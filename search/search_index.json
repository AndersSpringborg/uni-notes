{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Mathias Andresen - Noter \u00b6 Velkommen til min side med mine noter fra Aalborg Universitet. Jeg l\u00e6ser bachelor i Software. Siden indeholder lige nu noter fra: 3 Semester 4 Semester 5 Semester","title":"Forside"},{"location":"#mathias-andresen-noter","text":"Velkommen til min side med mine noter fra Aalborg Universitet. Jeg l\u00e6ser bachelor i Software. Siden indeholder lige nu noter fra: 3 Semester 4 Semester 5 Semester","title":"Mathias Andresen - Noter"},{"location":"3-semester/","text":"Indhold \u00b6 Kurser: AD1 - Algorithms and Datastructures DEB - Design and Evaluation of User Interfaces SU - Systems Development Moodle side: https://www.moodle.aau.dk/course/view.php?id=27324","title":"Index"},{"location":"3-semester/#indhold","text":"Kurser: AD1 - Algorithms and Datastructures DEB - Design and Evaluation of User Interfaces SU - Systems Development Moodle side: https://www.moodle.aau.dk/course/view.php?id=27324","title":"Indhold"},{"location":"3-semester/AD1/","text":"AD1 - ALGORITHMS AND DATA STRUCTURES \u00b6 Moodle side: https://www.moodle.aau.dk/course/view.php?id=27362","title":"Course"},{"location":"3-semester/AD1/#ad1-algorithms-and-data-structures","text":"Moodle side: https://www.moodle.aau.dk/course/view.php?id=27362","title":"AD1 - ALGORITHMS AND DATA STRUCTURES"},{"location":"3-semester/AD1/01-introduction/","text":"Linear Search \u00b6 Brute-force algorithm Binary Search \u00b6 Running Example \u00b6","title":"Introduction"},{"location":"3-semester/AD1/01-introduction/#linear-search","text":"Brute-force algorithm","title":"Linear Search"},{"location":"3-semester/AD1/01-introduction/#binary-search","text":"","title":"Binary Search"},{"location":"3-semester/AD1/01-introduction/#running-example","text":"","title":"Running Example"},{"location":"3-semester/AD1/02-analysing-algorithms/","text":"Insertion Sort \u00b6 Input : a sequence of n numbers (a_1, a_2,...,a_n) (a_1, a_2,...,a_n) Output : a permutation (reordering) (a'_1, a'_2 ,...,a'_n) (a'_1, a'_2 ,...,a'_n) of the input sequence such that a'_1\\leq a'_2\\leq...\\leq a'_n a'_1\\leq a'_2\\leq...\\leq a'_n Strategy \u00b6 Start with an \"empty hand\", say left hand. Insert a card in the correct position of left hand, where the numbers are already sorted Continue until all cards are inserted/sorted The RAM Model \u00b6 Instructions \u00b6 Primitive or atomic operations Each takes constant time, depending on the machine Instructions are executed one after another We consider instructions commonly found in real computers \u00b6 Arithmetic (add, subtract, multiply, etc.) Data movement (assignment) Control (branch, subroutine call, return) Comparison Data types - integers, characters, and floats \u00b6 ## Analysis of Insertion Sort t_j t_j is the number of times of the while loop test in line 5 is executed for a specific value of j. t_j t_j is the number of elements in A[1...j-1] A[1...j-1] which need to be checked in the j-th iteration of the for loop in line 5 t_j t_j may be different for different j. t_j t_j may be different for different input instances, e.g., best case or worst case Best/Worst/Average Case \u00b6 Worst case time complexity: W(n)=max_{1\\leq i\\leq k}T_i(n) W(n)=max_{1\\leq i\\leq k}T_i(n) \u00b6 The maximum running time over all k inputs of size n It is the most interesting/important! Comparing Algorithms\u2019 Efficiencies \u00b6 Question : How to compare two algorithms in terms of efficieny? \u200b E.g. linear search (linear) vs. binary search (logarithm) Answer : Look at how fast T(n) grows as n grows to a very large number (to the limit) Asymptotic complexity! Asymptotic Analysis \u00b6 Goal: To simplyfy analysis of running time by getting rid of \u201cdetails\u201d, which may be affected by specific implementation and hardware \u201crounding\u201d for numbers: 1.000.001 \\approx 1.000.000 1.000.001 \\approx 1.000.000 \u201crounding\u201d for functions 3n^2\\approx n^2 3n^2\\approx n^2 SE CLRS, p 56,Eq. 3.15 \u00b6","title":"Analysing Algorithms"},{"location":"3-semester/AD1/02-analysing-algorithms/#insertion-sort","text":"Input : a sequence of n numbers (a_1, a_2,...,a_n) (a_1, a_2,...,a_n) Output : a permutation (reordering) (a'_1, a'_2 ,...,a'_n) (a'_1, a'_2 ,...,a'_n) of the input sequence such that a'_1\\leq a'_2\\leq...\\leq a'_n a'_1\\leq a'_2\\leq...\\leq a'_n","title":"Insertion Sort"},{"location":"3-semester/AD1/02-analysing-algorithms/#strategy","text":"Start with an \"empty hand\", say left hand. Insert a card in the correct position of left hand, where the numbers are already sorted Continue until all cards are inserted/sorted","title":"Strategy"},{"location":"3-semester/AD1/02-analysing-algorithms/#the-ram-model","text":"","title":"The RAM Model"},{"location":"3-semester/AD1/02-analysing-algorithms/#instructions","text":"Primitive or atomic operations Each takes constant time, depending on the machine Instructions are executed one after another","title":"Instructions"},{"location":"3-semester/AD1/02-analysing-algorithms/#we-consider-instructions-commonly-found-in-real-computers","text":"Arithmetic (add, subtract, multiply, etc.) Data movement (assignment) Control (branch, subroutine call, return) Comparison","title":"We consider instructions commonly found in real computers"},{"location":"3-semester/AD1/02-analysing-algorithms/#data-types-integers-characters-and-floats","text":"## Analysis of Insertion Sort t_j t_j is the number of times of the while loop test in line 5 is executed for a specific value of j. t_j t_j is the number of elements in A[1...j-1] A[1...j-1] which need to be checked in the j-th iteration of the for loop in line 5 t_j t_j may be different for different j. t_j t_j may be different for different input instances, e.g., best case or worst case","title":"Data types - integers, characters, and floats"},{"location":"3-semester/AD1/02-analysing-algorithms/#bestworstaverage-case","text":"","title":"Best/Worst/Average Case"},{"location":"3-semester/AD1/02-analysing-algorithms/#worst-case-time-complexity-wnmax_1leq-ileq-kt_inwnmax_1leq-ileq-kt_in","text":"The maximum running time over all k inputs of size n It is the most interesting/important!","title":"Worst case time complexity: W(n)=max_{1\\leq i\\leq k}T_i(n)W(n)=max_{1\\leq i\\leq k}T_i(n)"},{"location":"3-semester/AD1/02-analysing-algorithms/#comparing-algorithms-efficiencies","text":"Question : How to compare two algorithms in terms of efficieny? \u200b E.g. linear search (linear) vs. binary search (logarithm) Answer : Look at how fast T(n) grows as n grows to a very large number (to the limit) Asymptotic complexity!","title":"Comparing Algorithms\u2019 Efficiencies"},{"location":"3-semester/AD1/02-analysing-algorithms/#asymptotic-analysis","text":"Goal: To simplyfy analysis of running time by getting rid of \u201cdetails\u201d, which may be affected by specific implementation and hardware \u201crounding\u201d for numbers: 1.000.001 \\approx 1.000.000 1.000.001 \\approx 1.000.000 \u201crounding\u201d for functions 3n^2\\approx n^2 3n^2\\approx n^2","title":"Asymptotic Analysis"},{"location":"3-semester/AD1/02-analysing-algorithms/#se-clrs-p-56eq-315","text":"","title":"SE CLRS, p 56,Eq. 3.15"},{"location":"3-semester/AD1/03-divide-and-conquer/","text":"Divide-and-Conquer (DaC) \u00b6 Basic Idea \u00b6 A problem with very small problem size can be solved trivially Sorting an array with only one single element is trivial Break the original big problem into several sub-problems, that are similar ti the original problem but smaller in size. Solve the sub-problems recusively unitl the sub-problem is with very small problem size that can be solved trivially. Combine the solutions to sub-problems to create a final solution to the original problem. Example: Factorial n! \u00b6 Recursive Non-recursive Divide-and-conquer method for algorithm design \u00b6 If the problem size is small enough to solve it in a straightforward manner, solve it. Otherwise do the following: Divide : Divide the problem into a number of disjoint sub-problems Conquer : Use divide-and-conquer recursively to solve the sub-problems. Combine : Take the solutions to the sub-problems and combine these solutions into a solution for the original problem Analyzing divide-and-conquer algorithms \u00b6 Recurrences \u00b6 Running times of algorithms with recursive calls can be desribed using recurrences A recurrence is an equation or inequality that describes a function in terms of its value on smaller inputs Assume that: If the problem size is smalle enough, the problem can be solved in constant time, i.e., \\Theta(1) \\Theta(1) the division of problem yields a sub-problems and each sub-problem is 1/b 1/b the size of the original We have: Example: Binary Search \u00b6 a=1,b=2 a=1,b=2 , having one sub-problem with half elements in the array D(n)=\\Theta(1) D(n)=\\Theta(1) , computing the middle index, constant time. C(n)=0 C(n)=0 , no need to combine Merge sort \u00b6 An algorithm that can solve the sorting problem and uses the DaC technique Assume that we are going to sort a sequence of numbers in array A: Divide If A has at least two elements, remove all the elements from A an put them into 2 sequences, A_1 A_1 and A_2 A_2 , each containing about half of the elements (i.e. A_1 A_1 contains the first \\lceil n/2 \\rceil \\lceil n/2 \\rceil elements and A_2 A_2 contains the remaining \\lfloor n/2 \\rfloor \\lfloor n/2 \\rfloor elements) Conquer Sort sequences A_1 A_1 and A_2 A_2 using Merge Sort. Combine Put back the elements into A by merging the sorted sequences A_1 A_1 and A_2 A_2 into one sorted sequence Running Example \u00b6 Running Time \u00b6 Analysis in lecture-3 slide 53-55","title":"Divide-and-Conquer"},{"location":"3-semester/AD1/03-divide-and-conquer/#divide-and-conquer-dac","text":"","title":"Divide-and-Conquer (DaC)"},{"location":"3-semester/AD1/03-divide-and-conquer/#basic-idea","text":"A problem with very small problem size can be solved trivially Sorting an array with only one single element is trivial Break the original big problem into several sub-problems, that are similar ti the original problem but smaller in size. Solve the sub-problems recusively unitl the sub-problem is with very small problem size that can be solved trivially. Combine the solutions to sub-problems to create a final solution to the original problem.","title":"Basic Idea"},{"location":"3-semester/AD1/03-divide-and-conquer/#example-factorial-n","text":"Recursive Non-recursive","title":"Example: Factorial n!"},{"location":"3-semester/AD1/03-divide-and-conquer/#divide-and-conquer-method-for-algorithm-design","text":"If the problem size is small enough to solve it in a straightforward manner, solve it. Otherwise do the following: Divide : Divide the problem into a number of disjoint sub-problems Conquer : Use divide-and-conquer recursively to solve the sub-problems. Combine : Take the solutions to the sub-problems and combine these solutions into a solution for the original problem","title":"Divide-and-conquer method for algorithm design"},{"location":"3-semester/AD1/03-divide-and-conquer/#analyzing-divide-and-conquer-algorithms","text":"","title":"Analyzing divide-and-conquer algorithms"},{"location":"3-semester/AD1/03-divide-and-conquer/#recurrences","text":"Running times of algorithms with recursive calls can be desribed using recurrences A recurrence is an equation or inequality that describes a function in terms of its value on smaller inputs Assume that: If the problem size is smalle enough, the problem can be solved in constant time, i.e., \\Theta(1) \\Theta(1) the division of problem yields a sub-problems and each sub-problem is 1/b 1/b the size of the original We have:","title":"Recurrences"},{"location":"3-semester/AD1/03-divide-and-conquer/#example-binary-search","text":"a=1,b=2 a=1,b=2 , having one sub-problem with half elements in the array D(n)=\\Theta(1) D(n)=\\Theta(1) , computing the middle index, constant time. C(n)=0 C(n)=0 , no need to combine","title":"Example: Binary Search"},{"location":"3-semester/AD1/03-divide-and-conquer/#merge-sort","text":"An algorithm that can solve the sorting problem and uses the DaC technique Assume that we are going to sort a sequence of numbers in array A: Divide If A has at least two elements, remove all the elements from A an put them into 2 sequences, A_1 A_1 and A_2 A_2 , each containing about half of the elements (i.e. A_1 A_1 contains the first \\lceil n/2 \\rceil \\lceil n/2 \\rceil elements and A_2 A_2 contains the remaining \\lfloor n/2 \\rfloor \\lfloor n/2 \\rfloor elements) Conquer Sort sequences A_1 A_1 and A_2 A_2 using Merge Sort. Combine Put back the elements into A by merging the sorted sequences A_1 A_1 and A_2 A_2 into one sorted sequence","title":"Merge sort"},{"location":"3-semester/AD1/03-divide-and-conquer/#running-example","text":"","title":"Running Example"},{"location":"3-semester/AD1/03-divide-and-conquer/#running-time","text":"Analysis in lecture-3 slide 53-55","title":"Running Time"},{"location":"3-semester/AD1/04-solving-recurrences/","text":"Solving Recurrences \u00b6 Repeated substitution method Expanding the recurrence by substition and noticing a patteren w.r.t. the step i. Identifying an appropriate i such that the base case can be plugged in. Recursion-trees Draw a tree to visualize what happens when a recurrence is iterated Master method Templates for different classes of recurrences Repeated Substitution \u00b6 Substitute Expand Substitute Expand ... Observe a pattern and write how your expression looks after the i-th substitution Find out what the value of i should be to get the base case of the recurrence T(1) Insert the value of T(1) and the expression of i into your expression. Example 1 \u00b6 Example 2 \u00b6 Recursion Tree \u00b6 A way to conveniently visualize what happens when a recurrence is iterated. Each node represents the cost of a single sub-problem. Sum the costs within each level of the tree to obtain a set of per\u2013level costs . Sum all the per-level costs to determine the total cost of all levels of teh recursion. Example \u00b6 T(n)=3*T(n/4)+\\Theta(n^2)=3*T(n/4)+c*n^2 T(n)=3*T(n/4)+\\Theta(n^2)=3*T(n/4)+c*n^2 Lower bound - \\Omega(n^2) \\Omega(n^2) We have to at least do level 1, so the lower bound is \\Omega(n^2) \\Omega(n^2) . Key steps \u00b6 T(n)=aT(n/b)+D(n)+C(n) T(n)=aT(n/b)+D(n)+C(n) How many levels in the tree? log_bn+1 log_bn+1 What is the cost per non-leaf level? Depends on the cost of dividing and combining What is the cost for the leaf level? Depends on how many leave nodes there are: n^{log_ba} n^{log_ba} Each leave node has constant cost Sum the per-level costs into the final cost The Master Method \u00b6 Providing a template method for solving recurrences of the form : T(n)=a*T(n/b)+f(n), T(n)=a*T(n/b)+f(n), \u200b where a\\geq 1 a\\geq 1 and b>1 b>1 are constant, \u200b and f(n) f(n) is asymptotically positive. T(n) T(n) is the runtime for an algorithm and we know that: a subproblems of size n/b are solved recursively, each in time T(n/b) T(n/b) f(n) f(n) is the cost of dividing the problem and combining the results f(n)=D(n)+C(n) f(n)=D(n)+C(n) First case: \u00b6 \u200b if f(n)=O(n^{log_ba-\\epsilon}) f(n)=O(n^{log_ba-\\epsilon}) for some constant \\epsilon>0 \\epsilon>0 , then T(n)=\\Theta(n^{log_ba}) T(n)=\\Theta(n^{log_ba}) Second case : \u00b6 \u200b if f(n)=\\Theta(n^{log_ba}) f(n)=\\Theta(n^{log_ba}) , then $$ T(n)=\\Theta(n^{log_ba}lg(n)) $$ Third case : \u00b6 \u200b if f(n)=\\Omega(n^{log_ba+\\epsilon}) f(n)=\\Omega(n^{log_ba+\\epsilon}) for some constant \\epsilon>0 \\epsilon>0 , and the regularity condition is also satisfied, then $$ T(n)=\\Theta(f(n)) $$ Regularity condition: a*f(n/b)\\leq c*f(n) a*f(n/b)\\leq c*f(n) for some constant c<1 c<1 and all sufficiently large n How to use \u00b6 Extract a, b, and f(n) from a given recurrence Determine n^{log_ba} n^{log_ba} Compare f(n) and n^{log_ba} n^{log_ba} asymptotically f(n) increases polynomially slower, case 1 The increase similarly, case 2 f(n) increases polynomially faster, case 3 Determine appropriate MM case and apply. Correctness of Algorithms \u00b6 The algorithm is correct if for any legal input it terminates, and produces the desired output Loop invariants \u00b6 Invariants - assertions (i-e, statements about the states of the execution) that are valid any time they are reached ( many times during the execution of an algorithm, e.g. in loops) We must show three things about loop invariants: Initialization - it is true prior to the first iteration Maintenance - if it is true before an iteration, then it remains true before the next iteration Termination - when loop terminates the invariant gives a useful property to show the correctness of the algorithm Example - Insertion Sort \u00b6 Invariant At the start of each for loop, A[1...j-1] A[1...j-1] consists of elements originally in A[1...j-1] A[1...j-1] but in sorted order. Initialization j = 2, the invariant trivially holds because A[1] is a sorted array Maintenance The inner while loop moves elements A[j-1],A[j-2],...,A[j-] A[j-1],A[j-2],...,A[j-] by one position to the right without chaning their order until it finds the proper position for A[j] A[j] . Then, A[j] A[j] is inserted into k-th position such that A[k-1]\\leq A[k]\\leq A[k+1] A[k-1]\\leq A[k]\\leq A[k+1] . Thus, A[1...j] A[1...j] consists of the elements originally in A[1...j] A[1...j] but sorted order. Termination The loop terminates, when j=n+1 j=n+1 . Then the invariant states: \u200b A[1...j-1] A[1...j-1] consists of elements originally in A[1...j-1] A[1...j-1] but in sorted order.","title":"Solving Recurrences"},{"location":"3-semester/AD1/04-solving-recurrences/#solving-recurrences","text":"Repeated substitution method Expanding the recurrence by substition and noticing a patteren w.r.t. the step i. Identifying an appropriate i such that the base case can be plugged in. Recursion-trees Draw a tree to visualize what happens when a recurrence is iterated Master method Templates for different classes of recurrences","title":"Solving Recurrences"},{"location":"3-semester/AD1/04-solving-recurrences/#repeated-substitution","text":"Substitute Expand Substitute Expand ... Observe a pattern and write how your expression looks after the i-th substitution Find out what the value of i should be to get the base case of the recurrence T(1) Insert the value of T(1) and the expression of i into your expression.","title":"Repeated Substitution"},{"location":"3-semester/AD1/04-solving-recurrences/#example-1","text":"","title":"Example 1"},{"location":"3-semester/AD1/04-solving-recurrences/#example-2","text":"","title":"Example 2"},{"location":"3-semester/AD1/04-solving-recurrences/#recursion-tree","text":"A way to conveniently visualize what happens when a recurrence is iterated. Each node represents the cost of a single sub-problem. Sum the costs within each level of the tree to obtain a set of per\u2013level costs . Sum all the per-level costs to determine the total cost of all levels of teh recursion.","title":"Recursion Tree"},{"location":"3-semester/AD1/04-solving-recurrences/#example","text":"T(n)=3*T(n/4)+\\Theta(n^2)=3*T(n/4)+c*n^2 T(n)=3*T(n/4)+\\Theta(n^2)=3*T(n/4)+c*n^2 Lower bound - \\Omega(n^2) \\Omega(n^2) We have to at least do level 1, so the lower bound is \\Omega(n^2) \\Omega(n^2) .","title":"Example"},{"location":"3-semester/AD1/04-solving-recurrences/#key-steps","text":"T(n)=aT(n/b)+D(n)+C(n) T(n)=aT(n/b)+D(n)+C(n) How many levels in the tree? log_bn+1 log_bn+1 What is the cost per non-leaf level? Depends on the cost of dividing and combining What is the cost for the leaf level? Depends on how many leave nodes there are: n^{log_ba} n^{log_ba} Each leave node has constant cost Sum the per-level costs into the final cost","title":"Key steps"},{"location":"3-semester/AD1/04-solving-recurrences/#the-master-method","text":"Providing a template method for solving recurrences of the form : T(n)=a*T(n/b)+f(n), T(n)=a*T(n/b)+f(n), \u200b where a\\geq 1 a\\geq 1 and b>1 b>1 are constant, \u200b and f(n) f(n) is asymptotically positive. T(n) T(n) is the runtime for an algorithm and we know that: a subproblems of size n/b are solved recursively, each in time T(n/b) T(n/b) f(n) f(n) is the cost of dividing the problem and combining the results f(n)=D(n)+C(n) f(n)=D(n)+C(n)","title":"The Master Method"},{"location":"3-semester/AD1/04-solving-recurrences/#first-case","text":"\u200b if f(n)=O(n^{log_ba-\\epsilon}) f(n)=O(n^{log_ba-\\epsilon}) for some constant \\epsilon>0 \\epsilon>0 , then T(n)=\\Theta(n^{log_ba}) T(n)=\\Theta(n^{log_ba})","title":"First case:"},{"location":"3-semester/AD1/04-solving-recurrences/#second-case","text":"\u200b if f(n)=\\Theta(n^{log_ba}) f(n)=\\Theta(n^{log_ba}) , then $$ T(n)=\\Theta(n^{log_ba}lg(n)) $$","title":"Second case:"},{"location":"3-semester/AD1/04-solving-recurrences/#third-case","text":"\u200b if f(n)=\\Omega(n^{log_ba+\\epsilon}) f(n)=\\Omega(n^{log_ba+\\epsilon}) for some constant \\epsilon>0 \\epsilon>0 , and the regularity condition is also satisfied, then $$ T(n)=\\Theta(f(n)) $$ Regularity condition: a*f(n/b)\\leq c*f(n) a*f(n/b)\\leq c*f(n) for some constant c<1 c<1 and all sufficiently large n","title":"Third case:"},{"location":"3-semester/AD1/04-solving-recurrences/#how-to-use","text":"Extract a, b, and f(n) from a given recurrence Determine n^{log_ba} n^{log_ba} Compare f(n) and n^{log_ba} n^{log_ba} asymptotically f(n) increases polynomially slower, case 1 The increase similarly, case 2 f(n) increases polynomially faster, case 3 Determine appropriate MM case and apply.","title":"How to use"},{"location":"3-semester/AD1/04-solving-recurrences/#correctness-of-algorithms","text":"The algorithm is correct if for any legal input it terminates, and produces the desired output","title":"Correctness of Algorithms"},{"location":"3-semester/AD1/04-solving-recurrences/#loop-invariants","text":"Invariants - assertions (i-e, statements about the states of the execution) that are valid any time they are reached ( many times during the execution of an algorithm, e.g. in loops) We must show three things about loop invariants: Initialization - it is true prior to the first iteration Maintenance - if it is true before an iteration, then it remains true before the next iteration Termination - when loop terminates the invariant gives a useful property to show the correctness of the algorithm","title":"Loop invariants"},{"location":"3-semester/AD1/04-solving-recurrences/#example-insertion-sort","text":"Invariant At the start of each for loop, A[1...j-1] A[1...j-1] consists of elements originally in A[1...j-1] A[1...j-1] but in sorted order. Initialization j = 2, the invariant trivially holds because A[1] is a sorted array Maintenance The inner while loop moves elements A[j-1],A[j-2],...,A[j-] A[j-1],A[j-2],...,A[j-] by one position to the right without chaning their order until it finds the proper position for A[j] A[j] . Then, A[j] A[j] is inserted into k-th position such that A[k-1]\\leq A[k]\\leq A[k+1] A[k-1]\\leq A[k]\\leq A[k+1] . Thus, A[1...j] A[1...j] consists of the elements originally in A[1...j] A[1...j] but sorted order. Termination The loop terminates, when j=n+1 j=n+1 . Then the invariant states: \u200b A[1...j-1] A[1...j-1] consists of elements originally in A[1...j-1] A[1...j-1] but in sorted order.","title":"Example - Insertion Sort"},{"location":"3-semester/AD1/05-sorting-algorithms/","text":"Sorting Algorithms \u00b6 Sorting: Input: A sequence of n numbers Output: A permutation (reordering) of the input sequence (slide 8) When in doubt, sort! - One of the principles of algorithm design. Insertion sort In-place [^in-place] sorting algorithm Worst case complexity \\Theta(n^2) \\Theta(n^2) Merge sort Uses the DaC technique Not in-place [^in-place] sorting, which requires additional storage with the size of the input array Worst case complexity \\Theta(n\\cdot lg(n)) \\Theta(n\\cdot lg(n)) Important properties Whether a sorting algorithm is in-place? What is the worst case complexity \\Theta(n^2) \\Theta(n^2) or \\Theta(n\\cdot lg(n)) \\Theta(n\\cdot lg(n)) ? [^in-place]: In-place : Only a constant number of elements of the input array are ever stored outside the array. Bubble Sort \u00b6 Popular but inefficient. Repeatedly swapping adjacent elements that are out of order. Example \u00b6 Selection Sort \u00b6 Pseudo Code : Example \u00b6 Quick Sort \u00b6 A DaC algorithm. Does not require additional array like Merge Sort Sorts in-place [^in-place] Example \u00b6","title":"Sorting Algorithms"},{"location":"3-semester/AD1/05-sorting-algorithms/#sorting-algorithms","text":"Sorting: Input: A sequence of n numbers Output: A permutation (reordering) of the input sequence (slide 8) When in doubt, sort! - One of the principles of algorithm design. Insertion sort In-place [^in-place] sorting algorithm Worst case complexity \\Theta(n^2) \\Theta(n^2) Merge sort Uses the DaC technique Not in-place [^in-place] sorting, which requires additional storage with the size of the input array Worst case complexity \\Theta(n\\cdot lg(n)) \\Theta(n\\cdot lg(n)) Important properties Whether a sorting algorithm is in-place? What is the worst case complexity \\Theta(n^2) \\Theta(n^2) or \\Theta(n\\cdot lg(n)) \\Theta(n\\cdot lg(n)) ? [^in-place]: In-place : Only a constant number of elements of the input array are ever stored outside the array.","title":"Sorting Algorithms"},{"location":"3-semester/AD1/05-sorting-algorithms/#bubble-sort","text":"Popular but inefficient. Repeatedly swapping adjacent elements that are out of order.","title":"Bubble Sort"},{"location":"3-semester/AD1/05-sorting-algorithms/#example","text":"","title":"Example"},{"location":"3-semester/AD1/05-sorting-algorithms/#selection-sort","text":"Pseudo Code :","title":"Selection Sort"},{"location":"3-semester/AD1/05-sorting-algorithms/#example_1","text":"","title":"Example"},{"location":"3-semester/AD1/05-sorting-algorithms/#quick-sort","text":"A DaC algorithm. Does not require additional array like Merge Sort Sorts in-place [^in-place]","title":"Quick Sort"},{"location":"3-semester/AD1/05-sorting-algorithms/#example_2","text":"","title":"Example"},{"location":"3-semester/AD1/06-heap-sort/","text":"Heap Sort \u00b6 Heapsort can be regarded as a selection sort with the help of a heap data structure. Heap \u00b6 Binary heap data structure* A*** Array Can be viewed as a nearly complete binary tree All levels, except the lowest one, are completely filled Heap property Max-heap The root is greater than or equal to all its children, and the left and right subtrees are again binary heaps. Min-heap The root is less than or equal to all its children, and the left and right subtrees are again binary heaps Two attributes: A.length : number of elements in the array A.heapsize : number of elements in the heap that is stored in the array 1 \\leq A.heapsize \\leq A.length 1 \\leq A.heapsize \\leq A.length A[1 ... A.length] A[1 ... A.length] may contain many elements, but only the elements in A[1 ... A.heapsize] A[1 ... A.heapsize] are valid elements of the heap Example \u00b6 Parent, Left Child, Right Child \u00b6 1 2 3 4 5 6 7 8 PARENT(i) return Floor(i/2) LEFT(i) return 2*i RIGHT(i) return 2*i+1 Heap property: A[Parent(i)] \\ge A[i] A[Parent(i)] \\ge A[i] The value of the node is at most the value of the parent. Maintaining the Heap Property (Heapify) \u00b6 Heapify Input: Array A and an index i into the array Assume: Binary trees rooted at Left(i) and Right(i) are heaps But, A[i] might be smaller than its two children, thus violating the heap property. The method Heapify makes A a heap by moving A[i] down the heap until the heap property is satisfied again. Pseudocode \u00b6 Example: Heapify(A,2) \u00b6 Analysis of Heapify \u00b6 Is Heapify recursive algorithm or not If yes, write down the recurrence and solve the recurrence If no, use the RAM model Identifying the recurrence for Heapify Dividing (lines 1-3): Figuring out the relationship among the elements A[i], A[l], and A[r]. Can be done in constant time, i.e., \u0398(1). Conquer (lines 4-6): Case 1: If A[i] is the largest among A[i], A[l], A[r] already, do nothing. Case 2: Otherwise, conquer the same problem (i.e., Heapfiy) on one of the subtree of node i. How many subproblems are you going to solve for each case? Case 1: 0 Case 2: 1 We at most need to solve 1 subproblem. Notes \u00b6 Sometimes, it is more important to write down a correct recurrence than solving the recurrence How many subproblems and what is the size of each subproblem Quick sort: best case v.s worst case Heapify: the largest size subproblem What is the cost of dividing and combining Quicksort: partition, dividing phase Merge sort: merge, combining phase Building a heap from an array \u00b6 Convert an array A[1 ... n] A[1 ... n] into a heap. Notice that the elements in the subarray A [(\u2514n/2\u2518+1)...n ] A [(\u2514n/2\u2518+1)...n ] are already 1-element heaps to begin with These elements have no children Call heapify from the \u2514n/2\u2518-th element down to the first element. 1 2 3 00 Build-Heap(A) 01 for i = floor(n/2) downto 1 do 02 Heapify(A,i) Example \u00b6 Sorting \u00b6 Running time \u00b6 Running time is \u200b O(n*lg(n)) O(n*lg(n)) like merge sort, but unlike selection-, insertion- or bubble-sorts. Sorts in-place - like insertion-, selection- or bubble-sorts, but unlike merge-sort.","title":"Heap Sort"},{"location":"3-semester/AD1/06-heap-sort/#heap-sort","text":"Heapsort can be regarded as a selection sort with the help of a heap data structure.","title":"Heap Sort"},{"location":"3-semester/AD1/06-heap-sort/#heap","text":"Binary heap data structure* A*** Array Can be viewed as a nearly complete binary tree All levels, except the lowest one, are completely filled Heap property Max-heap The root is greater than or equal to all its children, and the left and right subtrees are again binary heaps. Min-heap The root is less than or equal to all its children, and the left and right subtrees are again binary heaps Two attributes: A.length : number of elements in the array A.heapsize : number of elements in the heap that is stored in the array 1 \\leq A.heapsize \\leq A.length 1 \\leq A.heapsize \\leq A.length A[1 ... A.length] A[1 ... A.length] may contain many elements, but only the elements in A[1 ... A.heapsize] A[1 ... A.heapsize] are valid elements of the heap","title":"Heap"},{"location":"3-semester/AD1/06-heap-sort/#example","text":"","title":"Example"},{"location":"3-semester/AD1/06-heap-sort/#parent-left-child-right-child","text":"1 2 3 4 5 6 7 8 PARENT(i) return Floor(i/2) LEFT(i) return 2*i RIGHT(i) return 2*i+1 Heap property: A[Parent(i)] \\ge A[i] A[Parent(i)] \\ge A[i] The value of the node is at most the value of the parent.","title":"Parent, Left Child, Right Child"},{"location":"3-semester/AD1/06-heap-sort/#maintaining-the-heap-property-heapify","text":"Heapify Input: Array A and an index i into the array Assume: Binary trees rooted at Left(i) and Right(i) are heaps But, A[i] might be smaller than its two children, thus violating the heap property. The method Heapify makes A a heap by moving A[i] down the heap until the heap property is satisfied again.","title":"Maintaining the Heap Property (Heapify)"},{"location":"3-semester/AD1/06-heap-sort/#pseudocode","text":"","title":"Pseudocode"},{"location":"3-semester/AD1/06-heap-sort/#example-heapifya2","text":"","title":"Example: Heapify(A,2)"},{"location":"3-semester/AD1/06-heap-sort/#analysis-of-heapify","text":"Is Heapify recursive algorithm or not If yes, write down the recurrence and solve the recurrence If no, use the RAM model Identifying the recurrence for Heapify Dividing (lines 1-3): Figuring out the relationship among the elements A[i], A[l], and A[r]. Can be done in constant time, i.e., \u0398(1). Conquer (lines 4-6): Case 1: If A[i] is the largest among A[i], A[l], A[r] already, do nothing. Case 2: Otherwise, conquer the same problem (i.e., Heapfiy) on one of the subtree of node i. How many subproblems are you going to solve for each case? Case 1: 0 Case 2: 1 We at most need to solve 1 subproblem.","title":"Analysis of Heapify"},{"location":"3-semester/AD1/06-heap-sort/#notes","text":"Sometimes, it is more important to write down a correct recurrence than solving the recurrence How many subproblems and what is the size of each subproblem Quick sort: best case v.s worst case Heapify: the largest size subproblem What is the cost of dividing and combining Quicksort: partition, dividing phase Merge sort: merge, combining phase","title":"Notes"},{"location":"3-semester/AD1/06-heap-sort/#building-a-heap-from-an-array","text":"Convert an array A[1 ... n] A[1 ... n] into a heap. Notice that the elements in the subarray A [(\u2514n/2\u2518+1)...n ] A [(\u2514n/2\u2518+1)...n ] are already 1-element heaps to begin with These elements have no children Call heapify from the \u2514n/2\u2518-th element down to the first element. 1 2 3 00 Build-Heap(A) 01 for i = floor(n/2) downto 1 do 02 Heapify(A,i)","title":"Building a heap from an array"},{"location":"3-semester/AD1/06-heap-sort/#example_1","text":"","title":"Example"},{"location":"3-semester/AD1/06-heap-sort/#sorting","text":"","title":"Sorting"},{"location":"3-semester/AD1/06-heap-sort/#running-time","text":"Running time is \u200b O(n*lg(n)) O(n*lg(n)) like merge sort, but unlike selection-, insertion- or bubble-sorts. Sorts in-place - like insertion-, selection- or bubble-sorts, but unlike merge-sort.","title":"Running time"},{"location":"3-semester/AD1/07-data-structures/","text":"Data Structures \u00b6 An abstract data type (ADT) is a specification of: A set of data. A set of operations that can be performed on the data. ADT is abstract in the sense that it is independent of various concrete implementations. Encapsulates data structures and relecant algorithms. Provides access interface. Stack \u00b6 A pile of plates An object added to the stack goes on the \"top\" of the stack. ( push ) An object removed from the stack is taken from the \"top\" of the stack ( pop ) LIFO : Last in, First out Operations \u00b6 1 2 3 4 5 6 7 8 Push(S, x): inserts an element x into the stack S Pop(S): deletes the element on top of the stack S. Stack-Empty(S): returns whether the stack S is empty. Queue \u00b6 A real-life queue An element added to the queue foes to the \"end\" of the queue. ( enqueue ) The element which has been in the queue the longest can be removed from the queue. ( dequeue ) Elements are removed from a queue in the same order as they were inserted. FIFO : First in, First out. Operations \u00b6 1 2 3 4 5 Enqueue(Q, x): inserts an element x into the queue Q Dequeue(Q): deletes the head element in the queue Q. Linked List \u00b6 A sequence of elements Each element in the linked list is with: One key One ore more pointers There are different types depending on how the elements are linked. Singly linked list \u00b6 Element: A key One pointer: \"next\", pointing the the successor of the element. The last element points to a \"NIL\". Head pointer : Pointer \"head\": pointing to the first element of the list. Note: \"next\" are for elements (x.next) \"head\" is for whole list (L.head) Alternatively: \u00b6 Doubly linked list \u00b6 Element: A key Two pointers: \"next\" and \"prev\" Still has \"head\". Alternatively \u00b6 Operations \u00b6 1 2 3 4 5 6 7 8 Search(L, k): Find the first element with a key k in the list L Insert(L, x): Insert element x into list L Delete(L, x): Delete the element x from list L Priority Queue \u00b6 A priority queue (PQ) is a container for maintaining a set A of elements, each with an associated value called key . Operations \u00b6 1 2 3 4 5 6 7 8 Insert(A, x): insert element x in set A Maximum(A): returns the element of A with the largest key (i.e. highest priority) extract-Max(A): returns and removes the element of A with the largest key from A Heap Implementation \u00b6 Use a max-heap.","title":"Data Structures"},{"location":"3-semester/AD1/07-data-structures/#data-structures","text":"An abstract data type (ADT) is a specification of: A set of data. A set of operations that can be performed on the data. ADT is abstract in the sense that it is independent of various concrete implementations. Encapsulates data structures and relecant algorithms. Provides access interface.","title":"Data Structures"},{"location":"3-semester/AD1/07-data-structures/#stack","text":"A pile of plates An object added to the stack goes on the \"top\" of the stack. ( push ) An object removed from the stack is taken from the \"top\" of the stack ( pop ) LIFO : Last in, First out","title":"Stack"},{"location":"3-semester/AD1/07-data-structures/#operations","text":"1 2 3 4 5 6 7 8 Push(S, x): inserts an element x into the stack S Pop(S): deletes the element on top of the stack S. Stack-Empty(S): returns whether the stack S is empty.","title":"Operations"},{"location":"3-semester/AD1/07-data-structures/#queue","text":"A real-life queue An element added to the queue foes to the \"end\" of the queue. ( enqueue ) The element which has been in the queue the longest can be removed from the queue. ( dequeue ) Elements are removed from a queue in the same order as they were inserted. FIFO : First in, First out.","title":"Queue"},{"location":"3-semester/AD1/07-data-structures/#operations_1","text":"1 2 3 4 5 Enqueue(Q, x): inserts an element x into the queue Q Dequeue(Q): deletes the head element in the queue Q.","title":"Operations"},{"location":"3-semester/AD1/07-data-structures/#linked-list","text":"A sequence of elements Each element in the linked list is with: One key One ore more pointers There are different types depending on how the elements are linked.","title":"Linked List"},{"location":"3-semester/AD1/07-data-structures/#singly-linked-list","text":"Element: A key One pointer: \"next\", pointing the the successor of the element. The last element points to a \"NIL\". Head pointer : Pointer \"head\": pointing to the first element of the list. Note: \"next\" are for elements (x.next) \"head\" is for whole list (L.head)","title":"Singly linked list"},{"location":"3-semester/AD1/07-data-structures/#alternatively","text":"","title":"Alternatively:"},{"location":"3-semester/AD1/07-data-structures/#doubly-linked-list","text":"Element: A key Two pointers: \"next\" and \"prev\" Still has \"head\".","title":"Doubly linked list"},{"location":"3-semester/AD1/07-data-structures/#alternatively_1","text":"","title":"Alternatively"},{"location":"3-semester/AD1/07-data-structures/#operations_2","text":"1 2 3 4 5 6 7 8 Search(L, k): Find the first element with a key k in the list L Insert(L, x): Insert element x into list L Delete(L, x): Delete the element x from list L","title":"Operations"},{"location":"3-semester/AD1/07-data-structures/#priority-queue","text":"A priority queue (PQ) is a container for maintaining a set A of elements, each with an associated value called key .","title":"Priority Queue"},{"location":"3-semester/AD1/07-data-structures/#operations_3","text":"1 2 3 4 5 6 7 8 Insert(A, x): insert element x in set A Maximum(A): returns the element of A with the largest key (i.e. highest priority) extract-Max(A): returns and removes the element of A with the largest key from A","title":"Operations"},{"location":"3-semester/AD1/07-data-structures/#heap-implementation","text":"Use a max-heap.","title":"Heap Implementation"},{"location":"3-semester/AD1/08a-hash-tables/","text":"Dictionaries \u00b6 An element has a key part and a sattelite data part. Dictionaries store elements so that they can be located quickly using keys . Dictionary ADT 1 2 3 4 5 6 7 8 Search(S, k): access operation that returns an element where x.key = k Insert(S, x): a manipulation operation that adds element x to S. Delete(S, x): a manipulation operation that removes element x from S. Hash tables \u00b6 Like an array, but come up with a Hash function to map the large range (eg. 0 to 9999999) into a small one which we can manage. (eg. 0 to 4) Eg. Take the original key, modulo the (relatively small) size of the array, and use that as an index. Example : (96358904, Bill) into a hashed array with, say, 5 slots: 1 2 hash(96358904) = 96358904 mod 5 = 4 hash(96358902) = 96358902 mod 5 = 2 Lookup: 1 2 Search(\"96358904\"), hash(96358904)=4, then \"Bill\" Search(\"96358900\"), hash(96358900)=0, then \"\\\" Collisions \u00b6 When to elements has the same hashed key. Chaining \u00b6 Each entry in the table is pointer to a linked list. Elements with same hashed key are placed into a linked list. Linear Probing \u00b6 If the current location is used, try the next table location. Lookups walk along the table until the key or an empty slot is found. Open addressing \u00b6 Step i from 0, 1, 2, ..., m-1 Linear probing : $$ h(k,i)=(h'(k)+i)\\space mod\\space m $$ Quadratic probing : (c1 and c2 are constant) $$ h(k,i)=(h'(k)+c_1i+c_2i^2)\\space mod\\space m $$ Double hashing: $$ h(k,i)=h_1(k)+ih_2(k))\\space mod\\space m $$","title":"Hash Tables"},{"location":"3-semester/AD1/08a-hash-tables/#dictionaries","text":"An element has a key part and a sattelite data part. Dictionaries store elements so that they can be located quickly using keys . Dictionary ADT 1 2 3 4 5 6 7 8 Search(S, k): access operation that returns an element where x.key = k Insert(S, x): a manipulation operation that adds element x to S. Delete(S, x): a manipulation operation that removes element x from S.","title":"Dictionaries"},{"location":"3-semester/AD1/08a-hash-tables/#hash-tables","text":"Like an array, but come up with a Hash function to map the large range (eg. 0 to 9999999) into a small one which we can manage. (eg. 0 to 4) Eg. Take the original key, modulo the (relatively small) size of the array, and use that as an index. Example : (96358904, Bill) into a hashed array with, say, 5 slots: 1 2 hash(96358904) = 96358904 mod 5 = 4 hash(96358902) = 96358902 mod 5 = 2 Lookup: 1 2 Search(\"96358904\"), hash(96358904)=4, then \"Bill\" Search(\"96358900\"), hash(96358900)=0, then \"\\\"","title":"Hash tables"},{"location":"3-semester/AD1/08a-hash-tables/#collisions","text":"When to elements has the same hashed key.","title":"Collisions"},{"location":"3-semester/AD1/08a-hash-tables/#chaining","text":"Each entry in the table is pointer to a linked list. Elements with same hashed key are placed into a linked list.","title":"Chaining"},{"location":"3-semester/AD1/08a-hash-tables/#linear-probing","text":"If the current location is used, try the next table location. Lookups walk along the table until the key or an empty slot is found.","title":"Linear Probing"},{"location":"3-semester/AD1/08a-hash-tables/#open-addressing","text":"Step i from 0, 1, 2, ..., m-1 Linear probing : $$ h(k,i)=(h'(k)+i)\\space mod\\space m $$ Quadratic probing : (c1 and c2 are constant) $$ h(k,i)=(h'(k)+c_1i+c_2i^2)\\space mod\\space m $$ Double hashing: $$ h(k,i)=h_1(k)+ih_2(k))\\space mod\\space m $$","title":"Open addressing"},{"location":"3-semester/AD1/08b-binary-search-tree/","text":"Binary Search Tree \u00b6 Binary tree T satisfiyng binary-search-tree property. (Like max-heap property). Let x be a node in a binary search tree. If y is a node in the left subtree of x, then $$ y.key\\leq x.key $$ If y is a node in the right subtree of x, then y.key\\geq x.key y.key\\geq x.key Example: Note: more balanced, the better! Represent with Linked List \u00b6 Each node has 3 pointers. \"P\" points to parent. \"Left\" points to left child. \"Right\" points to right child. The tree has pointer \"Root\" pointing to the root of the tree. Tree walks \u00b6 Process of visiting each node in a tree data structure exactly once. Keys in the BST can be printed using \"tree walks\". Inorder tree walk \u00b6 The key of each node is visited (printed) between the keys in the left and right subtrees. 1 2 3 4 5 InorderTreeWalk(x) 01 if (x != NIL) then 02 InorderTreeWalk(x.left()) 03 print x.key() 04 InorderTreeWalk(x.right()) DaC algorith. Example \u00b6 Preorder tree walk \u00b6 Visits each node before visiting its children. Postorder tree walk \u00b6 Visits each node after visiting its children. Example \u00b6","title":"Binary Search Trees"},{"location":"3-semester/AD1/08b-binary-search-tree/#binary-search-tree","text":"Binary tree T satisfiyng binary-search-tree property. (Like max-heap property). Let x be a node in a binary search tree. If y is a node in the left subtree of x, then $$ y.key\\leq x.key $$ If y is a node in the right subtree of x, then y.key\\geq x.key y.key\\geq x.key Example: Note: more balanced, the better!","title":"Binary Search Tree"},{"location":"3-semester/AD1/08b-binary-search-tree/#represent-with-linked-list","text":"Each node has 3 pointers. \"P\" points to parent. \"Left\" points to left child. \"Right\" points to right child. The tree has pointer \"Root\" pointing to the root of the tree.","title":"Represent with Linked List"},{"location":"3-semester/AD1/08b-binary-search-tree/#tree-walks","text":"Process of visiting each node in a tree data structure exactly once. Keys in the BST can be printed using \"tree walks\".","title":"Tree walks"},{"location":"3-semester/AD1/08b-binary-search-tree/#inorder-tree-walk","text":"The key of each node is visited (printed) between the keys in the left and right subtrees. 1 2 3 4 5 InorderTreeWalk(x) 01 if (x != NIL) then 02 InorderTreeWalk(x.left()) 03 print x.key() 04 InorderTreeWalk(x.right()) DaC algorith.","title":"Inorder tree walk"},{"location":"3-semester/AD1/08b-binary-search-tree/#example","text":"","title":"Example"},{"location":"3-semester/AD1/08b-binary-search-tree/#preorder-tree-walk","text":"Visits each node before visiting its children.","title":"Preorder tree walk"},{"location":"3-semester/AD1/08b-binary-search-tree/#postorder-tree-walk","text":"Visits each node after visiting its children.","title":"Postorder tree walk"},{"location":"3-semester/AD1/08b-binary-search-tree/#example_1","text":"","title":"Example"},{"location":"3-semester/AD1/09-dynamic-programming/","text":"Dynamic Programming \u00b6 What if the sub-problems overlap? Sub-problems share sub-sub-problems. A DaC algorithm would do more work than necessary, because it needs to repeatedly solve the overlapped sub-sub-problems. A Dynamic Programming algorithm solves each sub-sub-problem only once and then saves its result (in array or hash table), thus avoiding the work of repeatedly solving the common sub-sub-problems. Example - Fibonacci Numbers \u00b6 A rabbit was born in the beginning. A rabbit starts producing offspring on the second generation after its birth and produces one child each generation. How many rabbits will there be after n generations? Straightforward Recursive \u00b6 F(n)=F(n-1)+F(n-2) F(n)=F(n-1)+F(n-2) F(0)=0,\\space F(1)=1 F(0)=0,\\space F(1)=1 0,1,1,2,3,5,8,13,21,34,... 0,1,1,2,3,5,8,13,21,34,... 1 2 3 FibonacciR(n) 01 if n <= 1 then return n 02 else return FibonacciR(n-1) + FibonacciR(n-2) This is slow! Here's why: The same value is calculated over and over! Sub-problems are overlapping \u2013 they share sub-sub-problems. Solution \u00b6 Dynamic Programming! We can calculate F(n) in linear time, by remembering solutions to the solved sub-problems. Compute solution in a bottom-up fashion . 1 2 3 4 5 6 Fibonacci(n) 01 F[0] = 0 02 F[1] = 1 03 for i = 2 to n do 04 F[i] = F[i-1] + F[i-2] 05 return F[n] Optimization Problems \u00b6 DP is typically applied to optimization problems . Optimization problems can have many possible solutions, each solution has a value, and we wish to find a solution with the optimal value (e.i. minimum or maximum). An algorithm should compute the optimal value plus, if needed an optimal solution. Example - Rod Cutting \u00b6 Problem: A steel rod of length n should be cut and sold in pieces. Pieces sold only in integer sizes according to a price table P[1..n] P[1..n] Goal: Cut up the rod to maximize profit. r_n r_n : the maximum profit of cutton a rod with length n . $$ r_n=max(P[1]+r_{n-1},P[2]+r_{n-2},...,P[n-1]+r_1,P[n]+r_0) $$ Having a rod with length 1, i.e., P[1], and the maximum profit of the remaining rod with length n-1, i.e., r_{n-2} r_{n-2} Having a rod with length 2, i.e., P[2], and the maximum profit of the remaining rod with length n - 2, i.e., r_{n-2} r_{n-2} \u2026 Having a rod with length n, i.e., P[n], and the maximum profit of the remaining rod with length 0, i.e., r_=0 r_=0 We say that the rod cutting problem exhibits optimal substructure . 1 2 3 4 5 6 Rod-Cut(P, n) //P: Price table as array, n: rod length as integer. 01 if n = 0 then return 0 02 q = -infinity 03 for i=1 to n do 04 q = max(q, P[i] + Rod-Cut(P, n-1)) 05 return q Running time is Exponential ! \u200b See analysis on lecture 9 slides, slide 28-30 Memoization - Top-Down \u00b6 Remember the solutions in an array or a hash-table. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Rod-Cut-M(P, n) 01 for i = 1 to n do 02 R[i] = -infinity 03 return Rod-Cut-M-Aux(P, n, R) Rod-Cut-M-Aux(P, n, R) 01 if R[n] >= 0 then return R[n] 02 if n = 0 then q = 0 03 else 04 q = -infinity 05 for i = 0 to n do 06 q = max(q, P[i]+Rod-Cut-M-Aux(P, n-1, R)) 07 R[n] = q 08 return q Running Example \u00b6 \u200b See on lecture 9 slides, slide 35-46 Run time is Quadratic .","title":"Dynamic Programming"},{"location":"3-semester/AD1/09-dynamic-programming/#dynamic-programming","text":"What if the sub-problems overlap? Sub-problems share sub-sub-problems. A DaC algorithm would do more work than necessary, because it needs to repeatedly solve the overlapped sub-sub-problems. A Dynamic Programming algorithm solves each sub-sub-problem only once and then saves its result (in array or hash table), thus avoiding the work of repeatedly solving the common sub-sub-problems.","title":"Dynamic Programming"},{"location":"3-semester/AD1/09-dynamic-programming/#example-fibonacci-numbers","text":"A rabbit was born in the beginning. A rabbit starts producing offspring on the second generation after its birth and produces one child each generation. How many rabbits will there be after n generations?","title":"Example - Fibonacci Numbers"},{"location":"3-semester/AD1/09-dynamic-programming/#straightforward-recursive","text":"F(n)=F(n-1)+F(n-2) F(n)=F(n-1)+F(n-2) F(0)=0,\\space F(1)=1 F(0)=0,\\space F(1)=1 0,1,1,2,3,5,8,13,21,34,... 0,1,1,2,3,5,8,13,21,34,... 1 2 3 FibonacciR(n) 01 if n <= 1 then return n 02 else return FibonacciR(n-1) + FibonacciR(n-2) This is slow! Here's why: The same value is calculated over and over! Sub-problems are overlapping \u2013 they share sub-sub-problems.","title":"Straightforward Recursive"},{"location":"3-semester/AD1/09-dynamic-programming/#solution","text":"Dynamic Programming! We can calculate F(n) in linear time, by remembering solutions to the solved sub-problems. Compute solution in a bottom-up fashion . 1 2 3 4 5 6 Fibonacci(n) 01 F[0] = 0 02 F[1] = 1 03 for i = 2 to n do 04 F[i] = F[i-1] + F[i-2] 05 return F[n]","title":"Solution"},{"location":"3-semester/AD1/09-dynamic-programming/#optimization-problems","text":"DP is typically applied to optimization problems . Optimization problems can have many possible solutions, each solution has a value, and we wish to find a solution with the optimal value (e.i. minimum or maximum). An algorithm should compute the optimal value plus, if needed an optimal solution.","title":"Optimization Problems"},{"location":"3-semester/AD1/09-dynamic-programming/#example-rod-cutting","text":"Problem: A steel rod of length n should be cut and sold in pieces. Pieces sold only in integer sizes according to a price table P[1..n] P[1..n] Goal: Cut up the rod to maximize profit. r_n r_n : the maximum profit of cutton a rod with length n . $$ r_n=max(P[1]+r_{n-1},P[2]+r_{n-2},...,P[n-1]+r_1,P[n]+r_0) $$ Having a rod with length 1, i.e., P[1], and the maximum profit of the remaining rod with length n-1, i.e., r_{n-2} r_{n-2} Having a rod with length 2, i.e., P[2], and the maximum profit of the remaining rod with length n - 2, i.e., r_{n-2} r_{n-2} \u2026 Having a rod with length n, i.e., P[n], and the maximum profit of the remaining rod with length 0, i.e., r_=0 r_=0 We say that the rod cutting problem exhibits optimal substructure . 1 2 3 4 5 6 Rod-Cut(P, n) //P: Price table as array, n: rod length as integer. 01 if n = 0 then return 0 02 q = -infinity 03 for i=1 to n do 04 q = max(q, P[i] + Rod-Cut(P, n-1)) 05 return q Running time is Exponential ! \u200b See analysis on lecture 9 slides, slide 28-30","title":"Example - Rod Cutting"},{"location":"3-semester/AD1/09-dynamic-programming/#memoization-top-down","text":"Remember the solutions in an array or a hash-table. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Rod-Cut-M(P, n) 01 for i = 1 to n do 02 R[i] = -infinity 03 return Rod-Cut-M-Aux(P, n, R) Rod-Cut-M-Aux(P, n, R) 01 if R[n] >= 0 then return R[n] 02 if n = 0 then q = 0 03 else 04 q = -infinity 05 for i = 0 to n do 06 q = max(q, P[i]+Rod-Cut-M-Aux(P, n-1, R)) 07 R[n] = q 08 return q","title":"Memoization - Top-Down"},{"location":"3-semester/AD1/09-dynamic-programming/#running-example","text":"\u200b See on lecture 9 slides, slide 35-46 Run time is Quadratic .","title":"Running Example"},{"location":"3-semester/AD1/10a-graph-theory/","text":"Graph Theory \u00b6 Directed graph , G G , is a pair (V,E) (V,E) , where V V is a finite set and E E is a binary relation on V V . The figure shows the directed graph G=(V,E) G=(V,E) , where V= \\{1,2,3,4,5,6\\} V= \\{1,2,3,4,5,6\\} and E=\\{(1,2), (2,2), (2,4), (2,5),(4,1),(4,5),(5,4),(6,3)\\} E=\\{(1,2), (2,2), (2,4), (2,5),(4,1),(4,5),(5,4),(6,3)\\} . The edge (2,2) is a self-loop . A \"point\" is called a vertex , (plural: vertices ). A \"connection\" is called an edges / edges . (E is the edge set ). An undirected graph is a graph where the edge set of E is unordered. This figure shows the undirected graph G=(V,E) G=(V,E) , where V=\\{1,2,3,4,5,6\\} V=\\{1,2,3,4,5,6\\} and E=\\{(1,2),(1,5),(2,5),(3,6)\\} E=\\{(1,2),(1,5),(2,5),(3,6)\\} In undirected graphs, self-loops are forbidden. (u,v) and (v,u) are considered the same edge. In a directed graph we say that (u,v) is incident from or leaves u, and is incident to or enters v. If the graph is undirected, (u,v) is incident on u and v. Incident from 6. Incident to 3. Leaves 6 and enters 6. If ( u,v ) is an edge in a graph, v is adjacent to u The degree of a vertex in an undirected graph is the number of edges incident on it. A vertex with a degree of 0 is isolated like vertex 4 in the undirected graph example. In a directed graph the out-degree of a vertex is the number of edges leaving it. The in-degree is the number of edges entering it. The degree is out-degree plus in-degree. A path of length k from u to u' is a sequence from u to u' going through k vertices. Mathematically: \u200b \\langle v_0, v_1,...,v_k\\rangle \\langle v_0, v_1,...,v_k\\rangle where u=v_0, u'=v_k u=v_0, u'=v_k and (v_{i-1},v_i)\\in E (v_{i-1},v_i)\\in E for i=1,2,...,k i=1,2,...,k The path contains the vertices v_0, v_1,...,v_k v_0, v_1,...,v_k and the edges (v_0,v_1),(v_1,v_2),...,(v_{k-1},v_k) (v_0,v_1),(v_1,v_2),...,(v_{k-1},v_k) A graph is sparse if |E| |E| is much less than |V|^2 |V|^2 \u200b while it is dense if |E| |E| is close to |V|^2 |V|^2 Representation of graphs \u00b6 There are two standard ways to represent a graph: A collection of adjacency lists An adjacency matrix If a graph is sparse the adjacency-list representation is of choice. If a graph is dense , adjacency-matrix may be of choice.","title":"Elementary graph algorithms (A)"},{"location":"3-semester/AD1/10a-graph-theory/#graph-theory","text":"Directed graph , G G , is a pair (V,E) (V,E) , where V V is a finite set and E E is a binary relation on V V . The figure shows the directed graph G=(V,E) G=(V,E) , where V= \\{1,2,3,4,5,6\\} V= \\{1,2,3,4,5,6\\} and E=\\{(1,2), (2,2), (2,4), (2,5),(4,1),(4,5),(5,4),(6,3)\\} E=\\{(1,2), (2,2), (2,4), (2,5),(4,1),(4,5),(5,4),(6,3)\\} . The edge (2,2) is a self-loop . A \"point\" is called a vertex , (plural: vertices ). A \"connection\" is called an edges / edges . (E is the edge set ). An undirected graph is a graph where the edge set of E is unordered. This figure shows the undirected graph G=(V,E) G=(V,E) , where V=\\{1,2,3,4,5,6\\} V=\\{1,2,3,4,5,6\\} and E=\\{(1,2),(1,5),(2,5),(3,6)\\} E=\\{(1,2),(1,5),(2,5),(3,6)\\} In undirected graphs, self-loops are forbidden. (u,v) and (v,u) are considered the same edge. In a directed graph we say that (u,v) is incident from or leaves u, and is incident to or enters v. If the graph is undirected, (u,v) is incident on u and v. Incident from 6. Incident to 3. Leaves 6 and enters 6. If ( u,v ) is an edge in a graph, v is adjacent to u The degree of a vertex in an undirected graph is the number of edges incident on it. A vertex with a degree of 0 is isolated like vertex 4 in the undirected graph example. In a directed graph the out-degree of a vertex is the number of edges leaving it. The in-degree is the number of edges entering it. The degree is out-degree plus in-degree. A path of length k from u to u' is a sequence from u to u' going through k vertices. Mathematically: \u200b \\langle v_0, v_1,...,v_k\\rangle \\langle v_0, v_1,...,v_k\\rangle where u=v_0, u'=v_k u=v_0, u'=v_k and (v_{i-1},v_i)\\in E (v_{i-1},v_i)\\in E for i=1,2,...,k i=1,2,...,k The path contains the vertices v_0, v_1,...,v_k v_0, v_1,...,v_k and the edges (v_0,v_1),(v_1,v_2),...,(v_{k-1},v_k) (v_0,v_1),(v_1,v_2),...,(v_{k-1},v_k) A graph is sparse if |E| |E| is much less than |V|^2 |V|^2 \u200b while it is dense if |E| |E| is close to |V|^2 |V|^2","title":"Graph Theory"},{"location":"3-semester/AD1/10a-graph-theory/#representation-of-graphs","text":"There are two standard ways to represent a graph: A collection of adjacency lists An adjacency matrix If a graph is sparse the adjacency-list representation is of choice. If a graph is dense , adjacency-matrix may be of choice.","title":"Representation of graphs"},{"location":"3-semester/AD1/10b-graph-algorithms/","text":"Searching a graph \u00b6 Systematically following its edges so as to visit its vertices. Can discover the structure of a graph. Many algorithms begin by searching their input graph to obtain the structure information. Searching a graph lies at the heart of the field of graph algorithms. Breadth-first search (BFS) \u00b6 Discovers all vertices at a distance k from s before discovering any vertices at distance k+1. \u200b \"Breadth First\" Input : A graph G=(V,E) and a source vertex s Aim : Systematically discovers every vertex that is reachable from s. Output : The distance from s to each reachable vertex. Distance = the smallest number of edges (unweighted graph) A Breadth-first tree with root s that contains all reachable vertices. Vertex attributes : Color : White : unexplored Gray : explored, but not all adjacent vertices has been explored. Black : explored + all adjacent explored. Distance : Distance to source s. Parent : Its parent in the breadth-first tree. Algorithm \u00b6 Running Example \u00b6 Breadt-first tree \u00b6 Running Time \u00b6 \u200b See analisys in lecture-10 slides, slide 26 Running time: $$ O(|V|+|E|) $$ Depth-first search (DFS) \u00b6 It searches \"deeper\" in the graph whenever possible. Input: A graph G=(V,E) Aim: Systematically visit every vertex in V. Output: A depth-first forest that is composed of several depth-first trees. Vertex attributes : Color (same as BFS): White : unexplored Gray : explored, but not all adjacent vertices has been explored. Black : explored + all adjacent explored. Timestamp : v.d: discovery time, ie. when v is first explored. v.f: finishing time, ie. when v finishes examining v's adjacency list. Parent : Its parent in the depth-first tree. Algorithm \u00b6 Running Example \u00b6 Depth-first forest \u00b6 Running Time \u00b6 Initializes all vertices (DFS: line 1-4): \u200b \\Theta(|V|) \\Theta(|V|) DFS-Visit is called exactly once for each vertex, when its white (DFS: line 5-6): \u200b \\Theta(|V|) \\Theta(|V|) For each vertex u, the loop executes |u.adjacent()|\u200b times (DFS-Visit: line 4-7). \u200b \\Sigma_{u\\in V}(|u.adjacent()|=|E|) \\Sigma_{u\\in V}(|u.adjacent()|=|E|) Thus: $$ \\Theta(|V|+|E|) $$ BFS vs. DFS \u00b6 BFS Search from one source Only visits vertices that are reachable from source. BFS tree. Often serves to find shortest paths and shortests path distances. O(|V|+|E|) O(|V|+|E|) DFS May search from multiple sources. Visit every vertex. DFS forest. Often as a subroutine in another algorithm, eg.: Classifying edges. Topological sort. Strongly connected components \\Theta(|V|+|E|) \\Theta(|V|+|E|) Edge Classification based on DFS \u00b6 Tree edges: Edges that are in the DFS forest From the example (u,v), (v,y), (y,x), (w,z) Non Tree edges : Back edges From descendant to ancestor in DFS tree (x,v) Self loops (z,z) Forward edges From ancestor to descendant in a DFS tree (u,x) Cross edges Remaining edges, between trees or subtrees (w,y) When exploring an edge (x,y), y's color tells something: If y is white - visit x, then y, edge (x, y) is a tree edge. If y is gray - visit y, later x, then y again, edge (x, y) is a back edge. If y is black , edge (x, y) is a forward or cross edge.","title":"Elementary graph algorithms (B)"},{"location":"3-semester/AD1/10b-graph-algorithms/#searching-a-graph","text":"Systematically following its edges so as to visit its vertices. Can discover the structure of a graph. Many algorithms begin by searching their input graph to obtain the structure information. Searching a graph lies at the heart of the field of graph algorithms.","title":"Searching a graph"},{"location":"3-semester/AD1/10b-graph-algorithms/#breadth-first-search-bfs","text":"Discovers all vertices at a distance k from s before discovering any vertices at distance k+1. \u200b \"Breadth First\" Input : A graph G=(V,E) and a source vertex s Aim : Systematically discovers every vertex that is reachable from s. Output : The distance from s to each reachable vertex. Distance = the smallest number of edges (unweighted graph) A Breadth-first tree with root s that contains all reachable vertices. Vertex attributes : Color : White : unexplored Gray : explored, but not all adjacent vertices has been explored. Black : explored + all adjacent explored. Distance : Distance to source s. Parent : Its parent in the breadth-first tree.","title":"Breadth-first search (BFS)"},{"location":"3-semester/AD1/10b-graph-algorithms/#algorithm","text":"","title":"Algorithm"},{"location":"3-semester/AD1/10b-graph-algorithms/#running-example","text":"","title":"Running Example"},{"location":"3-semester/AD1/10b-graph-algorithms/#breadt-first-tree","text":"","title":"Breadt-first tree"},{"location":"3-semester/AD1/10b-graph-algorithms/#running-time","text":"\u200b See analisys in lecture-10 slides, slide 26 Running time: $$ O(|V|+|E|) $$","title":"Running Time"},{"location":"3-semester/AD1/10b-graph-algorithms/#depth-first-search-dfs","text":"It searches \"deeper\" in the graph whenever possible. Input: A graph G=(V,E) Aim: Systematically visit every vertex in V. Output: A depth-first forest that is composed of several depth-first trees. Vertex attributes : Color (same as BFS): White : unexplored Gray : explored, but not all adjacent vertices has been explored. Black : explored + all adjacent explored. Timestamp : v.d: discovery time, ie. when v is first explored. v.f: finishing time, ie. when v finishes examining v's adjacency list. Parent : Its parent in the depth-first tree.","title":"Depth-first search (DFS)"},{"location":"3-semester/AD1/10b-graph-algorithms/#algorithm_1","text":"","title":"Algorithm"},{"location":"3-semester/AD1/10b-graph-algorithms/#running-example_1","text":"","title":"Running Example"},{"location":"3-semester/AD1/10b-graph-algorithms/#depth-first-forest","text":"","title":"Depth-first forest"},{"location":"3-semester/AD1/10b-graph-algorithms/#running-time_1","text":"Initializes all vertices (DFS: line 1-4): \u200b \\Theta(|V|) \\Theta(|V|) DFS-Visit is called exactly once for each vertex, when its white (DFS: line 5-6): \u200b \\Theta(|V|) \\Theta(|V|) For each vertex u, the loop executes |u.adjacent()|\u200b times (DFS-Visit: line 4-7). \u200b \\Sigma_{u\\in V}(|u.adjacent()|=|E|) \\Sigma_{u\\in V}(|u.adjacent()|=|E|) Thus: $$ \\Theta(|V|+|E|) $$","title":"Running Time"},{"location":"3-semester/AD1/10b-graph-algorithms/#bfs-vs-dfs","text":"BFS Search from one source Only visits vertices that are reachable from source. BFS tree. Often serves to find shortest paths and shortests path distances. O(|V|+|E|) O(|V|+|E|) DFS May search from multiple sources. Visit every vertex. DFS forest. Often as a subroutine in another algorithm, eg.: Classifying edges. Topological sort. Strongly connected components \\Theta(|V|+|E|) \\Theta(|V|+|E|)","title":"BFS vs. DFS"},{"location":"3-semester/AD1/10b-graph-algorithms/#edge-classification-based-on-dfs","text":"Tree edges: Edges that are in the DFS forest From the example (u,v), (v,y), (y,x), (w,z) Non Tree edges : Back edges From descendant to ancestor in DFS tree (x,v) Self loops (z,z) Forward edges From ancestor to descendant in a DFS tree (u,x) Cross edges Remaining edges, between trees or subtrees (w,y) When exploring an edge (x,y), y's color tells something: If y is white - visit x, then y, edge (x, y) is a tree edge. If y is gray - visit y, later x, then y again, edge (x, y) is a back edge. If y is black , edge (x, y) is a forward or cross edge.","title":"Edge Classification based on DFS"},{"location":"3-semester/AD1/10c-topological-sort/","text":"Directed Acyclic Graph (DAG) \u00b6 A DAG is a directed graph with no cycles. Applications: Indicate precedence relationships An edge e=( a, b ) from a to b means that event a must happen before event b . Dependency Graphs Example - Professor gets dressed in the morning. The professor must put on certain garments before others How to check DAG \u00b6 A directed graph is acyclic if and only if the graph has no back edges. Topological Sort \u00b6 CLRS page 612. Input: DAG G = (V, E) Aim: Introduce a linear ordering of all its vertices, such that for any edge ( u, v ) in the DAG, event u appears before event v in the ordering. Output : Topologically sorted DAG, ie. a linked list of vetices, showing an order. Reversely sort vertices according to the finishing times obtained from a DFS. If v.f < u.f v.f < u.f Event u happens before event v Algorithm \u00b6 1 2 3 4 Topological-Sort(G) 01 call DFS(G) to compute finishing times v.f for each vertex v 02 as each vertex is finished, insert it onto the front of a linked list 03 return the linked list of vertices Example \u00b6 Running Time \u00b6 DFS takes \\Theta(|V|+|E|) \\Theta(|V|+|E|) It takes constant time \\Theta(1) \\Theta(1) to insert a vertex onto the front of a linked list. In total, |V| vertices. Thus, \\Theta(|V|) \\Theta(|V|) In total: \\Theta(|V|+|E|) \\Theta(|V|+|E|) See Proof of correctness on lecture-10 slides, slide 47-48","title":"Elementary graph algorithms (C) - Topological Sort"},{"location":"3-semester/AD1/10c-topological-sort/#directed-acyclic-graph-dag","text":"A DAG is a directed graph with no cycles. Applications: Indicate precedence relationships An edge e=( a, b ) from a to b means that event a must happen before event b . Dependency Graphs Example - Professor gets dressed in the morning. The professor must put on certain garments before others","title":"Directed Acyclic Graph (DAG)"},{"location":"3-semester/AD1/10c-topological-sort/#how-to-check-dag","text":"A directed graph is acyclic if and only if the graph has no back edges.","title":"How to check DAG"},{"location":"3-semester/AD1/10c-topological-sort/#topological-sort","text":"CLRS page 612. Input: DAG G = (V, E) Aim: Introduce a linear ordering of all its vertices, such that for any edge ( u, v ) in the DAG, event u appears before event v in the ordering. Output : Topologically sorted DAG, ie. a linked list of vetices, showing an order. Reversely sort vertices according to the finishing times obtained from a DFS. If v.f < u.f v.f < u.f Event u happens before event v","title":"Topological Sort"},{"location":"3-semester/AD1/10c-topological-sort/#algorithm","text":"1 2 3 4 Topological-Sort(G) 01 call DFS(G) to compute finishing times v.f for each vertex v 02 as each vertex is finished, insert it onto the front of a linked list 03 return the linked list of vertices","title":"Algorithm"},{"location":"3-semester/AD1/10c-topological-sort/#example","text":"","title":"Example"},{"location":"3-semester/AD1/10c-topological-sort/#running-time","text":"DFS takes \\Theta(|V|+|E|) \\Theta(|V|+|E|) It takes constant time \\Theta(1) \\Theta(1) to insert a vertex onto the front of a linked list. In total, |V| vertices. Thus, \\Theta(|V|) \\Theta(|V|) In total: \\Theta(|V|+|E|) \\Theta(|V|+|E|) See Proof of correctness on lecture-10 slides, slide 47-48","title":"Running Time"},{"location":"3-semester/AD1/11a-strongly-connected-components/","text":"Strongly Connected Components \u00b6 A strongly connected component of a directed graph G=(V, E) is a maximal set of vertices C\\subseteq V C\\subseteq V , such that for every pair of vertices u and v in C, they are reachable from each other. Strongly connected components are: $$ {a,b,e},{c,d},{f,g},{h} $$ DFS and Transpose of a graph \u00b6 DFS often works as a subroutine in another algorithm Classifying edges Topological sort Strongly Connected Components Transpose of a graph : Given a graph G = (V, E) Its transpose graph is G^T=(V,E^T),where\\\\ E^T=\\{(u,v):(v,u)\\in E\\} G^T=(V,E^T),where\\\\ E^T=\\{(u,v):(v,u)\\in E\\} Transpose graph G^T G^T has the same vertex set as G, but has a different edge set from G, where directions of the edges are reversed. G and G^T G^T has exactly the same strongly connected components. Vertices u and v are reachable from each other in G if and only if they are reachable from each other in G^T G^T Algorithm \u00b6 1 2 3 4 5 6 7 Strongly-Connected-Components(G) 01 call DFS(G) to compute finishing times u.f for each vertex u 02 compute G^T 03 call DFS(G^T), but in the main loop of DFS, consider the vertices in order of decreasing u.f (as computed in line 1) 04 output the vertices of each tree in the depth-first forest formed in line 3 as a seperate strongly connected component Running Example \u00b6","title":"Strongly Connected Components"},{"location":"3-semester/AD1/11a-strongly-connected-components/#strongly-connected-components","text":"A strongly connected component of a directed graph G=(V, E) is a maximal set of vertices C\\subseteq V C\\subseteq V , such that for every pair of vertices u and v in C, they are reachable from each other. Strongly connected components are: $$ {a,b,e},{c,d},{f,g},{h} $$","title":"Strongly Connected Components"},{"location":"3-semester/AD1/11a-strongly-connected-components/#dfs-and-transpose-of-a-graph","text":"DFS often works as a subroutine in another algorithm Classifying edges Topological sort Strongly Connected Components Transpose of a graph : Given a graph G = (V, E) Its transpose graph is G^T=(V,E^T),where\\\\ E^T=\\{(u,v):(v,u)\\in E\\} G^T=(V,E^T),where\\\\ E^T=\\{(u,v):(v,u)\\in E\\} Transpose graph G^T G^T has the same vertex set as G, but has a different edge set from G, where directions of the edges are reversed. G and G^T G^T has exactly the same strongly connected components. Vertices u and v are reachable from each other in G if and only if they are reachable from each other in G^T G^T","title":"DFS and Transpose of a graph"},{"location":"3-semester/AD1/11a-strongly-connected-components/#algorithm","text":"1 2 3 4 5 6 7 Strongly-Connected-Components(G) 01 call DFS(G) to compute finishing times u.f for each vertex u 02 compute G^T 03 call DFS(G^T), but in the main loop of DFS, consider the vertices in order of decreasing u.f (as computed in line 1) 04 output the vertices of each tree in the depth-first forest formed in line 3 as a seperate strongly connected component","title":"Algorithm"},{"location":"3-semester/AD1/11a-strongly-connected-components/#running-example","text":"","title":"Running Example"},{"location":"3-semester/AD1/11b-minimum-spanning-tree/","text":"More Graph Concepts \u00b6 Weighted Graph G = (V, E), with a weight function \\bold{w}:E\\rarr\\R \\bold{w}:E\\rarr\\R Weight function w assigns a cost value to each edge in E. Eg. In a graph modeling a road netwok, the weight of an edge represents the length of a road. Eg. w(e)=10 w(e)=10 or w(u,v)=10 w(u,v)=10 , given e=(u) e=(u) Path A sequence of vertices <v_1,v_2,...,v_k> <v_1,v_2,...,v_k> such that vertex v_{i+1} v_{i+1} is adjacent to vertec v_i v_i for i=1 ... k-1 i=1 ... k-1 A sequence of edges <(v_1,v_2),(v_2,v_3),...,(v_{k-1},v_k)> <(v_1,v_2),(v_2,v_3),...,(v_{k-1},v_k)> A sequence of edges <e_1, e_2, ...,e_{k-1}> <e_1, e_2, ...,e_{k-1}> , where e_1=(v_1,v_2) e_1=(v_1,v_2) e_2=(v_2,v_3),..., e_2=(v_2,v_3),..., and e_{k-1}=(v_{k-1},v_k) e_{k-1}=(v_{k-1},v_k) Sub-Graph A subset of vertices and edges. Connected Graph Any two vertices in the graph are connected by some path Tree Connected undirected graph without cycles. Spanning Tree \u00b6 A spanning tree T of a connected, undirected graph G = (V, E) is a sub-graph of G, satisfying: T contains all vertices of G; T connnects any two vertices of G; T\\subseteq E T\\subseteq E and T is acyclic. T is a tree, since T is acyclic and connects any two vertices of the undirected graph G. Minimum Spanning Tree (MST) \u00b6 There are more than one spanning tree. MST of a connected, undirected, weighted graph G is a spanning tree T: Satisfying all conditions of a spanning tree. Has the minimum value of w(T)=\\Sigma_{(u,v)\\in T}(w(u,v)) w(T)=\\Sigma_{(u,v)\\in T}(w(u,v)) , among all possible spanning trees. Finding MST is an optimization problem. Growing MST \u00b6 Input Connected, undirected, weigthed graph G = (V, E) A weight function w:E\\rarr \\R w:E\\rarr \\R Output An MST A , ie. a set of edges. Intuition - Greedy search: Initialize A = \u00f8, and A is a subset of some MST, ie. a tree. Add one edge (u, v) to A at a time, such that A \\cup\\{((u,v)\\} A \\cup\\{((u,v)\\} is a subset of some MST. Key part: How to determine an edge (u, v) to add? Edge (u,v)\\in E (u,v)\\in E but (u,v)\\notin A (u,v)\\notin A What else? 1 2 3 4 5 6 Generic-MST(G, w) 01 A = \u00d8 02 while A does not form a spanning tree 03 find an edge (u,v) that is safe for A 04 A = A U {(u,v)} 05 return A Prim's Algorithm \u00b6 https://www.youtube.com/watch?v=YyLaRffCdk4 A special case of the generic MST method. Input Connected, undirected, weighted graph G = (V, E) A weight function w:E\\rarr \\R w:E\\rarr \\R A random vertex r to start with. Output MST where each vertex v has two attributes Parent, v.parent : v's parent in the MST Key, v.key: the least weight of any edge connecting v to a vertex in the MST. Intuition A vertex based algorithm The algorithm maintains a tree. Add one vertex to a tree at a time, until all are added -- MSP Safe edge : the least weight edge that connects a vertex v not in the tree, to a vertex in the tree, ie. greedy feature, -- add v. Pseudocode \u00b6 Running Example \u00b6 See explanation in lecture-11 slides, slide 24-28! Complexity \u00b6 See analysis in lecture-11 slides, slide 31 O(|E|*lg(|V|)) O(|E|*lg(|V|)) Kruskal Algorithm \u00b6 https://www.youtube.com/watch?v=5xosHRdxqHA A special case of the generic MST method. Input Connected, undirected, weighted graph G = (V, E) A weight function w:E\\rarr\\R w:E\\rarr\\R Output MST Intuition An edge based algorithm The algorithm maintains a forest , where each vertex is treated as a distinct tree in the beginning. Add one edge from G to MST at a time. Safe edge: the least weight edge amon all edges in G that connects to distinct trees in the forest , ie. greedy feature. The algorithm keeps adding a sefe edge (u, v) to the MST, if (u, v) satisfies: C1: has the least weight among all edges in G C2: connects two different trees in the forest - (u, v) is not in MST. If u and v belong to the same tree in the forest, u and v are a part of a MST - adding (u, v) creates a cycle for MST. Pseudocode \u00b6 Running Example \u00b6 See explanation in lecture-11 slides, slide 36-41 Generic Algorithm \u00b6 A cut of an undirected graph G = (V, E) is a partition of vertices, denoted as (S, V-S) , where S\\sub V S\\sub V An edge (u,v)\\in E (u,v)\\in E crosses the cut if u is in S and v is in V-S , or v is in S and u is in V-S Light edge is an edge crossing the cut an has the minimum weight of any edge crossing the cut. Given a cut (S, V-S) as a partition of G = (V, E). Considering a set A of edges, we say the cut respects A if no edge in A crosses the cut. 1 2 3 4 5 6 7 MST(G) 01 A = \u00d8 02 while A does not form a spanning tree do 03.1 Make a cut (S, V-S) of G that respects A 03.2 Take the light edge (u,v) crossing S to V-S 04 A = A U {(u,v)} 05 return A 3.1: If an edge belongs to A, ie. a part of MST, it does not cross S to V-S, this makes sure edges in A are not safe edges. 3.2: The light edge (u, v) is safe for A, satisfying: (u, v) crosses S to V-S: (u, v) does not belong to A (u, v) has the minimum weight of any edge crossing the cut: greedy. Essence: Find a possible cut. Take a light edge. Example \u00b6 From the AD (DAT/SW) Exam 2015 Prim's vs Generic Algorithm \u00b6 See more in lecture-11 slide 48 Kruskals's vs Generic Algorithm \u00b6 See more in lecture-11 slide 49-51","title":"Minimum Spanning Tree"},{"location":"3-semester/AD1/11b-minimum-spanning-tree/#more-graph-concepts","text":"Weighted Graph G = (V, E), with a weight function \\bold{w}:E\\rarr\\R \\bold{w}:E\\rarr\\R Weight function w assigns a cost value to each edge in E. Eg. In a graph modeling a road netwok, the weight of an edge represents the length of a road. Eg. w(e)=10 w(e)=10 or w(u,v)=10 w(u,v)=10 , given e=(u) e=(u) Path A sequence of vertices <v_1,v_2,...,v_k> <v_1,v_2,...,v_k> such that vertex v_{i+1} v_{i+1} is adjacent to vertec v_i v_i for i=1 ... k-1 i=1 ... k-1 A sequence of edges <(v_1,v_2),(v_2,v_3),...,(v_{k-1},v_k)> <(v_1,v_2),(v_2,v_3),...,(v_{k-1},v_k)> A sequence of edges <e_1, e_2, ...,e_{k-1}> <e_1, e_2, ...,e_{k-1}> , where e_1=(v_1,v_2) e_1=(v_1,v_2) e_2=(v_2,v_3),..., e_2=(v_2,v_3),..., and e_{k-1}=(v_{k-1},v_k) e_{k-1}=(v_{k-1},v_k) Sub-Graph A subset of vertices and edges. Connected Graph Any two vertices in the graph are connected by some path Tree Connected undirected graph without cycles.","title":"More Graph Concepts"},{"location":"3-semester/AD1/11b-minimum-spanning-tree/#spanning-tree","text":"A spanning tree T of a connected, undirected graph G = (V, E) is a sub-graph of G, satisfying: T contains all vertices of G; T connnects any two vertices of G; T\\subseteq E T\\subseteq E and T is acyclic. T is a tree, since T is acyclic and connects any two vertices of the undirected graph G.","title":"Spanning Tree"},{"location":"3-semester/AD1/11b-minimum-spanning-tree/#minimum-spanning-tree-mst","text":"There are more than one spanning tree. MST of a connected, undirected, weighted graph G is a spanning tree T: Satisfying all conditions of a spanning tree. Has the minimum value of w(T)=\\Sigma_{(u,v)\\in T}(w(u,v)) w(T)=\\Sigma_{(u,v)\\in T}(w(u,v)) , among all possible spanning trees. Finding MST is an optimization problem.","title":"Minimum Spanning Tree (MST)"},{"location":"3-semester/AD1/11b-minimum-spanning-tree/#growing-mst","text":"Input Connected, undirected, weigthed graph G = (V, E) A weight function w:E\\rarr \\R w:E\\rarr \\R Output An MST A , ie. a set of edges. Intuition - Greedy search: Initialize A = \u00f8, and A is a subset of some MST, ie. a tree. Add one edge (u, v) to A at a time, such that A \\cup\\{((u,v)\\} A \\cup\\{((u,v)\\} is a subset of some MST. Key part: How to determine an edge (u, v) to add? Edge (u,v)\\in E (u,v)\\in E but (u,v)\\notin A (u,v)\\notin A What else? 1 2 3 4 5 6 Generic-MST(G, w) 01 A = \u00d8 02 while A does not form a spanning tree 03 find an edge (u,v) that is safe for A 04 A = A U {(u,v)} 05 return A","title":"Growing MST"},{"location":"3-semester/AD1/11b-minimum-spanning-tree/#prims-algorithm","text":"https://www.youtube.com/watch?v=YyLaRffCdk4 A special case of the generic MST method. Input Connected, undirected, weighted graph G = (V, E) A weight function w:E\\rarr \\R w:E\\rarr \\R A random vertex r to start with. Output MST where each vertex v has two attributes Parent, v.parent : v's parent in the MST Key, v.key: the least weight of any edge connecting v to a vertex in the MST. Intuition A vertex based algorithm The algorithm maintains a tree. Add one vertex to a tree at a time, until all are added -- MSP Safe edge : the least weight edge that connects a vertex v not in the tree, to a vertex in the tree, ie. greedy feature, -- add v.","title":"Prim's Algorithm"},{"location":"3-semester/AD1/11b-minimum-spanning-tree/#pseudocode","text":"","title":"Pseudocode"},{"location":"3-semester/AD1/11b-minimum-spanning-tree/#running-example","text":"See explanation in lecture-11 slides, slide 24-28!","title":"Running Example"},{"location":"3-semester/AD1/11b-minimum-spanning-tree/#complexity","text":"See analysis in lecture-11 slides, slide 31 O(|E|*lg(|V|)) O(|E|*lg(|V|))","title":"Complexity"},{"location":"3-semester/AD1/11b-minimum-spanning-tree/#kruskal-algorithm","text":"https://www.youtube.com/watch?v=5xosHRdxqHA A special case of the generic MST method. Input Connected, undirected, weighted graph G = (V, E) A weight function w:E\\rarr\\R w:E\\rarr\\R Output MST Intuition An edge based algorithm The algorithm maintains a forest , where each vertex is treated as a distinct tree in the beginning. Add one edge from G to MST at a time. Safe edge: the least weight edge amon all edges in G that connects to distinct trees in the forest , ie. greedy feature. The algorithm keeps adding a sefe edge (u, v) to the MST, if (u, v) satisfies: C1: has the least weight among all edges in G C2: connects two different trees in the forest - (u, v) is not in MST. If u and v belong to the same tree in the forest, u and v are a part of a MST - adding (u, v) creates a cycle for MST.","title":"Kruskal Algorithm"},{"location":"3-semester/AD1/11b-minimum-spanning-tree/#pseudocode_1","text":"","title":"Pseudocode"},{"location":"3-semester/AD1/11b-minimum-spanning-tree/#running-example_1","text":"See explanation in lecture-11 slides, slide 36-41","title":"Running Example"},{"location":"3-semester/AD1/11b-minimum-spanning-tree/#generic-algorithm","text":"A cut of an undirected graph G = (V, E) is a partition of vertices, denoted as (S, V-S) , where S\\sub V S\\sub V An edge (u,v)\\in E (u,v)\\in E crosses the cut if u is in S and v is in V-S , or v is in S and u is in V-S Light edge is an edge crossing the cut an has the minimum weight of any edge crossing the cut. Given a cut (S, V-S) as a partition of G = (V, E). Considering a set A of edges, we say the cut respects A if no edge in A crosses the cut. 1 2 3 4 5 6 7 MST(G) 01 A = \u00d8 02 while A does not form a spanning tree do 03.1 Make a cut (S, V-S) of G that respects A 03.2 Take the light edge (u,v) crossing S to V-S 04 A = A U {(u,v)} 05 return A 3.1: If an edge belongs to A, ie. a part of MST, it does not cross S to V-S, this makes sure edges in A are not safe edges. 3.2: The light edge (u, v) is safe for A, satisfying: (u, v) crosses S to V-S: (u, v) does not belong to A (u, v) has the minimum weight of any edge crossing the cut: greedy. Essence: Find a possible cut. Take a light edge.","title":"Generic Algorithm"},{"location":"3-semester/AD1/11b-minimum-spanning-tree/#example","text":"From the AD (DAT/SW) Exam 2015","title":"Example"},{"location":"3-semester/AD1/11b-minimum-spanning-tree/#prims-vs-generic-algorithm","text":"See more in lecture-11 slide 48","title":"Prim's vs Generic Algorithm"},{"location":"3-semester/AD1/11b-minimum-spanning-tree/#kruskalss-vs-generic-algorithm","text":"See more in lecture-11 slide 49-51","title":"Kruskals's vs Generic Algorithm"},{"location":"3-semester/AD1/12-finding-shortest-paths/","text":"The Shortests Path Problem \u00b6 Lars plans to go from Aalborg to Aarhus and he wants to save fuel. Assuming fuel consumption is prorpotional to travel distance One possible solution is: Model road network as weighted graph Enumerate all paths from Aalborg to Aarhus Add upp the lengths of roads in each path and selected the paht with shortest sum of lengths This is verty ineffecient because it examines a lot of paths that are note worth considering. Weighted, directed graph G = (V, E) with a weight function w:E\\rarr \\R w:E\\rarr \\R A path on G: \u200b p=<(v_0,v_1),(v_1,v_2),...,(v,_{k-1},v_k)> p=<(v_0,v_1),(v_1,v_2),...,(v,_{k-1},v_k)> \u200b w(v_0,v_1)\\space w(v_1,v_2)\\space\\space w(v,_{k-1},v_k) w(v_0,v_1)\\space w(v_1,v_2)\\space\\space w(v,_{k-1},v_k) Weight of a path is: \u200b w(p)=\\Sigma^{k-1}_{i=0}w(v_i,v_{i+1}) w(p)=\\Sigma^{k-1}_{i=0}w(v_i,v_{i+1}) Given two vertices u and v in V, More than one path exists to go from u to v, eg. p_1,p_2,...,p_n p_1,p_2,...,p_n Each path has a weight w(p_1),w(p_2),...,w(p_n) w(p_1),w(p_2),...,w(p_n) The shortests-path weight, denoted as \\delta(u, v) \\delta(u, v) , from u to v: min(w(p_1),w(p_2),...,w(p_n)) min(w(p_1),w(p_2),...,w(p_n)) defined as: The shortest path between u and v is a path p with the short-path weight w(p)=\\delta(u,v) w(p)=\\delta(u,v) Problems \u00b6 Single-source Find a shortest path from a given source (vertex s) to each of the vertices that are reachable from s. Single-pair Given two vertices, find a shortest path between them. Solution to single-source also solves this. All-pairs Find shortest-paths for every pair of vertices. Running a single-source algorithm once from each vertex More efficient method: Not covered in AD1 Shortest-paths for un-weighted graphs . BFS, lecture 10. Negative weights \u00b6 If weights are non-negative, shortest paths are well-defined. Negative weights effect shortest-path weights? Cycles \u00b6 A path <v_0,v_1,...,v_i,...,v_j,...,v_k> <v_0,v_1,...,v_i,...,v_j,...,v_k> forms a cycle, if v_i=v_j v_i=v_j Shortest paths have NO cycles. Negative-weight cycles No. Otherwise, shortest paths are not well-defined anymore. Positive-weight cycles No. Otherwise, we could get shorter paths by removing the cycles. 0-weight cycles No. We can just repeatedly remove the 0-weight cycles to form another shortest path, until the path becomes cycle free. Any acyclic path in a graph G = (V, E) contains at most |V| distinct vertices and at most |V|-1 edges. This is used by Bellman-Ford algorithm Shortest Paths Tree \u00b6 The result of shortest path algorithms. A shortest paths tree . The shortest paths tree is a tree with the source vertex s as the root It records a shortest path from the source vertex s to each vertex v that is reachable from s. Each vertex v v.parent() records the predecessor of v in its shortest path v.d() records a shortest-path weight from s to v. Relaxation Technique \u00b6 For each vertex v in the graph, we maintain v. d() : Estimate the weight of a shortest path from s to v. Initialize v.d() to \\infin \\infin in the beginning. Update, ie. decrease the value of v.d() during the search. Intuition Check whether a new path from s, via u, to v, can improve the existing shortest path from s to v. w(s,u)+w(u,v) w(s,u)+w(u,v) vs w(s,v) w(s,v) is u.d +w(u,v)<v.d u.d +w(u,v)<v.d ? 1 2 3 4 Relax(u,v,G) 01 if v.d > u.d + G.w(u,v) then 02 v.d = u.d + G.w(u,v) 03 v.parent = u Dijkstra's Algorithm \u00b6 Works fro graphs with non-negative edge weights Input Directed, weighted graph G = (V, E) A weight function w:E\\rarr\\R w:E\\rarr\\R A source vertex s. Output A set of vertices S, |S|=|V| Each vertex u\\in S u\\in S has a value for u.d() and for u.parent() If u is not reachable from s, u.d() = \\infin \\infin Intuition Maintain a set S of visited vertices, and each time 1) select a vertex u that is the \"closest\"* from s, and add it to S 2) relax all edges from u, ie. check whether going through u can improve the shortest-path weight of u's neighbours. * \"Closest\" = Least shortest-path weights (a priority queue prioritized on u.d()) Pseudocode \u00b6 A little different from book in syntax, see CLRS p. 658 Running Example \u00b6 Explained in Lecture-12 slide 18-21 Running Time \u00b6 Analysis in Lecture-12 slide 23-24 Correctness \u00b6 Analysis in Lecture-12 slide 25-28 Bellman-Ford Algorithm \u00b6 Dijkstra's doesn't work when there are negative edges. Intuition - We cannot guarantee that the length of a path increases when more edges are included. Bellman-Ford alg. can handle a graph where edges have negative weights (but no negative-weight cycles) Input Directed, weighted graph G = (V, E) A weight function w:E\\rarr\\R w:E\\rarr\\R A source vertex s. Output Boolean value: False = detects negative-weight cycles True = returns the shortest path-tree If Boolean value = true, a set of vertices S, |S|=|V| Each vertex u\\in S u\\in S has a value for u.d() and for u.parent() If u is not reachable from s, u.d() = \\infin \\infin Pseudocode \u00b6 Running Example \u00b6 Explained in Lecture-11 slide 30-34 Correctness \u00b6 Based on Path Relaxation Property \u200b Analysis in lecture-12 slide 35-37 Shortest Path in DAGs \u00b6 Topologically sort the DAG Relax the edges of vertices according to the topologically sorted order of vertices. Running Example \u00b6 Correctness \u00b6 Based on Path Relaxation Property \u200b Analysis in lecture-12 slide 41 + 35-37","title":"Finding Shortest Paths"},{"location":"3-semester/AD1/12-finding-shortest-paths/#the-shortests-path-problem","text":"Lars plans to go from Aalborg to Aarhus and he wants to save fuel. Assuming fuel consumption is prorpotional to travel distance One possible solution is: Model road network as weighted graph Enumerate all paths from Aalborg to Aarhus Add upp the lengths of roads in each path and selected the paht with shortest sum of lengths This is verty ineffecient because it examines a lot of paths that are note worth considering. Weighted, directed graph G = (V, E) with a weight function w:E\\rarr \\R w:E\\rarr \\R A path on G: \u200b p=<(v_0,v_1),(v_1,v_2),...,(v,_{k-1},v_k)> p=<(v_0,v_1),(v_1,v_2),...,(v,_{k-1},v_k)> \u200b w(v_0,v_1)\\space w(v_1,v_2)\\space\\space w(v,_{k-1},v_k) w(v_0,v_1)\\space w(v_1,v_2)\\space\\space w(v,_{k-1},v_k) Weight of a path is: \u200b w(p)=\\Sigma^{k-1}_{i=0}w(v_i,v_{i+1}) w(p)=\\Sigma^{k-1}_{i=0}w(v_i,v_{i+1}) Given two vertices u and v in V, More than one path exists to go from u to v, eg. p_1,p_2,...,p_n p_1,p_2,...,p_n Each path has a weight w(p_1),w(p_2),...,w(p_n) w(p_1),w(p_2),...,w(p_n) The shortests-path weight, denoted as \\delta(u, v) \\delta(u, v) , from u to v: min(w(p_1),w(p_2),...,w(p_n)) min(w(p_1),w(p_2),...,w(p_n)) defined as: The shortest path between u and v is a path p with the short-path weight w(p)=\\delta(u,v) w(p)=\\delta(u,v)","title":"The Shortests Path Problem"},{"location":"3-semester/AD1/12-finding-shortest-paths/#problems","text":"Single-source Find a shortest path from a given source (vertex s) to each of the vertices that are reachable from s. Single-pair Given two vertices, find a shortest path between them. Solution to single-source also solves this. All-pairs Find shortest-paths for every pair of vertices. Running a single-source algorithm once from each vertex More efficient method: Not covered in AD1 Shortest-paths for un-weighted graphs . BFS, lecture 10.","title":"Problems"},{"location":"3-semester/AD1/12-finding-shortest-paths/#negative-weights","text":"If weights are non-negative, shortest paths are well-defined. Negative weights effect shortest-path weights?","title":"Negative weights"},{"location":"3-semester/AD1/12-finding-shortest-paths/#cycles","text":"A path <v_0,v_1,...,v_i,...,v_j,...,v_k> <v_0,v_1,...,v_i,...,v_j,...,v_k> forms a cycle, if v_i=v_j v_i=v_j Shortest paths have NO cycles. Negative-weight cycles No. Otherwise, shortest paths are not well-defined anymore. Positive-weight cycles No. Otherwise, we could get shorter paths by removing the cycles. 0-weight cycles No. We can just repeatedly remove the 0-weight cycles to form another shortest path, until the path becomes cycle free. Any acyclic path in a graph G = (V, E) contains at most |V| distinct vertices and at most |V|-1 edges. This is used by Bellman-Ford algorithm","title":"Cycles"},{"location":"3-semester/AD1/12-finding-shortest-paths/#shortest-paths-tree","text":"The result of shortest path algorithms. A shortest paths tree . The shortest paths tree is a tree with the source vertex s as the root It records a shortest path from the source vertex s to each vertex v that is reachable from s. Each vertex v v.parent() records the predecessor of v in its shortest path v.d() records a shortest-path weight from s to v.","title":"Shortest Paths Tree"},{"location":"3-semester/AD1/12-finding-shortest-paths/#relaxation-technique","text":"For each vertex v in the graph, we maintain v. d() : Estimate the weight of a shortest path from s to v. Initialize v.d() to \\infin \\infin in the beginning. Update, ie. decrease the value of v.d() during the search. Intuition Check whether a new path from s, via u, to v, can improve the existing shortest path from s to v. w(s,u)+w(u,v) w(s,u)+w(u,v) vs w(s,v) w(s,v) is u.d +w(u,v)<v.d u.d +w(u,v)<v.d ? 1 2 3 4 Relax(u,v,G) 01 if v.d > u.d + G.w(u,v) then 02 v.d = u.d + G.w(u,v) 03 v.parent = u","title":"Relaxation Technique"},{"location":"3-semester/AD1/12-finding-shortest-paths/#dijkstras-algorithm","text":"Works fro graphs with non-negative edge weights Input Directed, weighted graph G = (V, E) A weight function w:E\\rarr\\R w:E\\rarr\\R A source vertex s. Output A set of vertices S, |S|=|V| Each vertex u\\in S u\\in S has a value for u.d() and for u.parent() If u is not reachable from s, u.d() = \\infin \\infin Intuition Maintain a set S of visited vertices, and each time 1) select a vertex u that is the \"closest\"* from s, and add it to S 2) relax all edges from u, ie. check whether going through u can improve the shortest-path weight of u's neighbours. * \"Closest\" = Least shortest-path weights (a priority queue prioritized on u.d())","title":"Dijkstra's Algorithm"},{"location":"3-semester/AD1/12-finding-shortest-paths/#pseudocode","text":"A little different from book in syntax, see CLRS p. 658","title":"Pseudocode"},{"location":"3-semester/AD1/12-finding-shortest-paths/#running-example","text":"Explained in Lecture-12 slide 18-21","title":"Running Example"},{"location":"3-semester/AD1/12-finding-shortest-paths/#running-time","text":"Analysis in Lecture-12 slide 23-24","title":"Running Time"},{"location":"3-semester/AD1/12-finding-shortest-paths/#correctness","text":"Analysis in Lecture-12 slide 25-28","title":"Correctness"},{"location":"3-semester/AD1/12-finding-shortest-paths/#bellman-ford-algorithm","text":"Dijkstra's doesn't work when there are negative edges. Intuition - We cannot guarantee that the length of a path increases when more edges are included. Bellman-Ford alg. can handle a graph where edges have negative weights (but no negative-weight cycles) Input Directed, weighted graph G = (V, E) A weight function w:E\\rarr\\R w:E\\rarr\\R A source vertex s. Output Boolean value: False = detects negative-weight cycles True = returns the shortest path-tree If Boolean value = true, a set of vertices S, |S|=|V| Each vertex u\\in S u\\in S has a value for u.d() and for u.parent() If u is not reachable from s, u.d() = \\infin \\infin","title":"Bellman-Ford Algorithm"},{"location":"3-semester/AD1/12-finding-shortest-paths/#pseudocode_1","text":"","title":"Pseudocode"},{"location":"3-semester/AD1/12-finding-shortest-paths/#running-example_1","text":"Explained in Lecture-11 slide 30-34","title":"Running Example"},{"location":"3-semester/AD1/12-finding-shortest-paths/#correctness_1","text":"Based on Path Relaxation Property \u200b Analysis in lecture-12 slide 35-37","title":"Correctness"},{"location":"3-semester/AD1/12-finding-shortest-paths/#shortest-path-in-dags","text":"Topologically sort the DAG Relax the edges of vertices according to the topologically sorted order of vertices.","title":"Shortest Path in DAGs"},{"location":"3-semester/AD1/12-finding-shortest-paths/#running-example_2","text":"","title":"Running Example"},{"location":"3-semester/AD1/12-finding-shortest-paths/#correctness_2","text":"Based on Path Relaxation Property \u200b Analysis in lecture-12 slide 41 + 35-37","title":"Correctness"},{"location":"3-semester/AD1/extra1-big-o-cheat-sheet/","text":"Big O Cheat Sheet \u00b6 Complexity \u00b6 Chart \u00b6 Table \u00b6 Algorithms and Data structures \u00b6 Sorting Algorithms \u00b6 Data Structures \u00b6","title":"Big-O Cheat-Sheet"},{"location":"3-semester/AD1/extra1-big-o-cheat-sheet/#big-o-cheat-sheet","text":"","title":"Big O Cheat Sheet"},{"location":"3-semester/AD1/extra1-big-o-cheat-sheet/#complexity","text":"","title":"Complexity"},{"location":"3-semester/AD1/extra1-big-o-cheat-sheet/#chart","text":"","title":"Chart"},{"location":"3-semester/AD1/extra1-big-o-cheat-sheet/#table","text":"","title":"Table"},{"location":"3-semester/AD1/extra1-big-o-cheat-sheet/#algorithms-and-data-structures","text":"","title":"Algorithms and Data structures"},{"location":"3-semester/AD1/extra1-big-o-cheat-sheet/#sorting-algorithms","text":"","title":"Sorting Algorithms"},{"location":"3-semester/AD1/extra1-big-o-cheat-sheet/#data-structures","text":"","title":"Data Structures"},{"location":"3-semester/AD1/extra2-standard-notation-and-common-functions/","text":"Standard Notation and Common Functions \u00b6 See CLRS p. 53- Exponentials \u00b6 For all real a > 0, m, and n, we have the following identities Logarithms \u00b6 We shall be using the following notations: An important notational convention we shall adopt is that logarithm functions will apply only to the next term in the formula,so that lg\\space n + k lg\\space n + k will mean lg(n)+k lg(n)+k and not lg(n+k) lg(n+k) . If we hold b> 1 b> 1 constant, then for n>0 n>0 , the function log_b n log_b n is strictly increasing.","title":"Standard Notation and Common Functions"},{"location":"3-semester/AD1/extra2-standard-notation-and-common-functions/#standard-notation-and-common-functions","text":"See CLRS p. 53-","title":"Standard Notation and Common Functions"},{"location":"3-semester/AD1/extra2-standard-notation-and-common-functions/#exponentials","text":"For all real a > 0, m, and n, we have the following identities","title":"Exponentials"},{"location":"3-semester/AD1/extra2-standard-notation-and-common-functions/#logarithms","text":"We shall be using the following notations: An important notational convention we shall adopt is that logarithm functions will apply only to the next term in the formula,so that lg\\space n + k lg\\space n + k will mean lg(n)+k lg(n)+k and not lg(n+k) lg(n+k) . If we hold b> 1 b> 1 constant, then for n>0 n>0 , the function log_b n log_b n is strictly increasing.","title":"Logarithms"},{"location":"3-semester/AD1/extra3-standard-knowledge/","text":"Standard Knowledge \u00b6 Logarithm \u00b6 Log_{10}(200)=2.3 Log_{10}(200)=2.3 because 200=10^{2.3} 200=10^{2.3} Log_{10}(10^x)=x Log_{10}(10^x)=x i-th Logarithm: What to raise i in to get the result. log_i(x) log_i(x) : solve equation: \u200b find y: x=i^y x=i^y Others \u00b6 \\frac{a}{c}\\cdot\\frac{b}{d}=\\frac{ab}{cd} \\frac{a}{c}\\cdot\\frac{b}{d}=\\frac{ab}{cd} \\frac{a}{b}:\\frac{c}{d}=\\frac{a}{b}\\cdot\\frac{d}{c} \\frac{a}{b}:\\frac{c}{d}=\\frac{a}{b}\\cdot\\frac{d}{c}","title":"Earlier Knowledge"},{"location":"3-semester/AD1/extra3-standard-knowledge/#standard-knowledge","text":"","title":"Standard Knowledge"},{"location":"3-semester/AD1/extra3-standard-knowledge/#logarithm","text":"Log_{10}(200)=2.3 Log_{10}(200)=2.3 because 200=10^{2.3} 200=10^{2.3} Log_{10}(10^x)=x Log_{10}(10^x)=x i-th Logarithm: What to raise i in to get the result. log_i(x) log_i(x) : solve equation: \u200b find y: x=i^y x=i^y","title":"Logarithm"},{"location":"3-semester/AD1/extra3-standard-knowledge/#others","text":"\\frac{a}{c}\\cdot\\frac{b}{d}=\\frac{ab}{cd} \\frac{a}{c}\\cdot\\frac{b}{d}=\\frac{ab}{cd} \\frac{a}{b}:\\frac{c}{d}=\\frac{a}{b}\\cdot\\frac{d}{c} \\frac{a}{b}:\\frac{c}{d}=\\frac{a}{b}\\cdot\\frac{d}{c}","title":"Others"},{"location":"3-semester/DEB/","text":"DEB - DESIGN AND EVALUATION OF USER INTERFACES \u00b6 Moodle side: https://www.moodle.aau.dk/course/view.php?id=27350","title":"Course"},{"location":"3-semester/DEB/#deb-design-and-evaluation-of-user-interfaces","text":"Moodle side: https://www.moodle.aau.dk/course/view.php?id=27350","title":"DEB - DESIGN AND EVALUATION OF USER INTERFACES"},{"location":"3-semester/DEB/03-envisionment/","text":"Envisionment \u00b6 Making ideas visible Representing design products to ourselves and others Generation, communication and evaluation of ideas Types of representations : Exploration e.g. sketches and wireframes Accuracy e.g. prototypes Needs to be accurate enough, but not confusing (main features must stand out) Basic Techniques \u00b6 Sketch and Wireframe Examples \u00b6 Two examples from the same example project Sketch \u00b6 Wireframe \u00b6 A wireframe is a step up from a sketch Navigation maps \u00b6 Prototypes \u00b6 A prototype is something you can interact with. Sketch/wireframe provides a static view while a prototype provides an interactive view. There are two types of prototypes: Lo-fi prototypes and Hi-fi prototypes. Lo-fi prototypes \u00b6 AKA \u201cpaper prototypes\u201d More explorative. Quick to produce (throw-away) Broad emphasis on: Content Structure Key functions Navigation Hi-fi prototypes \u00b6 \"Plug in the power cord\". High level of detail (layout and graphics). Useful for evaluating specific design elements and features. Interactive. Used when design ideas are stabilizing. Hi-fi vs Lo-fi \u00b6 Lo-fi Cheap Lack of detail Not realistic Low level of validity when testing Hi-fi Expensive Realistic High level of validity when testing Testing is done at a later stage Clients believe they are real! Premature commitment Reluctance in making radical changes, because you as a designer has spend time on the prototype. Be careful with making prototypes too real. The clients can think that the system is done, and wonder why you need to spend more time. That's why the sketch example looks like a drawing. It is made in a tool, but made to look like a drawing, so that there is no doubt that it is not a finished product. Tools \u00b6 Exploration : Balsamiq Accuracy : Axure Power Point More tools: http://wiki.c2.com/?GuiPrototypingTools","title":"Envisionment"},{"location":"3-semester/DEB/03-envisionment/#envisionment","text":"Making ideas visible Representing design products to ourselves and others Generation, communication and evaluation of ideas Types of representations : Exploration e.g. sketches and wireframes Accuracy e.g. prototypes Needs to be accurate enough, but not confusing (main features must stand out)","title":"Envisionment"},{"location":"3-semester/DEB/03-envisionment/#basic-techniques","text":"","title":"Basic Techniques"},{"location":"3-semester/DEB/03-envisionment/#sketch-and-wireframe-examples","text":"Two examples from the same example project","title":"Sketch and Wireframe Examples"},{"location":"3-semester/DEB/03-envisionment/#sketch","text":"","title":"Sketch"},{"location":"3-semester/DEB/03-envisionment/#wireframe","text":"A wireframe is a step up from a sketch","title":"Wireframe"},{"location":"3-semester/DEB/03-envisionment/#navigation-maps","text":"","title":"Navigation maps"},{"location":"3-semester/DEB/03-envisionment/#prototypes","text":"A prototype is something you can interact with. Sketch/wireframe provides a static view while a prototype provides an interactive view. There are two types of prototypes: Lo-fi prototypes and Hi-fi prototypes.","title":"Prototypes"},{"location":"3-semester/DEB/03-envisionment/#lo-fi-prototypes","text":"AKA \u201cpaper prototypes\u201d More explorative. Quick to produce (throw-away) Broad emphasis on: Content Structure Key functions Navigation","title":"Lo-fi prototypes"},{"location":"3-semester/DEB/03-envisionment/#hi-fi-prototypes","text":"\"Plug in the power cord\". High level of detail (layout and graphics). Useful for evaluating specific design elements and features. Interactive. Used when design ideas are stabilizing.","title":"Hi-fi prototypes"},{"location":"3-semester/DEB/03-envisionment/#hi-fi-vs-lo-fi","text":"Lo-fi Cheap Lack of detail Not realistic Low level of validity when testing Hi-fi Expensive Realistic High level of validity when testing Testing is done at a later stage Clients believe they are real! Premature commitment Reluctance in making radical changes, because you as a designer has spend time on the prototype. Be careful with making prototypes too real. The clients can think that the system is done, and wonder why you need to spend more time. That's why the sketch example looks like a drawing. It is made in a tool, but made to look like a drawing, so that there is no doubt that it is not a finished product.","title":"Hi-fi vs Lo-fi"},{"location":"3-semester/DEB/03-envisionment/#tools","text":"Exploration : Balsamiq Accuracy : Axure Power Point More tools: http://wiki.c2.com/?GuiPrototypingTools","title":"Tools"},{"location":"3-semester/DEB/04-physical-design/","text":"Physical Design \u00b6 Literature: Benyon, D. (3rd edition): Chapter 4, section 4.5 + Chapter 9, sections 9.5 and 9.6 + Chapter 12, sections 12.1-12.3. Nielsen (1994): Enhancing the Explanatory Power of Usability Heuristics ( PDF ) Petrie (2012): What do users really care about?: a comparison of usability problems found by users and experts on highly interactive websites ( PDF ) Abstract vs. concrete design. Abstract (Conceptual design): Determining the following: Logic Functions Structure Content Concrete design: Physical realization of the same (logic, functions, ...) The UI: \"Everything in the system that people come in contact with\" Difference user interfaces: Command prompt and GUI (Graphical User Interface). Command Prompt \u00b6 Some things can be done fast by expert users. Graphical User Interface \u00b6 Direct Manipulation Example: Windows File Manager Continuous representation of the object of interest Physical actions or labeled button presses instead of complex syntax Rapid incremental reversible operations whose impact on the object of interest is immediately visible User friendly for novice user. WIMP Windows Icons Menus Pointers Icons \u00b6 Holton's icon checklist Understandable Familiar Unambiguous Memorable Informative Few Distinct Attractive Legible (Easily readable) Compact Coherent Extensible Menus \u00b6 Different types of menus: Cascading Pop-up Contextual Cascading Menu \u00b6 It cascades/unfolds. Compacting the information visible to the user. Popup Menu \u00b6 Contextual Menu \u00b6 Typically right click on folder/file you want to access. Adapts based on the context. Widget Guidelines \u00b6 Radio Buttons \u00b6 You can only select one item. Mutual exclusiveness. Checkboxes \u00b6 You can select more than one. Toolbars \u00b6 More examples: \u00b6 Textbox Dropdowns Date Textarea Password Tel URL And many more Nielsen's Heuristics \u00b6 Visibility of systems status Match between system and real world User control and freedom Consistency and standards Error prevention Recognition rather than recall Flexibility and efficiency of use Petrie and Powers' heuristics \u00b6 Physical presentation: Make text and interactive elements large and clear enough Make page layout clear Avoid short time-outs and display timeouts Make key content and elements and changes to them salient[^salient] Content: Provide relevant and appropriate content Provide sufficient but not excessive content Provide clear terms, abbreviations, avoid jargon Information Architecture: Provide clear, well-organized information structures Interactivity: How and why? Clear labels and instructions Avoid duplication/excessive effort by users Make input formats clear and easy Provide feedback on user actions and system progress Make the sequence of interaction logical Provide a logical and complete set of options Follow conventions for interaction Provide the interactive functionality users will need and expect Indicate if links go to an external site or to another webpage Interactive and non-interactive elements should be clearly distinguished Group interactive elements clearly and logically Provide informative error messages and error recovery [^salient]: most noticeable or important. (Dansk: fremtr\u00e6dende)","title":"Physical Design"},{"location":"3-semester/DEB/04-physical-design/#physical-design","text":"Literature: Benyon, D. (3rd edition): Chapter 4, section 4.5 + Chapter 9, sections 9.5 and 9.6 + Chapter 12, sections 12.1-12.3. Nielsen (1994): Enhancing the Explanatory Power of Usability Heuristics ( PDF ) Petrie (2012): What do users really care about?: a comparison of usability problems found by users and experts on highly interactive websites ( PDF ) Abstract vs. concrete design. Abstract (Conceptual design): Determining the following: Logic Functions Structure Content Concrete design: Physical realization of the same (logic, functions, ...) The UI: \"Everything in the system that people come in contact with\" Difference user interfaces: Command prompt and GUI (Graphical User Interface).","title":"Physical Design"},{"location":"3-semester/DEB/04-physical-design/#command-prompt","text":"Some things can be done fast by expert users.","title":"Command Prompt"},{"location":"3-semester/DEB/04-physical-design/#graphical-user-interface","text":"Direct Manipulation Example: Windows File Manager Continuous representation of the object of interest Physical actions or labeled button presses instead of complex syntax Rapid incremental reversible operations whose impact on the object of interest is immediately visible User friendly for novice user. WIMP Windows Icons Menus Pointers","title":"Graphical User Interface"},{"location":"3-semester/DEB/04-physical-design/#icons","text":"Holton's icon checklist Understandable Familiar Unambiguous Memorable Informative Few Distinct Attractive Legible (Easily readable) Compact Coherent Extensible","title":"Icons"},{"location":"3-semester/DEB/04-physical-design/#menus","text":"Different types of menus: Cascading Pop-up Contextual","title":"Menus"},{"location":"3-semester/DEB/04-physical-design/#cascading-menu","text":"It cascades/unfolds. Compacting the information visible to the user.","title":"Cascading Menu"},{"location":"3-semester/DEB/04-physical-design/#popup-menu","text":"","title":"Popup Menu"},{"location":"3-semester/DEB/04-physical-design/#contextual-menu","text":"Typically right click on folder/file you want to access. Adapts based on the context.","title":"Contextual Menu"},{"location":"3-semester/DEB/04-physical-design/#widget-guidelines","text":"","title":"Widget Guidelines"},{"location":"3-semester/DEB/04-physical-design/#radio-buttons","text":"You can only select one item. Mutual exclusiveness.","title":"Radio Buttons"},{"location":"3-semester/DEB/04-physical-design/#checkboxes","text":"You can select more than one.","title":"Checkboxes"},{"location":"3-semester/DEB/04-physical-design/#toolbars","text":"","title":"Toolbars"},{"location":"3-semester/DEB/04-physical-design/#more-examples","text":"Textbox Dropdowns Date Textarea Password Tel URL And many more","title":"More examples:"},{"location":"3-semester/DEB/04-physical-design/#nielsens-heuristics","text":"Visibility of systems status Match between system and real world User control and freedom Consistency and standards Error prevention Recognition rather than recall Flexibility and efficiency of use","title":"Nielsen's Heuristics"},{"location":"3-semester/DEB/04-physical-design/#petrie-and-powers-heuristics","text":"Physical presentation: Make text and interactive elements large and clear enough Make page layout clear Avoid short time-outs and display timeouts Make key content and elements and changes to them salient[^salient] Content: Provide relevant and appropriate content Provide sufficient but not excessive content Provide clear terms, abbreviations, avoid jargon Information Architecture: Provide clear, well-organized information structures Interactivity: How and why? Clear labels and instructions Avoid duplication/excessive effort by users Make input formats clear and easy Provide feedback on user actions and system progress Make the sequence of interaction logical Provide a logical and complete set of options Follow conventions for interaction Provide the interactive functionality users will need and expect Indicate if links go to an external site or to another webpage Interactive and non-interactive elements should be clearly distinguished Group interactive elements clearly and logically Provide informative error messages and error recovery [^salient]: most noticeable or important. (Dansk: fremtr\u00e6dende)","title":"Petrie and Powers' heuristics"},{"location":"3-semester/DEB/05-physical-design-memory-and-attention/","text":"Memory \u00b6 Working memory \u00b6 Central executive is part of working memory and part of the brain. Consists of two slave systems, and helps humans focus on/remember things they see or hear fairly detailed, but not for a long time Working memory is persistent for about 30 seconds. Dialog boxes are persistent until users take action, otherwise they might forget. Visuo-spatial sketchpad (visualization) Articulatory loop (audiotory) Called \"loop\" because humans will repeat things to remember them For example phone numbers Chunking \u00b6 Division of a thing to remember, into smaller parts, which then helps remember more Phone numbers 25687462 => 25 68 74 62 Area codes are often separated from the rest of the number Reduce the memory load by chunking Sub-items need to be strongly connected to the super category Another example is in the Word tool bar, that groups different actions together Theories about recollection \u00b6 How many things can a human recall, based on free memory, without any cues to help remember. Helps decide how many menu items to include in a UI. Miller (1956) 5 to 9 items \u00b6 Cowan (2002) 3 to 5 items \u00b6 A menu with many items is not necessarily bad design. Humans can't remember that much, needs to be shown what they need , rather than let them recall. Can be described as a form of direct manipulation (constant overview of objects, reverse actions and see them). Principles \u00b6 For reducing the memory load Recognition rather than recall Match between system and the real world An icon with a design from the real world, will let users recognize what it does Linked to recognition rather than recall Attention \u00b6 What we focus upon. Focus can be directed to surroundings while interacting. Concentration of mental effort on sensory (things that are seen or heard) or mental (thoughts) events. The Stroop Effect \u00b6 Shows how bad the human attention span is. When asked to say the color of the font, and not the color that is written, it is easier when there is a correspondence. Requires a larger mental effort when missing correspondence. Attention is directed towards reading the word (mental stimuli) but the font color is seen. Forms of attention \u00b6 Selective attention \u00b6 Where attention is directed towards one element An example is sitting on the phone at a bus stop. It is selective attention, but can quickly become divided if bus arrives. Must realize how environment can affect attention when designing. Divided attention \u00b6 When attention is shifted from one element to another An example is driving while being on the phone Another example is BT, shows lots of ads and articles, so difficult to be selective with attention Stress \u00b6 Stress will affect attention, and is described as external (noise, light vibration) and mental (anger, fear) stimuli affecting level of arousal. Yerkes-Dodson law \u00b6 As the stress/arousal level increases, so will the performance, until a certain level. More stress can be handled with a simple task than with a complex task, before performance drops. An optimal level of stress exist, but will vary between users, typically because of skill. If a system is designed in a stressful environment, the performance level needs to be considered. Examples \u00b6 A news website with lots of ads Ambulances Other safety critical environments Flight cockpits Environments with lots of stimuli Mental workload \u00b6 Describes how busy the user is and how difficult their task is. NASA Task Load Index \u00b6 Helps measure mental workload through questionnaire form Result of a NASA TLX Helps measure the perceived workload Visual search \u00b6 When having to locate an item in a visual scene. Easy to locate an item when using attention drawing elements, such as the item being larger or another color. Attention is grabbed. Humans do not use a particular strategy to search for items. They will often look at images however, so images can be \"dangerous\" to use, as they can stand out too much compared to text. Alerts and error messages \u00b6 If an error occurs or there is a change in the system state, the users attention needs to be grabbed. Can be done in two ways: Unobtrusive When an email is received and a small popup appears General Windows popups The message can be seen, but does not interfere with the users current task Obtrusive Master alerts in planes (will happen if an engine goes out, blinks and beeps). Grabs attention from flying, but is more important. Can't have too many obtrusive alerts as it will slow and/or hinder their work More detailed guidelines for error messages: Careful wording (direct, but not rude) Avoid threatening words (catastrophe etc) Do not use no double negation Be specific (e.g. \"enter your name\") No uppercase letters (Don't SHOUT at the user. Humans reads through shapes) Use attention-grabbing techniques with caution","title":"Physical Design - Memory & Attention"},{"location":"3-semester/DEB/05-physical-design-memory-and-attention/#memory","text":"","title":"Memory"},{"location":"3-semester/DEB/05-physical-design-memory-and-attention/#working-memory","text":"Central executive is part of working memory and part of the brain. Consists of two slave systems, and helps humans focus on/remember things they see or hear fairly detailed, but not for a long time Working memory is persistent for about 30 seconds. Dialog boxes are persistent until users take action, otherwise they might forget. Visuo-spatial sketchpad (visualization) Articulatory loop (audiotory) Called \"loop\" because humans will repeat things to remember them For example phone numbers","title":"Working memory"},{"location":"3-semester/DEB/05-physical-design-memory-and-attention/#chunking","text":"Division of a thing to remember, into smaller parts, which then helps remember more Phone numbers 25687462 => 25 68 74 62 Area codes are often separated from the rest of the number Reduce the memory load by chunking Sub-items need to be strongly connected to the super category Another example is in the Word tool bar, that groups different actions together","title":"Chunking"},{"location":"3-semester/DEB/05-physical-design-memory-and-attention/#theories-about-recollection","text":"How many things can a human recall, based on free memory, without any cues to help remember. Helps decide how many menu items to include in a UI.","title":"Theories about recollection"},{"location":"3-semester/DEB/05-physical-design-memory-and-attention/#miller-1956-5-to-9-items","text":"","title":"Miller (1956)   5 to 9 items"},{"location":"3-semester/DEB/05-physical-design-memory-and-attention/#cowan-2002-3-to-5-items","text":"A menu with many items is not necessarily bad design. Humans can't remember that much, needs to be shown what they need , rather than let them recall. Can be described as a form of direct manipulation (constant overview of objects, reverse actions and see them).","title":"Cowan (2002) 3 to 5 items"},{"location":"3-semester/DEB/05-physical-design-memory-and-attention/#principles","text":"For reducing the memory load Recognition rather than recall Match between system and the real world An icon with a design from the real world, will let users recognize what it does Linked to recognition rather than recall","title":"Principles"},{"location":"3-semester/DEB/05-physical-design-memory-and-attention/#attention","text":"What we focus upon. Focus can be directed to surroundings while interacting. Concentration of mental effort on sensory (things that are seen or heard) or mental (thoughts) events.","title":"Attention"},{"location":"3-semester/DEB/05-physical-design-memory-and-attention/#the-stroop-effect","text":"Shows how bad the human attention span is. When asked to say the color of the font, and not the color that is written, it is easier when there is a correspondence. Requires a larger mental effort when missing correspondence. Attention is directed towards reading the word (mental stimuli) but the font color is seen.","title":"The Stroop Effect"},{"location":"3-semester/DEB/05-physical-design-memory-and-attention/#forms-of-attention","text":"","title":"Forms of attention"},{"location":"3-semester/DEB/05-physical-design-memory-and-attention/#selective-attention","text":"Where attention is directed towards one element An example is sitting on the phone at a bus stop. It is selective attention, but can quickly become divided if bus arrives. Must realize how environment can affect attention when designing.","title":"Selective attention"},{"location":"3-semester/DEB/05-physical-design-memory-and-attention/#divided-attention","text":"When attention is shifted from one element to another An example is driving while being on the phone Another example is BT, shows lots of ads and articles, so difficult to be selective with attention","title":"Divided attention"},{"location":"3-semester/DEB/05-physical-design-memory-and-attention/#stress","text":"Stress will affect attention, and is described as external (noise, light vibration) and mental (anger, fear) stimuli affecting level of arousal.","title":"Stress"},{"location":"3-semester/DEB/05-physical-design-memory-and-attention/#yerkes-dodson-law","text":"As the stress/arousal level increases, so will the performance, until a certain level. More stress can be handled with a simple task than with a complex task, before performance drops. An optimal level of stress exist, but will vary between users, typically because of skill. If a system is designed in a stressful environment, the performance level needs to be considered.","title":"Yerkes-Dodson law"},{"location":"3-semester/DEB/05-physical-design-memory-and-attention/#examples","text":"A news website with lots of ads Ambulances Other safety critical environments Flight cockpits Environments with lots of stimuli","title":"Examples"},{"location":"3-semester/DEB/05-physical-design-memory-and-attention/#mental-workload","text":"Describes how busy the user is and how difficult their task is.","title":"Mental workload"},{"location":"3-semester/DEB/05-physical-design-memory-and-attention/#nasa-task-load-index","text":"Helps measure mental workload through questionnaire form Result of a NASA TLX Helps measure the perceived workload","title":"NASA Task Load Index"},{"location":"3-semester/DEB/05-physical-design-memory-and-attention/#visual-search","text":"When having to locate an item in a visual scene. Easy to locate an item when using attention drawing elements, such as the item being larger or another color. Attention is grabbed. Humans do not use a particular strategy to search for items. They will often look at images however, so images can be \"dangerous\" to use, as they can stand out too much compared to text.","title":"Visual search"},{"location":"3-semester/DEB/05-physical-design-memory-and-attention/#alerts-and-error-messages","text":"If an error occurs or there is a change in the system state, the users attention needs to be grabbed. Can be done in two ways: Unobtrusive When an email is received and a small popup appears General Windows popups The message can be seen, but does not interfere with the users current task Obtrusive Master alerts in planes (will happen if an engine goes out, blinks and beeps). Grabs attention from flying, but is more important. Can't have too many obtrusive alerts as it will slow and/or hinder their work More detailed guidelines for error messages: Careful wording (direct, but not rude) Avoid threatening words (catastrophe etc) Do not use no double negation Be specific (e.g. \"enter your name\") No uppercase letters (Don't SHOUT at the user. Humans reads through shapes) Use attention-grabbing techniques with caution","title":"Alerts and error messages"},{"location":"3-semester/DEB/06-perception-and-navigation/","text":"Perception and Navigation \u00b6 Perception \u00b6 Perceptual set: Our previous experiences that makes us experience depth and patterns in a particular way. Depth perception \u00b6 Critical in the design of 3D applications How we understand depth: Primary cues Retinal disparity (two separate images) Stereopsis (image combining process) Accommodation (muscular process to create image focus) Convergence (muscular process for image focus on short distances) Secondary cues (2D plane, monocular) Light and shade Linear perspective Height in horizontal plane Motion parallax Overlap Also makes use of light and shade Relative size Texture gradient Pattern recognition \u00b6 Based on our perceptual set Gestalt laws of perception Psychology terms Proximity \"Elements that are positioned close together are belonging together\" Two separate groups of stars Continuity \"We tend to perceive smooth, continuous patterns rather than disjoint, interrupted ones.\" Similarity \"Similar figures tend to be grouped together.\" Round vs diamond shapes -> These have different properties Closure \"Closed figures are perceived more easily than incomplete (or open) figures\" \"Images that are not complete that nevertheless provides a complete image\" Affordance \u00b6 \"The affordance of the environment are what it offers animals, what it provides or furnishes, for good or ill\" - Gibson So, an affordance is a resource that the environment offers an animal and... ...the animal must possess the capabilities to perceive and use the resource. \"It is perceived affordances that tell the user what actions can be performed on an object and, to some extent how to do them\" - Norman Examples \u00b6 Salt and pepper shaker. We know through our perceptual set, that we use more salt than pepper in European countries, so the one with more holes is for salt. There being only plates but no door handles provides the affordance, that we need to push the door. Through our perceptual set. This is a juice presser. But that is not clear from the leftmost picture. The affordance of this product is unclear. Navigation \u00b6 Object identification Identifying categories and clusters of objects Wayfinding Working out how to reach a destination Exploration Understanding what exists in an environment How objects are related Three types of signs: Informational signs (object identification) Directional signs (wayfinding) Warning and reassurance signs (exploration) Informational signs \u00b6 Orienting oneself (Passini 1994) Providing information on objects Aid in object identification and classification of objects Directional signs \u00b6 Choosing the correct route (Passini, 1994) Providing information on routes and route hierarchies Apple Guidelines: Provide one path for one destination Make the path logical We need to provide a clear visual hierarchy Warning and reassurance signs \u00b6 Monitoring chosen route (Passini, 1994) Recognizing when the destination is reached (Passini, 1994) Providing feedback on actual location Providing information on possible actions within the environment Apple Guidelines: Provide markers of where you are Traceability","title":"Physical Design - Perception & Navigation"},{"location":"3-semester/DEB/06-perception-and-navigation/#perception-and-navigation","text":"","title":"Perception and Navigation"},{"location":"3-semester/DEB/06-perception-and-navigation/#perception","text":"Perceptual set: Our previous experiences that makes us experience depth and patterns in a particular way.","title":"Perception"},{"location":"3-semester/DEB/06-perception-and-navigation/#depth-perception","text":"Critical in the design of 3D applications How we understand depth: Primary cues Retinal disparity (two separate images) Stereopsis (image combining process) Accommodation (muscular process to create image focus) Convergence (muscular process for image focus on short distances) Secondary cues (2D plane, monocular) Light and shade Linear perspective Height in horizontal plane Motion parallax Overlap Also makes use of light and shade Relative size Texture gradient","title":"Depth perception"},{"location":"3-semester/DEB/06-perception-and-navigation/#pattern-recognition","text":"Based on our perceptual set Gestalt laws of perception Psychology terms Proximity \"Elements that are positioned close together are belonging together\" Two separate groups of stars Continuity \"We tend to perceive smooth, continuous patterns rather than disjoint, interrupted ones.\" Similarity \"Similar figures tend to be grouped together.\" Round vs diamond shapes -> These have different properties Closure \"Closed figures are perceived more easily than incomplete (or open) figures\" \"Images that are not complete that nevertheless provides a complete image\"","title":"Pattern recognition"},{"location":"3-semester/DEB/06-perception-and-navigation/#affordance","text":"\"The affordance of the environment are what it offers animals, what it provides or furnishes, for good or ill\" - Gibson So, an affordance is a resource that the environment offers an animal and... ...the animal must possess the capabilities to perceive and use the resource. \"It is perceived affordances that tell the user what actions can be performed on an object and, to some extent how to do them\" - Norman","title":"Affordance"},{"location":"3-semester/DEB/06-perception-and-navigation/#examples","text":"Salt and pepper shaker. We know through our perceptual set, that we use more salt than pepper in European countries, so the one with more holes is for salt. There being only plates but no door handles provides the affordance, that we need to push the door. Through our perceptual set. This is a juice presser. But that is not clear from the leftmost picture. The affordance of this product is unclear.","title":"Examples"},{"location":"3-semester/DEB/06-perception-and-navigation/#navigation","text":"Object identification Identifying categories and clusters of objects Wayfinding Working out how to reach a destination Exploration Understanding what exists in an environment How objects are related Three types of signs: Informational signs (object identification) Directional signs (wayfinding) Warning and reassurance signs (exploration)","title":"Navigation"},{"location":"3-semester/DEB/06-perception-and-navigation/#informational-signs","text":"Orienting oneself (Passini 1994) Providing information on objects Aid in object identification and classification of objects","title":"Informational signs"},{"location":"3-semester/DEB/06-perception-and-navigation/#directional-signs","text":"Choosing the correct route (Passini, 1994) Providing information on routes and route hierarchies Apple Guidelines: Provide one path for one destination Make the path logical We need to provide a clear visual hierarchy","title":"Directional signs"},{"location":"3-semester/DEB/06-perception-and-navigation/#warning-and-reassurance-signs","text":"Monitoring chosen route (Passini, 1994) Recognizing when the destination is reached (Passini, 1994) Providing feedback on actual location Providing information on possible actions within the environment Apple Guidelines: Provide markers of where you are Traceability","title":"Warning and reassurance signs"},{"location":"3-semester/DEB/07-evaluation/","text":"Evaluation - What is Usability and Usability Testing \u00b6 Literature: Benyon - Designing Interactive Systems (chap. 4 section 4.3 + chap. 10) Rubin & Chisnell - Handbook of Usability Testing (chap. 3) (PDF) Rubin & Chisnell - Handbook of Usability Testing (chap. 5) (PDF) Nielsen & Molich - Heuristic Evaluation of User Interfaces ( PDF ) Nielsen and Molic's heuristics ( PDF ) Usability \u00b6 ISO 9241 definition: \u00b6 \"The effectiveness, efficiency and satisfaction with which specified users achieve specified goals in particular envirionments\" Effectiveness : The accuracyand completeness with which specified users can achieve specified goals in particular envirionments. \u200b How well the users can complete their tasks. Efficiency : The resource expended in relation to the accuracy and completeness of goals achieved. \u200b How much eg. time needed to solve their tasks. Satisfaction: The comfort and acceptability of the work system to its users and other people affected by its use. \u200b Subjective measure. Jacob Nielsen definition \u00b6 Learnability: How easy is it for users to accomplish basic tasks the first time they encounter the design? Efficiency : Once users have learned the design, how quickly can they perform the tasks? Memorability : When users return to the design after a period of not using it, how easily can they reestablish proficiency? Errors: How many errors do users make, how severe are these errors, and how easily can they recover from the errors? Satisfaction : How pleasant is it to use the design? Usability Testing \u00b6 Purpose Identifying usability problems in a system Starting point for refinements of design. Outcome A ranked list of usability problems Knowledge about what works well How do we evaluate usability? Inquiry We try to understand users. Also part of doing PACT. Testing Users test product designs. Inspection Testing of a design by an expert. When to test? \u00b6 Lab vs. Field Test \u00b6 Lab Test \u00b6 Strengths \u00b6 The least obtrusive way to collect data Allows communication \"behind the scenes\" Allows many observers High replicability and control Demand characteristics Weaknesses \u00b6 Somewhat \" sterile \" environment Test participants may feel like \" lab monkeys \" Questionable realism (ecological validity) Testing \u00b6 Representative users interact with design. Task solving and/or \" thinking-aloud \". Produces a ranked list of usability problems. Pros Identifies problems very precisely Gives first-hand insight into use Cons Test situation can be unnatural Difficult and very time consuming Testing Process (Participant perspective) \u00b6 Testing Process (Our Perspective) \u00b6 Activities \u00b6 Planning \u00b6 Test Participants \u00b6 Representative for the user group Demographics Experience Number of test-subjects Generalizability Quantitative conclusions Statistics Deciding on the Tasks \u00b6 What are the basic tasks that representative users do with the system? Is the whole system part of the evaluation? Can we create a crystal clear task description? How long does it take to solve the tasks? Useful rules: Make the tasks realistic Make the tasks actionable Avoid clues and describing the steps Good Tasks : Represent real use of the system Describe the end result Motivate (why should they be solved?) Include relevant data (eg. names) Don't force the users to use their own logins for example. Group smaller sub-tasks together Typical bad tasks: Vague, unclear or general Provides too much help Contain jargon and unfamiliar terms Forces the user into a specified sequence Deciding What to Measure and How \u00b6 Are all the components of usability relevant? How will we collect data? What are we going to measure? Think aloud? Usability Metrics \u00b6 Objective metrics: Effectiveness How many tasks were completed Efficiency How fast were they completed Subjective (perceived) metrics: Interview data Questionnaires (for example SUS, USE questionnaires) Heuristic inspection \u00b6 Experts inspects a design using a checklist (heuristic) Scenarios + relevant tasks can structure process. Produces a ranked list of usability problems. Pros Quick and easy to conduct No users required 3-5 inspections finds 70% of all problems Cons High proportion of \"cosmetic\" problems \"False\" usability problems","title":"Evaluation - What is Usability and Usability Testing"},{"location":"3-semester/DEB/07-evaluation/#evaluation-what-is-usability-and-usability-testing","text":"Literature: Benyon - Designing Interactive Systems (chap. 4 section 4.3 + chap. 10) Rubin & Chisnell - Handbook of Usability Testing (chap. 3) (PDF) Rubin & Chisnell - Handbook of Usability Testing (chap. 5) (PDF) Nielsen & Molich - Heuristic Evaluation of User Interfaces ( PDF ) Nielsen and Molic's heuristics ( PDF )","title":"Evaluation - What is Usability and Usability Testing"},{"location":"3-semester/DEB/07-evaluation/#usability","text":"","title":"Usability"},{"location":"3-semester/DEB/07-evaluation/#iso-9241-definition","text":"\"The effectiveness, efficiency and satisfaction with which specified users achieve specified goals in particular envirionments\" Effectiveness : The accuracyand completeness with which specified users can achieve specified goals in particular envirionments. \u200b How well the users can complete their tasks. Efficiency : The resource expended in relation to the accuracy and completeness of goals achieved. \u200b How much eg. time needed to solve their tasks. Satisfaction: The comfort and acceptability of the work system to its users and other people affected by its use. \u200b Subjective measure.","title":"ISO 9241 definition:"},{"location":"3-semester/DEB/07-evaluation/#jacob-nielsen-definition","text":"Learnability: How easy is it for users to accomplish basic tasks the first time they encounter the design? Efficiency : Once users have learned the design, how quickly can they perform the tasks? Memorability : When users return to the design after a period of not using it, how easily can they reestablish proficiency? Errors: How many errors do users make, how severe are these errors, and how easily can they recover from the errors? Satisfaction : How pleasant is it to use the design?","title":"Jacob Nielsen definition"},{"location":"3-semester/DEB/07-evaluation/#usability-testing","text":"Purpose Identifying usability problems in a system Starting point for refinements of design. Outcome A ranked list of usability problems Knowledge about what works well How do we evaluate usability? Inquiry We try to understand users. Also part of doing PACT. Testing Users test product designs. Inspection Testing of a design by an expert.","title":"Usability Testing"},{"location":"3-semester/DEB/07-evaluation/#when-to-test","text":"","title":"When to test?"},{"location":"3-semester/DEB/07-evaluation/#lab-vs-field-test","text":"","title":"Lab vs. Field Test"},{"location":"3-semester/DEB/07-evaluation/#lab-test","text":"","title":"Lab Test"},{"location":"3-semester/DEB/07-evaluation/#strengths","text":"The least obtrusive way to collect data Allows communication \"behind the scenes\" Allows many observers High replicability and control Demand characteristics","title":"Strengths"},{"location":"3-semester/DEB/07-evaluation/#weaknesses","text":"Somewhat \" sterile \" environment Test participants may feel like \" lab monkeys \" Questionable realism (ecological validity)","title":"Weaknesses"},{"location":"3-semester/DEB/07-evaluation/#testing","text":"Representative users interact with design. Task solving and/or \" thinking-aloud \". Produces a ranked list of usability problems. Pros Identifies problems very precisely Gives first-hand insight into use Cons Test situation can be unnatural Difficult and very time consuming","title":"Testing"},{"location":"3-semester/DEB/07-evaluation/#testing-process-participant-perspective","text":"","title":"Testing Process (Participant perspective)"},{"location":"3-semester/DEB/07-evaluation/#testing-process-our-perspective","text":"","title":"Testing Process (Our Perspective)"},{"location":"3-semester/DEB/07-evaluation/#activities","text":"","title":"Activities"},{"location":"3-semester/DEB/07-evaluation/#planning","text":"","title":"Planning"},{"location":"3-semester/DEB/07-evaluation/#test-participants","text":"Representative for the user group Demographics Experience Number of test-subjects Generalizability Quantitative conclusions Statistics","title":"Test Participants"},{"location":"3-semester/DEB/07-evaluation/#deciding-on-the-tasks","text":"What are the basic tasks that representative users do with the system? Is the whole system part of the evaluation? Can we create a crystal clear task description? How long does it take to solve the tasks? Useful rules: Make the tasks realistic Make the tasks actionable Avoid clues and describing the steps Good Tasks : Represent real use of the system Describe the end result Motivate (why should they be solved?) Include relevant data (eg. names) Don't force the users to use their own logins for example. Group smaller sub-tasks together Typical bad tasks: Vague, unclear or general Provides too much help Contain jargon and unfamiliar terms Forces the user into a specified sequence","title":"Deciding on the Tasks"},{"location":"3-semester/DEB/07-evaluation/#deciding-what-to-measure-and-how","text":"Are all the components of usability relevant? How will we collect data? What are we going to measure? Think aloud?","title":"Deciding What to Measure and How"},{"location":"3-semester/DEB/07-evaluation/#usability-metrics","text":"Objective metrics: Effectiveness How many tasks were completed Efficiency How fast were they completed Subjective (perceived) metrics: Interview data Questionnaires (for example SUS, USE questionnaires)","title":"Usability Metrics"},{"location":"3-semester/DEB/07-evaluation/#heuristic-inspection","text":"Experts inspects a design using a checklist (heuristic) Scenarios + relevant tasks can structure process. Produces a ranked list of usability problems. Pros Quick and easy to conduct No users required 3-5 inspections finds 70% of all problems Cons High proportion of \"cosmetic\" problems \"False\" usability problems","title":"Heuristic inspection"},{"location":"3-semester/DEB/08-evaluation-identifying-problems/","text":"After usability test \u00b6 A usability test has been conducted, what to do now: Transcription of log files \u00b6 Have to transcribe what the user does. Catches the user interaction in textual form. Provides an overview and shows problems with Can be done live, by a test logger, and then finished afterwards by looking at videos. Above shows an example of a transcript. First column shows a timestamp, middle shows the transcription, and right column shows the exact problem, interpreted by who ever writes the transcript. No interpretation in the transcript Data summary \u00b6 Main outcome is list of problems, but also want a summary of user performance. Information like how long time tasks generally took, if all tasks were solved, how much stress etc, how many problems in each category, how many unique problems (unique problem is only experienced by a single user. Need to decide if they should be dealt with, as only one user encounters them) Data analysis \u00b6 What is a usability problem \u00b6 When talking user-based evaluation, a usability problem is a problem experienced by a specific user while interacting with a specific system. Can be observed if: Not always clear why the problem is. Often clear what the problem is, but not why the user has the problem Problematic behavior from test moderator \u00b6 Don't tell the user to not click something, as they can get nervous about breaking the test Don't take control over mouse to remove notification Support rather than control. Remember to wait before taking control (Lecturer thinks non-condescending tone is very important, has two examples where the moderator doesn't sound condescending, but rather makes normal conversation with the test user. Just something to be aware of, if a question about speaking tone appears) Describing usability problems \u00b6 The problem list is the primary outcome of a evaluation. It is: Produced by looking at the transcript. Example below shows a problem list, a typical layout Might need more detail, if test done for others. Can be done providing extra document with extra details, describing the problems in more detail. Sometimes extra details can be for serious problems Categorizing problems \u00b6 Users may experience different level of severity. When making the problem list, use most critical level Critical \u00b6 The user is unable to continue Feels the system behaves strongly irritating Critical difference between believed and actual state of system Critical (Added by Nielsen) \u00b6 More than one user experiences one critical problem, independently of each other Serious \u00b6 Cosmetic \u00b6 Image above shows schema a decide on a problem category. Again, pick the most severe/critical category. Not set in stone, but guidelines Documentation (report) \u00b6 (This is an entry in the lecture, but not talked about, other than mentioned at the start)","title":"Evaluation - Identifying Problems"},{"location":"3-semester/DEB/08-evaluation-identifying-problems/#after-usability-test","text":"A usability test has been conducted, what to do now:","title":"After usability test"},{"location":"3-semester/DEB/08-evaluation-identifying-problems/#transcription-of-log-files","text":"Have to transcribe what the user does. Catches the user interaction in textual form. Provides an overview and shows problems with Can be done live, by a test logger, and then finished afterwards by looking at videos. Above shows an example of a transcript. First column shows a timestamp, middle shows the transcription, and right column shows the exact problem, interpreted by who ever writes the transcript. No interpretation in the transcript","title":"Transcription of log files"},{"location":"3-semester/DEB/08-evaluation-identifying-problems/#data-summary","text":"Main outcome is list of problems, but also want a summary of user performance. Information like how long time tasks generally took, if all tasks were solved, how much stress etc, how many problems in each category, how many unique problems (unique problem is only experienced by a single user. Need to decide if they should be dealt with, as only one user encounters them)","title":"Data summary"},{"location":"3-semester/DEB/08-evaluation-identifying-problems/#data-analysis","text":"","title":"Data analysis"},{"location":"3-semester/DEB/08-evaluation-identifying-problems/#what-is-a-usability-problem","text":"When talking user-based evaluation, a usability problem is a problem experienced by a specific user while interacting with a specific system. Can be observed if: Not always clear why the problem is. Often clear what the problem is, but not why the user has the problem","title":"What is a usability problem"},{"location":"3-semester/DEB/08-evaluation-identifying-problems/#problematic-behavior-from-test-moderator","text":"Don't tell the user to not click something, as they can get nervous about breaking the test Don't take control over mouse to remove notification Support rather than control. Remember to wait before taking control (Lecturer thinks non-condescending tone is very important, has two examples where the moderator doesn't sound condescending, but rather makes normal conversation with the test user. Just something to be aware of, if a question about speaking tone appears)","title":"Problematic behavior from test moderator"},{"location":"3-semester/DEB/08-evaluation-identifying-problems/#describing-usability-problems","text":"The problem list is the primary outcome of a evaluation. It is: Produced by looking at the transcript. Example below shows a problem list, a typical layout Might need more detail, if test done for others. Can be done providing extra document with extra details, describing the problems in more detail. Sometimes extra details can be for serious problems","title":"Describing usability problems"},{"location":"3-semester/DEB/08-evaluation-identifying-problems/#categorizing-problems","text":"Users may experience different level of severity. When making the problem list, use most critical level","title":"Categorizing problems"},{"location":"3-semester/DEB/08-evaluation-identifying-problems/#critical","text":"The user is unable to continue Feels the system behaves strongly irritating Critical difference between believed and actual state of system","title":"Critical"},{"location":"3-semester/DEB/08-evaluation-identifying-problems/#critical-added-by-nielsen","text":"More than one user experiences one critical problem, independently of each other","title":"Critical (Added by Nielsen)"},{"location":"3-semester/DEB/08-evaluation-identifying-problems/#serious","text":"","title":"Serious"},{"location":"3-semester/DEB/08-evaluation-identifying-problems/#cosmetic","text":"Image above shows schema a decide on a problem category. Again, pick the most severe/critical category. Not set in stone, but guidelines","title":"Cosmetic"},{"location":"3-semester/DEB/08-evaluation-identifying-problems/#documentation-report","text":"(This is an entry in the lecture, but not talked about, other than mentioned at the start)","title":"Documentation (report)"},{"location":"3-semester/DEB/09a-user-experience/","text":"User Experience \u00b6 Literature: Bargas-Avila and Hornb\u00e6k - Old wine in new bottles or novel challenges? (PDF) Kjeldskov et al. - Instant Data Analysis (PDF) Recall the definition of Usability in Lecture 07. User Experience: From ISO 9241-210 A person's perceptions and responses that result from the use or anticipated use of a product, system or service... This is a broader term than usability. Emotion is a key aspect! How is UX data collected? \u00b6 AttrakDiff2 \u00b6 From Marc Hassenzahl et al. Lavie and Tractinsky \u00b6 Purely aesthetics. Pitfalls? \u00b6 What does the participant remember most from the interaction? If he had a bad interaction in the end, is this what is reflected in the questionnaire? Or is it the mean of the whole interaction? Emotions \u00b6 What are emotions? James-Lange (1884): \"... the result of physical changes in autonomic and motor functions\" Sensory input creates bodily responses Awareness of bodily changes constitutes an emotion. Scherer (2005): Mobilization and synchronization of organismic subsystems A response to a cognitive evaluation of stimulus events Events are \"of major concerns for the organism\" Moods/Attitudes vs. Emotions (Scherer, 2005) Emotions are intense and short-lived (connected to specific events) Moods are of low intensity and may last for days 6 Basic Emotions Emotion Graph How can we measure emotions? \u00b6 Self-Assessment-Manikin (SAM) \u00b6 Studies show that when we evaluate emotions in retrospect, the major movements in emotions will dominate. If one really cool thing makes us happy, this will dominate the evaluation Galvanic Skin Response (GSR) \u00b6 Used during the interactions Electromyography (EMG) \u00b6 Facereader Software \u00b6 Cued Recall Debrief \u00b6 Use the graph data to pick points of interests. Show the user video clips of the POI's Ask them to reflect on what happens","title":"Evaluation - Alternative methods and UX"},{"location":"3-semester/DEB/09a-user-experience/#user-experience","text":"Literature: Bargas-Avila and Hornb\u00e6k - Old wine in new bottles or novel challenges? (PDF) Kjeldskov et al. - Instant Data Analysis (PDF) Recall the definition of Usability in Lecture 07. User Experience: From ISO 9241-210 A person's perceptions and responses that result from the use or anticipated use of a product, system or service... This is a broader term than usability. Emotion is a key aspect!","title":"User Experience"},{"location":"3-semester/DEB/09a-user-experience/#how-is-ux-data-collected","text":"","title":"How is UX data collected?"},{"location":"3-semester/DEB/09a-user-experience/#attrakdiff2","text":"From Marc Hassenzahl et al.","title":"AttrakDiff2"},{"location":"3-semester/DEB/09a-user-experience/#lavie-and-tractinsky","text":"Purely aesthetics.","title":"Lavie and Tractinsky"},{"location":"3-semester/DEB/09a-user-experience/#pitfalls","text":"What does the participant remember most from the interaction? If he had a bad interaction in the end, is this what is reflected in the questionnaire? Or is it the mean of the whole interaction?","title":"Pitfalls?"},{"location":"3-semester/DEB/09a-user-experience/#emotions","text":"What are emotions? James-Lange (1884): \"... the result of physical changes in autonomic and motor functions\" Sensory input creates bodily responses Awareness of bodily changes constitutes an emotion. Scherer (2005): Mobilization and synchronization of organismic subsystems A response to a cognitive evaluation of stimulus events Events are \"of major concerns for the organism\" Moods/Attitudes vs. Emotions (Scherer, 2005) Emotions are intense and short-lived (connected to specific events) Moods are of low intensity and may last for days 6 Basic Emotions Emotion Graph","title":"Emotions"},{"location":"3-semester/DEB/09a-user-experience/#how-can-we-measure-emotions","text":"","title":"How can we measure emotions?"},{"location":"3-semester/DEB/09a-user-experience/#self-assessment-manikin-sam","text":"Studies show that when we evaluate emotions in retrospect, the major movements in emotions will dominate. If one really cool thing makes us happy, this will dominate the evaluation","title":"Self-Assessment-Manikin (SAM)"},{"location":"3-semester/DEB/09a-user-experience/#galvanic-skin-response-gsr","text":"Used during the interactions","title":"Galvanic Skin Response (GSR)"},{"location":"3-semester/DEB/09a-user-experience/#electromyography-emg","text":"","title":"Electromyography (EMG)"},{"location":"3-semester/DEB/09a-user-experience/#facereader-software","text":"","title":"Facereader Software"},{"location":"3-semester/DEB/09a-user-experience/#cued-recall-debrief","text":"Use the graph data to pick points of interests. Show the user video clips of the POI's Ask them to reflect on what happens","title":"Cued Recall Debrief"},{"location":"3-semester/DEB/09b-alternative-methods-for-testing-usability/","text":"Alternative Methods for Testing Usability \u00b6 Instant Data Analysis \u00b6 Main Difference: You do not analyze video data! Analysis done immediately after the last session Brainstorm Tasks Logger's notes Presence during IDA: Test moderator Logger Facilitator Test moderator and data logger starts by brainstorming all the usability problems that they can remember from the sessions. The facilitator notes this down, and creates an overview of the problems. So that the moderator and logger can focus only on remembering the problems. Comparison \u00b6 From the study (Kjeldskov et al. - Instant Data Analysis (PDF) ) Overlap \u00b6 White box is an unidentified problem, Black box is an identified problem. Time Spent \u00b6 IDA: 4 person hours VBA: 40 person hours Remote Usability testing \u00b6 Remote Asynchronous \u00b6 Logging/Analytics You know what users do, but not why. Usability Questionnaire Usability Problems \u00b6 Definition: Usability problems are present when the system is: Unuseful: You cannot find the documents or functions you wneed to solve your tasks. Difficult to learn : It takes a long time to learn how to use the system. Difficult to remember : It takes a long time to find elements in the system, which you have used previously. Ineffective to use: It takes a long time to solve certain tasks with the system. Unsatisfying to use: The system does not feel comfortable to use, it is not joyful to work with it. This is basically an inverted edition of Jacob Nielsen's definition from lecture 07 .","title":"Evaluation - Alternative methods and UX"},{"location":"3-semester/DEB/09b-alternative-methods-for-testing-usability/#alternative-methods-for-testing-usability","text":"","title":"Alternative Methods for Testing Usability"},{"location":"3-semester/DEB/09b-alternative-methods-for-testing-usability/#instant-data-analysis","text":"Main Difference: You do not analyze video data! Analysis done immediately after the last session Brainstorm Tasks Logger's notes Presence during IDA: Test moderator Logger Facilitator Test moderator and data logger starts by brainstorming all the usability problems that they can remember from the sessions. The facilitator notes this down, and creates an overview of the problems. So that the moderator and logger can focus only on remembering the problems.","title":"Instant Data Analysis"},{"location":"3-semester/DEB/09b-alternative-methods-for-testing-usability/#comparison","text":"From the study (Kjeldskov et al. - Instant Data Analysis (PDF) )","title":"Comparison"},{"location":"3-semester/DEB/09b-alternative-methods-for-testing-usability/#overlap","text":"White box is an unidentified problem, Black box is an identified problem.","title":"Overlap"},{"location":"3-semester/DEB/09b-alternative-methods-for-testing-usability/#time-spent","text":"IDA: 4 person hours VBA: 40 person hours","title":"Time Spent"},{"location":"3-semester/DEB/09b-alternative-methods-for-testing-usability/#remote-usability-testing","text":"","title":"Remote Usability testing"},{"location":"3-semester/DEB/09b-alternative-methods-for-testing-usability/#remote-asynchronous","text":"Logging/Analytics You know what users do, but not why. Usability Questionnaire","title":"Remote Asynchronous"},{"location":"3-semester/DEB/09b-alternative-methods-for-testing-usability/#usability-problems","text":"Definition: Usability problems are present when the system is: Unuseful: You cannot find the documents or functions you wneed to solve your tasks. Difficult to learn : It takes a long time to learn how to use the system. Difficult to remember : It takes a long time to find elements in the system, which you have used previously. Ineffective to use: It takes a long time to solve certain tasks with the system. Unsatisfying to use: The system does not feel comfortable to use, it is not joyful to work with it. This is basically an inverted edition of Jacob Nielsen's definition from lecture 07 .","title":"Usability Problems"},{"location":"3-semester/SU/","text":"SU - SYSTEMS DEVELOPMENT \u00b6 Moodle side: https://www.moodle.aau.dk/course/view.php?id=27347","title":"Course"},{"location":"3-semester/SU/#su-systems-development","text":"Moodle side: https://www.moodle.aau.dk/course/view.php?id=27347","title":"SU - SYSTEMS DEVELOPMENT"},{"location":"3-semester/SU/1-classes-and-objects/","text":"Classes and Objects \u00b6 Class: A description of a collection of objects sharing: structure, behavioral pattern, and attributes. Object: An entity with identity, state, and behavior. Relation: An object is created from the description of a class. An object belongs to a class (from which it was created). Each class contains a set of objects; we refer to them as the objects of the class.","title":"Classes and Objects"},{"location":"3-semester/SU/1-classes-and-objects/#classes-and-objects","text":"Class: A description of a collection of objects sharing: structure, behavioral pattern, and attributes. Object: An entity with identity, state, and behavior. Relation: An object is created from the description of a class. An object belongs to a class (from which it was created). Each class contains a set of objects; we refer to them as the objects of the class.","title":"Classes and Objects"},{"location":"3-semester/SU/2-factor/","text":"System Definition \u00b6 FACTOR \u00b6 See page 40-41 in OOAD F - Functionality \u00b6 The system functions that support the application-domain tasks. A - Application domain \u00b6 Those parts of an organization that administrate, monitor, or control a problem domain. C - Conditions \u00b6 The conditions under which the system will be developed and used. T - Technology \u00b6 Both the technology used to develop the system and the technology on which the system will run. O - Objects \u00b6 The main objects in the problem domain. R - Responsibility \u00b6 The system's overall responsibility in relation to its context. Wiums \u00b6 System Definition \u00b6 After analyzing the case study at Wiums Renseri, the problem with the current system is order management being done manually by hand. A modern system, that keeps track of order information, could reduce the number of human errors and potentially increase productivity. Modelling the company\u2019s desired situation and looking at existing solution had confirmed the possible solution being a computerized order management system. This results in the following FACTOR criterion. FACTOR \u00b6 Functionality : Hold information about orders, customers and products. The system should be able to print tags for tracking orders. Assist the employees in administrating customers and payments. Support managers by generating sales statistics, and facilitate payments. Application domain : Employees such as managers, the receptionist and the people in the production are all part of the application domain, because they either monitor, administrate or control orders, by using the system. The credit card terminal will also interact with the system, by receiving transaction amount and confirming if a transaction is accepted. Therefore, it is also a part of the application domain. Conditions : The system should be developed in close cooperation with Wiums Renseri, to understand the appropriate steps of the process. The employees should have knowledge about the processes in Wiums Renseri, in order to understand the systems sequential steps. Technology : The system will be based on a modern computer with a monitor. A receipt printer and waterproof label printer should be supported by the system, as to make it possible to print receipts and tags for the textiles. Furthermore, scanners will be needed to scan the tags. A credit card terminal should help facilitate payments. Objects : The most important objects are products, orders and customers. Responsibility : Tool that assist in order administration, by maintaining information about products, orders and customers. Notifies customers of order completion, and passing relevant information to e-conomic. Definition \u00b6 Using the FACTOR model the following system definition is made: A computerized system used to assist in order management, by allowing the employees to monitor and register new orders and keep track of payments and pricing of products, as well as tracking individual orders at certain stages of service. The system should also be able to print notes, tags and receipts for these orders, as well as handle customer notifications of completed orders and support integration with e-conomic. The system should primarily serve as a tool to improve the order management process, by maintaining information about orders, products and customers. Secondarily, it should serve as a tool to automate filling out invoices in e-conomic for business customers, as well as generating statistics from all sales. The system should be based on regular modern PCs and monitors, a receipt printer, a waterproof label printer, a credit card terminal and several scanners for the tags. The system should be developed in close cooperation with Wiums Renseri and it should be assumed that employees are experienced in the processes at Wiums Renseri.","title":"FACTOR"},{"location":"3-semester/SU/2-factor/#system-definition","text":"","title":"System Definition"},{"location":"3-semester/SU/2-factor/#factor","text":"See page 40-41 in OOAD","title":"FACTOR"},{"location":"3-semester/SU/2-factor/#f-functionality","text":"The system functions that support the application-domain tasks.","title":"F - Functionality"},{"location":"3-semester/SU/2-factor/#a-application-domain","text":"Those parts of an organization that administrate, monitor, or control a problem domain.","title":"A - Application domain"},{"location":"3-semester/SU/2-factor/#c-conditions","text":"The conditions under which the system will be developed and used.","title":"C - Conditions"},{"location":"3-semester/SU/2-factor/#t-technology","text":"Both the technology used to develop the system and the technology on which the system will run.","title":"T - Technology"},{"location":"3-semester/SU/2-factor/#o-objects","text":"The main objects in the problem domain.","title":"O - Objects"},{"location":"3-semester/SU/2-factor/#r-responsibility","text":"The system's overall responsibility in relation to its context.","title":"R - Responsibility"},{"location":"3-semester/SU/2-factor/#wiums","text":"","title":"Wiums"},{"location":"3-semester/SU/2-factor/#system-definition_1","text":"After analyzing the case study at Wiums Renseri, the problem with the current system is order management being done manually by hand. A modern system, that keeps track of order information, could reduce the number of human errors and potentially increase productivity. Modelling the company\u2019s desired situation and looking at existing solution had confirmed the possible solution being a computerized order management system. This results in the following FACTOR criterion.","title":"System Definition"},{"location":"3-semester/SU/2-factor/#factor_1","text":"Functionality : Hold information about orders, customers and products. The system should be able to print tags for tracking orders. Assist the employees in administrating customers and payments. Support managers by generating sales statistics, and facilitate payments. Application domain : Employees such as managers, the receptionist and the people in the production are all part of the application domain, because they either monitor, administrate or control orders, by using the system. The credit card terminal will also interact with the system, by receiving transaction amount and confirming if a transaction is accepted. Therefore, it is also a part of the application domain. Conditions : The system should be developed in close cooperation with Wiums Renseri, to understand the appropriate steps of the process. The employees should have knowledge about the processes in Wiums Renseri, in order to understand the systems sequential steps. Technology : The system will be based on a modern computer with a monitor. A receipt printer and waterproof label printer should be supported by the system, as to make it possible to print receipts and tags for the textiles. Furthermore, scanners will be needed to scan the tags. A credit card terminal should help facilitate payments. Objects : The most important objects are products, orders and customers. Responsibility : Tool that assist in order administration, by maintaining information about products, orders and customers. Notifies customers of order completion, and passing relevant information to e-conomic.","title":"FACTOR"},{"location":"3-semester/SU/2-factor/#definition","text":"Using the FACTOR model the following system definition is made: A computerized system used to assist in order management, by allowing the employees to monitor and register new orders and keep track of payments and pricing of products, as well as tracking individual orders at certain stages of service. The system should also be able to print notes, tags and receipts for these orders, as well as handle customer notifications of completed orders and support integration with e-conomic. The system should primarily serve as a tool to improve the order management process, by maintaining information about orders, products and customers. Secondarily, it should serve as a tool to automate filling out invoices in e-conomic for business customers, as well as generating statistics from all sales. The system should be based on regular modern PCs and monitors, a receipt printer, a waterproof label printer, a credit card terminal and several scanners for the tags. The system should be developed in close cooperation with Wiums Renseri and it should be assumed that employees are experienced in the processes at Wiums Renseri.","title":"Definition"},{"location":"3-semester/SU/3-structure/","text":"Structure \u00b6 A class with no objects is called abstract class Structures between classes \u00b6 Generalization Structure \u00b6 Generalization : A general class (the super class) describes properties common to a group of specialized classes (the subclasses) Specialized Classes are subclasses Generalized classes are superclasses Example \u00b6 The classes \u201cTaxi\u201d and \u201cPrivate Car\u201d might be specializations of the general class \u201cPassenger Car\u201d which might be a specialization of the general class \u201cVehicle\u201d \u200b Think programming! Cluster Structure \u00b6 Cluster : Collection of related classes Example \u00b6 From an automobile register Structures between objects \u00b6 Two types: aggregation and association . Aggregation Structure \u00b6 Aggregation : A superior object (the whole) consists of a number of inferior objects (the parts) See the picture below, \u201cCar\u201d consists of \u201cEngine\u201d which consists of \u201cCylinder\u201d An Engine consists of at least 2 Cylinder 's, this is marked by the 2...* 2...* and is called multiplicity Association Structure \u00b6 Association : A meaningful relation between a number of objects For example, maybe a car is owned by one or more people, and a Person owns 0 or more cars. It It makes no sense to say that a person contains a car. We can name the association, for example call this line \u201cownership\u201d (This is often because we a missing a class though) Find candidates for structure \u00b6 Identify Generalizations \u00b6 First approach; we take every pair and determine if one of the two is a generalization of the other Second approach; we determine if a relevant generalization exists for pairs of selected classes. Third approach; we take each of the selected classes and attempt to define a relevant generalization or specialization. Identify Aggregations \u00b6 To find candidates for aggregation, systematically examine selected classes individually in pairs. There are three typical applications of aggregation structure: Whole-Part ; the whole is the sum of the parts Container-Content ; the whole is a container for the parts Union-Member ; the whole is an organized union of members The whole is considered to be superior to its parts. (Vertical placement in the class diagram) Identify Associations \u00b6 Look at remaining class pairs to see if the can be meaningfully related. Identify Clusters \u00b6 To increase the diagrams clarity Can use other structures as a starting point for generating clusters. It is NOT allowed to place a class in 2 different clusters!","title":"Structure"},{"location":"3-semester/SU/3-structure/#structure","text":"A class with no objects is called abstract class","title":"Structure"},{"location":"3-semester/SU/3-structure/#structures-between-classes","text":"","title":"Structures between classes"},{"location":"3-semester/SU/3-structure/#generalization-structure","text":"Generalization : A general class (the super class) describes properties common to a group of specialized classes (the subclasses) Specialized Classes are subclasses Generalized classes are superclasses","title":"Generalization Structure"},{"location":"3-semester/SU/3-structure/#example","text":"The classes \u201cTaxi\u201d and \u201cPrivate Car\u201d might be specializations of the general class \u201cPassenger Car\u201d which might be a specialization of the general class \u201cVehicle\u201d \u200b Think programming!","title":"Example"},{"location":"3-semester/SU/3-structure/#cluster-structure","text":"Cluster : Collection of related classes","title":"Cluster Structure"},{"location":"3-semester/SU/3-structure/#example_1","text":"From an automobile register","title":"Example"},{"location":"3-semester/SU/3-structure/#structures-between-objects","text":"Two types: aggregation and association .","title":"Structures between objects"},{"location":"3-semester/SU/3-structure/#aggregation-structure","text":"Aggregation : A superior object (the whole) consists of a number of inferior objects (the parts) See the picture below, \u201cCar\u201d consists of \u201cEngine\u201d which consists of \u201cCylinder\u201d An Engine consists of at least 2 Cylinder 's, this is marked by the 2...* 2...* and is called multiplicity","title":"Aggregation Structure"},{"location":"3-semester/SU/3-structure/#association-structure","text":"Association : A meaningful relation between a number of objects For example, maybe a car is owned by one or more people, and a Person owns 0 or more cars. It It makes no sense to say that a person contains a car. We can name the association, for example call this line \u201cownership\u201d (This is often because we a missing a class though)","title":"Association Structure"},{"location":"3-semester/SU/3-structure/#find-candidates-for-structure","text":"","title":"Find candidates for structure"},{"location":"3-semester/SU/3-structure/#identify-generalizations","text":"First approach; we take every pair and determine if one of the two is a generalization of the other Second approach; we determine if a relevant generalization exists for pairs of selected classes. Third approach; we take each of the selected classes and attempt to define a relevant generalization or specialization.","title":"Identify Generalizations"},{"location":"3-semester/SU/3-structure/#identify-aggregations","text":"To find candidates for aggregation, systematically examine selected classes individually in pairs. There are three typical applications of aggregation structure: Whole-Part ; the whole is the sum of the parts Container-Content ; the whole is a container for the parts Union-Member ; the whole is an organized union of members The whole is considered to be superior to its parts. (Vertical placement in the class diagram)","title":"Identify Aggregations"},{"location":"3-semester/SU/3-structure/#identify-associations","text":"Look at remaining class pairs to see if the can be meaningfully related.","title":"Identify Associations"},{"location":"3-semester/SU/3-structure/#identify-clusters","text":"To increase the diagrams clarity Can use other structures as a starting point for generating clusters. It is NOT allowed to place a class in 2 different clusters!","title":"Identify Clusters"},{"location":"3-semester/SU/4-usage/","text":"Usage \u00b6 Actor: An abstraction of users or other systems that interact with the target system Use Case: A pattern for interaction between the system and actors in the application domain Involve the user, by presenting use cases through prototypes. To illustrate relations between actors and use-cases you can use either action table or use case diagrams . UML recommends use-case diagrams, while the book prefers actor table (because of space). Actor table example for automatic payment system: Find Actors and Use Cases \u00b6 Who will use the system? How will it be used? If several roles appear the same to the system, consider consolidating them into one. Actor Specifications \u00b6 The target system\u2019s actors described in specifications. Consists of 3 parts: goal, characteristics, and examples. Describes (as precisely as possible) the actor\u2019s role in relation to the target system. Example from automatic payment system Use Case Specification | Statechart Diagram \u00b6 Use case specification is more detailed than statechart diagram - Should not contain unnecessary details thought! Use case specification example from automatic payment system : Statechart diagram example from automatic payment system:","title":"Usage"},{"location":"3-semester/SU/4-usage/#usage","text":"Actor: An abstraction of users or other systems that interact with the target system Use Case: A pattern for interaction between the system and actors in the application domain Involve the user, by presenting use cases through prototypes. To illustrate relations between actors and use-cases you can use either action table or use case diagrams . UML recommends use-case diagrams, while the book prefers actor table (because of space). Actor table example for automatic payment system:","title":"Usage"},{"location":"3-semester/SU/4-usage/#find-actors-and-use-cases","text":"Who will use the system? How will it be used? If several roles appear the same to the system, consider consolidating them into one.","title":"Find Actors and Use Cases"},{"location":"3-semester/SU/4-usage/#actor-specifications","text":"The target system\u2019s actors described in specifications. Consists of 3 parts: goal, characteristics, and examples. Describes (as precisely as possible) the actor\u2019s role in relation to the target system. Example from automatic payment system","title":"Actor Specifications"},{"location":"3-semester/SU/4-usage/#use-case-specification-statechart-diagram","text":"Use case specification is more detailed than statechart diagram - Should not contain unnecessary details thought! Use case specification example from automatic payment system : Statechart diagram example from automatic payment system:","title":"Use Case Specification | Statechart Diagram"},{"location":"4-semester/","text":"Indhold \u00b6 Kurser: PSS - Principles of Operating Systems and Concurrency SPO - Languages and Compilers SS - Syntax og Semantik Moodle side: https://www.moodle.aau.dk/course/view.php?id=28797","title":"Index"},{"location":"4-semester/#indhold","text":"Kurser: PSS - Principles of Operating Systems and Concurrency SPO - Languages and Compilers SS - Syntax og Semantik Moodle side: https://www.moodle.aau.dk/course/view.php?id=28797","title":"Indhold"},{"location":"4-semester/PSS/","text":"PSS - PRINCIPLES OF OPERATING SYSTEMS AND CONCURRENCY \u00b6 Moodle side: https://www.moodle.aau.dk/course/view.php?id=28773","title":"Course"},{"location":"4-semester/PSS/#pss-principles-of-operating-systems-and-concurrency","text":"Moodle side: https://www.moodle.aau.dk/course/view.php?id=28773","title":"PSS - PRINCIPLES OF OPERATING SYSTEMS AND CONCURRENCY"},{"location":"4-semester/PSS/1-intro/","text":"Et program i hukommelsen \u00b6 Datalaget: Konstante variable, globale variable Heap Stack Code En process er et k\u00f8rende program med alt det tilh\u00f8rende data.","title":"Introduction to Operating Systems"},{"location":"4-semester/PSS/1-intro/#et-program-i-hukommelsen","text":"Datalaget: Konstante variable, globale variable Heap Stack Code En process er et k\u00f8rende program med alt det tilh\u00f8rende data.","title":"Et program i hukommelsen"},{"location":"4-semester/PSS/5-memory-management/","text":"Memory Management \u00b6 Address space : Running program's (abstract) view of memory","title":"Memory Management"},{"location":"4-semester/PSS/5-memory-management/#memory-management","text":"Address space : Running program's (abstract) view of memory","title":"Memory Management"},{"location":"4-semester/PSS/exam/","text":"PSS - Exam \u00b6 1 Processes and Threads 2 Scheduler 3 Memory Management 4 Paged Memory 5 Concurrency 6 Concurrency Problems 7 I/O, Device Drivers","title":"Exam"},{"location":"4-semester/PSS/exam/#pss-exam","text":"1 Processes and Threads 2 Scheduler 3 Memory Management 4 Paged Memory 5 Concurrency 6 Concurrency Problems 7 I/O, Device Drivers","title":"PSS - Exam"},{"location":"4-semester/PSS/exam/1-processes-and-threads/","text":"1 - Processes and Threads \u00b6 Keywords: Definition of process/thread, process-/thread-control block, 5-state process model, process creation, process image, process/thread switching, multithreading, implementation strategies for thread support, user-/kernel-mode. Litterature \u00b6 OSTEP Chapters 3, 4, 5 Learning Goals \u00b6 After lecture 1 you can: Define and explain the concept of a process \u200bExplain what a process image is. Explain what a process control block is, what it is used for and why it is needed. Explain in general terms, how process creation, switching and termination works Define and discuss process states Define and explain the concept of a thread Discuss when, where and how (multi-)threads are usefull Explain what a thread control block is, what it is used for and why it is needed Discuss implementation strategies for thread support and explain the associated trade-offs Noter \u00b6 Process \u00b6 En process er k\u00f8rende program med alt tilh\u00f8rende data. Et program er bare data p\u00e5 disken. Process Image \u00b6 Et process image er en samling af process relateret data. Det best\u00e5r af: Process control block (PCB) Program (code) Stack Heap (user data) Process Control Block (PCB) \u00b6 PCB er en datastruktur der indeholder data om en process. Indeholder ting som: Identifier State (tilstand) Prioritet Program Counter (PC) Memory counter I/O Status Context (gemte registre til hvis processen skal k\u00f8re igen) XV6 Implementering i xv6 kan findes i filen proc.h En PCB bruges til at kunne skifte mellem processer. Task listen, eller process tabellen i operativsystemet er en liste over PCB'er. Den har dermed al den information den skal bruge om de processer den skal skifte imellem. Process creation, switching and termination \u00b6 XV6 Implementering i xv6 kan findes i filen proc.c Creation \u00b6 N\u00e5r en process bliver oprettet, l\u00e6ses programmet og program data ind i memory. I simple operativsystemer, g\u00f8res dette eagerly , hvilket betyder at al data l\u00e6ses ind inden programmet starter. I mere moderne systemer g\u00f8res dette lazily , hvilket betyder at data l\u00e6ses ind n\u00e5r det skal bruges. Herefter allokeres memory til stakken ( runtime-stack ) og eventuelt heap . Stakken bruges i C til lokale variable, funktionsparametre, return adresser. Heapen bruges i C til explicit requested, dynamisk allokeret memory, som ved malloc() kald. Switching \u00b6 N\u00e5r en process bliver scheduled, skifter dens tilstand fra ready til running. F\u00f8rst bliver PCB for den k\u00f8rende process oprettet, og information lagres. Derefter udpakkes PCB for den skiftede til process. Registre s\u00e6ttes, dette kaldes context switch. Den nye process k\u00f8res, via den nyindstillede PC (program counter) Ekstra info https://medium.com/@ppan.brian/context-switch-from-xv6-aedcb1246cd Process state \u00b6 En process's tilstande bruges af scheduleren til at vide hvilke processer den kan k\u00f8re. OSTEP definerer f\u00f8lgende tilstande. Running: Processen k\u00f8rer lige nu p\u00e5 en processor, instruktioner udf\u00f8res. Ready: Processen er klar til at k\u00f8re, men af en eller anden grund, har OS valgt ikke at k\u00f8re den. Blocked: Processen er ikke klar til at k\u00f8re. Et eksempel kan v\u00e6re mens den venter p\u00e5 I/O. xv6 har tilmed: EMBROYO : den er oprettet, men ikke udfyldt med n\u00f8dvendig data endnu. ZOMBIE : den er termineret, men endnu ikke opryttet af OS endnu. (Bruges af f.eks. parent processes, til at se return code) 5-State Process Model \u00b6 Thread \u00b6 En tr\u00e5d ( thread ) er den mindste eksekverbare del af en process. En process kan have 1 eller flere tr\u00e5de. Multithreading \u00b6 Opsplitningen af en process i flere tr\u00e5de. Multithreading kan eksempelvis v\u00e6re nyttigt ved GUI. Nogle tr\u00e5de arbejder p\u00e5 hovedarbejdet (baggrunden) Nogle tr\u00e5de arbejder p\u00e5 at opdatere GUI, s\u00e5 den virker mere responsiv. Eksempel: Der trykkes p\u00e5 en opdateringsknap med der opdatere en liste med data fra internettet. Ved single-thread, vil GUI'en \"holde stille\" og vente p\u00e5 al data. Ved multithreading, kan man hente data i en anden tr\u00e5d, og dermed er GUI stadig brugbar imens. Low cost creation, switching and termination Interthread communication ( memory sharing ) Kan udnytte multi-processor/-core platforme. Skaber bedre arkitektur / design. Modularitet Thread-local data: thread-ID priority stack Process-local data: Address space, heap, open files process-ID parent, ownership, CPU reg (copy) Shared data: Program text Thread Control Block \u00b6 Ligesom process control block (PCB), men indeholder information om tr\u00e5den som thread-ID, som beskrevet ovenfor. Thread Support Implementation Strategies \u00b6 Amdahls Law \u00b6 Potientel performance speedup: $$ Speedup=\\frac{1}{(1-f)+\\frac{f}{N}} $$ hvor \u200b f f er \"parallelisable fraction\" (hvor meget execution time der bliver paralleliseret) \u200b N N er antallet af tilg\u00e6ngelige cores/CPUs","title":"1 - Processes and Threads"},{"location":"4-semester/PSS/exam/1-processes-and-threads/#1-processes-and-threads","text":"Keywords: Definition of process/thread, process-/thread-control block, 5-state process model, process creation, process image, process/thread switching, multithreading, implementation strategies for thread support, user-/kernel-mode.","title":"1 - Processes and Threads"},{"location":"4-semester/PSS/exam/1-processes-and-threads/#litterature","text":"OSTEP Chapters 3, 4, 5","title":"Litterature"},{"location":"4-semester/PSS/exam/1-processes-and-threads/#learning-goals","text":"After lecture 1 you can: Define and explain the concept of a process \u200bExplain what a process image is. Explain what a process control block is, what it is used for and why it is needed. Explain in general terms, how process creation, switching and termination works Define and discuss process states Define and explain the concept of a thread Discuss when, where and how (multi-)threads are usefull Explain what a thread control block is, what it is used for and why it is needed Discuss implementation strategies for thread support and explain the associated trade-offs","title":"Learning Goals"},{"location":"4-semester/PSS/exam/1-processes-and-threads/#noter","text":"","title":"Noter"},{"location":"4-semester/PSS/exam/1-processes-and-threads/#process","text":"En process er k\u00f8rende program med alt tilh\u00f8rende data. Et program er bare data p\u00e5 disken.","title":"Process"},{"location":"4-semester/PSS/exam/1-processes-and-threads/#process-image","text":"Et process image er en samling af process relateret data. Det best\u00e5r af: Process control block (PCB) Program (code) Stack Heap (user data)","title":"Process Image"},{"location":"4-semester/PSS/exam/1-processes-and-threads/#process-control-block-pcb","text":"PCB er en datastruktur der indeholder data om en process. Indeholder ting som: Identifier State (tilstand) Prioritet Program Counter (PC) Memory counter I/O Status Context (gemte registre til hvis processen skal k\u00f8re igen) XV6 Implementering i xv6 kan findes i filen proc.h En PCB bruges til at kunne skifte mellem processer. Task listen, eller process tabellen i operativsystemet er en liste over PCB'er. Den har dermed al den information den skal bruge om de processer den skal skifte imellem.","title":"Process Control Block (PCB)"},{"location":"4-semester/PSS/exam/1-processes-and-threads/#process-creation-switching-and-termination","text":"XV6 Implementering i xv6 kan findes i filen proc.c","title":"Process creation, switching and termination"},{"location":"4-semester/PSS/exam/1-processes-and-threads/#creation","text":"N\u00e5r en process bliver oprettet, l\u00e6ses programmet og program data ind i memory. I simple operativsystemer, g\u00f8res dette eagerly , hvilket betyder at al data l\u00e6ses ind inden programmet starter. I mere moderne systemer g\u00f8res dette lazily , hvilket betyder at data l\u00e6ses ind n\u00e5r det skal bruges. Herefter allokeres memory til stakken ( runtime-stack ) og eventuelt heap . Stakken bruges i C til lokale variable, funktionsparametre, return adresser. Heapen bruges i C til explicit requested, dynamisk allokeret memory, som ved malloc() kald.","title":"Creation"},{"location":"4-semester/PSS/exam/1-processes-and-threads/#switching","text":"N\u00e5r en process bliver scheduled, skifter dens tilstand fra ready til running. F\u00f8rst bliver PCB for den k\u00f8rende process oprettet, og information lagres. Derefter udpakkes PCB for den skiftede til process. Registre s\u00e6ttes, dette kaldes context switch. Den nye process k\u00f8res, via den nyindstillede PC (program counter) Ekstra info https://medium.com/@ppan.brian/context-switch-from-xv6-aedcb1246cd","title":"Switching"},{"location":"4-semester/PSS/exam/1-processes-and-threads/#process-state","text":"En process's tilstande bruges af scheduleren til at vide hvilke processer den kan k\u00f8re. OSTEP definerer f\u00f8lgende tilstande. Running: Processen k\u00f8rer lige nu p\u00e5 en processor, instruktioner udf\u00f8res. Ready: Processen er klar til at k\u00f8re, men af en eller anden grund, har OS valgt ikke at k\u00f8re den. Blocked: Processen er ikke klar til at k\u00f8re. Et eksempel kan v\u00e6re mens den venter p\u00e5 I/O. xv6 har tilmed: EMBROYO : den er oprettet, men ikke udfyldt med n\u00f8dvendig data endnu. ZOMBIE : den er termineret, men endnu ikke opryttet af OS endnu. (Bruges af f.eks. parent processes, til at se return code)","title":"Process state"},{"location":"4-semester/PSS/exam/1-processes-and-threads/#5-state-process-model","text":"","title":"5-State Process Model"},{"location":"4-semester/PSS/exam/1-processes-and-threads/#thread","text":"En tr\u00e5d ( thread ) er den mindste eksekverbare del af en process. En process kan have 1 eller flere tr\u00e5de.","title":"Thread"},{"location":"4-semester/PSS/exam/1-processes-and-threads/#multithreading","text":"Opsplitningen af en process i flere tr\u00e5de. Multithreading kan eksempelvis v\u00e6re nyttigt ved GUI. Nogle tr\u00e5de arbejder p\u00e5 hovedarbejdet (baggrunden) Nogle tr\u00e5de arbejder p\u00e5 at opdatere GUI, s\u00e5 den virker mere responsiv. Eksempel: Der trykkes p\u00e5 en opdateringsknap med der opdatere en liste med data fra internettet. Ved single-thread, vil GUI'en \"holde stille\" og vente p\u00e5 al data. Ved multithreading, kan man hente data i en anden tr\u00e5d, og dermed er GUI stadig brugbar imens. Low cost creation, switching and termination Interthread communication ( memory sharing ) Kan udnytte multi-processor/-core platforme. Skaber bedre arkitektur / design. Modularitet Thread-local data: thread-ID priority stack Process-local data: Address space, heap, open files process-ID parent, ownership, CPU reg (copy) Shared data: Program text","title":"Multithreading"},{"location":"4-semester/PSS/exam/1-processes-and-threads/#thread-control-block","text":"Ligesom process control block (PCB), men indeholder information om tr\u00e5den som thread-ID, som beskrevet ovenfor.","title":"Thread Control Block"},{"location":"4-semester/PSS/exam/1-processes-and-threads/#thread-support-implementation-strategies","text":"","title":"Thread Support Implementation Strategies"},{"location":"4-semester/PSS/exam/1-processes-and-threads/#amdahls-law","text":"Potientel performance speedup: $$ Speedup=\\frac{1}{(1-f)+\\frac{f}{N}} $$ hvor \u200b f f er \"parallelisable fraction\" (hvor meget execution time der bliver paralleliseret) \u200b N N er antallet af tilg\u00e6ngelige cores/CPUs","title":"Amdahls Law"},{"location":"4-semester/PSS/exam/2-scheduler/","text":"2 - Scheduling \u00b6 Keywords : Metrics for scheduling (turnaround time, response time), simple process model, scheduling policies (FIFO, SJF, STCF, Round Robin, MLFQ, lottery scheduler). Litterature \u00b6 OSTEP Chapters 6, 7, 8, 9 Learning Goals \u00b6 After lecture 2 you: Can explain the notion of limited direct execution and how it relates to scheduling Will know the simplified process model Will know and can explain important metrics for measuring a scheduling policy: Fairness Turnaround time Response time Can explain important scheduling policies and their pros and cons: FIFO (First In First Out) SJF (Shortest Job First) STCF (Shortest Time-to-Completion First) Round Robin MLFQ Lottery scheduling Noter \u00b6 Limited Direct Execution (LDE) \u00b6 The direct execution part: Run the program directly on the CPU Direct execution is fast, running directly on the CPU. How do allow a process to perform restricted operations without giving complete control? For at undg\u00e5 malicious processer, findes 2 modes. User - og kernel-mode . Restriktede operationer, kan kun laves i kernel mode. System kald , er kald som user-processer kan eksekvere, der kan eksekvere kode i kernel mode. For at eksekvere system kald, eksekvere programmer en trap instruktion. N\u00e5r systemet er f\u00e6rdig, eksekveres en return-from-trap instruktion, der returnerer til det kaldende bruger-program. Trap \u00b6 N\u00e5r et trap eksekveres, gemmes kalderens registrer, s\u00e5 den kan returnere igen. x86 pusher PC, flags og andre registre til en kernel stack . Som s\u00e5 bliver popped igen n\u00e5r return-from-trap eksekveres. Trap Table \u00b6 For at kernel ved hvad der m\u00e5 eksekveres under et trap, ops\u00e6ttes et trap-table under boot. Trap table fort\u00e6ller hardware hvilket kode der skal eksekveres under hvilke events ( trap handlers ). Et system-call number er assigned til hvert system kald. User programmet placerer dette nummer i et bestemt register, eller bestemt sted i stakken. OS tjekker dette nummers validitet og k\u00f8rer det tilsvarende kode. Denne indirekthed fungerer som besykttelse. xv6 System call numbers i xv6 kan ses i syscall.h . De gemmes i %eax. Og kaldes ved at eksekvere trap med T_SYSCALL . Switching Process (Scheduling) \u00b6 2 method: cooperative approach og non-cooperative . Cooperative Approach \u00b6 Processer st\u00e5r selv for at overgive kontrollen til OS. Eksempelvis gennem system kald eller yield kald. Overgiver ogs\u00e5 kontrollen ved fejl, ex divide-by-zero. Generer trap. Non-Cooperative \u00b6 Timer-interrupts : en timer k\u00f8rer i hardware og interrupter. Under interrupt, pauses den k\u00f8rende process, og OS's interrupt-handler k\u00f8rer. Hvilket kode der skal k\u00f8re (interrupt-handler) meddeles til hardware under boot. Timer startes under boot. Context Switch \u00b6 Scheduler bestemmer om der skal skiftes process. Hvis der skiftes process udf\u00f8res context switch . Registre gemmes fra k\u00f8rende process (til kernel stack ), og registre genoprettes fra soon-to-be-running process (fra kernel stack ). N\u00e5r return-from-trap udf\u00f8res, returneres til process B. Simplified Process Model \u00b6 Each job runs for the same amount of time All jobs arrive at the same time Once started each job runs to completion All jobs only use the CPU (no I/O) The run-time of each job is known Terminologi: job==process\\ |\\ job == thread job==process\\ |\\ job == thread Scheduling Metrics \u00b6 Turnaround time : Tid fra arrival til f\u00e6rdigg\u00f8rrelse $$ T_{turnaround}=T_{complition}-T_{arrival} $$ Dette er en performance metric. Response time: Tid fra arrival til f\u00f8rste schedule $$ T_{response}=T_{firstrun}-T_{arrival} $$ First In, First Out (FIFO) \u00b6 AKA First Come, First Served (FCFS) Nem at implementere Average turnaround time: \u200b \\frac{10+20+30}{3}=20 \\frac{10+20+30}{3}=20 Men hvis vi relaxer assumtion 1: Convoy effect : Korte jobs bliver sat i k\u00f8 bag store jobs. Average turnaround time h\u00f8j: \u200b \\frac{100+110+120}{3}=110 \\frac{100+110+120}{3}=110 D\u00e5rlig response time Shortest Job First (SJF) \u00b6 Average turnaround time: \u200b \\frac{10+20+120}{3}=50 \\frac{10+20+120}{3}=50 Givet vores assumptions om at alle jobs ankommer p\u00e5 samme tid, kan SJF bevises til at v\u00e6re optimal scheduling algoritme. Men hvis vi relaxer assumption 2. Hvis A ankommer f\u00f8r B og C vil den f\u00e5 h\u00f8j turnaround time igen. \u200b ( 103.33 103.33 ) Skidt for response time Shortest Time-to-Completion First (STCF) \u00b6 AKA Preemptive Shortest Job First (PSJF) Vi relaxer assumption 3. Giver os bedre average turnaround time: 50 50 Givet vores nuv\u00e6rende assumptions er STCF beviseligt optimal. Skidt for response time Round Robin (RR) \u00b6 RR k\u00f8rer jobs for en time slice (ogs\u00e5 kaldt scheduling quantum ) Time slices skal v\u00e6re dividerbar med timer-interrupt perioden. Jo kortere time slice, jo bedre response time. MEN: Hvis time slice bliver s\u00e5 kort, at kosten ved context switching dominerer overall performance. Armortization : hvis noget har en fixed cost, s\u00e5 kald denne operation s\u00e5 f\u00e5 gange som muligt. Average turnaround time: 14, rimelig skidt. RR er en af de v\u00e6rste policies set fra Turnaround time Generalt, policies, som RR, som er fair (deler cpu ligeligt bland aktive processer i lille tidsscala) performer d\u00e5lrigt p\u00e5 metrics som turnaround time. Multi Level Feedback Queue (MLFQ) \u00b6 Goal: Optimer turnaround time og minmer response time Hver process f\u00e5r en priorietet Hvert prioritet level har sin egen process k\u00f8 (queue) Basic rules: \\begin{equation} \\text{if } Pri(A)>Pri(B) \\text{ run } A \\tag{Rule 1}\\label{rule1} \\end{equation} \\begin{equation} \\text{if } Pri(A)>Pri(B) \\text{ run } A \\tag{Rule 1}\\label{rule1} \\end{equation} \\begin{equation} \\text{if } Pri(A)=Pri(B) \\text{ use RR for } A \\text{ and } B \\tag{Rule 2} \\end{equation} \\begin{equation} \\text{if } Pri(A)=Pri(B) \\text{ use RR for } A \\text{ and } B \\tag{Rule 2} \\end{equation} Handler om hvordan scheduleren s\u00e6tter prioriteter. Et program der ofte giver afkald p\u00e5 CPU for at vente p\u00e5 I/O f\u00e5r h\u00f8j prioritet. Regler for prioritets\u00e6ndring Rule 3: Nye processer starter p\u00e5 h\u00f8jeste prioritet Rule 4a: Prioritet af process der bruger al sin time-slice bliver reduceret Rule 4b: En process der slipper CPU f\u00f8r tid, bliver p\u00e5 samme prioritet Problemer \u00b6 Starvation: Hvis der er for mange interaktive jobs, tager de al CPU-tid, og starverer de lange jobs. En smart bruger kan rewrite et program til at game the scheduler Et lang job kan \u00e6ndre sin behavior og blive interaktiv efter noget tid. Dette bliver ikke rewardet i nuv\u00e6rende system. Ny regel \u00b6 Rule 5: Efter noget tid S , boostes alle jobs til \u00f8verste prioritet (k\u00f8) Rule 4 (samling af 4a og 4b): Altid reducer prioriteten p\u00e5 et job, n\u00e5r den bar brugt alt tildelt (alloted) tid. S\u00e5 vi ender ud med: Proportional Share (Lottery Scheduling) \u00b6 AKA fair-share scheduler Princip: Et antal tickets er i spil. Efter hver time slice, holdes lotteri. Processen, der ejer den ticket der bliver trukket, bliver k\u00f8rt. Ticket Mechanics \u00b6 Ticket currency : Lader brugeren allokere tickets mellem dens egen jobs. Currency converteres til rigtige tickets. Ticket transfer: En process kan midlertidigt give sine tickets til en anden process. Ex en client/server struktur. Klienten kan give serveren sine tickets n\u00e5r den beder den om noget arbejde. Ticket inflation : En process kan midlertidigt h\u00e6ve eller s\u00e6nke dets antal tickets. Giver ikke mening i et kompetitativt milj\u00f8. Unfairness Metric \u00b6 U U , tiden det f\u00f8rste job er f\u00e6rdigt divideret med tiden det andet job er f\u00e6rdigt. Hvis job A er f\u00e6rdig efter 10 og job B efter 20: $$ U=\\frac{10}{20}=0.5 $$ N\u00e5r jobs bliver f\u00e6rdigt n\u00e6sten sammentidigt vil U U n\u00e6rme sig 1. En perfectly fair scheduler har U=1 U=1","title":"2 - Scheduling"},{"location":"4-semester/PSS/exam/2-scheduler/#2-scheduling","text":"Keywords : Metrics for scheduling (turnaround time, response time), simple process model, scheduling policies (FIFO, SJF, STCF, Round Robin, MLFQ, lottery scheduler).","title":"2 - Scheduling"},{"location":"4-semester/PSS/exam/2-scheduler/#litterature","text":"OSTEP Chapters 6, 7, 8, 9","title":"Litterature"},{"location":"4-semester/PSS/exam/2-scheduler/#learning-goals","text":"After lecture 2 you: Can explain the notion of limited direct execution and how it relates to scheduling Will know the simplified process model Will know and can explain important metrics for measuring a scheduling policy: Fairness Turnaround time Response time Can explain important scheduling policies and their pros and cons: FIFO (First In First Out) SJF (Shortest Job First) STCF (Shortest Time-to-Completion First) Round Robin MLFQ Lottery scheduling","title":"Learning Goals"},{"location":"4-semester/PSS/exam/2-scheduler/#noter","text":"","title":"Noter"},{"location":"4-semester/PSS/exam/2-scheduler/#limited-direct-execution-lde","text":"The direct execution part: Run the program directly on the CPU Direct execution is fast, running directly on the CPU. How do allow a process to perform restricted operations without giving complete control? For at undg\u00e5 malicious processer, findes 2 modes. User - og kernel-mode . Restriktede operationer, kan kun laves i kernel mode. System kald , er kald som user-processer kan eksekvere, der kan eksekvere kode i kernel mode. For at eksekvere system kald, eksekvere programmer en trap instruktion. N\u00e5r systemet er f\u00e6rdig, eksekveres en return-from-trap instruktion, der returnerer til det kaldende bruger-program.","title":"Limited Direct Execution (LDE)"},{"location":"4-semester/PSS/exam/2-scheduler/#trap","text":"N\u00e5r et trap eksekveres, gemmes kalderens registrer, s\u00e5 den kan returnere igen. x86 pusher PC, flags og andre registre til en kernel stack . Som s\u00e5 bliver popped igen n\u00e5r return-from-trap eksekveres.","title":"Trap"},{"location":"4-semester/PSS/exam/2-scheduler/#trap-table","text":"For at kernel ved hvad der m\u00e5 eksekveres under et trap, ops\u00e6ttes et trap-table under boot. Trap table fort\u00e6ller hardware hvilket kode der skal eksekveres under hvilke events ( trap handlers ). Et system-call number er assigned til hvert system kald. User programmet placerer dette nummer i et bestemt register, eller bestemt sted i stakken. OS tjekker dette nummers validitet og k\u00f8rer det tilsvarende kode. Denne indirekthed fungerer som besykttelse. xv6 System call numbers i xv6 kan ses i syscall.h . De gemmes i %eax. Og kaldes ved at eksekvere trap med T_SYSCALL .","title":"Trap Table"},{"location":"4-semester/PSS/exam/2-scheduler/#switching-process-scheduling","text":"2 method: cooperative approach og non-cooperative .","title":"Switching Process (Scheduling)"},{"location":"4-semester/PSS/exam/2-scheduler/#cooperative-approach","text":"Processer st\u00e5r selv for at overgive kontrollen til OS. Eksempelvis gennem system kald eller yield kald. Overgiver ogs\u00e5 kontrollen ved fejl, ex divide-by-zero. Generer trap.","title":"Cooperative Approach"},{"location":"4-semester/PSS/exam/2-scheduler/#non-cooperative","text":"Timer-interrupts : en timer k\u00f8rer i hardware og interrupter. Under interrupt, pauses den k\u00f8rende process, og OS's interrupt-handler k\u00f8rer. Hvilket kode der skal k\u00f8re (interrupt-handler) meddeles til hardware under boot. Timer startes under boot.","title":"Non-Cooperative"},{"location":"4-semester/PSS/exam/2-scheduler/#context-switch","text":"Scheduler bestemmer om der skal skiftes process. Hvis der skiftes process udf\u00f8res context switch . Registre gemmes fra k\u00f8rende process (til kernel stack ), og registre genoprettes fra soon-to-be-running process (fra kernel stack ). N\u00e5r return-from-trap udf\u00f8res, returneres til process B.","title":"Context Switch"},{"location":"4-semester/PSS/exam/2-scheduler/#simplified-process-model","text":"Each job runs for the same amount of time All jobs arrive at the same time Once started each job runs to completion All jobs only use the CPU (no I/O) The run-time of each job is known Terminologi: job==process\\ |\\ job == thread job==process\\ |\\ job == thread","title":"Simplified Process Model"},{"location":"4-semester/PSS/exam/2-scheduler/#scheduling-metrics","text":"Turnaround time : Tid fra arrival til f\u00e6rdigg\u00f8rrelse $$ T_{turnaround}=T_{complition}-T_{arrival} $$ Dette er en performance metric. Response time: Tid fra arrival til f\u00f8rste schedule $$ T_{response}=T_{firstrun}-T_{arrival} $$","title":"Scheduling Metrics"},{"location":"4-semester/PSS/exam/2-scheduler/#first-in-first-out-fifo","text":"AKA First Come, First Served (FCFS) Nem at implementere Average turnaround time: \u200b \\frac{10+20+30}{3}=20 \\frac{10+20+30}{3}=20 Men hvis vi relaxer assumtion 1: Convoy effect : Korte jobs bliver sat i k\u00f8 bag store jobs. Average turnaround time h\u00f8j: \u200b \\frac{100+110+120}{3}=110 \\frac{100+110+120}{3}=110 D\u00e5rlig response time","title":"First In, First Out (FIFO)"},{"location":"4-semester/PSS/exam/2-scheduler/#shortest-job-first-sjf","text":"Average turnaround time: \u200b \\frac{10+20+120}{3}=50 \\frac{10+20+120}{3}=50 Givet vores assumptions om at alle jobs ankommer p\u00e5 samme tid, kan SJF bevises til at v\u00e6re optimal scheduling algoritme. Men hvis vi relaxer assumption 2. Hvis A ankommer f\u00f8r B og C vil den f\u00e5 h\u00f8j turnaround time igen. \u200b ( 103.33 103.33 ) Skidt for response time","title":"Shortest Job First (SJF)"},{"location":"4-semester/PSS/exam/2-scheduler/#shortest-time-to-completion-first-stcf","text":"AKA Preemptive Shortest Job First (PSJF) Vi relaxer assumption 3. Giver os bedre average turnaround time: 50 50 Givet vores nuv\u00e6rende assumptions er STCF beviseligt optimal. Skidt for response time","title":"Shortest Time-to-Completion First (STCF)"},{"location":"4-semester/PSS/exam/2-scheduler/#round-robin-rr","text":"RR k\u00f8rer jobs for en time slice (ogs\u00e5 kaldt scheduling quantum ) Time slices skal v\u00e6re dividerbar med timer-interrupt perioden. Jo kortere time slice, jo bedre response time. MEN: Hvis time slice bliver s\u00e5 kort, at kosten ved context switching dominerer overall performance. Armortization : hvis noget har en fixed cost, s\u00e5 kald denne operation s\u00e5 f\u00e5 gange som muligt. Average turnaround time: 14, rimelig skidt. RR er en af de v\u00e6rste policies set fra Turnaround time Generalt, policies, som RR, som er fair (deler cpu ligeligt bland aktive processer i lille tidsscala) performer d\u00e5lrigt p\u00e5 metrics som turnaround time.","title":"Round Robin (RR)"},{"location":"4-semester/PSS/exam/2-scheduler/#multi-level-feedback-queue-mlfq","text":"Goal: Optimer turnaround time og minmer response time Hver process f\u00e5r en priorietet Hvert prioritet level har sin egen process k\u00f8 (queue) Basic rules: \\begin{equation} \\text{if } Pri(A)>Pri(B) \\text{ run } A \\tag{Rule 1}\\label{rule1} \\end{equation} \\begin{equation} \\text{if } Pri(A)>Pri(B) \\text{ run } A \\tag{Rule 1}\\label{rule1} \\end{equation} \\begin{equation} \\text{if } Pri(A)=Pri(B) \\text{ use RR for } A \\text{ and } B \\tag{Rule 2} \\end{equation} \\begin{equation} \\text{if } Pri(A)=Pri(B) \\text{ use RR for } A \\text{ and } B \\tag{Rule 2} \\end{equation} Handler om hvordan scheduleren s\u00e6tter prioriteter. Et program der ofte giver afkald p\u00e5 CPU for at vente p\u00e5 I/O f\u00e5r h\u00f8j prioritet. Regler for prioritets\u00e6ndring Rule 3: Nye processer starter p\u00e5 h\u00f8jeste prioritet Rule 4a: Prioritet af process der bruger al sin time-slice bliver reduceret Rule 4b: En process der slipper CPU f\u00f8r tid, bliver p\u00e5 samme prioritet","title":"Multi Level Feedback Queue (MLFQ)"},{"location":"4-semester/PSS/exam/2-scheduler/#problemer","text":"Starvation: Hvis der er for mange interaktive jobs, tager de al CPU-tid, og starverer de lange jobs. En smart bruger kan rewrite et program til at game the scheduler Et lang job kan \u00e6ndre sin behavior og blive interaktiv efter noget tid. Dette bliver ikke rewardet i nuv\u00e6rende system.","title":"Problemer"},{"location":"4-semester/PSS/exam/2-scheduler/#ny-regel","text":"Rule 5: Efter noget tid S , boostes alle jobs til \u00f8verste prioritet (k\u00f8) Rule 4 (samling af 4a og 4b): Altid reducer prioriteten p\u00e5 et job, n\u00e5r den bar brugt alt tildelt (alloted) tid. S\u00e5 vi ender ud med:","title":"Ny regel"},{"location":"4-semester/PSS/exam/2-scheduler/#proportional-share-lottery-scheduling","text":"AKA fair-share scheduler Princip: Et antal tickets er i spil. Efter hver time slice, holdes lotteri. Processen, der ejer den ticket der bliver trukket, bliver k\u00f8rt.","title":"Proportional Share (Lottery Scheduling)"},{"location":"4-semester/PSS/exam/2-scheduler/#ticket-mechanics","text":"Ticket currency : Lader brugeren allokere tickets mellem dens egen jobs. Currency converteres til rigtige tickets. Ticket transfer: En process kan midlertidigt give sine tickets til en anden process. Ex en client/server struktur. Klienten kan give serveren sine tickets n\u00e5r den beder den om noget arbejde. Ticket inflation : En process kan midlertidigt h\u00e6ve eller s\u00e6nke dets antal tickets. Giver ikke mening i et kompetitativt milj\u00f8.","title":"Ticket Mechanics"},{"location":"4-semester/PSS/exam/2-scheduler/#unfairness-metric","text":"U U , tiden det f\u00f8rste job er f\u00e6rdigt divideret med tiden det andet job er f\u00e6rdigt. Hvis job A er f\u00e6rdig efter 10 og job B efter 20: $$ U=\\frac{10}{20}=0.5 $$ N\u00e5r jobs bliver f\u00e6rdigt n\u00e6sten sammentidigt vil U U n\u00e6rme sig 1. En perfectly fair scheduler har U=1 U=1","title":"Unfairness Metric"},{"location":"4-semester/PSS/exam/3-memory-management/","text":"3 - Memory Management \u00b6 Keywords : Memory hierarchy, goals for memory management (transparency, efficiency, isolation), address space, challenges for memory management, features (relocation, protection, and sharing), virtual addresses vs. physical addresses, address translation, base and bound registers, simple allocation, static allocation (nonuniform), dynamic allocation, virtual memory, segmentation. Litterature \u00b6 OSTEP Chapter 12, 13, (14), 15, 16, 17 Kapitler med parenteser skimmes: (x) Learning Goals \u00b6 After Lecture 5 you: ... will know and can discuss the three goals of memory management: Transparency Efficiency Protection (isolation) ... can explain what an address space is ... define and explain the notion of virtual memory ... perform simple address translation from virtual to physical ... can explain the need for and use of base/bound registers ... define and explain the use of segmentation Noter \u00b6 XV6 XV6 memory management i vm.c memlayout.h mmu.h kalloc.c Adress Space \u00b6 Addres space er en abstraktion af den fysiske hukommelse. S\u00e5 ser det ud fra hver process' synspunkt at de har det plads. Dette kaldes virtualizing memory . Hvis eksempelvis process A vil l\u00e6se fra adresse 0 ( virtual address ), s\u00e5 vil OS i samarbejde med hardware overs\u00e6tte til den fysiske (virkelige) adresse. M\u00e5l med Memory Management \u00b6 Transparency : OS skal implementere virtuel memory s\u00e5 det er usynligt for det k\u00f8rende program. (Programmet skal ikke vide at det er virtuel hukommelse) Efficiency: Virtualizeringen skal v\u00e6re s\u00e5 effektiv som muligt. Tid og plads Udnytter hardware support som TLB'er Protection: Beskyt processer fra hinanden, og OS fra processer. De skal ikke kunne skrive og l\u00e6se uden for deres egen adress space. Isolation processer er isoleret fra hinanden Adress Translation \u00b6 AKA hardware-based address translation Hardware overs\u00e6tter hver memory adresse, virtuel \\rightarrow \\rightarrow physical . OS manages memory . Antagelser \u00b6 Sammenh\u00e6ngende allokation (Contiguous allocation) Lille address space (mindre end fysisk hukommelse) Fixed st\u00f8rrelse address space Eksempel \u00b6 Base and Bounds \u00b6 AKA dynamic relocation 2 Hardware registre: base - og bounds - (ogs\u00e5 kaldt limit ) register. Lader os placere adress space hvor vi vil i fysisk. Memory referancer overs\u00e6ttes nu med: $$ Address_{physical}=Address_{virtual}+base\\ \\textbf{address translation} $$ Teknikken kaldes ofte dynamic relocation . Bounds bruges til protection . Undg\u00e5 at process l\u00e6ser uden for address space. Delen af CPU der hj\u00e6lper med adress translation kaldes memory management unit (MMU) . Eksempler: Hardware Requirements OS Problemer \u00b6 OS har en free list , en liste over ledigt hukommelse. Under context-switch skal base og bounds registre gemmes og l\u00e6ses. Segmentation \u00b6 Base og bounds par pr segment address space. Et segment er et sammenh\u00e6ngende portion address space. Logisk: code, stack og heap. Segmentation lader os placere dem forskellige steder i fysisk. Ubrugt address space kaldes sparce address space En referance til en adresse uden for segmentet -> segmentation violation eller segmentation fault Vi kan bruge bits i virtuelle adresse til at pr\u00e6cisere segment ( explicit approach ) 1 2 3 4 5 6 7 Segment = ( VirtualAddress & SEG_MASK ) >> SEG_SHIFT Offset = VirtualAddress & OFFSET_MASK if ( Offset >= Bounds [ Segment ]) RaiseException ( PROTECTION_FAULT ) else PhysAddr = Base [ Segment ] + Offset Register = AccessMemory ( PhysAddr ) 1 2 3 SEG_MASK = 0x3000 SEG_SHIFT = 12 OFFSET_MASK = 0xFFF Implicit approach: Hardware bestemmer segment ved at se p\u00e5 hvor addressen blev dannet. (Eks. fra PC, s\u00e5 er addr. fra code segment) The Stack \u00b6 Stack vokser bagud. Vi tilf\u00f8jer bit til hardware der fort\u00e6ller om adresser vokser fremad. Sharing \u00b6 Noget hukommelse kan deles mellem address spaces. code sharing . Vi tilf\u00f8jer protection bits til hukommelse. Fine- vs Coars-grained Segmentation \u00b6 Det vi har ovenover kaldes coarse-grained segmentation. Fine-grained segmentation: Mange sm\u00e5 segments. Kr\u00e6ver et segment table i memory. OS kan l\u00e6re hvordan de forskellige segments bruges. Og derved optimere. OS Support \u00b6 Problem: external fragmentation : fysisk hukommelse bliver fyldt med sm\u00e5 huller af ubrugt plads, som er for sm\u00e5 til et segment. L\u00f8sning kan v\u00e6re at compact fysisk memory. Dyrt, memory-intensivt at kopiere segments. Kan g\u00f8re segment-growing requests sv\u00e6re at servere. Simplere l\u00f8sning: free-list management algoritme. best-fit holder liste af frit lager, og giver den der passer bedst i st\u00f8rrelse. worst-fit first-fit Modsat internal fragmentation : Allokeret lager, er for stort og meget ubrugt. Free Space Management \u00b6 Free-list XV6 Se line 22 in kalloc.c Splitting: Request for 1 byte. Allocator finder free chunk, splitter den op. Returnerer den ene del, og anden del bliver i listen. Coalescing: Der kaldes free(10) . List vil se ud som f\u00f8lger: Vi samler frie segmenter der er sammenh\u00e6ngende: Header \u00b6 Bruges til at holde styr p\u00e5 st\u00f8rrelsen p\u00e5 allokerede regioner. XV6 kan ses i umalloc.c Embed Free List \u00b6 Listen laves i det frie hukommelse 1 2 3 4 typedef struct __node_t { int size ; struct __node_t * next ; } node_t Basic Strategies \u00b6 Best Fit \u00b6 Find de chunks der er s\u00e5 stor eller st\u00f8rre end det \u00f8nskede. Returner det mindste af dem. Kan koste meget performance n\u00e5r den skal s\u00f8ge efter den bedste frie blok. Worst Fit \u00b6 Find den st\u00f8rste chunk og returner den \u00f8nskede st\u00f8rrelse. Behold resten p\u00e5 free-list Kan v\u00e6re dyr i performance. Studier viser at den performer d\u00e5rligt. Leder til excess fragmentation, mens den stadig har h\u00f8j overheads. First Fit \u00b6 Finder den f\u00f8rste blok der er stor nok. Returnerer den \u00f8nskede st\u00f8rrelse. Hurtig metode. Fylder dog nogen gange starten op med sm\u00e5 objekter. Hvordan allocator manager free-listens r\u00e6kkef\u00f8lge betyder noget. En l\u00f8sning er address-based ordering G\u00f8r det nemmere at coalesce, og fragmentering er reduceret. Next Fit \u00b6 Ligesom first-fit, men algoritmen holder en pointer til der hvor den ledte sidst. Spreder s\u00f8gningen mere uniformt. H\u00f8j performance, som first-fit. Buddy Allocation \u00b6","title":"3 - Memory Management"},{"location":"4-semester/PSS/exam/3-memory-management/#3-memory-management","text":"Keywords : Memory hierarchy, goals for memory management (transparency, efficiency, isolation), address space, challenges for memory management, features (relocation, protection, and sharing), virtual addresses vs. physical addresses, address translation, base and bound registers, simple allocation, static allocation (nonuniform), dynamic allocation, virtual memory, segmentation.","title":"3 - Memory Management"},{"location":"4-semester/PSS/exam/3-memory-management/#litterature","text":"OSTEP Chapter 12, 13, (14), 15, 16, 17 Kapitler med parenteser skimmes: (x)","title":"Litterature"},{"location":"4-semester/PSS/exam/3-memory-management/#learning-goals","text":"After Lecture 5 you: ... will know and can discuss the three goals of memory management: Transparency Efficiency Protection (isolation) ... can explain what an address space is ... define and explain the notion of virtual memory ... perform simple address translation from virtual to physical ... can explain the need for and use of base/bound registers ... define and explain the use of segmentation","title":"Learning Goals"},{"location":"4-semester/PSS/exam/3-memory-management/#noter","text":"XV6 XV6 memory management i vm.c memlayout.h mmu.h kalloc.c","title":"Noter"},{"location":"4-semester/PSS/exam/3-memory-management/#adress-space","text":"Addres space er en abstraktion af den fysiske hukommelse. S\u00e5 ser det ud fra hver process' synspunkt at de har det plads. Dette kaldes virtualizing memory . Hvis eksempelvis process A vil l\u00e6se fra adresse 0 ( virtual address ), s\u00e5 vil OS i samarbejde med hardware overs\u00e6tte til den fysiske (virkelige) adresse.","title":"Adress Space"},{"location":"4-semester/PSS/exam/3-memory-management/#mal-med-memory-management","text":"Transparency : OS skal implementere virtuel memory s\u00e5 det er usynligt for det k\u00f8rende program. (Programmet skal ikke vide at det er virtuel hukommelse) Efficiency: Virtualizeringen skal v\u00e6re s\u00e5 effektiv som muligt. Tid og plads Udnytter hardware support som TLB'er Protection: Beskyt processer fra hinanden, og OS fra processer. De skal ikke kunne skrive og l\u00e6se uden for deres egen adress space. Isolation processer er isoleret fra hinanden","title":"M\u00e5l med Memory Management"},{"location":"4-semester/PSS/exam/3-memory-management/#adress-translation","text":"AKA hardware-based address translation Hardware overs\u00e6tter hver memory adresse, virtuel \\rightarrow \\rightarrow physical . OS manages memory .","title":"Adress Translation"},{"location":"4-semester/PSS/exam/3-memory-management/#antagelser","text":"Sammenh\u00e6ngende allokation (Contiguous allocation) Lille address space (mindre end fysisk hukommelse) Fixed st\u00f8rrelse address space","title":"Antagelser"},{"location":"4-semester/PSS/exam/3-memory-management/#eksempel","text":"","title":"Eksempel"},{"location":"4-semester/PSS/exam/3-memory-management/#base-and-bounds","text":"AKA dynamic relocation 2 Hardware registre: base - og bounds - (ogs\u00e5 kaldt limit ) register. Lader os placere adress space hvor vi vil i fysisk. Memory referancer overs\u00e6ttes nu med: $$ Address_{physical}=Address_{virtual}+base\\ \\textbf{address translation} $$ Teknikken kaldes ofte dynamic relocation . Bounds bruges til protection . Undg\u00e5 at process l\u00e6ser uden for address space. Delen af CPU der hj\u00e6lper med adress translation kaldes memory management unit (MMU) . Eksempler: Hardware Requirements","title":"Base and Bounds"},{"location":"4-semester/PSS/exam/3-memory-management/#os-problemer","text":"OS har en free list , en liste over ledigt hukommelse. Under context-switch skal base og bounds registre gemmes og l\u00e6ses.","title":"OS Problemer"},{"location":"4-semester/PSS/exam/3-memory-management/#segmentation","text":"Base og bounds par pr segment address space. Et segment er et sammenh\u00e6ngende portion address space. Logisk: code, stack og heap. Segmentation lader os placere dem forskellige steder i fysisk. Ubrugt address space kaldes sparce address space En referance til en adresse uden for segmentet -> segmentation violation eller segmentation fault Vi kan bruge bits i virtuelle adresse til at pr\u00e6cisere segment ( explicit approach ) 1 2 3 4 5 6 7 Segment = ( VirtualAddress & SEG_MASK ) >> SEG_SHIFT Offset = VirtualAddress & OFFSET_MASK if ( Offset >= Bounds [ Segment ]) RaiseException ( PROTECTION_FAULT ) else PhysAddr = Base [ Segment ] + Offset Register = AccessMemory ( PhysAddr ) 1 2 3 SEG_MASK = 0x3000 SEG_SHIFT = 12 OFFSET_MASK = 0xFFF Implicit approach: Hardware bestemmer segment ved at se p\u00e5 hvor addressen blev dannet. (Eks. fra PC, s\u00e5 er addr. fra code segment)","title":"Segmentation"},{"location":"4-semester/PSS/exam/3-memory-management/#the-stack","text":"Stack vokser bagud. Vi tilf\u00f8jer bit til hardware der fort\u00e6ller om adresser vokser fremad.","title":"The Stack"},{"location":"4-semester/PSS/exam/3-memory-management/#sharing","text":"Noget hukommelse kan deles mellem address spaces. code sharing . Vi tilf\u00f8jer protection bits til hukommelse.","title":"Sharing"},{"location":"4-semester/PSS/exam/3-memory-management/#fine-vs-coars-grained-segmentation","text":"Det vi har ovenover kaldes coarse-grained segmentation. Fine-grained segmentation: Mange sm\u00e5 segments. Kr\u00e6ver et segment table i memory. OS kan l\u00e6re hvordan de forskellige segments bruges. Og derved optimere.","title":"Fine- vs Coars-grained Segmentation"},{"location":"4-semester/PSS/exam/3-memory-management/#os-support","text":"Problem: external fragmentation : fysisk hukommelse bliver fyldt med sm\u00e5 huller af ubrugt plads, som er for sm\u00e5 til et segment. L\u00f8sning kan v\u00e6re at compact fysisk memory. Dyrt, memory-intensivt at kopiere segments. Kan g\u00f8re segment-growing requests sv\u00e6re at servere. Simplere l\u00f8sning: free-list management algoritme. best-fit holder liste af frit lager, og giver den der passer bedst i st\u00f8rrelse. worst-fit first-fit Modsat internal fragmentation : Allokeret lager, er for stort og meget ubrugt.","title":"OS Support"},{"location":"4-semester/PSS/exam/3-memory-management/#free-space-management","text":"Free-list XV6 Se line 22 in kalloc.c Splitting: Request for 1 byte. Allocator finder free chunk, splitter den op. Returnerer den ene del, og anden del bliver i listen. Coalescing: Der kaldes free(10) . List vil se ud som f\u00f8lger: Vi samler frie segmenter der er sammenh\u00e6ngende:","title":"Free Space Management"},{"location":"4-semester/PSS/exam/3-memory-management/#header","text":"Bruges til at holde styr p\u00e5 st\u00f8rrelsen p\u00e5 allokerede regioner. XV6 kan ses i umalloc.c","title":"Header"},{"location":"4-semester/PSS/exam/3-memory-management/#embed-free-list","text":"Listen laves i det frie hukommelse 1 2 3 4 typedef struct __node_t { int size ; struct __node_t * next ; } node_t","title":"Embed Free List"},{"location":"4-semester/PSS/exam/3-memory-management/#basic-strategies","text":"","title":"Basic Strategies"},{"location":"4-semester/PSS/exam/3-memory-management/#best-fit","text":"Find de chunks der er s\u00e5 stor eller st\u00f8rre end det \u00f8nskede. Returner det mindste af dem. Kan koste meget performance n\u00e5r den skal s\u00f8ge efter den bedste frie blok.","title":"Best Fit"},{"location":"4-semester/PSS/exam/3-memory-management/#worst-fit","text":"Find den st\u00f8rste chunk og returner den \u00f8nskede st\u00f8rrelse. Behold resten p\u00e5 free-list Kan v\u00e6re dyr i performance. Studier viser at den performer d\u00e5rligt. Leder til excess fragmentation, mens den stadig har h\u00f8j overheads.","title":"Worst Fit"},{"location":"4-semester/PSS/exam/3-memory-management/#first-fit","text":"Finder den f\u00f8rste blok der er stor nok. Returnerer den \u00f8nskede st\u00f8rrelse. Hurtig metode. Fylder dog nogen gange starten op med sm\u00e5 objekter. Hvordan allocator manager free-listens r\u00e6kkef\u00f8lge betyder noget. En l\u00f8sning er address-based ordering G\u00f8r det nemmere at coalesce, og fragmentering er reduceret.","title":"First Fit"},{"location":"4-semester/PSS/exam/3-memory-management/#next-fit","text":"Ligesom first-fit, men algoritmen holder en pointer til der hvor den ledte sidst. Spreder s\u00f8gningen mere uniformt. H\u00f8j performance, som first-fit.","title":"Next Fit"},{"location":"4-semester/PSS/exam/3-memory-management/#buddy-allocation","text":"","title":"Buddy Allocation"},{"location":"4-semester/PSS/exam/4-paged-memory/","text":"4 - Paged Memory \u00b6 Keywords : Address types (physical, relative, virtual), address translation (page tables), virtual memory, swapping, paging, shared memory, memory use for OS, page replacement algorithms (OPT, LRU, FIFO, CLOCK). Litterature \u00b6 OSTEP Chapter 18, 19, 20, 21, 22, (23), 24 Kapitler med parenteser skimmes: (x) Learning Goals \u00b6 After Paged Memory you can: ... define and explain paging and how paged memory works ... perform simple address translation from paged (virtual) memory to physical memory ... explain how paged memory supports shared memory ... explain organisation of page tables (direct, two-level) ... define, explain, and discuss various page replacement algorithms and their pros and cons Noter \u00b6 Paging \u00b6 Chopping up space into fixed size pieces. Simple example: For at holde styr p\u00e5 virtual pages, OS holder en per-process data struktur kaldet page table . Holder address translations for hver virtual page Virtuel adresse splittes op i virtual page number (VPN) og offset 16 bit page. 2 bit vpn. Vi indexer nu page table. Page 1 ligger i page frame 7 i ovenst\u00e5ende billede. Dette er physical frame number (PFN) aka physical page number (PPN) 7. Page Tables \u00b6 Page tables kan blive meget store. 32-bit adresse, 4KB pages. Virtuel adresse splittes til 20-bit VPN og 12-bit offset. 20-bit VPN betyder 2^{20} 2^{20} translations. 4 bytes per page table entry (PTE) giver 4MB per page table! Derfor er page tables ikke i MMU (hardware memory management unit) Vi holder page tables i memory. Linear Page Table \u00b6 Simpelt array af page table entries (PTE). Indexes med VPN, for at finde PFN. Page table entry \u00b6 En valid bit er normalt. Indikerer om translation er valid. Eksempel, stack og heap der vokser mod hinanden. Alt imellem er invalid . Access af invalid lager generer trap. Protection bits indikerer om page m\u00e5 l\u00e6ses fra, skrives til eller executes fra. Present bit indikerer om denne page er i fysisk memory eller p\u00e5 disk. swapped out Dirty bit indikerer om den er \u00e6ndret siden den blev bragt til hukommelse. Reference bit aka accessed bit : indikerer om en page har v\u00e6ret tilg\u00e5et buges i page replacement (P) present bit, (R/W) read/write, (U/S) user/supervisor, (PWT, PCD, PAT, G) bruges i hardware caching system, (A) accessed, (D) dirty bit, (PFN). Translation \u00b6 Lad os sige at page-table base register indeholder den fysiske adresse p\u00e5 start lokationen for page table. Giver os: 1 2 3 4 5 6 7 8 9 VPN = ( VirtualAddress & VPN_MASK ) >> SHIFT PTEAddr = PageTableBaseRegister + ( VPN * sizeof ( PTE )) VPN_MASK = 0x30 SHIFT = 4 offset = VirtualAddress & OFFSET_MASK PhysAddr = ( PFN << SHIFT ) | offset Translation Lookaside Buffer (TLB) \u00b6 For at g\u00f8re translation hurtigere tilf\u00f8jes tanslation-lookaside buffer (TLB) til hardware (MMU). En hardware cache af popul\u00e6re v2p translations. Kunne kaldes address-translation cache Ved hver virtual memory referance, tjekkes TLB for at se om den indeholder translation'en. Hvis TLB indeholder translation, har vi TLB hit . Hvis ikke, har vi TLB miss Simpelt algoritme inds\u00e6tter translation i TLB ved TLB miss. Det er vigtigt at vi oftest f\u00e5r TLB hit. Locality \u00b6 Spacial locality : access af elementer der ligger t\u00e6t p\u00e5 hinanden giver h\u00f8jere hit rate Temporal locality : hurtig re-referencing af elementeri tid giver h\u00f8jere hit rate. TLB Miss Handling \u00b6 Kan h\u00e5ndteres af Hardware eller OS. Eksempel p\u00e5 hardware-managed TLB er Intel x86. Bruger multi-level page table . Current page table bliver pointed p\u00e5 af CR3 register. Software-managed TLB : Hardware raiser exception, og trap handler h\u00e5ndtere TLB miss Return-from trap er anderledes end ved system call, da vi skal kalde den for\u00e5sagende instruktion. Denne gang med TLB hit. Man skal s\u00f8rge for at undg\u00e5 infinite loop, eksempelvis ved at holde TLB miss handlers i fysisk memory. Eller reservere entries i TLB for permanente translations. Software-managed l\u00f8sning giver flexibilitet og simplicitet TLB Indhold \u00b6 Typisk TLB har 32, 64 eller 128 entries, og er fully associative. En translation kan v\u00e6re overalt i TLB Hele TLB s\u00f8ges i parallel. En entry kan se ud som: $$ \\text{VPN}\\ |\\ \\text{PFN}\\ |\\ \\text{other bits} $$ Other bits : valid bit : har entry en valid translation protection bit : hvordan kan page tilg\u00e5s (som i page table) address-space identifier , dirty bit osv. TLB Problemer \u00b6 TLB indeholder v2p translations kun gyldige for nuv\u00e6rende process. N\u00e5r der skiftes process skal hardware, OS eller begge sikre sig at den n\u00e6ste process ikke bruge forkeret translations. En mulig l\u00f8sning er at flush TLB ved context switch. S\u00e6tter alle valid bits til 0 Kan v\u00e6re kostbart, da der vil v\u00e6re TLB miss'es efter hver context switch. Nogle systemer har address space identifer (ASID) felt i TLB. Kan t\u00e6nkes som process identifer (PID) men ofte f\u00e6rre bits Formindsk Page Tables \u00b6 En l\u00f8sning er st\u00f8rre pages . 32-bit addresser igen. Denne gang 16KB pages. Giver 18-bit VPN plus 14-bit offset. PTE (4 bytes) giver: 2^{18} 2^{18} entries, derfor 1MB per page table. Leder til internal fragmentation Hybrid apporach Et page table per logisk segment (code, heap og stack). Vi bruger base til holde fysisk adresse p\u00e5 page table. Og bound til at holde slutningen p\u00e5 page table. Eksempel: 32-bit adress space, 4KB pages, adress space splittet i 4 segments. Registers skal skiftes ved context-switch. 1 2 3 SN = ( VirtualAddress & SEG_MASK ) >> SN_SHIFT VPN = ( VirtualAddress & VPN_MASK ) >> VPN_SHIFT AddressOfPTE = Base [ SN ] + ( VPN * sizeof ( PTE )) Eksempel: Hvis code kun bruger f\u00f8rste 3 pages, vil code page table kun have 3 entries, og bounds er sat til 3. Fordele: Ubrugte pages mellem stack og heap fylder ikke i page table. Ulemper : Kr\u00e6ver at segmentation brgues. Giver external fragmentation Multi-Level Page Tables kan l\u00f8se problemet. Multi-Level Page Tables \u00b6 Sk\u00e6r page table op i page-sized stykker. Hver page table passer i en enkelt page. Hvis en hel page af PTE (page-table-entries) er invalid, allokeres ikke plads. Ellers bruges en ny struktur: page directory . Page directory (i 2-level) indeholder et antal page directory entries (PDE) . En PDE har som minium en valid bit og page frame number (PFN) . Hvis PDE er valid, er mindst en af pages valid. Hvis PDE ikke er valid, er resten af PDE ikke defineret. Fordele: Allokerer kun page table space i proportion til antal adress space brugt. Hvis carefully constructed, passer hver portion i en page, hvilket g\u00f8re det nemmere at manage memory. Lader os placere page-table pages hvor vi vil i memory. Ulemper Performance cost: ved TLB miss, kr\u00e6ves 2 loads. 1 for page directory og 1 for PTE. time-space trade-off complexity : Mere complex at implementere. Eksempel med 256 entry page table More Than 2 Levels \u00b6 Hvad hvis page directory bliver for stor? Eksempel: 30-bit virtual address space, 512 byte page. PTE 4 bytes. Giver: 21-bit VPN og 9-bit offset 128 PTE's per page. 7 bits til index 14 bits PDI: 2^{14} 2^{14} entries. Fylder 128 pages. Vi bygger endnu et niveau p\u00e5. Inverted Page Tables \u00b6 Her holder vi \u00e9t page table, der har en entry for hver fysisk page. Hver entry fort\u00e6ller os hvilken process der bruger denne page, og hvilken virtuel page der mapper til denne fysiske page. S\u00f8g gennem den struktur for at finde den korrekte. Bruger ofte hash-tables, da linear search er dyrt. Swapping \u00b6 Vi t\u00e6nker nu p\u00e5 at alle address spaces tilsammen kan v\u00e6re st\u00f8rre end fysisk memory. Vi bruger et ekstra level i memory hierarchy Et eksempel p\u00e5 memory hierarchy: Vi bruger hard disk til at opbevare portioner af adress spaces, der ikke er i great demand. F\u00f8r i tiden brugt man, memory overlays , hvor programm\u00f8rer selv skulle flytte data ind og ud af memory. Swap Space \u00b6 Vi reservere noget plads p\u00e5 disk, til at flytte pages frem og tilbage imellem. Vi kalder dette for swap space . Vi swap per pages ud af memory til det, og memory ind i memory fra det. Derfor skal OS huske disk address for en given page. Hver page table entry (PTE) f\u00e5r en present bit . Siger om page er i fysisk memory. Page Fault \u00b6 Accessing af en page der ikke er i fysisk kaldes page fault aka page miss. Kalder page-fault handler i OS Disk adresse kan stores i PTE. OS finder adresse, og requester disk fetch. Opdaterer page table (markerer present) Pr\u00f8ver instruktion igen. Dette kan generere TLB miss. Alternativt kan man opdatere TLB under page fault (Under I/O vil processen v\u00e6re i blocked state , og andre processer kan k\u00f8re) Hvad Hvis Memory Er Fyldt? \u00b6 Hvis der ikke er plads til at page in en page, vil OS f\u00f8rst page ud en eller flere pages. Dette styres af page-replacement policy For at holde en lille smule memory frit, bruger de fleste OS: HW og LW Low watermark (LW): His der er f\u00e6rre end LW pages tilg\u00e6ngelige, kaldes en baggrundstr\u00e5d til at frig\u00f8re memory. High watermark (HW) : Dette g\u00f8res indtil der er HW pages ledige. Denne baggrundstr\u00e5d kaldes ofte swap daemon eller page daemon . Swapping Policies \u00b6 Vi kan kalde main memory for cache for virtual memory. Vores m\u00e5l er at minimere cache misses . Samme som at maximere cache hits Average memory access time (AMAT) \\begin{align*} AMAT &= T_M+(P_{Miss}*T_D),\\\\\\\\ &\\text{hvor } T_M : \\text{cost of accessing memory}\\\\ &\\text{and } T_D : \\text{cost of accessing disk}\\\\ &\\text{and } P_{Miss} :\\text{chance of cache miss } (0.0<P_{Miss}<1.0) \\end{align*} \\begin{align*} AMAT &= T_M+(P_{Miss}*T_D),\\\\\\\\ &\\text{hvor } T_M : \\text{cost of accessing memory}\\\\ &\\text{and } T_D : \\text{cost of accessing disk}\\\\ &\\text{and } P_{Miss} :\\text{chance of cache miss } (0.0<P_{Miss}<1.0) \\end{align*} Optimal Replacement Policy \u00b6 (Impossible to implement) Udskifter den page der vil bliver accessed l\u00e6ngst ude i fremtiden Bruges til at sammenligne med. FIFO (First In First Out) \u00b6 Simpelt at implementere Kan ikke bestemme vigtigheden af en page Random \u00b6 Simpel at implementere Least-Recently-Used (LRU) \u00b6 Bruger history. ( frequency eller recency ) Replaces the least-recently-used page. Som Least-Frequently-Used (LFU) Workload Eksempler \u00b6 Ingen lokalitet: \u00b6 100 unikke pages, v\u00e6lger random. 10k gange. Ingen lokalitet: Ligegyldigt hvilken policy. 80-20 workload: \u00b6 80% af referancer er til 20% af pages. LRU er den bedste. Looping Sequential Workload \u00b6 Referencer til 50 pages i sekvens startende fra 0. Random er den bedste op til n\u00e5r cache bliver 50 eller mere. Worst-case for b\u00e5de LRU og FIFO. Random har ingen \"wierd\" corner cases. Aproximating LRU \u00b6 For at undg\u00e5 at kigge en hel liste igennem for at finde least-recently-used page, kan man aproximere LRU. Bruger en use bit aka reference bit . 1 per page. N\u00e5r en page bliver referenced s\u00e6ttes use bit til 1 af hardware. Clock algorithm Forestil alle pages i en cirkul\u00e6r liste. En clock hand peger p\u00e5 en page. N\u00e5r replacement sker tjekker OS om den pegede p\u00e5 page P har use bit 1 eller 0. Hvis use bit er 1, er det ikke en god replacement kandidat. Use bit s\u00e6ttes til 0 (cleared) og clock hand incrementes. Forts\u00e6tter til en page med use bit 0 findes. Tilf\u00f8jelse: dirty bit . Man kan tilf\u00f8je dirty bit, som fort\u00e6ller om page er modified ( dirty ). Dette betyder at der skal skrives til disk hvilket er dyrt. Thrashing \u00b6 Hvis memory er oversubscribed, og memory demand er over fysisk memory, vil systemet konstant page, kaldet thrashing .","title":"4 - Paged Memory"},{"location":"4-semester/PSS/exam/4-paged-memory/#4-paged-memory","text":"Keywords : Address types (physical, relative, virtual), address translation (page tables), virtual memory, swapping, paging, shared memory, memory use for OS, page replacement algorithms (OPT, LRU, FIFO, CLOCK).","title":"4 - Paged Memory"},{"location":"4-semester/PSS/exam/4-paged-memory/#litterature","text":"OSTEP Chapter 18, 19, 20, 21, 22, (23), 24 Kapitler med parenteser skimmes: (x)","title":"Litterature"},{"location":"4-semester/PSS/exam/4-paged-memory/#learning-goals","text":"After Paged Memory you can: ... define and explain paging and how paged memory works ... perform simple address translation from paged (virtual) memory to physical memory ... explain how paged memory supports shared memory ... explain organisation of page tables (direct, two-level) ... define, explain, and discuss various page replacement algorithms and their pros and cons","title":"Learning Goals"},{"location":"4-semester/PSS/exam/4-paged-memory/#noter","text":"","title":"Noter"},{"location":"4-semester/PSS/exam/4-paged-memory/#paging","text":"Chopping up space into fixed size pieces. Simple example: For at holde styr p\u00e5 virtual pages, OS holder en per-process data struktur kaldet page table . Holder address translations for hver virtual page Virtuel adresse splittes op i virtual page number (VPN) og offset 16 bit page. 2 bit vpn. Vi indexer nu page table. Page 1 ligger i page frame 7 i ovenst\u00e5ende billede. Dette er physical frame number (PFN) aka physical page number (PPN) 7.","title":"Paging"},{"location":"4-semester/PSS/exam/4-paged-memory/#page-tables","text":"Page tables kan blive meget store. 32-bit adresse, 4KB pages. Virtuel adresse splittes til 20-bit VPN og 12-bit offset. 20-bit VPN betyder 2^{20} 2^{20} translations. 4 bytes per page table entry (PTE) giver 4MB per page table! Derfor er page tables ikke i MMU (hardware memory management unit) Vi holder page tables i memory.","title":"Page Tables"},{"location":"4-semester/PSS/exam/4-paged-memory/#linear-page-table","text":"Simpelt array af page table entries (PTE). Indexes med VPN, for at finde PFN.","title":"Linear Page Table"},{"location":"4-semester/PSS/exam/4-paged-memory/#page-table-entry","text":"En valid bit er normalt. Indikerer om translation er valid. Eksempel, stack og heap der vokser mod hinanden. Alt imellem er invalid . Access af invalid lager generer trap. Protection bits indikerer om page m\u00e5 l\u00e6ses fra, skrives til eller executes fra. Present bit indikerer om denne page er i fysisk memory eller p\u00e5 disk. swapped out Dirty bit indikerer om den er \u00e6ndret siden den blev bragt til hukommelse. Reference bit aka accessed bit : indikerer om en page har v\u00e6ret tilg\u00e5et buges i page replacement (P) present bit, (R/W) read/write, (U/S) user/supervisor, (PWT, PCD, PAT, G) bruges i hardware caching system, (A) accessed, (D) dirty bit, (PFN).","title":"Page table entry"},{"location":"4-semester/PSS/exam/4-paged-memory/#translation","text":"Lad os sige at page-table base register indeholder den fysiske adresse p\u00e5 start lokationen for page table. Giver os: 1 2 3 4 5 6 7 8 9 VPN = ( VirtualAddress & VPN_MASK ) >> SHIFT PTEAddr = PageTableBaseRegister + ( VPN * sizeof ( PTE )) VPN_MASK = 0x30 SHIFT = 4 offset = VirtualAddress & OFFSET_MASK PhysAddr = ( PFN << SHIFT ) | offset","title":"Translation"},{"location":"4-semester/PSS/exam/4-paged-memory/#translation-lookaside-buffer-tlb","text":"For at g\u00f8re translation hurtigere tilf\u00f8jes tanslation-lookaside buffer (TLB) til hardware (MMU). En hardware cache af popul\u00e6re v2p translations. Kunne kaldes address-translation cache Ved hver virtual memory referance, tjekkes TLB for at se om den indeholder translation'en. Hvis TLB indeholder translation, har vi TLB hit . Hvis ikke, har vi TLB miss Simpelt algoritme inds\u00e6tter translation i TLB ved TLB miss. Det er vigtigt at vi oftest f\u00e5r TLB hit.","title":"Translation Lookaside Buffer (TLB)"},{"location":"4-semester/PSS/exam/4-paged-memory/#locality","text":"Spacial locality : access af elementer der ligger t\u00e6t p\u00e5 hinanden giver h\u00f8jere hit rate Temporal locality : hurtig re-referencing af elementeri tid giver h\u00f8jere hit rate.","title":"Locality"},{"location":"4-semester/PSS/exam/4-paged-memory/#tlb-miss-handling","text":"Kan h\u00e5ndteres af Hardware eller OS. Eksempel p\u00e5 hardware-managed TLB er Intel x86. Bruger multi-level page table . Current page table bliver pointed p\u00e5 af CR3 register. Software-managed TLB : Hardware raiser exception, og trap handler h\u00e5ndtere TLB miss Return-from trap er anderledes end ved system call, da vi skal kalde den for\u00e5sagende instruktion. Denne gang med TLB hit. Man skal s\u00f8rge for at undg\u00e5 infinite loop, eksempelvis ved at holde TLB miss handlers i fysisk memory. Eller reservere entries i TLB for permanente translations. Software-managed l\u00f8sning giver flexibilitet og simplicitet","title":"TLB Miss Handling"},{"location":"4-semester/PSS/exam/4-paged-memory/#tlb-indhold","text":"Typisk TLB har 32, 64 eller 128 entries, og er fully associative. En translation kan v\u00e6re overalt i TLB Hele TLB s\u00f8ges i parallel. En entry kan se ud som: $$ \\text{VPN}\\ |\\ \\text{PFN}\\ |\\ \\text{other bits} $$ Other bits : valid bit : har entry en valid translation protection bit : hvordan kan page tilg\u00e5s (som i page table) address-space identifier , dirty bit osv.","title":"TLB Indhold"},{"location":"4-semester/PSS/exam/4-paged-memory/#tlb-problemer","text":"TLB indeholder v2p translations kun gyldige for nuv\u00e6rende process. N\u00e5r der skiftes process skal hardware, OS eller begge sikre sig at den n\u00e6ste process ikke bruge forkeret translations. En mulig l\u00f8sning er at flush TLB ved context switch. S\u00e6tter alle valid bits til 0 Kan v\u00e6re kostbart, da der vil v\u00e6re TLB miss'es efter hver context switch. Nogle systemer har address space identifer (ASID) felt i TLB. Kan t\u00e6nkes som process identifer (PID) men ofte f\u00e6rre bits","title":"TLB Problemer"},{"location":"4-semester/PSS/exam/4-paged-memory/#formindsk-page-tables","text":"En l\u00f8sning er st\u00f8rre pages . 32-bit addresser igen. Denne gang 16KB pages. Giver 18-bit VPN plus 14-bit offset. PTE (4 bytes) giver: 2^{18} 2^{18} entries, derfor 1MB per page table. Leder til internal fragmentation Hybrid apporach Et page table per logisk segment (code, heap og stack). Vi bruger base til holde fysisk adresse p\u00e5 page table. Og bound til at holde slutningen p\u00e5 page table. Eksempel: 32-bit adress space, 4KB pages, adress space splittet i 4 segments. Registers skal skiftes ved context-switch. 1 2 3 SN = ( VirtualAddress & SEG_MASK ) >> SN_SHIFT VPN = ( VirtualAddress & VPN_MASK ) >> VPN_SHIFT AddressOfPTE = Base [ SN ] + ( VPN * sizeof ( PTE )) Eksempel: Hvis code kun bruger f\u00f8rste 3 pages, vil code page table kun have 3 entries, og bounds er sat til 3. Fordele: Ubrugte pages mellem stack og heap fylder ikke i page table. Ulemper : Kr\u00e6ver at segmentation brgues. Giver external fragmentation Multi-Level Page Tables kan l\u00f8se problemet.","title":"Formindsk Page Tables"},{"location":"4-semester/PSS/exam/4-paged-memory/#multi-level-page-tables","text":"Sk\u00e6r page table op i page-sized stykker. Hver page table passer i en enkelt page. Hvis en hel page af PTE (page-table-entries) er invalid, allokeres ikke plads. Ellers bruges en ny struktur: page directory . Page directory (i 2-level) indeholder et antal page directory entries (PDE) . En PDE har som minium en valid bit og page frame number (PFN) . Hvis PDE er valid, er mindst en af pages valid. Hvis PDE ikke er valid, er resten af PDE ikke defineret. Fordele: Allokerer kun page table space i proportion til antal adress space brugt. Hvis carefully constructed, passer hver portion i en page, hvilket g\u00f8re det nemmere at manage memory. Lader os placere page-table pages hvor vi vil i memory. Ulemper Performance cost: ved TLB miss, kr\u00e6ves 2 loads. 1 for page directory og 1 for PTE. time-space trade-off complexity : Mere complex at implementere. Eksempel med 256 entry page table","title":"Multi-Level Page Tables"},{"location":"4-semester/PSS/exam/4-paged-memory/#more-than-2-levels","text":"Hvad hvis page directory bliver for stor? Eksempel: 30-bit virtual address space, 512 byte page. PTE 4 bytes. Giver: 21-bit VPN og 9-bit offset 128 PTE's per page. 7 bits til index 14 bits PDI: 2^{14} 2^{14} entries. Fylder 128 pages. Vi bygger endnu et niveau p\u00e5.","title":"More Than 2 Levels"},{"location":"4-semester/PSS/exam/4-paged-memory/#inverted-page-tables","text":"Her holder vi \u00e9t page table, der har en entry for hver fysisk page. Hver entry fort\u00e6ller os hvilken process der bruger denne page, og hvilken virtuel page der mapper til denne fysiske page. S\u00f8g gennem den struktur for at finde den korrekte. Bruger ofte hash-tables, da linear search er dyrt.","title":"Inverted Page Tables"},{"location":"4-semester/PSS/exam/4-paged-memory/#swapping","text":"Vi t\u00e6nker nu p\u00e5 at alle address spaces tilsammen kan v\u00e6re st\u00f8rre end fysisk memory. Vi bruger et ekstra level i memory hierarchy Et eksempel p\u00e5 memory hierarchy: Vi bruger hard disk til at opbevare portioner af adress spaces, der ikke er i great demand. F\u00f8r i tiden brugt man, memory overlays , hvor programm\u00f8rer selv skulle flytte data ind og ud af memory.","title":"Swapping"},{"location":"4-semester/PSS/exam/4-paged-memory/#swap-space","text":"Vi reservere noget plads p\u00e5 disk, til at flytte pages frem og tilbage imellem. Vi kalder dette for swap space . Vi swap per pages ud af memory til det, og memory ind i memory fra det. Derfor skal OS huske disk address for en given page. Hver page table entry (PTE) f\u00e5r en present bit . Siger om page er i fysisk memory.","title":"Swap Space"},{"location":"4-semester/PSS/exam/4-paged-memory/#page-fault","text":"Accessing af en page der ikke er i fysisk kaldes page fault aka page miss. Kalder page-fault handler i OS Disk adresse kan stores i PTE. OS finder adresse, og requester disk fetch. Opdaterer page table (markerer present) Pr\u00f8ver instruktion igen. Dette kan generere TLB miss. Alternativt kan man opdatere TLB under page fault (Under I/O vil processen v\u00e6re i blocked state , og andre processer kan k\u00f8re)","title":"Page Fault"},{"location":"4-semester/PSS/exam/4-paged-memory/#hvad-hvis-memory-er-fyldt","text":"Hvis der ikke er plads til at page in en page, vil OS f\u00f8rst page ud en eller flere pages. Dette styres af page-replacement policy For at holde en lille smule memory frit, bruger de fleste OS: HW og LW Low watermark (LW): His der er f\u00e6rre end LW pages tilg\u00e6ngelige, kaldes en baggrundstr\u00e5d til at frig\u00f8re memory. High watermark (HW) : Dette g\u00f8res indtil der er HW pages ledige. Denne baggrundstr\u00e5d kaldes ofte swap daemon eller page daemon .","title":"Hvad Hvis Memory Er Fyldt?"},{"location":"4-semester/PSS/exam/4-paged-memory/#swapping-policies","text":"Vi kan kalde main memory for cache for virtual memory. Vores m\u00e5l er at minimere cache misses . Samme som at maximere cache hits Average memory access time (AMAT) \\begin{align*} AMAT &= T_M+(P_{Miss}*T_D),\\\\\\\\ &\\text{hvor } T_M : \\text{cost of accessing memory}\\\\ &\\text{and } T_D : \\text{cost of accessing disk}\\\\ &\\text{and } P_{Miss} :\\text{chance of cache miss } (0.0<P_{Miss}<1.0) \\end{align*} \\begin{align*} AMAT &= T_M+(P_{Miss}*T_D),\\\\\\\\ &\\text{hvor } T_M : \\text{cost of accessing memory}\\\\ &\\text{and } T_D : \\text{cost of accessing disk}\\\\ &\\text{and } P_{Miss} :\\text{chance of cache miss } (0.0<P_{Miss}<1.0) \\end{align*}","title":"Swapping Policies"},{"location":"4-semester/PSS/exam/4-paged-memory/#optimal-replacement-policy","text":"(Impossible to implement) Udskifter den page der vil bliver accessed l\u00e6ngst ude i fremtiden Bruges til at sammenligne med.","title":"Optimal Replacement Policy"},{"location":"4-semester/PSS/exam/4-paged-memory/#fifo-first-in-first-out","text":"Simpelt at implementere Kan ikke bestemme vigtigheden af en page","title":"FIFO (First In First Out)"},{"location":"4-semester/PSS/exam/4-paged-memory/#random","text":"Simpel at implementere","title":"Random"},{"location":"4-semester/PSS/exam/4-paged-memory/#least-recently-used-lru","text":"Bruger history. ( frequency eller recency ) Replaces the least-recently-used page. Som Least-Frequently-Used (LFU)","title":"Least-Recently-Used (LRU)"},{"location":"4-semester/PSS/exam/4-paged-memory/#workload-eksempler","text":"","title":"Workload Eksempler"},{"location":"4-semester/PSS/exam/4-paged-memory/#ingen-lokalitet","text":"100 unikke pages, v\u00e6lger random. 10k gange. Ingen lokalitet: Ligegyldigt hvilken policy.","title":"Ingen lokalitet:"},{"location":"4-semester/PSS/exam/4-paged-memory/#80-20-workload","text":"80% af referancer er til 20% af pages. LRU er den bedste.","title":"80-20 workload:"},{"location":"4-semester/PSS/exam/4-paged-memory/#looping-sequential-workload","text":"Referencer til 50 pages i sekvens startende fra 0. Random er den bedste op til n\u00e5r cache bliver 50 eller mere. Worst-case for b\u00e5de LRU og FIFO. Random har ingen \"wierd\" corner cases.","title":"Looping Sequential Workload"},{"location":"4-semester/PSS/exam/4-paged-memory/#aproximating-lru","text":"For at undg\u00e5 at kigge en hel liste igennem for at finde least-recently-used page, kan man aproximere LRU. Bruger en use bit aka reference bit . 1 per page. N\u00e5r en page bliver referenced s\u00e6ttes use bit til 1 af hardware. Clock algorithm Forestil alle pages i en cirkul\u00e6r liste. En clock hand peger p\u00e5 en page. N\u00e5r replacement sker tjekker OS om den pegede p\u00e5 page P har use bit 1 eller 0. Hvis use bit er 1, er det ikke en god replacement kandidat. Use bit s\u00e6ttes til 0 (cleared) og clock hand incrementes. Forts\u00e6tter til en page med use bit 0 findes. Tilf\u00f8jelse: dirty bit . Man kan tilf\u00f8je dirty bit, som fort\u00e6ller om page er modified ( dirty ). Dette betyder at der skal skrives til disk hvilket er dyrt.","title":"Aproximating LRU"},{"location":"4-semester/PSS/exam/4-paged-memory/#thrashing","text":"Hvis memory er oversubscribed, og memory demand er over fysisk memory, vil systemet konstant page, kaldet thrashing .","title":"Thrashing"},{"location":"4-semester/PSS/exam/5-concurrency/","text":"5 - Concurrency \u00b6 Keywords : Multi-threading, implementation strategies for multi-threading (concurrency), concurrency vs. parallelism, inter-process communication, race conditions, mutual exclusion, ensuring mutual exclusion (algorithms, hardware supported, mutexes, semaphores, monitors). Litterature \u00b6 OSTEP Chapter 25, 26, (27), 28, (29), (30), 31, 32, (33), 34 Kapitler med parenteser skimmes: (x) Learning Goals \u00b6 After this lecture, you ... can define what a race condition is ... can explain how mutual exclusion can be used to avoid race conditions ... can explain strategies for achieving and implementing mutual exclusion ... can define the notions of mutex , semaphore , and monitor and explain how they work and where they are useful ... can explain how to synchronise two (or more) threads and why it may be necessary Noter \u00b6 Threads \u00b6 Threads kan ses som en abstrahering i processer. En tr\u00e5d er lige som en seperate process, botset fra at de deler adress space, og kan derfor access samme data. Hver tr\u00e5d har sit eget s\u00e6t registre Hvis 2 tr\u00e5de k\u00f8rer p\u00e5 samme CPU skal der laves context switch hvis CPU skal skifte tr\u00e5d. Address space forbliver det samme Ligesom ved processer har vi en datastruktur til at gemme informationen. Her en thread control block (TCB) i stedet for PCB. Hver tr\u00e5d har sin egen stack: thread-local storage Hvorfor Tr\u00e5de? \u00b6 Parallelism En opgave splittes op, og kan k\u00f8re p\u00e5 flere CPU'er. Parallelization . I/O For at undg\u00e5 at programmet er blocked pga. langsom I/O. Mange server-based applikationer bruger tr\u00e5de. (web servere, databaser osv.) Race Conditions \u00b6 Hvis 2 tr\u00e5de arbejder p\u00e5 den samme variabel, kan der opst\u00e5 det der kaldes race condition eller data race . Hvis begge henter variabel k ind p\u00e5 samme med mens den er 42. S\u00e5 t\u00e6ller de den begge op, og resultatet bliver 43, selvom det burde v\u00e6re 44. Definitioner \u00b6 Race condition: N\u00e5r resultatet af en beregning afh\u00e6nger af relative speed af de individuelle tr\u00e5de Med andre ord: Resultatet afh\u00e6nger af interleaving af tr\u00e5dene. Sv\u00e6re at debug Critical region (critical section) : Program fragment s\u00e5rbar overfor race conditions \"Kritisk region\" Kritiske regioner skal eksekveres under mutual exclusion Mutual exclusion (mutex) : N\u00e5r kun en tr\u00e5d (blandt mange) kan tilg\u00e5 en given resource eller execute en specifik del af program-text. \"gensidig udelukkelse\" Atomic Event eller sekvens af events som sker uafbrudt. Locks \u00b6 Critical region kode omgives a lock og unlock. 1 2 3 4 5 lock_t mutex ; ... lock ( & mutex ); balance = balance + 1 ; unlock ( & mutex ); Her er mutex lock variable (lock for short). Lock variablen holder lock'ens tilstand. En lock er enten available (unlocked/free) Ingen tr\u00e5de holder lock'en Eller aquired (locked/held) Pr\u00e6cis 1 tr\u00e5d holder lock'en Kald til lock() fors\u00f8ger at aquire l\u00e5sen. Hvis ingen tr\u00e5de holder l\u00e5sen (l\u00e5sen er available), vil tr\u00e5den aquire l\u00e5sen, og eksekvere den critical region. Hvis l\u00e5sen er held, vil tr\u00e5den vente p\u00e5 at l\u00e5sen bliver available Kald til unlock() vil g\u00f8re l\u00e5sen available igen, og hvis nogle tr\u00e5de venter p\u00e5 l\u00e5sen, vil de nu aquire l\u00e5sen, og eksekvere critical region. Building a Lock \u00b6 M\u00e5l: Correctness: Provides mutual exclusion Fairness : F\u00e5r alle tr\u00e5de et fair fors\u00f8g p\u00e5 at aquire. (Er der nogle tr\u00e5de der starver , og dermed aldrig f\u00e5r fat i l\u00e5sen) Performance Tidlig l\u00f8sning for single-processor systemer var at disable interrupts under critical region 1 2 void lock () { DisableInterrupts ();} void unlock () { EnableInterrupts ();} Simpelt at implementere Ulemper: Lader alle tr\u00e5de kalde priviligeret instruktion, kan abuses Virker ikke p\u00e5 multi-processor systemer. Hvis 2 tr\u00e5de k\u00f8rer p\u00e5 2 forskellige processorer, er det ligegyldigt om interrupts er disabled Kan g\u00f8re til at interrupts bliver tabt. Eksempelvis hvis CPU misser at disk l\u00e6sning er f\u00e6rdig Ineffektivt Lock Variables \u00b6 VIRKER IKKE Race conditions Dekkers Algorithm \u00b6 Ikke effektiv Compiler vil m\u00e5ske allokere nogle variable i registers (ikke delt mellem tr\u00e5de) Sv\u00e6r (umulig?) at scalere til mere end 2 tr\u00e5de. Spin Lock Med Test-And-Set \u00b6 Hardware instruktion test-and-set (atomic exchange) G\u00f8r f\u00f8lgende atomically: 1 2 3 4 5 int TestAndSet ( int * old_ptr , int new ) { int old = * old_ptr ; // fetch old value at old_ptr * old_ptr = new ; // store 'new' into old_ptr return old ; // return the old value } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 typedef struct __lock_t { int flag ; } lock_t ; void init ( lock_t * lock ) { // 0: lock is available, 1: lock is held lock -> flag = 0 ; } void lock ( lock_t * lock ) { while ( TestAndSet ( & lock -> flag , 1 ) == 1 ) ; // spin-wait (do nothing) } void unlock ( lock_t * lock ) { lock -> flag = 0 ; } Spin-locks kr\u00e6ver preemptive scheduler (interrupter via timer). Ellers clogger de CPU Evaluering af Spin Locks \u00b6 Correctness : Ja, lader kun 1 tr\u00e5d eksekvere critical region af gangen. Fairness: Nej, ingen garanti for ikke at starve Perfomance Ikke s\u00e5 god p\u00e5 single CPU. Hvis tr\u00e5d bliver afbrudt midt i critcal region S\u00e5 kan alle andre tr\u00e5de, ventende, st\u00e5 at spinne i en hel time slice Rimelig god p\u00e5 multiple CPU, hvis antallet af CPUs ca. passer med antallet af tr\u00e5de Compare-And-Swap \u00b6 Ny hardware instruktion, atomically : 1 2 3 4 5 6 int CompareAndSwap ( int * ptr , int expected , int new ) { int original = * ptr ; if ( original == expected ) * ptr = new ; return original ; } Implementering: 1 2 3 4 void lock ( lock_t * lock ) { while ( CompareAndSwap ( & lock -> flag , 0 , 1 ) == 1 ) ; // spin } Fetch-And-Add \u00b6 Hardware instruktion, atomically: 1 2 3 4 5 int FetchAndAdd ( int * ptr ) { int old = * ptr ; * ptr = old + 1 ; return old ; } Sikrer progress for alle tr\u00e5de. Laver en \"k\u00f8\". Undg\u00e5 Tidsspild Ved Spinning \u00b6 Hvis der sker context-switch midt i en critical region, vil en tr\u00e5d der venter p\u00e5 lock, bare spinne uden at g\u00f8re noget, hele time slice. Kaldes busy wait . Mulig l\u00f8sning: yield. I stedt for at g\u00f8re ingenting, s\u00e5 kald yield Stadig ikke helt god. 100 threads der venter: RR scheduler, waster 99 cycles Stadig bedre end at wase 99 time slices Brug af Queues: Sleeping Instead of Spinning \u00b6 Semaphores \u00b6 Semaphore: Et objekt med en integer v\u00e6rdi der kan manipuleres med 2 routiner. I POSIX er det: sem_wait() sem_post() Initialiseres med 1 2 3 #include <semaphore.h> sem_t s ; sem_init ( & s , 0 , 1 ); // initilizere den til 1, arg2 (0) betyder at den er delt mellem tr\u00e5de i den samme process 1 2 3 4 5 6 7 8 9 int sem_wait ( sem_t * s ) { /* decrement the value of semaphore s by one wait if value of semaphore s is negative */ } int sem_post ( sem_t * s ) { /* increment the value of semaphore s by one if there are one or more threads waiting, wake one */ } Binary Semaphore (Lock) \u00b6 1 2 3 4 5 6 sem_t m ; sem_init ( & m , 0 , 1 ); // initialize to 1; sem_wait ( & m ); // critical section here sem_post ( & m ); Semaphores For Ordering \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 sem_t s ; void * child ( void * arg ) { printf ( \"child \\n \" ); sem_post ( & s ); // signal here: child is done return NULL ; } int main ( int argc , char * argv []) { sem_init ( & s , 0 , 0 ); printf ( \"parent: begin \\n \" ); pthread_t c ; Pthread_create ( & c , NULL , child , NULL ); sem_wait ( & s ); // wait here for child printf ( \"parent: end \\n \" ); return 0 ; } The Producer/Consumer Problem (Bounded Buffer Problem) \u00b6 En eller flere producer threads Genererer data items, placerer dem i en buffer En eller flere consumer threads Tager items fra bufferen og consumer dem Virker hvis MAX er 1 Hvis MAX er eks 10, f\u00e5r vi race condition Reader/Writer Lock \u00b6 Thread Throtting with Semaphores \u00b6 Hvis vi vil kontrollere max antal tr\u00e5de, kan vi g\u00f8re det med semaphores. Kaldes throttling , en form for admission control Eksempelvis hvis en masse memory heavy arbejde, og der k\u00f8rer en masse tr\u00e5de p\u00e5 en gang, kan det v\u00e6re at fysisk memory overstiges. Vi kan initialisere en semaphore med antallet af max tr\u00e5de.","title":"5 - Concurrency"},{"location":"4-semester/PSS/exam/5-concurrency/#5-concurrency","text":"Keywords : Multi-threading, implementation strategies for multi-threading (concurrency), concurrency vs. parallelism, inter-process communication, race conditions, mutual exclusion, ensuring mutual exclusion (algorithms, hardware supported, mutexes, semaphores, monitors).","title":"5 - Concurrency"},{"location":"4-semester/PSS/exam/5-concurrency/#litterature","text":"OSTEP Chapter 25, 26, (27), 28, (29), (30), 31, 32, (33), 34 Kapitler med parenteser skimmes: (x)","title":"Litterature"},{"location":"4-semester/PSS/exam/5-concurrency/#learning-goals","text":"After this lecture, you ... can define what a race condition is ... can explain how mutual exclusion can be used to avoid race conditions ... can explain strategies for achieving and implementing mutual exclusion ... can define the notions of mutex , semaphore , and monitor and explain how they work and where they are useful ... can explain how to synchronise two (or more) threads and why it may be necessary","title":"Learning Goals"},{"location":"4-semester/PSS/exam/5-concurrency/#noter","text":"","title":"Noter"},{"location":"4-semester/PSS/exam/5-concurrency/#threads","text":"Threads kan ses som en abstrahering i processer. En tr\u00e5d er lige som en seperate process, botset fra at de deler adress space, og kan derfor access samme data. Hver tr\u00e5d har sit eget s\u00e6t registre Hvis 2 tr\u00e5de k\u00f8rer p\u00e5 samme CPU skal der laves context switch hvis CPU skal skifte tr\u00e5d. Address space forbliver det samme Ligesom ved processer har vi en datastruktur til at gemme informationen. Her en thread control block (TCB) i stedet for PCB. Hver tr\u00e5d har sin egen stack: thread-local storage","title":"Threads"},{"location":"4-semester/PSS/exam/5-concurrency/#hvorfor-trade","text":"Parallelism En opgave splittes op, og kan k\u00f8re p\u00e5 flere CPU'er. Parallelization . I/O For at undg\u00e5 at programmet er blocked pga. langsom I/O. Mange server-based applikationer bruger tr\u00e5de. (web servere, databaser osv.)","title":"Hvorfor Tr\u00e5de?"},{"location":"4-semester/PSS/exam/5-concurrency/#race-conditions","text":"Hvis 2 tr\u00e5de arbejder p\u00e5 den samme variabel, kan der opst\u00e5 det der kaldes race condition eller data race . Hvis begge henter variabel k ind p\u00e5 samme med mens den er 42. S\u00e5 t\u00e6ller de den begge op, og resultatet bliver 43, selvom det burde v\u00e6re 44.","title":"Race Conditions"},{"location":"4-semester/PSS/exam/5-concurrency/#definitioner","text":"Race condition: N\u00e5r resultatet af en beregning afh\u00e6nger af relative speed af de individuelle tr\u00e5de Med andre ord: Resultatet afh\u00e6nger af interleaving af tr\u00e5dene. Sv\u00e6re at debug Critical region (critical section) : Program fragment s\u00e5rbar overfor race conditions \"Kritisk region\" Kritiske regioner skal eksekveres under mutual exclusion Mutual exclusion (mutex) : N\u00e5r kun en tr\u00e5d (blandt mange) kan tilg\u00e5 en given resource eller execute en specifik del af program-text. \"gensidig udelukkelse\" Atomic Event eller sekvens af events som sker uafbrudt.","title":"Definitioner"},{"location":"4-semester/PSS/exam/5-concurrency/#locks","text":"Critical region kode omgives a lock og unlock. 1 2 3 4 5 lock_t mutex ; ... lock ( & mutex ); balance = balance + 1 ; unlock ( & mutex ); Her er mutex lock variable (lock for short). Lock variablen holder lock'ens tilstand. En lock er enten available (unlocked/free) Ingen tr\u00e5de holder lock'en Eller aquired (locked/held) Pr\u00e6cis 1 tr\u00e5d holder lock'en Kald til lock() fors\u00f8ger at aquire l\u00e5sen. Hvis ingen tr\u00e5de holder l\u00e5sen (l\u00e5sen er available), vil tr\u00e5den aquire l\u00e5sen, og eksekvere den critical region. Hvis l\u00e5sen er held, vil tr\u00e5den vente p\u00e5 at l\u00e5sen bliver available Kald til unlock() vil g\u00f8re l\u00e5sen available igen, og hvis nogle tr\u00e5de venter p\u00e5 l\u00e5sen, vil de nu aquire l\u00e5sen, og eksekvere critical region.","title":"Locks"},{"location":"4-semester/PSS/exam/5-concurrency/#building-a-lock","text":"M\u00e5l: Correctness: Provides mutual exclusion Fairness : F\u00e5r alle tr\u00e5de et fair fors\u00f8g p\u00e5 at aquire. (Er der nogle tr\u00e5de der starver , og dermed aldrig f\u00e5r fat i l\u00e5sen) Performance Tidlig l\u00f8sning for single-processor systemer var at disable interrupts under critical region 1 2 void lock () { DisableInterrupts ();} void unlock () { EnableInterrupts ();} Simpelt at implementere Ulemper: Lader alle tr\u00e5de kalde priviligeret instruktion, kan abuses Virker ikke p\u00e5 multi-processor systemer. Hvis 2 tr\u00e5de k\u00f8rer p\u00e5 2 forskellige processorer, er det ligegyldigt om interrupts er disabled Kan g\u00f8re til at interrupts bliver tabt. Eksempelvis hvis CPU misser at disk l\u00e6sning er f\u00e6rdig Ineffektivt","title":"Building a Lock"},{"location":"4-semester/PSS/exam/5-concurrency/#lock-variables","text":"VIRKER IKKE Race conditions","title":"Lock Variables"},{"location":"4-semester/PSS/exam/5-concurrency/#dekkers-algorithm","text":"Ikke effektiv Compiler vil m\u00e5ske allokere nogle variable i registers (ikke delt mellem tr\u00e5de) Sv\u00e6r (umulig?) at scalere til mere end 2 tr\u00e5de.","title":"Dekkers Algorithm"},{"location":"4-semester/PSS/exam/5-concurrency/#spin-lock-med-test-and-set","text":"Hardware instruktion test-and-set (atomic exchange) G\u00f8r f\u00f8lgende atomically: 1 2 3 4 5 int TestAndSet ( int * old_ptr , int new ) { int old = * old_ptr ; // fetch old value at old_ptr * old_ptr = new ; // store 'new' into old_ptr return old ; // return the old value } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 typedef struct __lock_t { int flag ; } lock_t ; void init ( lock_t * lock ) { // 0: lock is available, 1: lock is held lock -> flag = 0 ; } void lock ( lock_t * lock ) { while ( TestAndSet ( & lock -> flag , 1 ) == 1 ) ; // spin-wait (do nothing) } void unlock ( lock_t * lock ) { lock -> flag = 0 ; } Spin-locks kr\u00e6ver preemptive scheduler (interrupter via timer). Ellers clogger de CPU","title":"Spin Lock Med Test-And-Set"},{"location":"4-semester/PSS/exam/5-concurrency/#evaluering-af-spin-locks","text":"Correctness : Ja, lader kun 1 tr\u00e5d eksekvere critical region af gangen. Fairness: Nej, ingen garanti for ikke at starve Perfomance Ikke s\u00e5 god p\u00e5 single CPU. Hvis tr\u00e5d bliver afbrudt midt i critcal region S\u00e5 kan alle andre tr\u00e5de, ventende, st\u00e5 at spinne i en hel time slice Rimelig god p\u00e5 multiple CPU, hvis antallet af CPUs ca. passer med antallet af tr\u00e5de","title":"Evaluering af Spin Locks"},{"location":"4-semester/PSS/exam/5-concurrency/#compare-and-swap","text":"Ny hardware instruktion, atomically : 1 2 3 4 5 6 int CompareAndSwap ( int * ptr , int expected , int new ) { int original = * ptr ; if ( original == expected ) * ptr = new ; return original ; } Implementering: 1 2 3 4 void lock ( lock_t * lock ) { while ( CompareAndSwap ( & lock -> flag , 0 , 1 ) == 1 ) ; // spin }","title":"Compare-And-Swap"},{"location":"4-semester/PSS/exam/5-concurrency/#fetch-and-add","text":"Hardware instruktion, atomically: 1 2 3 4 5 int FetchAndAdd ( int * ptr ) { int old = * ptr ; * ptr = old + 1 ; return old ; } Sikrer progress for alle tr\u00e5de. Laver en \"k\u00f8\".","title":"Fetch-And-Add"},{"location":"4-semester/PSS/exam/5-concurrency/#undga-tidsspild-ved-spinning","text":"Hvis der sker context-switch midt i en critical region, vil en tr\u00e5d der venter p\u00e5 lock, bare spinne uden at g\u00f8re noget, hele time slice. Kaldes busy wait . Mulig l\u00f8sning: yield. I stedt for at g\u00f8re ingenting, s\u00e5 kald yield Stadig ikke helt god. 100 threads der venter: RR scheduler, waster 99 cycles Stadig bedre end at wase 99 time slices","title":"Undg\u00e5 Tidsspild Ved Spinning"},{"location":"4-semester/PSS/exam/5-concurrency/#brug-af-queues-sleeping-instead-of-spinning","text":"","title":"Brug af Queues: Sleeping Instead of Spinning"},{"location":"4-semester/PSS/exam/5-concurrency/#semaphores","text":"Semaphore: Et objekt med en integer v\u00e6rdi der kan manipuleres med 2 routiner. I POSIX er det: sem_wait() sem_post() Initialiseres med 1 2 3 #include <semaphore.h> sem_t s ; sem_init ( & s , 0 , 1 ); // initilizere den til 1, arg2 (0) betyder at den er delt mellem tr\u00e5de i den samme process 1 2 3 4 5 6 7 8 9 int sem_wait ( sem_t * s ) { /* decrement the value of semaphore s by one wait if value of semaphore s is negative */ } int sem_post ( sem_t * s ) { /* increment the value of semaphore s by one if there are one or more threads waiting, wake one */ }","title":"Semaphores"},{"location":"4-semester/PSS/exam/5-concurrency/#binary-semaphore-lock","text":"1 2 3 4 5 6 sem_t m ; sem_init ( & m , 0 , 1 ); // initialize to 1; sem_wait ( & m ); // critical section here sem_post ( & m );","title":"Binary Semaphore (Lock)"},{"location":"4-semester/PSS/exam/5-concurrency/#semaphores-for-ordering","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 sem_t s ; void * child ( void * arg ) { printf ( \"child \\n \" ); sem_post ( & s ); // signal here: child is done return NULL ; } int main ( int argc , char * argv []) { sem_init ( & s , 0 , 0 ); printf ( \"parent: begin \\n \" ); pthread_t c ; Pthread_create ( & c , NULL , child , NULL ); sem_wait ( & s ); // wait here for child printf ( \"parent: end \\n \" ); return 0 ; }","title":"Semaphores For Ordering"},{"location":"4-semester/PSS/exam/5-concurrency/#the-producerconsumer-problem-bounded-buffer-problem","text":"En eller flere producer threads Genererer data items, placerer dem i en buffer En eller flere consumer threads Tager items fra bufferen og consumer dem Virker hvis MAX er 1 Hvis MAX er eks 10, f\u00e5r vi race condition","title":"The Producer/Consumer Problem (Bounded Buffer Problem)"},{"location":"4-semester/PSS/exam/5-concurrency/#readerwriter-lock","text":"","title":"Reader/Writer Lock"},{"location":"4-semester/PSS/exam/5-concurrency/#thread-throtting-with-semaphores","text":"Hvis vi vil kontrollere max antal tr\u00e5de, kan vi g\u00f8re det med semaphores. Kaldes throttling , en form for admission control Eksempelvis hvis en masse memory heavy arbejde, og der k\u00f8rer en masse tr\u00e5de p\u00e5 en gang, kan det v\u00e6re at fysisk memory overstiges. Vi kan initialisere en semaphore med antallet af max tr\u00e5de.","title":"Thread Throtting with Semaphores"},{"location":"4-semester/PSS/exam/6-concurrency-problems/","text":"6 - Concurrency Problems \u00b6 Keywords : Definition of deadlock, mutual exclusion, resource allocation graph, Coffman\u2019s conditions, solution strategies (prevention, avoidance, detection and recovery), how to achieve deadlock prevention (breaking Coffman\u2019s conditions), safe states and deadlock avoidance, deadlock detection and recovery, livelock, priority inversion. Litterature \u00b6 OSTEP Chapter 25, 26, (27), 28, (29), (30), 31, 32, (33), 34 Kapitler med parenteser skimmes: (x) Learning Goals \u00b6 After today\u2019s lecture, you: ... can define and explain the concept of deadlock ... can define and explain Coffman\u2019s conditions for deadlock ... can explain and use deadlock prevention strategies : prevention avoidance detect-and-recover ... can define and explain the following concepts livelock priority inversion ... can use the Dining Philosophers example to explain concurrency issues Noter \u00b6 Concurrency bugs in modern applications Non-Deadlock Bugs \u00b6 97 % af non-deadlock bugs er en af f\u00f8lgende: Atomicity Violation Order Violation Atomicity Violation \u00b6 Eksempel fra MySQL: 1 2 3 4 5 6 7 Thread 1 :: if ( thd -> proc_info ) { fputs ( thd -> proc_info , ... ); } Thread 2 :: thd -> proc_info = NULL Koden i thread 1, har en atomicity assumption , den tror at kode line 2-3 sker i et hug. Men hvis den laver null-check f\u00f8rst, og s\u00e5 tr\u00e5d 2 s\u00e6tter den til null f\u00f8r linje 3 k\u00f8res, s\u00e5 er der fejl L\u00f8sning: Locks rundt om lines 2-4 + line 7 Order-Violation \u00b6 1 float T ; Koden i thread 2, tror at T er initialiseret. L\u00f8sning : Kan l\u00f8ses med en semaphore Deadlock \u00b6 Se The Dining Philosphers Eksempel Sker hvis lad os sige Tr\u00e5d 1 holder en lock L1 og venter p\u00e5 L2 . Og Tr\u00e5d 2 holder L2 og venter p\u00e5 L1 Sker blandt andet pga store code bases og encapsulation Conditions for Deadlock \u00b6 Fire conditions skal holde f\u00f8r en deadlock kan ske: Mutual exclusion: Tr\u00e5de holder eksklusiv kontrol over en resource som de skal bruge. (eks. en lock) Hold-and-wait: Tr\u00e5de holder resourcer allokeret til dem, eks. aquired locks, mens de venter p\u00e5 flere resourcer (eks. locks de gerne vil aquire). No preemption: Resourcer (eks. locks) kan ikke blive forcefully removed fra tr\u00e5de der holder dem. Circular wait: Der eksistere en cirkular k\u00e6de af tr\u00e5de, s\u00e5 at hver tr\u00e5d holder en eller flere resourcer (eks. locks) som bliver requestet af den n\u00e6ste tr\u00e5d i k\u00e6den. Alle disse conditions skal overholdes f\u00f8r der kan ske en deadlock. Prevention \u00b6 Circular Wait \u00b6 Nok den mest praktiske prevention teknik. Undg\u00e5 circular wait. Den mest ligetil l\u00f8sning er total ordering . Hvis der kun er 2 locks i systemet, s\u00e5 s\u00f8rg for at eks L1 altid aquires f\u00f8r L2 . Kan v\u00e6re sv\u00e6rt i et system med mere end 2 locks. Partial ordering: En r\u00e6kkef\u00f8lge for forskellige locks. Kr\u00e6ver careful design. Er kun en convention, og en \"doven\" programm\u00f8r kan ignorere dem. Kr\u00e6ver dyb forst\u00e5else for kode basen. Hold-and-wait \u00b6 Kan undg\u00e5s ved at tage alle locks p\u00e5 en gang: 1 2 3 4 5 pthread_mutex_lock(prevention); // begin aquisition pthread_mutex_lock(L1); pthread_mutex_lock(L2); ... pthread_mutex_unlock(prevention); // end Problematisk: Encapsulation arbejder imod os. Kr\u00e6ver at vi ved hvilke locksder skal holdes, og at de skal aquires f\u00f8r tid. Neds\u00e6tter concurrency, da alle locks skal aquires p\u00e5 en gang i stedet for n\u00e5r de rigtigt skal bruges No Preemption \u00b6 Flere libraries tilbyder flexibelt s\u00e6t interfaces til at hj\u00e6lpe. Routinen: pthread_mutex_trylock() grabber enten lock'en hvis den er klar og returnerer sucess, eller returnerer error hvis l\u00e5sen holdes. Kan bruges til at lave en deadlock-free ordering-robust lock aquisition protocol: 1 2 3 4 5 6 top : pthread_mutex_lock ( L1 ); if ( pthread_mutex_trylock ( L2 ) != 0 ) { pthread_mutex_unlock ( L1 ); goto top ; } Et nyt problem er dog skabt: livelock Det kan ske (dog usansynligt) at 2 tr\u00e5de begge pr\u00f8ver og fejler gentagne gange p\u00e5 at aquire begge locks. Begge tr\u00e5de k\u00f8rer gennem koden om og om igen (alts\u00e5 ingen deadlock), men der sker intet, derfor livelock. En l\u00f8sning p\u00e5 dette kan v\u00e6re at inf\u00f8re et random delay. Dog: Problem igen, encapsulation Hvis koden har aquired andre resources, skal den huske at release dem igen. Tilf\u00f8jer ikke preemption. men bruger trylock til at lade udvilkeren tr\u00e6kke sig ud af et lock ownership. Mutual Exclusion \u00b6 Undg\u00e5 behovet for mutual exclusion. Ide: design data strukturer uden locks. ( lock-free and wait-free ) Brug hardware instruktioner. Eksempel vi bruger compare-and-swap til at atomically increment en v\u00e6rdi. 1 2 3 4 5 void AtomicIncrement ( int * value , int amount ) { do { int old = * value ; } while ( CompareAndSwap ( value , old , old + amount ) == 0 ); } Istedet for at aquire en lock, update og release lock, har vi en metode der bliver ved med at pr\u00f8ve at opdatere value med compare-and-swap. Deadlock Avoidance via Scheduling \u00b6 Lad os sige at vi har 2 CPU'er og 4 tr\u00e5de, samt 2 locks. Lad os sige at vi ved at de 2 locks aquires af tr\u00e5dene som vist i tabellen: En smart scheduler kan nu vide at s\u00e5 l\u00e6nge at T1 og T2 ikke k\u00f8re p\u00e5 samme tid kan der ikke ske deadlock. Eksempel p\u00e5 schedule: Andet eksempel: Static scheduling koster i performance Detect and Recover \u00b6 Sidste generelle strategi er at lade deadlocks v\u00e6re \"tilladet\", og s\u00e5 take action hvis en deadlock bliver detected. Eksempel, hvis OS fryser, kan det genstartes. Mange databasesystemer k\u00f8rer deadlock detection. Bygger en resource graf, og tjekker for cycles. Hvis det findes skal systemet genstartes. The Dining Philosophers \u00b6 5 filosoffer Mellem hver er en enkelt gaffel. En filosof kan enten t\u00e6nke eller spise T\u00e6nke: ingen gaffel Spise: Kr\u00e6ver 2 gafler Her er basic loop for hver philospher, hvor hver filosof har en thread idientifier p fra 0 til 4 1 2 3 4 5 6 while (1) { think(); get_forks(p); eat(); put_forks(p); } Udfordringen er at skrive routinerne get_forks() og put_forks() s\u00e5dan at: Der er ingen deadlock Ingen filosof starves H\u00f8j concurrency (s\u00e5 mange filosoffer som muligt spiser p\u00e5 samme tid) Vi bruger f\u00f8lgende helper funktioner: 1 2 int left ( int p ) { return p ; } int right ( int p ) { return ( p + 1 ) % 5 ; } N\u00e5r en filisof vil have gaflen til venstre kaldes left(p) og samme for h\u00f8jre right(p) . Vi har ogs\u00e5 nogle semaphores, lad os sige vi har 5, en for hver fork: sem_t forks[5] Umiddelbare L\u00f8sning (FORKERT) \u00b6 1 2 3 4 5 6 7 8 9 void get_forks ( int p ) { sem_wait ( & forks [ left ( p )]); sem_wait ( & forks [ right ( p )]); } void put_forks ( int p ) { sem_post ( & forks [ left ( p )]); sem_post ( & forks [ right ( p )]); } ==DETTE ER FORKERT== Det f\u00f8rer til deadlock . Hvis hver filosof tager deres gaffel til venstre f\u00f8r nogen n\u00e5r at tage deres gaffel til h\u00f8jre, vil de alle sammen v\u00e6re stuck ventende p\u00e5 at deres h\u00f8jre gaffel bliver ledig. L\u00f8sning: Breaking The Dependency \u00b6 Vi \u00e6ndre p\u00e5 hvordan gafler bliver taget hos mindst en af filosofferne. 1 2 3 4 5 6 7 8 9 void get_forks ( int p ) { if ( p == 4 ) { sem_wait ( & forks [ right ( p )]); sem_wait ( & forks [ left ( p )]); } else { sem_wait ( & forks [ left ( p )]); sem_wait ( & forks [ right ( p )]); } }","title":"6 - Concurrency Problems"},{"location":"4-semester/PSS/exam/6-concurrency-problems/#6-concurrency-problems","text":"Keywords : Definition of deadlock, mutual exclusion, resource allocation graph, Coffman\u2019s conditions, solution strategies (prevention, avoidance, detection and recovery), how to achieve deadlock prevention (breaking Coffman\u2019s conditions), safe states and deadlock avoidance, deadlock detection and recovery, livelock, priority inversion.","title":"6 - Concurrency Problems"},{"location":"4-semester/PSS/exam/6-concurrency-problems/#litterature","text":"OSTEP Chapter 25, 26, (27), 28, (29), (30), 31, 32, (33), 34 Kapitler med parenteser skimmes: (x)","title":"Litterature"},{"location":"4-semester/PSS/exam/6-concurrency-problems/#learning-goals","text":"After today\u2019s lecture, you: ... can define and explain the concept of deadlock ... can define and explain Coffman\u2019s conditions for deadlock ... can explain and use deadlock prevention strategies : prevention avoidance detect-and-recover ... can define and explain the following concepts livelock priority inversion ... can use the Dining Philosophers example to explain concurrency issues","title":"Learning Goals"},{"location":"4-semester/PSS/exam/6-concurrency-problems/#noter","text":"Concurrency bugs in modern applications","title":"Noter"},{"location":"4-semester/PSS/exam/6-concurrency-problems/#non-deadlock-bugs","text":"97 % af non-deadlock bugs er en af f\u00f8lgende: Atomicity Violation Order Violation","title":"Non-Deadlock Bugs"},{"location":"4-semester/PSS/exam/6-concurrency-problems/#atomicity-violation","text":"Eksempel fra MySQL: 1 2 3 4 5 6 7 Thread 1 :: if ( thd -> proc_info ) { fputs ( thd -> proc_info , ... ); } Thread 2 :: thd -> proc_info = NULL Koden i thread 1, har en atomicity assumption , den tror at kode line 2-3 sker i et hug. Men hvis den laver null-check f\u00f8rst, og s\u00e5 tr\u00e5d 2 s\u00e6tter den til null f\u00f8r linje 3 k\u00f8res, s\u00e5 er der fejl L\u00f8sning: Locks rundt om lines 2-4 + line 7","title":"Atomicity Violation"},{"location":"4-semester/PSS/exam/6-concurrency-problems/#order-violation","text":"1 float T ; Koden i thread 2, tror at T er initialiseret. L\u00f8sning : Kan l\u00f8ses med en semaphore","title":"Order-Violation"},{"location":"4-semester/PSS/exam/6-concurrency-problems/#deadlock","text":"Se The Dining Philosphers Eksempel Sker hvis lad os sige Tr\u00e5d 1 holder en lock L1 og venter p\u00e5 L2 . Og Tr\u00e5d 2 holder L2 og venter p\u00e5 L1 Sker blandt andet pga store code bases og encapsulation","title":"Deadlock"},{"location":"4-semester/PSS/exam/6-concurrency-problems/#conditions-for-deadlock","text":"Fire conditions skal holde f\u00f8r en deadlock kan ske: Mutual exclusion: Tr\u00e5de holder eksklusiv kontrol over en resource som de skal bruge. (eks. en lock) Hold-and-wait: Tr\u00e5de holder resourcer allokeret til dem, eks. aquired locks, mens de venter p\u00e5 flere resourcer (eks. locks de gerne vil aquire). No preemption: Resourcer (eks. locks) kan ikke blive forcefully removed fra tr\u00e5de der holder dem. Circular wait: Der eksistere en cirkular k\u00e6de af tr\u00e5de, s\u00e5 at hver tr\u00e5d holder en eller flere resourcer (eks. locks) som bliver requestet af den n\u00e6ste tr\u00e5d i k\u00e6den. Alle disse conditions skal overholdes f\u00f8r der kan ske en deadlock.","title":"Conditions for Deadlock"},{"location":"4-semester/PSS/exam/6-concurrency-problems/#prevention","text":"","title":"Prevention"},{"location":"4-semester/PSS/exam/6-concurrency-problems/#circular-wait","text":"Nok den mest praktiske prevention teknik. Undg\u00e5 circular wait. Den mest ligetil l\u00f8sning er total ordering . Hvis der kun er 2 locks i systemet, s\u00e5 s\u00f8rg for at eks L1 altid aquires f\u00f8r L2 . Kan v\u00e6re sv\u00e6rt i et system med mere end 2 locks. Partial ordering: En r\u00e6kkef\u00f8lge for forskellige locks. Kr\u00e6ver careful design. Er kun en convention, og en \"doven\" programm\u00f8r kan ignorere dem. Kr\u00e6ver dyb forst\u00e5else for kode basen.","title":"Circular Wait"},{"location":"4-semester/PSS/exam/6-concurrency-problems/#hold-and-wait","text":"Kan undg\u00e5s ved at tage alle locks p\u00e5 en gang: 1 2 3 4 5 pthread_mutex_lock(prevention); // begin aquisition pthread_mutex_lock(L1); pthread_mutex_lock(L2); ... pthread_mutex_unlock(prevention); // end Problematisk: Encapsulation arbejder imod os. Kr\u00e6ver at vi ved hvilke locksder skal holdes, og at de skal aquires f\u00f8r tid. Neds\u00e6tter concurrency, da alle locks skal aquires p\u00e5 en gang i stedet for n\u00e5r de rigtigt skal bruges","title":"Hold-and-wait"},{"location":"4-semester/PSS/exam/6-concurrency-problems/#no-preemption","text":"Flere libraries tilbyder flexibelt s\u00e6t interfaces til at hj\u00e6lpe. Routinen: pthread_mutex_trylock() grabber enten lock'en hvis den er klar og returnerer sucess, eller returnerer error hvis l\u00e5sen holdes. Kan bruges til at lave en deadlock-free ordering-robust lock aquisition protocol: 1 2 3 4 5 6 top : pthread_mutex_lock ( L1 ); if ( pthread_mutex_trylock ( L2 ) != 0 ) { pthread_mutex_unlock ( L1 ); goto top ; } Et nyt problem er dog skabt: livelock Det kan ske (dog usansynligt) at 2 tr\u00e5de begge pr\u00f8ver og fejler gentagne gange p\u00e5 at aquire begge locks. Begge tr\u00e5de k\u00f8rer gennem koden om og om igen (alts\u00e5 ingen deadlock), men der sker intet, derfor livelock. En l\u00f8sning p\u00e5 dette kan v\u00e6re at inf\u00f8re et random delay. Dog: Problem igen, encapsulation Hvis koden har aquired andre resources, skal den huske at release dem igen. Tilf\u00f8jer ikke preemption. men bruger trylock til at lade udvilkeren tr\u00e6kke sig ud af et lock ownership.","title":"No Preemption"},{"location":"4-semester/PSS/exam/6-concurrency-problems/#mutual-exclusion","text":"Undg\u00e5 behovet for mutual exclusion. Ide: design data strukturer uden locks. ( lock-free and wait-free ) Brug hardware instruktioner. Eksempel vi bruger compare-and-swap til at atomically increment en v\u00e6rdi. 1 2 3 4 5 void AtomicIncrement ( int * value , int amount ) { do { int old = * value ; } while ( CompareAndSwap ( value , old , old + amount ) == 0 ); } Istedet for at aquire en lock, update og release lock, har vi en metode der bliver ved med at pr\u00f8ve at opdatere value med compare-and-swap.","title":"Mutual Exclusion"},{"location":"4-semester/PSS/exam/6-concurrency-problems/#deadlock-avoidance-via-scheduling","text":"Lad os sige at vi har 2 CPU'er og 4 tr\u00e5de, samt 2 locks. Lad os sige at vi ved at de 2 locks aquires af tr\u00e5dene som vist i tabellen: En smart scheduler kan nu vide at s\u00e5 l\u00e6nge at T1 og T2 ikke k\u00f8re p\u00e5 samme tid kan der ikke ske deadlock. Eksempel p\u00e5 schedule: Andet eksempel: Static scheduling koster i performance","title":"Deadlock Avoidance via Scheduling"},{"location":"4-semester/PSS/exam/6-concurrency-problems/#detect-and-recover","text":"Sidste generelle strategi er at lade deadlocks v\u00e6re \"tilladet\", og s\u00e5 take action hvis en deadlock bliver detected. Eksempel, hvis OS fryser, kan det genstartes. Mange databasesystemer k\u00f8rer deadlock detection. Bygger en resource graf, og tjekker for cycles. Hvis det findes skal systemet genstartes.","title":"Detect and Recover"},{"location":"4-semester/PSS/exam/6-concurrency-problems/#the-dining-philosophers","text":"5 filosoffer Mellem hver er en enkelt gaffel. En filosof kan enten t\u00e6nke eller spise T\u00e6nke: ingen gaffel Spise: Kr\u00e6ver 2 gafler Her er basic loop for hver philospher, hvor hver filosof har en thread idientifier p fra 0 til 4 1 2 3 4 5 6 while (1) { think(); get_forks(p); eat(); put_forks(p); } Udfordringen er at skrive routinerne get_forks() og put_forks() s\u00e5dan at: Der er ingen deadlock Ingen filosof starves H\u00f8j concurrency (s\u00e5 mange filosoffer som muligt spiser p\u00e5 samme tid) Vi bruger f\u00f8lgende helper funktioner: 1 2 int left ( int p ) { return p ; } int right ( int p ) { return ( p + 1 ) % 5 ; } N\u00e5r en filisof vil have gaflen til venstre kaldes left(p) og samme for h\u00f8jre right(p) . Vi har ogs\u00e5 nogle semaphores, lad os sige vi har 5, en for hver fork: sem_t forks[5]","title":"The Dining Philosophers"},{"location":"4-semester/PSS/exam/6-concurrency-problems/#umiddelbare-lsning-forkert","text":"1 2 3 4 5 6 7 8 9 void get_forks ( int p ) { sem_wait ( & forks [ left ( p )]); sem_wait ( & forks [ right ( p )]); } void put_forks ( int p ) { sem_post ( & forks [ left ( p )]); sem_post ( & forks [ right ( p )]); } ==DETTE ER FORKERT== Det f\u00f8rer til deadlock . Hvis hver filosof tager deres gaffel til venstre f\u00f8r nogen n\u00e5r at tage deres gaffel til h\u00f8jre, vil de alle sammen v\u00e6re stuck ventende p\u00e5 at deres h\u00f8jre gaffel bliver ledig.","title":"Umiddelbare L\u00f8sning (FORKERT)"},{"location":"4-semester/PSS/exam/6-concurrency-problems/#lsning-breaking-the-dependency","text":"Vi \u00e6ndre p\u00e5 hvordan gafler bliver taget hos mindst en af filosofferne. 1 2 3 4 5 6 7 8 9 void get_forks ( int p ) { if ( p == 4 ) { sem_wait ( & forks [ right ( p )]); sem_wait ( & forks [ left ( p )]); } else { sem_wait ( & forks [ left ( p )]); sem_wait ( & forks [ right ( p )]); } }","title":"L\u00f8sning: Breaking The Dependency"},{"location":"4-semester/PSS/exam/7-io-and-device-drivers/","text":"7 - I/O and Device Drivers \u00b6 Keywords : Types of I/O (programmed, interrupt-driven, DMA), implementation of I/O (as system calls), definition and implementation of system call, device drivers. Litterature \u00b6 OSTEP Chapter 35, 36, 37, (38), (39), (40), [41], [42], [43], [44], [45] Kapitler med parenteser skimmes: (x) Kapitler med kantede paranteser er optional: [x] Noter \u00b6 System Arkitektur \u00b6 \"Klassisk\" diagram af et system: Hvorfor har vi hierakisk struktur? Fysik Jo hurtigere en bus er, jo kortere skal den v\u00e6re Pris Det er dyrt at lave high performance busses Intel's Z270 Chipset: CPU connecter med en I/O chip via Intel's proprietary DMI (Direct Media Interface) Resten af devices connecter til I/O chuppen via forskellige interconnects. Harddrives via eSATA ATA - AT Attachement SATA - Serial ATA eSATA - external SATA USB (Universal Serial Bus) connections Low performance devices Higher performance devices gennem PCIe (Peripheral Component Interconnect Express) Network NVMe storage devices En Canonical Device \u00b6 Interface som pr\u00e6senteres til systemet. Internal structure . Denne enhed best\u00e5r af 3 registre: status : l\u00e6ses for at se enhedens nuv\u00e6rende status command : fort\u00e6ller enheden hvilken opgave den skal udf\u00f8re data: til at videregive data til enheden, eller til at modtage data fra enheden. Typisk simpel interaktion som OS kan have med enheden: 1 2 3 4 5 6 7 While (STATUS == BUSY) ; // wait until device is not busy Write data to the DATA register Write command to the COMMAND register (starts device and executes the command) While (STATUS == BUSY) ; // wait ontil devices is done with your request OS venter p\u00e5 at enheden er klar ved at l\u00e6se STATUS ( polling the device) (line 1-2) OS sender data til DATA (line 3) N\u00e5r main CPU er involveret i data flytning kaldes det programmed I/O (PIO) OS skriver kommando til COMMAND Fort\u00e6ller implicit til enheden b\u00e5de at data er tilstede og at den skal begynde kommandoen OS venter p\u00e5 enheden f\u00e6rdigg\u00f8rer arbejdet ved polling Simpel men ineffektiv protokol. CPU spilder tid ved polling. L\u00f8sning: Interrupts \u00b6 OS laver I/O request, putter processen i sleep, laver context switch. N\u00e5r I/O enheden er f\u00e6rdig, laver den et hardware interrupt . Interrupt handler aka interrupt service routine (ISR) tager over. Ikke altid den bedste l\u00f8sning. Hvis en enhed er meget hurtig, vil interrupts s\u00e6nke farten. Polling bedre. Hybrid: poll'er lidt i starten og bruger interrupts hvis ikke enheden er f\u00e6rdig. Networks Kan v\u00e6re skidt at bruge interrupts, hvis der kommer en masse packets der genererer interrupts. OS kan g\u00e5 i livelock hvor det ikke laver andet end at behandle interrupts Polling giver lidt mere kontrol Coalescing er en anden optimering. En device der skal interrupt, venter f\u00f8rst med at sende. Mens den venter, kan andre requests klares, og flere interrupts kan samles (coalesced). Effektiv Data Overf\u00f8rsel Med DMA \u00b6 Det kr\u00e6ver tid for CPU'en at overf\u00f8re data. (Markeret med gr\u00e5 'c' bokse) En l\u00f8sning p\u00e5 dette problem er Direct Memory Access (DMA) En DMA engine, er en specifik enhed i systemet der orkestrerer overf\u00f8rseler mellem enheder og main memory uden meget CPU intervention. OS programmerer DMA engine ved at fort\u00e6lle den hvor dataen befinder sig, hvor meget data, og hvor det skal sendes hen. Device Interaction \u00b6 2 forskellige m\u00e5der at kommunikere med enheder er udviklet over tid. Explicit I/O instructions . Hardware instruktioner der specificere en m\u00e5de for OS at sende data til device registre. x86 har in og out instruktioner Caller specificerer register med data og en specifik port . Instruktioner er priviligerede . Memory-mapped I/O Hardware g\u00f8r device registre ledige som var de memory locations. OS kalder load eller store til adressen, og hardware router til enheden i stedet for main memory. Device Drivers \u00b6 Abstraction Et stykke software i OS, device driver kender i detajler hvordan en enhed fungerer. Linux File System Stack: Et file system, er ligeglad med hvilken disk den bruger. Raw interface er ogs\u00e5 stillet til r\u00e5dighed. Lader specielle applikationer som file-system checker eller disk defragmentation tools l\u00e6se og skrive direkte. Kan have ulemper: Hvis en device har specielle capabilities, men skal have en generic interface, g\u00e5r disse capabilities til spilde. Studier viser at over 70% af Linux OS code er device drives. IDE Disk \u00b6 Interface: 4 registre: Control Command block Status Error Tilg\u00e5s ved at l\u00e6se de specifikke I/O adresser med in og out instruktionerne i x86. Basic protocol er som f\u00f8lger: Wait for drive to be ready : L\u00e6s Status Register until drevet er READY og ikke BUSY Skriv parametre til command registre : Skriv sector count, logical block address (LBA) p\u00e5 sektorerne der skal tilg\u00e5s, og drive number (master=0x00 eller slave=0x10) til command registre Start I/O : ved at issue read/write til command registre. Data transfer (ved writes): vent til drive status er READY og DRQ (drive request for data). Skriv data til data port. H\u00e5ndter interrupts Error handling: efter hver operation, l\u00e6s status register. Hvis ERROR bit er 1 l\u00e6s error register for detaljer. xv6 IDE drive i xv6 findes i ide.c Hard Disk Drive \u00b6 Best\u00e5r af et stort antal sektorer (512-byte blocks) Som hver kan l\u00e6ses eller skrives Numereret fra 0 til n-1 n-1 p\u00e5 en disk med n n sektorer Kan ses som et array af sektorer 0 til n-1 n-1 er address space for drevet Multi-sektor operationer er mulige. Mange fil-systemer l\u00e6ser eller skriver 4KB af gangen. Eneste garanti er at 512-byte writes er atomic Hvis der sker power loss, er det m\u00e5ske kun en del af en skrivning der f\u00e6rdigg\u00f8res ( torn write ) Assumptions som klienter af disk dreve g\u00f8r sig (unwritten contract) Access af 2 blocks t\u00e6t p\u00e5 hinanden er hurtiger end 2 blocks langt fra hinanden Access af blocks i en sammenh\u00e6ngende chunk er hurtigst Disk Geometri \u00b6 Vi har en platter en cirkul\u00e6r plade med en h\u00e5rd overflade Hver platter har 2 sider ( surface ) En disk har en eller flere platters Ofte lavet af aluminium Coated med tyndt magnetisk lag Platters er bundet sammen omkring spindle Som er connected til en moter Spinner ofte mellem 7,200 og 15,000 RPM (rotations per minute) 10,000 RPM \\rightarrow \\rightarrow 1 rotation tager ca 6 ms Data er encoded p\u00e5 hver surface i koncentriske cirkler af sektorer. ( track ) Disk head l\u00e6ser og skriver sektorer ved at l\u00e6se eller p\u00e5virke magnetic patterns 1 per surface Attached til en disk arm Simple Disk Drive \u00b6 Vi tager udgangspunkt i figur 37.2 ovenfor. Hvis vi laver request til block 0. S\u00e5 skal disken bare vente p\u00e5 at 0 roterer under disk head. Kaldes rotational delay aka rotation delay Moderne diske har flere millioner tracks. Eksempel med flere tracks: Request til sektor 11. F\u00f8rst skal disk armen bev\u00e6ge sig til det korrekte track. Kaldet et seek . Seeks sammen med rotations er de dyreste disk operationer. Seek har flere faser: acceleration coasting deceleration settling : settling time er signifikant e.g., 0.5 til 2 ms drev skal v\u00e6re sikker p\u00e5 at finde det rigtige track. N\u00e5r sector 11 passerer under disk head kan sidste fase af I/O ske ( transfer ). Ofte bruges track skew S\u00e5 n\u00e5r head repositioner armen, passer det bedre. Ydre tracks har ofte flere sektorer (pga geometri). Kaldes multi-zoned disk drives: Disk er organiseret i mange zoner. Hver zone har det samme antal sektorer per track. Ydre zoner har flere sektoerer end indre zoner. Cache \u00b6 Ogs\u00e5 kaldet track buffer . Lille stykke memory, ofte omkring 8- eller 16 MB Disk skal v\u00e6lge ved writes, om den vil anerkende et write n\u00e5r den har puttet data i memory, eller f\u00f8rst n\u00e5r den har skrevet p\u00e5 disk? Write back: n\u00e5r det er skrevet i memory (ogs\u00e5 kaldet immediate repporting ) Kan f\u00e5 drevet til at se hurtigere ud, men kan v\u00e6re farligt hvis filsystemet kr\u00e6ver at data skrives i en bestemt r\u00e6kkef\u00f8lge. Write through : n\u00e5r der er skrevet til disk. I/O Time \u00b6 I/O Time: $$ T_{I/O}=T_{seek}+T_{rotation}+T_{transfer} $$ Rate of I/O: $$ R_{I/O}=\\frac{Size_{Transfer}}{T_{I/O}} $$ Eksempler \u00b6 Random workload laver sm\u00e5 reads (eg 4KB) til random lokationer Sequential workload l\u00e6ser et stort antal sammenh\u00e6ngende sektorer Disk Scheduling \u00b6 Da I/O er dyrt spiller OS en rolle i scheduling. Disk scheduler I/O \"job\"-tid er nogenlunde kendt i mods\u00e6tning til job-scheduling. Pr\u00f8ver at f\u00f8lge principle of SJF (shortest job first) SSTF: Shortest Seek Time First \u00b6 AKA shortest-seek-first (SSF) . Rangerer k\u00f8en af I/O request baseret p\u00e5 track. V\u00e6lger den request p\u00e5 det n\u00e6rmeste track f\u00f8rst. Kender dog ikke geometrien af disk, ses som array af blocks. Kan derfor implementere nearest-block-first (NBF) i stedet for SSTF. Kan f\u00f8re til starvation , hvis der kommer str\u00f8m af requests til de indre track eksempelvis. Elevator (AKA SCAN or C-SCAN) \u00b6 Algoritmen bev\u00e6ger sig frem og tilbage over disken, og h\u00e5ndterer request i r\u00e6kkef\u00f8lge over tracks. Et enkelt pass over disken kaldes et sweep . Varianter: F-SCAN: fryser k\u00f8en n\u00e5r den sweeper. Requests der kommer ind under sweep s\u00e6ttes i en sekund\u00e6r k\u00f8 Undg\u00e5r at far-away requests bliver starved ved at delay sent-ankomne (men t\u00e6ttere p\u00e5) requests. C-SCAN : Circular SCAN I stedet for frem og til bage, k\u00f8re den i \"cirkel\". Mere fair over for de midterste tracks. Hverken SCAN eller SSTF er den bedste scheduling technology, de adhere ikke til principle of SJF som de kunne. De ignorerer rotation. SPTF: Shortest Positioning Time First \u00b6 Hvis det tager l\u00e6ngere tid at rotere, giver det mere mening at lave det lange seek, ex 8, s\u00e5 den ikke skal rotere s\u00e5 meget. P\u00e5 moderne drives er seek og rotation ca. ens. Derfor er SPTF brugbart og increaser performance. Sv\u00e6rt at implementere i OS, s\u00e5 ofte implementeret inde i drevet. Andre Scheduling Problemer \u00b6 I/O Merging : Der kommer 3 request til 33, s\u00e5 8, s\u00e5 34. Scheduler burde her merge 33 og 34 indtil en two-block request. Hvor lang tid skal OS vente med at sende request? Naiv approach: work-conserving: send s\u00e5 snart der er en I/O. Harddisk arbejder hele tiden. Research om anticpatory disk scheduling viser at det kan betale sige at vente lidt. Kaldet non-work-conserving","title":"7 - I/O and Device Drivers"},{"location":"4-semester/PSS/exam/7-io-and-device-drivers/#7-io-and-device-drivers","text":"Keywords : Types of I/O (programmed, interrupt-driven, DMA), implementation of I/O (as system calls), definition and implementation of system call, device drivers.","title":"7 - I/O and Device Drivers"},{"location":"4-semester/PSS/exam/7-io-and-device-drivers/#litterature","text":"OSTEP Chapter 35, 36, 37, (38), (39), (40), [41], [42], [43], [44], [45] Kapitler med parenteser skimmes: (x) Kapitler med kantede paranteser er optional: [x]","title":"Litterature"},{"location":"4-semester/PSS/exam/7-io-and-device-drivers/#noter","text":"","title":"Noter"},{"location":"4-semester/PSS/exam/7-io-and-device-drivers/#system-arkitektur","text":"\"Klassisk\" diagram af et system: Hvorfor har vi hierakisk struktur? Fysik Jo hurtigere en bus er, jo kortere skal den v\u00e6re Pris Det er dyrt at lave high performance busses Intel's Z270 Chipset: CPU connecter med en I/O chip via Intel's proprietary DMI (Direct Media Interface) Resten af devices connecter til I/O chuppen via forskellige interconnects. Harddrives via eSATA ATA - AT Attachement SATA - Serial ATA eSATA - external SATA USB (Universal Serial Bus) connections Low performance devices Higher performance devices gennem PCIe (Peripheral Component Interconnect Express) Network NVMe storage devices","title":"System Arkitektur"},{"location":"4-semester/PSS/exam/7-io-and-device-drivers/#en-canonical-device","text":"Interface som pr\u00e6senteres til systemet. Internal structure . Denne enhed best\u00e5r af 3 registre: status : l\u00e6ses for at se enhedens nuv\u00e6rende status command : fort\u00e6ller enheden hvilken opgave den skal udf\u00f8re data: til at videregive data til enheden, eller til at modtage data fra enheden. Typisk simpel interaktion som OS kan have med enheden: 1 2 3 4 5 6 7 While (STATUS == BUSY) ; // wait until device is not busy Write data to the DATA register Write command to the COMMAND register (starts device and executes the command) While (STATUS == BUSY) ; // wait ontil devices is done with your request OS venter p\u00e5 at enheden er klar ved at l\u00e6se STATUS ( polling the device) (line 1-2) OS sender data til DATA (line 3) N\u00e5r main CPU er involveret i data flytning kaldes det programmed I/O (PIO) OS skriver kommando til COMMAND Fort\u00e6ller implicit til enheden b\u00e5de at data er tilstede og at den skal begynde kommandoen OS venter p\u00e5 enheden f\u00e6rdigg\u00f8rer arbejdet ved polling Simpel men ineffektiv protokol. CPU spilder tid ved polling.","title":"En Canonical Device"},{"location":"4-semester/PSS/exam/7-io-and-device-drivers/#lsning-interrupts","text":"OS laver I/O request, putter processen i sleep, laver context switch. N\u00e5r I/O enheden er f\u00e6rdig, laver den et hardware interrupt . Interrupt handler aka interrupt service routine (ISR) tager over. Ikke altid den bedste l\u00f8sning. Hvis en enhed er meget hurtig, vil interrupts s\u00e6nke farten. Polling bedre. Hybrid: poll'er lidt i starten og bruger interrupts hvis ikke enheden er f\u00e6rdig. Networks Kan v\u00e6re skidt at bruge interrupts, hvis der kommer en masse packets der genererer interrupts. OS kan g\u00e5 i livelock hvor det ikke laver andet end at behandle interrupts Polling giver lidt mere kontrol Coalescing er en anden optimering. En device der skal interrupt, venter f\u00f8rst med at sende. Mens den venter, kan andre requests klares, og flere interrupts kan samles (coalesced).","title":"L\u00f8sning: Interrupts"},{"location":"4-semester/PSS/exam/7-io-and-device-drivers/#effektiv-data-overfrsel-med-dma","text":"Det kr\u00e6ver tid for CPU'en at overf\u00f8re data. (Markeret med gr\u00e5 'c' bokse) En l\u00f8sning p\u00e5 dette problem er Direct Memory Access (DMA) En DMA engine, er en specifik enhed i systemet der orkestrerer overf\u00f8rseler mellem enheder og main memory uden meget CPU intervention. OS programmerer DMA engine ved at fort\u00e6lle den hvor dataen befinder sig, hvor meget data, og hvor det skal sendes hen.","title":"Effektiv Data Overf\u00f8rsel Med DMA"},{"location":"4-semester/PSS/exam/7-io-and-device-drivers/#device-interaction","text":"2 forskellige m\u00e5der at kommunikere med enheder er udviklet over tid. Explicit I/O instructions . Hardware instruktioner der specificere en m\u00e5de for OS at sende data til device registre. x86 har in og out instruktioner Caller specificerer register med data og en specifik port . Instruktioner er priviligerede . Memory-mapped I/O Hardware g\u00f8r device registre ledige som var de memory locations. OS kalder load eller store til adressen, og hardware router til enheden i stedet for main memory.","title":"Device Interaction"},{"location":"4-semester/PSS/exam/7-io-and-device-drivers/#device-drivers","text":"Abstraction Et stykke software i OS, device driver kender i detajler hvordan en enhed fungerer. Linux File System Stack: Et file system, er ligeglad med hvilken disk den bruger. Raw interface er ogs\u00e5 stillet til r\u00e5dighed. Lader specielle applikationer som file-system checker eller disk defragmentation tools l\u00e6se og skrive direkte. Kan have ulemper: Hvis en device har specielle capabilities, men skal have en generic interface, g\u00e5r disse capabilities til spilde. Studier viser at over 70% af Linux OS code er device drives.","title":"Device Drivers"},{"location":"4-semester/PSS/exam/7-io-and-device-drivers/#ide-disk","text":"Interface: 4 registre: Control Command block Status Error Tilg\u00e5s ved at l\u00e6se de specifikke I/O adresser med in og out instruktionerne i x86. Basic protocol er som f\u00f8lger: Wait for drive to be ready : L\u00e6s Status Register until drevet er READY og ikke BUSY Skriv parametre til command registre : Skriv sector count, logical block address (LBA) p\u00e5 sektorerne der skal tilg\u00e5s, og drive number (master=0x00 eller slave=0x10) til command registre Start I/O : ved at issue read/write til command registre. Data transfer (ved writes): vent til drive status er READY og DRQ (drive request for data). Skriv data til data port. H\u00e5ndter interrupts Error handling: efter hver operation, l\u00e6s status register. Hvis ERROR bit er 1 l\u00e6s error register for detaljer. xv6 IDE drive i xv6 findes i ide.c","title":"IDE Disk"},{"location":"4-semester/PSS/exam/7-io-and-device-drivers/#hard-disk-drive","text":"Best\u00e5r af et stort antal sektorer (512-byte blocks) Som hver kan l\u00e6ses eller skrives Numereret fra 0 til n-1 n-1 p\u00e5 en disk med n n sektorer Kan ses som et array af sektorer 0 til n-1 n-1 er address space for drevet Multi-sektor operationer er mulige. Mange fil-systemer l\u00e6ser eller skriver 4KB af gangen. Eneste garanti er at 512-byte writes er atomic Hvis der sker power loss, er det m\u00e5ske kun en del af en skrivning der f\u00e6rdigg\u00f8res ( torn write ) Assumptions som klienter af disk dreve g\u00f8r sig (unwritten contract) Access af 2 blocks t\u00e6t p\u00e5 hinanden er hurtiger end 2 blocks langt fra hinanden Access af blocks i en sammenh\u00e6ngende chunk er hurtigst","title":"Hard Disk Drive"},{"location":"4-semester/PSS/exam/7-io-and-device-drivers/#disk-geometri","text":"Vi har en platter en cirkul\u00e6r plade med en h\u00e5rd overflade Hver platter har 2 sider ( surface ) En disk har en eller flere platters Ofte lavet af aluminium Coated med tyndt magnetisk lag Platters er bundet sammen omkring spindle Som er connected til en moter Spinner ofte mellem 7,200 og 15,000 RPM (rotations per minute) 10,000 RPM \\rightarrow \\rightarrow 1 rotation tager ca 6 ms Data er encoded p\u00e5 hver surface i koncentriske cirkler af sektorer. ( track ) Disk head l\u00e6ser og skriver sektorer ved at l\u00e6se eller p\u00e5virke magnetic patterns 1 per surface Attached til en disk arm","title":"Disk Geometri"},{"location":"4-semester/PSS/exam/7-io-and-device-drivers/#simple-disk-drive","text":"Vi tager udgangspunkt i figur 37.2 ovenfor. Hvis vi laver request til block 0. S\u00e5 skal disken bare vente p\u00e5 at 0 roterer under disk head. Kaldes rotational delay aka rotation delay Moderne diske har flere millioner tracks. Eksempel med flere tracks: Request til sektor 11. F\u00f8rst skal disk armen bev\u00e6ge sig til det korrekte track. Kaldet et seek . Seeks sammen med rotations er de dyreste disk operationer. Seek har flere faser: acceleration coasting deceleration settling : settling time er signifikant e.g., 0.5 til 2 ms drev skal v\u00e6re sikker p\u00e5 at finde det rigtige track. N\u00e5r sector 11 passerer under disk head kan sidste fase af I/O ske ( transfer ). Ofte bruges track skew S\u00e5 n\u00e5r head repositioner armen, passer det bedre. Ydre tracks har ofte flere sektorer (pga geometri). Kaldes multi-zoned disk drives: Disk er organiseret i mange zoner. Hver zone har det samme antal sektorer per track. Ydre zoner har flere sektoerer end indre zoner.","title":"Simple Disk Drive"},{"location":"4-semester/PSS/exam/7-io-and-device-drivers/#cache","text":"Ogs\u00e5 kaldet track buffer . Lille stykke memory, ofte omkring 8- eller 16 MB Disk skal v\u00e6lge ved writes, om den vil anerkende et write n\u00e5r den har puttet data i memory, eller f\u00f8rst n\u00e5r den har skrevet p\u00e5 disk? Write back: n\u00e5r det er skrevet i memory (ogs\u00e5 kaldet immediate repporting ) Kan f\u00e5 drevet til at se hurtigere ud, men kan v\u00e6re farligt hvis filsystemet kr\u00e6ver at data skrives i en bestemt r\u00e6kkef\u00f8lge. Write through : n\u00e5r der er skrevet til disk.","title":"Cache"},{"location":"4-semester/PSS/exam/7-io-and-device-drivers/#io-time","text":"I/O Time: $$ T_{I/O}=T_{seek}+T_{rotation}+T_{transfer} $$ Rate of I/O: $$ R_{I/O}=\\frac{Size_{Transfer}}{T_{I/O}} $$","title":"I/O Time"},{"location":"4-semester/PSS/exam/7-io-and-device-drivers/#eksempler","text":"Random workload laver sm\u00e5 reads (eg 4KB) til random lokationer Sequential workload l\u00e6ser et stort antal sammenh\u00e6ngende sektorer","title":"Eksempler"},{"location":"4-semester/PSS/exam/7-io-and-device-drivers/#disk-scheduling","text":"Da I/O er dyrt spiller OS en rolle i scheduling. Disk scheduler I/O \"job\"-tid er nogenlunde kendt i mods\u00e6tning til job-scheduling. Pr\u00f8ver at f\u00f8lge principle of SJF (shortest job first)","title":"Disk Scheduling"},{"location":"4-semester/PSS/exam/7-io-and-device-drivers/#sstf-shortest-seek-time-first","text":"AKA shortest-seek-first (SSF) . Rangerer k\u00f8en af I/O request baseret p\u00e5 track. V\u00e6lger den request p\u00e5 det n\u00e6rmeste track f\u00f8rst. Kender dog ikke geometrien af disk, ses som array af blocks. Kan derfor implementere nearest-block-first (NBF) i stedet for SSTF. Kan f\u00f8re til starvation , hvis der kommer str\u00f8m af requests til de indre track eksempelvis.","title":"SSTF: Shortest Seek Time First"},{"location":"4-semester/PSS/exam/7-io-and-device-drivers/#elevator-aka-scan-or-c-scan","text":"Algoritmen bev\u00e6ger sig frem og tilbage over disken, og h\u00e5ndterer request i r\u00e6kkef\u00f8lge over tracks. Et enkelt pass over disken kaldes et sweep . Varianter: F-SCAN: fryser k\u00f8en n\u00e5r den sweeper. Requests der kommer ind under sweep s\u00e6ttes i en sekund\u00e6r k\u00f8 Undg\u00e5r at far-away requests bliver starved ved at delay sent-ankomne (men t\u00e6ttere p\u00e5) requests. C-SCAN : Circular SCAN I stedet for frem og til bage, k\u00f8re den i \"cirkel\". Mere fair over for de midterste tracks. Hverken SCAN eller SSTF er den bedste scheduling technology, de adhere ikke til principle of SJF som de kunne. De ignorerer rotation.","title":"Elevator (AKA SCAN or C-SCAN)"},{"location":"4-semester/PSS/exam/7-io-and-device-drivers/#sptf-shortest-positioning-time-first","text":"Hvis det tager l\u00e6ngere tid at rotere, giver det mere mening at lave det lange seek, ex 8, s\u00e5 den ikke skal rotere s\u00e5 meget. P\u00e5 moderne drives er seek og rotation ca. ens. Derfor er SPTF brugbart og increaser performance. Sv\u00e6rt at implementere i OS, s\u00e5 ofte implementeret inde i drevet.","title":"SPTF: Shortest Positioning Time First"},{"location":"4-semester/PSS/exam/7-io-and-device-drivers/#andre-scheduling-problemer","text":"I/O Merging : Der kommer 3 request til 33, s\u00e5 8, s\u00e5 34. Scheduler burde her merge 33 og 34 indtil en two-block request. Hvor lang tid skal OS vente med at sende request? Naiv approach: work-conserving: send s\u00e5 snart der er en I/O. Harddisk arbejder hele tiden. Research om anticpatory disk scheduling viser at det kan betale sige at vente lidt. Kaldet non-work-conserving","title":"Andre Scheduling Problemer"},{"location":"4-semester/PSS/exam/8-xv6/","text":"8 - XV6 Exercise of Choice \u00b6 Potential XV6 Exercises: \u00b6 Processes and Threads \u00b6 (5) Find the code implementing the shell Discuss how the shell works, e.g., make a flow diagram for a single shell command; how does one add \"commands\" to the shell? Find the code the implements directy change, i.e., the cd shell command; why does it have to be built directly into the shell? Discuss how scripting could be added to the shell Discuss if the shell is part of the operating system. Should it be? Scheduling \u00b6 (1) Find the code for the scheduler of XV6. Is the scheduler preemptive or cooperative? Is that a good or bad design decision? Explain how the scheduler works, in particular, how does XV6 choose the next process to run? Discuss the scheduling policy: what are the advantages and disadvantages, is it a good choice for a desktop OS? What does \"good choice\" mean? Discuss how to implement a different scheduler, e.g., a fixed priority scheduler that always picks the process with the highest priority. Memory Management \u00b6 (1) Find the code that defines the memory layout of XV6 Explain the memory layout of XV6. Find and explain the code that sets up memory layout for processes in XV6, i.e., memory layout for users of XV6. (2) Find the code that handles the virtual memory of XV6 Explain the format for virtual addresses used in XV6. Find and explain the code for converting between \"real\" and virtual addresses. Explain how segments are used in the kernel. Find and explain the implementation of XV6's page tables . How are page tables organised (how many levels)? How big (potentially) is a page table? Paged Memory \u00b6 (1) Find the code that handles the virtual memory of XV6 Explain the format for virtual addresses used in XV6. Find and explain the code for converting between \"real\" and virtual addresses. Explain how segments are used in the kernel. Find and explain the implementation of XV6's page tables . How are page tables organised (how many levels)? How big (potentially) is a page table?","title":"8 - XV6 Exercise of Choice"},{"location":"4-semester/PSS/exam/8-xv6/#8-xv6-exercise-of-choice","text":"","title":"8 - XV6 Exercise of Choice"},{"location":"4-semester/PSS/exam/8-xv6/#potential-xv6-exercises","text":"","title":"Potential XV6 Exercises:"},{"location":"4-semester/PSS/exam/8-xv6/#processes-and-threads","text":"(5) Find the code implementing the shell Discuss how the shell works, e.g., make a flow diagram for a single shell command; how does one add \"commands\" to the shell? Find the code the implements directy change, i.e., the cd shell command; why does it have to be built directly into the shell? Discuss how scripting could be added to the shell Discuss if the shell is part of the operating system. Should it be?","title":"Processes and Threads"},{"location":"4-semester/PSS/exam/8-xv6/#scheduling","text":"(1) Find the code for the scheduler of XV6. Is the scheduler preemptive or cooperative? Is that a good or bad design decision? Explain how the scheduler works, in particular, how does XV6 choose the next process to run? Discuss the scheduling policy: what are the advantages and disadvantages, is it a good choice for a desktop OS? What does \"good choice\" mean? Discuss how to implement a different scheduler, e.g., a fixed priority scheduler that always picks the process with the highest priority.","title":"Scheduling"},{"location":"4-semester/PSS/exam/8-xv6/#memory-management","text":"(1) Find the code that defines the memory layout of XV6 Explain the memory layout of XV6. Find and explain the code that sets up memory layout for processes in XV6, i.e., memory layout for users of XV6. (2) Find the code that handles the virtual memory of XV6 Explain the format for virtual addresses used in XV6. Find and explain the code for converting between \"real\" and virtual addresses. Explain how segments are used in the kernel. Find and explain the implementation of XV6's page tables . How are page tables organised (how many levels)? How big (potentially) is a page table?","title":"Memory Management"},{"location":"4-semester/PSS/exam/8-xv6/#paged-memory","text":"(1) Find the code that handles the virtual memory of XV6 Explain the format for virtual addresses used in XV6. Find and explain the code for converting between \"real\" and virtual addresses. Explain how segments are used in the kernel. Find and explain the implementation of XV6's page tables . How are page tables organised (how many levels)? How big (potentially) is a page table?","title":"Paged Memory"},{"location":"4-semester/SPO/","text":"SPO - LANGAUGES AND COMPILERS \u00b6 Moodle side: https://www.moodle.aau.dk/course/view.php?id=28774","title":"Course"},{"location":"4-semester/SPO/#spo-langauges-and-compilers","text":"Moodle side: https://www.moodle.aau.dk/course/view.php?id=28774","title":"SPO - LANGAUGES AND COMPILERS"},{"location":"4-semester/SPO/01-programming-languages-and-compilers/","text":"Compiler \u00b6 Translates human-oriented programming languages into computer-oriented machine languages. A compiler allows virtually all computer users to ignore machine independent details of machine language. Making programs and programming expertise portable across wide variety of computers. Compiler Distinction Compilers may be distinguished by: By the kind of machine code they generate By the format of the target code they generate Machine Code Generated by Compilers \u00b6 May generate any of three: Pure Machine Code Augmented Machine Code Virtual Machine Code Pure Machine Code \u00b6 Code for a particular machines instruction set. Called pure code , because it only includes instructions part of that instruction set. Rare because most compilers rely on runtime libraries and OS's Most commonly used in compilers for system implementation languages , which are inteded for implementing OS's ore embedded applications. Can execute on bare hardware without dependence on any other software. Augmented Machine Code \u00b6 Augmented with OS routines and runtime language support routines. Requires that a particular OS be present. Virtual Machine Code \u00b6 Consists of entirely of virtual instructions. Increases portability An interpreter for the virtual machine (VM) is written for every target architecure.I Example: Java Bootstrapping a Compiler \u00b6 If a compiler accepts source language L . Any instance of this compiler can translate from L to VM instructions. If the compiler itself is written in L , the compiler can compile itself into the VM instructions. Language Criteria \u00b6 Sebesta's Language Criteria","title":"Programming Languages and Compilers"},{"location":"4-semester/SPO/01-programming-languages-and-compilers/#compiler","text":"Translates human-oriented programming languages into computer-oriented machine languages. A compiler allows virtually all computer users to ignore machine independent details of machine language. Making programs and programming expertise portable across wide variety of computers. Compiler Distinction Compilers may be distinguished by: By the kind of machine code they generate By the format of the target code they generate","title":"Compiler"},{"location":"4-semester/SPO/01-programming-languages-and-compilers/#machine-code-generated-by-compilers","text":"May generate any of three: Pure Machine Code Augmented Machine Code Virtual Machine Code","title":"Machine Code Generated by Compilers"},{"location":"4-semester/SPO/01-programming-languages-and-compilers/#pure-machine-code","text":"Code for a particular machines instruction set. Called pure code , because it only includes instructions part of that instruction set. Rare because most compilers rely on runtime libraries and OS's Most commonly used in compilers for system implementation languages , which are inteded for implementing OS's ore embedded applications. Can execute on bare hardware without dependence on any other software.","title":"Pure Machine Code"},{"location":"4-semester/SPO/01-programming-languages-and-compilers/#augmented-machine-code","text":"Augmented with OS routines and runtime language support routines. Requires that a particular OS be present.","title":"Augmented Machine Code"},{"location":"4-semester/SPO/01-programming-languages-and-compilers/#virtual-machine-code","text":"Consists of entirely of virtual instructions. Increases portability An interpreter for the virtual machine (VM) is written for every target architecure.I Example: Java","title":"Virtual Machine Code"},{"location":"4-semester/SPO/01-programming-languages-and-compilers/#bootstrapping-a-compiler","text":"If a compiler accepts source language L . Any instance of this compiler can translate from L to VM instructions. If the compiler itself is written in L , the compiler can compile itself into the VM instructions.","title":"Bootstrapping a Compiler"},{"location":"4-semester/SPO/01-programming-languages-and-compilers/#language-criteria","text":"Sebesta's Language Criteria","title":"Language Criteria"},{"location":"4-semester/SPO/02-tombstone-diagrams/","text":"Tombstone Diagrams \u00b6 Diagrams consisting out of a set of \"puzzle pieces\", we use to reason about language processores and programs Differnt kinds of pieces Combination rules (not all diagrams are well formed) Combination Rules \u00b6 Example \u00b6 Cross Compilation \u00b6 A cross compiler is a compiler that runs on one machine ( host machine ), but emits code for another machine ( target machine ). This is what happens when compiling an app for android on a x86 machine for example. Two Stage Compilation \u00b6 Two-stage translator : a composition of two translators. The output of the first translator is provided as input for the second translator. Generate C code from Java, compile the C code with C-compiler. Interpreters \u00b6 Interpreter : A language processor implemented in software. Terminology: abstract (virtual) machine versus real machine . Example : The Java Virtual Machine Compiler vs Interpreter \u00b6 Compilers offer advantages when: Programs are deployed in a production setting Programs are repetative The instructions of the programming language are complex Interpreters are better choice when: In development/testing/debugging stage Programs are run once and then discarded The instructions of the language are simple The execution speed is overshadowed by other factors. Portable Compilers \u00b6 Example: It is easier to port Kit 2.","title":"Tombstone Diagrams"},{"location":"4-semester/SPO/02-tombstone-diagrams/#tombstone-diagrams","text":"Diagrams consisting out of a set of \"puzzle pieces\", we use to reason about language processores and programs Differnt kinds of pieces Combination rules (not all diagrams are well formed)","title":"Tombstone Diagrams"},{"location":"4-semester/SPO/02-tombstone-diagrams/#combination-rules","text":"","title":"Combination Rules"},{"location":"4-semester/SPO/02-tombstone-diagrams/#example","text":"","title":"Example"},{"location":"4-semester/SPO/02-tombstone-diagrams/#cross-compilation","text":"A cross compiler is a compiler that runs on one machine ( host machine ), but emits code for another machine ( target machine ). This is what happens when compiling an app for android on a x86 machine for example.","title":"Cross Compilation"},{"location":"4-semester/SPO/02-tombstone-diagrams/#two-stage-compilation","text":"Two-stage translator : a composition of two translators. The output of the first translator is provided as input for the second translator. Generate C code from Java, compile the C code with C-compiler.","title":"Two Stage Compilation"},{"location":"4-semester/SPO/02-tombstone-diagrams/#interpreters","text":"Interpreter : A language processor implemented in software. Terminology: abstract (virtual) machine versus real machine . Example : The Java Virtual Machine","title":"Interpreters"},{"location":"4-semester/SPO/02-tombstone-diagrams/#compiler-vs-interpreter","text":"Compilers offer advantages when: Programs are deployed in a production setting Programs are repetative The instructions of the programming language are complex Interpreters are better choice when: In development/testing/debugging stage Programs are run once and then discarded The instructions of the language are simple The execution speed is overshadowed by other factors.","title":"Compiler vs Interpreter"},{"location":"4-semester/SPO/02-tombstone-diagrams/#portable-compilers","text":"Example: It is easier to port Kit 2.","title":"Portable Compilers"},{"location":"4-semester/SPO/03a-the-phases-of-a-compiler/","text":"The \"Phases\" of a Compiler \u00b6 Syntax Analysis \u00b6 Syntax Lexical analysis Regular expressions Parsing Context Free Grammar Contextual analysis \u00b6 Contextual constraints Scope checking Scope rules (static semantics) Type checking Type rules (static semantics) Code Generations \u00b6 Semantics (dynamic semantics) Organization of a Compiler \u00b6 Scanner Source program -> tokens Part of Syntax analysis Parser tokens -> abstract syntax tree (AST) Part of Syntax analysis Symbol table Created from AST Part of Contextual analysis Semantic analysis: AST decoration Part of code generation","title":"The Phases of a Compiler"},{"location":"4-semester/SPO/03a-the-phases-of-a-compiler/#the-phases-of-a-compiler","text":"","title":"The \"Phases\" of a Compiler"},{"location":"4-semester/SPO/03a-the-phases-of-a-compiler/#syntax-analysis","text":"Syntax Lexical analysis Regular expressions Parsing Context Free Grammar","title":"Syntax Analysis"},{"location":"4-semester/SPO/03a-the-phases-of-a-compiler/#contextual-analysis","text":"Contextual constraints Scope checking Scope rules (static semantics) Type checking Type rules (static semantics)","title":"Contextual analysis"},{"location":"4-semester/SPO/03a-the-phases-of-a-compiler/#code-generations","text":"Semantics (dynamic semantics)","title":"Code Generations"},{"location":"4-semester/SPO/03a-the-phases-of-a-compiler/#organization-of-a-compiler","text":"Scanner Source program -> tokens Part of Syntax analysis Parser tokens -> abstract syntax tree (AST) Part of Syntax analysis Symbol table Created from AST Part of Contextual analysis Semantic analysis: AST decoration Part of code generation","title":"Organization of a Compiler"},{"location":"4-semester/SPO/03b-the-ac-language-and-compiler/","text":"The Language 'ac' \u00b6 ac: adding calculator Informal Definition \u00b6 Types: integer float : 5 fractional digits after the decimal point. Automatic type conversion from int to float Keywords f: float i: integer p: print Variables 23 names from lowercase Roman alph. except the 3 reserved keywords. Flat scope Names are visible in the program when they are declared Target of translation: dc (desk calculator) Reverse Polish Notation (RPN) Example Program \u00b6 Example ac program: 1 2 3 4 5 6 f b // declare var b as float i a // declare var a as int a = 5 // assign a the value 5 b = a + 3.2 // assign b teh result of calculation a + 3.2 p b // print content of b $ // end of input Corresponding dc program: 1 2 3 4 5 6 7 8 5 sa la 3.2 + sb lb p Formal Definition \u00b6 Syntax specification: context-free grammar (CFG) Token specification: Regular Expressions No formal definition of Type Rules or Runtime semantics (in Fisher et. Al.) Context Free Grammar (CFG) \u00b6 A set of productions or rewriting rules Eg.: ```CFG Stmt -> id assign Val Expr | print id // A statement (Stmt) can either be line 1 or line 2 ``` Two kinds of symbols: Terminals - Cannot be rewritten Eg. id, assign, print Start symbol: Prog Empty or null string: \\lambda \\lambda (some references use \\varepsilon \\varepsilon for empty string) End of input stream or file: $ Nonterminals Eg. Val , Expr Note: nonterminals begin with uppercase in Fisher et. al. LHS - left-hand side ( Prog ) RHS - right-hand side ( Dcls Stmts $ ) We begin with the start-symbol usually the LHS of the first rule. We proceed by replacing it with the RHS of some production for that symbol We continue choosing some nonterminal symbol in out derived string of symbols, finding a production for that nonterminal, replacing it with the string of symbols on the production's RHS. We continue until no nonterminal remain. Any string of terminals produced in this manner is considered syntactically valid. \u200b Any other string has a syntax error Syntax Specification with CFG for 'ac' \u00b6 Example Derivation of an 'ac' program \u00b6 Parse Tree \u00b6 Token Specification \u00b6 Using Regular Expressions Phases of an ac compiler \u00b6 Scanning The scanner reads a src ac program as a text file and produces a stream of tokens Parsing Determine if the stream of tokens conforms to the language's grammar spec. And creates an abstract syntax tree (AST) . For ac, recursive descent is used Symbol Table The AST is traversed to create a symbol table , associating type and other contextual information with variables used in an 'ac' program. Semantic Analysis The AST is traversed to perform Semantic Analysis. In 'ac' its fairly minimal, in most languages, multiple passes over the AST may be required. Often decorates or transforms portions of the AST. The actual meaning of those portions become clear Translation The AST is traversed to generate a Translation of the original program. Scanning \u00b6 Translating stream of chars into a stream of tokens. Each token found by the scanner has the following two components: A token's type explain s the token's membership in the terminal alphabet. All instances of a given terminal have the same token type. A token's semantic value provides additional information about the token. For terminals such as plus no semantic info is required (only one token (+) can correspond to it). Other, such as id and num require semantic info, so the compiler can record which identifier or number has been scanned. Scanner pseudocode \u00b6 Parsing \u00b6 Responsible for determining if the stream of tokens provided by the scanner conforms to the language's grammar spec. The parsing technique used for the ac compiler is called Recursive descent . Mutually recursive parsing routines that in effect descent through a derivation tree. We look at recursive descent parsing procedures for Stmt and Stmts Pseudocode for Stmt and Stmts \u00b6 First it examines the next input token to predict which production to apply. Ex. Stmt offers two productions: 1 2 Stmt -> id assign Val Expr Stmt -> print id This is done at marker 1 and 6 in figure 2.7. If 'id' is the next input token, then the parse must proceed with a rule that generates 'id' as its first terminal. We say that the predict set for Stmt -> id assign Val Expr is {id} The predict set for Stmt -> print id is {print} Semantic Analysis \u00b6 Code Generation \u00b6 Assumes that program has been thoroughly checked and is well formed (scope and type rules) Takes semantics of both the source and target language into account Transforms source program into target code Tree Traversal \u00b6 \"Traditional\" OO approach Visitor Approach GOF Using static overloading Reflective \"Functional\" approach Active patterns in Scala (or F#) Traditional \u00b6 Method for each phase in every node Visitor \u00b6 GOF \u00b6 1 2 3 4 5 6 7 8 @Override void visitAssigning ( Assigning n ) { // TODO Auto-generated method stub n . child1 . accept ( this ); emit ( \" s\" ); emit ( n . id ); emit ( \" 0 k \" ); } Static Overloading \u00b6 1 2 3 4 5 6 7 8 @Override void visit ( Assigning n ) { // TODO Auto-generated method stub n . child1 . accept ( this ); emit ( \" s\" ); emit ( n . id ); emit ( \" 0 k \" ); } Reflective \u00b6 Using double dispatch Visitor decides the best class / method to use Functional \u00b6 If-else chain or switch Scala Active Patterns \u00b6","title":"The 'ac' Language and Compiler"},{"location":"4-semester/SPO/03b-the-ac-language-and-compiler/#the-language-ac","text":"ac: adding calculator","title":"The Language 'ac'"},{"location":"4-semester/SPO/03b-the-ac-language-and-compiler/#informal-definition","text":"Types: integer float : 5 fractional digits after the decimal point. Automatic type conversion from int to float Keywords f: float i: integer p: print Variables 23 names from lowercase Roman alph. except the 3 reserved keywords. Flat scope Names are visible in the program when they are declared Target of translation: dc (desk calculator) Reverse Polish Notation (RPN)","title":"Informal Definition"},{"location":"4-semester/SPO/03b-the-ac-language-and-compiler/#example-program","text":"Example ac program: 1 2 3 4 5 6 f b // declare var b as float i a // declare var a as int a = 5 // assign a the value 5 b = a + 3.2 // assign b teh result of calculation a + 3.2 p b // print content of b $ // end of input Corresponding dc program: 1 2 3 4 5 6 7 8 5 sa la 3.2 + sb lb p","title":"Example Program"},{"location":"4-semester/SPO/03b-the-ac-language-and-compiler/#formal-definition","text":"Syntax specification: context-free grammar (CFG) Token specification: Regular Expressions No formal definition of Type Rules or Runtime semantics (in Fisher et. Al.)","title":"Formal Definition"},{"location":"4-semester/SPO/03b-the-ac-language-and-compiler/#context-free-grammar-cfg","text":"A set of productions or rewriting rules Eg.: ```CFG Stmt -> id assign Val Expr | print id // A statement (Stmt) can either be line 1 or line 2 ``` Two kinds of symbols: Terminals - Cannot be rewritten Eg. id, assign, print Start symbol: Prog Empty or null string: \\lambda \\lambda (some references use \\varepsilon \\varepsilon for empty string) End of input stream or file: $ Nonterminals Eg. Val , Expr Note: nonterminals begin with uppercase in Fisher et. al. LHS - left-hand side ( Prog ) RHS - right-hand side ( Dcls Stmts $ ) We begin with the start-symbol usually the LHS of the first rule. We proceed by replacing it with the RHS of some production for that symbol We continue choosing some nonterminal symbol in out derived string of symbols, finding a production for that nonterminal, replacing it with the string of symbols on the production's RHS. We continue until no nonterminal remain. Any string of terminals produced in this manner is considered syntactically valid. \u200b Any other string has a syntax error","title":"Context Free Grammar (CFG)"},{"location":"4-semester/SPO/03b-the-ac-language-and-compiler/#syntax-specification-with-cfg-for-ac","text":"","title":"Syntax Specification with CFG for 'ac'"},{"location":"4-semester/SPO/03b-the-ac-language-and-compiler/#example-derivation-of-an-ac-program","text":"","title":"Example Derivation of an 'ac' program"},{"location":"4-semester/SPO/03b-the-ac-language-and-compiler/#parse-tree","text":"","title":"Parse Tree"},{"location":"4-semester/SPO/03b-the-ac-language-and-compiler/#token-specification","text":"Using Regular Expressions","title":"Token Specification"},{"location":"4-semester/SPO/03b-the-ac-language-and-compiler/#phases-of-an-ac-compiler","text":"Scanning The scanner reads a src ac program as a text file and produces a stream of tokens Parsing Determine if the stream of tokens conforms to the language's grammar spec. And creates an abstract syntax tree (AST) . For ac, recursive descent is used Symbol Table The AST is traversed to create a symbol table , associating type and other contextual information with variables used in an 'ac' program. Semantic Analysis The AST is traversed to perform Semantic Analysis. In 'ac' its fairly minimal, in most languages, multiple passes over the AST may be required. Often decorates or transforms portions of the AST. The actual meaning of those portions become clear Translation The AST is traversed to generate a Translation of the original program.","title":"Phases of an ac compiler"},{"location":"4-semester/SPO/03b-the-ac-language-and-compiler/#scanning","text":"Translating stream of chars into a stream of tokens. Each token found by the scanner has the following two components: A token's type explain s the token's membership in the terminal alphabet. All instances of a given terminal have the same token type. A token's semantic value provides additional information about the token. For terminals such as plus no semantic info is required (only one token (+) can correspond to it). Other, such as id and num require semantic info, so the compiler can record which identifier or number has been scanned.","title":"Scanning"},{"location":"4-semester/SPO/03b-the-ac-language-and-compiler/#scanner-pseudocode","text":"","title":"Scanner pseudocode"},{"location":"4-semester/SPO/03b-the-ac-language-and-compiler/#parsing","text":"Responsible for determining if the stream of tokens provided by the scanner conforms to the language's grammar spec. The parsing technique used for the ac compiler is called Recursive descent . Mutually recursive parsing routines that in effect descent through a derivation tree. We look at recursive descent parsing procedures for Stmt and Stmts","title":"Parsing"},{"location":"4-semester/SPO/03b-the-ac-language-and-compiler/#pseudocode-for-stmt-and-stmts","text":"First it examines the next input token to predict which production to apply. Ex. Stmt offers two productions: 1 2 Stmt -> id assign Val Expr Stmt -> print id This is done at marker 1 and 6 in figure 2.7. If 'id' is the next input token, then the parse must proceed with a rule that generates 'id' as its first terminal. We say that the predict set for Stmt -> id assign Val Expr is {id} The predict set for Stmt -> print id is {print}","title":"Pseudocode for Stmt and Stmts"},{"location":"4-semester/SPO/03b-the-ac-language-and-compiler/#semantic-analysis","text":"","title":"Semantic Analysis"},{"location":"4-semester/SPO/03b-the-ac-language-and-compiler/#code-generation","text":"Assumes that program has been thoroughly checked and is well formed (scope and type rules) Takes semantics of both the source and target language into account Transforms source program into target code","title":"Code Generation"},{"location":"4-semester/SPO/03b-the-ac-language-and-compiler/#tree-traversal","text":"\"Traditional\" OO approach Visitor Approach GOF Using static overloading Reflective \"Functional\" approach Active patterns in Scala (or F#)","title":"Tree Traversal"},{"location":"4-semester/SPO/03b-the-ac-language-and-compiler/#traditional","text":"Method for each phase in every node","title":"Traditional"},{"location":"4-semester/SPO/03b-the-ac-language-and-compiler/#visitor","text":"","title":"Visitor"},{"location":"4-semester/SPO/03b-the-ac-language-and-compiler/#gof","text":"1 2 3 4 5 6 7 8 @Override void visitAssigning ( Assigning n ) { // TODO Auto-generated method stub n . child1 . accept ( this ); emit ( \" s\" ); emit ( n . id ); emit ( \" 0 k \" ); }","title":"GOF"},{"location":"4-semester/SPO/03b-the-ac-language-and-compiler/#static-overloading","text":"1 2 3 4 5 6 7 8 @Override void visit ( Assigning n ) { // TODO Auto-generated method stub n . child1 . accept ( this ); emit ( \" s\" ); emit ( n . id ); emit ( \" 0 k \" ); }","title":"Static Overloading"},{"location":"4-semester/SPO/03b-the-ac-language-and-compiler/#reflective","text":"Using double dispatch Visitor decides the best class / method to use","title":"Reflective"},{"location":"4-semester/SPO/03b-the-ac-language-and-compiler/#functional","text":"If-else chain or switch","title":"Functional"},{"location":"4-semester/SPO/03b-the-ac-language-and-compiler/#scala-active-patterns","text":"","title":"Scala Active Patterns"},{"location":"4-semester/SPO/04-language-specification/","text":"Language Specification \u00b6 A communication device between people who need to have a common understanding of the PL. What to specify? What is a well formed program Syntax Contextual constraints (static semantics) sope rules type rules What is the meaning of (well formed) programs. Semantics (runtime semantics) How to specify? Formal specification Some kind of precisely defined formalism Informal specification Description in eg. English Usually a mix of both (eg. Java spec.) Syntax -> Formal spec. using RE and CFG Contextual constraints and semantics -> informal Formal semantics has been retrofitted. Trend towards more formality (C#, Fortress) Terminology \u00b6 A sentence is a string of characters over some alphabet A language is a set of sentences A lexeme is the lowest level syntactic unit of a language (eg. * , sum , begin ) A token is a category of lexemes (eg. identifier) Generators A device that generates sentences of a language One can determine if syntax of particular sentence is syntactically correct, by comparing it to the structure of the generator. Recognizers A recognition device reads input strings over the alphabet of the language and decides whether the input strings belong to the language Ex: syntax analysis part of a compiler Syntax Analysis \u00b6 Portion of a language processor nearly always consists of two parts. A low-level part called a lexical analyzer. Finite automaton based on regular grammar. A high-level part called syntax analyzer, or parser. Push-down automaton based on a CFG or BNF Context-Free Grammars and BNF \u00b6 CFG Developed by Noam Chomsky in mid-1950's Language generators, meant to describe syntax of natural languages. A finite set of terminal symbols (tokens) A finite set of non-terminal symbols A start symbol A finite set of production rules CFG's are written in BNF notation. A production rule in BNF is written as: \u200b N::=\\alpha N::=\\alpha where N is a non terminal and \\alpha \\alpha a sequence of terminals and non-terminals \u200b N::=\\alpha|\\beta|.. N::=\\alpha|\\beta|.. is an abbreviation for several rules with N as LHS Example \u00b6 BNF Fundamentals \u00b6 Abstractions are used to represent classes of syntactic structures They act like syntactic variables (also called non-terminal symbols ) Terminals are lexemes or tokens A rule has a LHS, which is nonterminal and a RHS which is a string of terminals and/or nonterminals Nonterminals are often enclosed in angle brackets Grammar: a finite non-empty set of rules A start symbol is a special element of nonterminals of a grammar. An abstraction (or nonterminal symbol) can have more than one RHS Alternative rules are written with | Syntactic list are described using recursion A derivation is a repeated application of rules, starting with the start symbol and ending with a sentence (all terminal symbols) Example \u00b6 Example derivation \u00b6 Derivations \u00b6 Every string of symbols in a derivation is a sentential form A sentence is a sentential form that has only terminal symbols A leftmost derivation is one in which the leftmost nonterminal in each sentential form is the one that is expanded A rightmost derivation is one in which the rightmost nonterminal in each sentential form is the one that is expanded A derivation may be neither leftmost nor rightmost Ambiguity in Grammars \u00b6 A grammar is ambiguous if and only if it generates a sentential form that has two or more distinct parse trees. Example \u00b6 Fix the example \u00b6 If we use the parse tree to indicate precedence levels of the operators, we cannot have ambiguity. Associativity of Operators \u00b6 Operator associativity can also be indicated by a grammar. Extended BNF \u00b6 Optional parts are placed in brackets [] 1 <proc_call> -> ident [(<expr_list>)] Alternative parts of RHS are placed inside parenthethes and seperated via vertical bars 1 <term> -> <term> (+|-) const Repetitions (0 or more) are placed inside braces {} 1 <ident> -> letter {letter|digit} BNF vs EBNF \u00b6 BNF EBNF Important \u00b6 Syntax is the visible part of a programming language Programming language designers can waste a lot of time discussing unimportant details of syntax The language paradigm is the next most visible part The choice of paradigm, and therefor language, depends on how humans best think about the problem There are no right models of computations - just different models of computation, some more suited for cerain classes of problems than others. The most invisible part is the language semantics Clear semantics usually leads to simple and efficient implementations.","title":"Language Specification"},{"location":"4-semester/SPO/04-language-specification/#language-specification","text":"A communication device between people who need to have a common understanding of the PL. What to specify? What is a well formed program Syntax Contextual constraints (static semantics) sope rules type rules What is the meaning of (well formed) programs. Semantics (runtime semantics) How to specify? Formal specification Some kind of precisely defined formalism Informal specification Description in eg. English Usually a mix of both (eg. Java spec.) Syntax -> Formal spec. using RE and CFG Contextual constraints and semantics -> informal Formal semantics has been retrofitted. Trend towards more formality (C#, Fortress)","title":"Language Specification"},{"location":"4-semester/SPO/04-language-specification/#terminology","text":"A sentence is a string of characters over some alphabet A language is a set of sentences A lexeme is the lowest level syntactic unit of a language (eg. * , sum , begin ) A token is a category of lexemes (eg. identifier) Generators A device that generates sentences of a language One can determine if syntax of particular sentence is syntactically correct, by comparing it to the structure of the generator. Recognizers A recognition device reads input strings over the alphabet of the language and decides whether the input strings belong to the language Ex: syntax analysis part of a compiler","title":"Terminology"},{"location":"4-semester/SPO/04-language-specification/#syntax-analysis","text":"Portion of a language processor nearly always consists of two parts. A low-level part called a lexical analyzer. Finite automaton based on regular grammar. A high-level part called syntax analyzer, or parser. Push-down automaton based on a CFG or BNF","title":"Syntax Analysis"},{"location":"4-semester/SPO/04-language-specification/#context-free-grammars-and-bnf","text":"CFG Developed by Noam Chomsky in mid-1950's Language generators, meant to describe syntax of natural languages. A finite set of terminal symbols (tokens) A finite set of non-terminal symbols A start symbol A finite set of production rules CFG's are written in BNF notation. A production rule in BNF is written as: \u200b N::=\\alpha N::=\\alpha where N is a non terminal and \\alpha \\alpha a sequence of terminals and non-terminals \u200b N::=\\alpha|\\beta|.. N::=\\alpha|\\beta|.. is an abbreviation for several rules with N as LHS","title":"Context-Free Grammars and BNF"},{"location":"4-semester/SPO/04-language-specification/#example","text":"","title":"Example"},{"location":"4-semester/SPO/04-language-specification/#bnf-fundamentals","text":"Abstractions are used to represent classes of syntactic structures They act like syntactic variables (also called non-terminal symbols ) Terminals are lexemes or tokens A rule has a LHS, which is nonterminal and a RHS which is a string of terminals and/or nonterminals Nonterminals are often enclosed in angle brackets Grammar: a finite non-empty set of rules A start symbol is a special element of nonterminals of a grammar. An abstraction (or nonterminal symbol) can have more than one RHS Alternative rules are written with | Syntactic list are described using recursion A derivation is a repeated application of rules, starting with the start symbol and ending with a sentence (all terminal symbols)","title":"BNF Fundamentals"},{"location":"4-semester/SPO/04-language-specification/#example_1","text":"","title":"Example"},{"location":"4-semester/SPO/04-language-specification/#example-derivation","text":"","title":"Example derivation"},{"location":"4-semester/SPO/04-language-specification/#derivations","text":"Every string of symbols in a derivation is a sentential form A sentence is a sentential form that has only terminal symbols A leftmost derivation is one in which the leftmost nonterminal in each sentential form is the one that is expanded A rightmost derivation is one in which the rightmost nonterminal in each sentential form is the one that is expanded A derivation may be neither leftmost nor rightmost","title":"Derivations"},{"location":"4-semester/SPO/04-language-specification/#ambiguity-in-grammars","text":"A grammar is ambiguous if and only if it generates a sentential form that has two or more distinct parse trees.","title":"Ambiguity in Grammars"},{"location":"4-semester/SPO/04-language-specification/#example_2","text":"","title":"Example"},{"location":"4-semester/SPO/04-language-specification/#fix-the-example","text":"If we use the parse tree to indicate precedence levels of the operators, we cannot have ambiguity.","title":"Fix the example"},{"location":"4-semester/SPO/04-language-specification/#associativity-of-operators","text":"Operator associativity can also be indicated by a grammar.","title":"Associativity of Operators"},{"location":"4-semester/SPO/04-language-specification/#extended-bnf","text":"Optional parts are placed in brackets [] 1 <proc_call> -> ident [(<expr_list>)] Alternative parts of RHS are placed inside parenthethes and seperated via vertical bars 1 <term> -> <term> (+|-) const Repetitions (0 or more) are placed inside braces {} 1 <ident> -> letter {letter|digit}","title":"Extended BNF"},{"location":"4-semester/SPO/04-language-specification/#bnf-vs-ebnf","text":"BNF EBNF","title":"BNF vs EBNF"},{"location":"4-semester/SPO/04-language-specification/#important","text":"Syntax is the visible part of a programming language Programming language designers can waste a lot of time discussing unimportant details of syntax The language paradigm is the next most visible part The choice of paradigm, and therefor language, depends on how humans best think about the problem There are no right models of computations - just different models of computation, some more suited for cerain classes of problems than others. The most invisible part is the language semantics Clear semantics usually leads to simple and efficient implementations.","title":"Important"},{"location":"4-semester/SPO/05a-context-free-grammars/","text":"Context Free Grammar (CFG) \u00b6 A finite set of terminal symbols A finite set of non-terminal symbols A start symbol A finite set of production rules How to Design a Grammar \u00b6 Lets write a CFG for C-style function prototypes. Examples void myf1(int x, double y); int myf2(); int myf3(double z); double myf4(int, int w, int); void myf5(void); One Possible Grammar \u00b6 Examples \u00b6 1 2 3 4 5 void ident(int ident, double ident); int ident(); int ident(double ident); double ident(int, int ident, int); void ident(void); Another Possible Grammar \u00b6 Examples \u00b6 1 2 3 4 5 void ident(int ident, double ident); int ident(); int ident(double ident); int ident(int, int ident, int); void ident(void); Definition \u00b6 Components: G=(N,\\Sigma,P,S) G=(N,\\Sigma,P,S) A finite terminal alphabet \\Sigma \\Sigma The set of tokens produced by the scanner A finite nonterminal alphabet N variables of the grammar A start symbol S S \\in N S \\in N that initiates all derivations (also called goal symbol) A finite set of productions P: P:A \\rightarrow X_1...X_m, \\space \\text{where} \\space A \\in N, X_i \\in N \\cup \\Sigma, 1\\leq i \\leq m \\space \\text{and} \\space m \\geq 0 P:A \\rightarrow X_1...X_m, \\space \\text{where} \\space A \\in N, X_i \\in N \\cup \\Sigma, 1\\leq i \\leq m \\space \\text{and} \\space m \\geq 0 (also called rewriting rules) Vocabulary V=N\\cup \\Sigma V=N\\cup \\Sigma N \\cap \\Sigma = \\phi N \\cap \\Sigma = \\phi Notation \u00b6 The book Fisher et. al. uses the following notation: Names Beginning With Represent Symbols In Examples Uppercase N A, B, C, Prefix Lowercase and punctuation \\Sigma \\Sigma a, b, c, if, then, (, ; \\mathcal{X,Y} \\mathcal{X,Y} N \\cup \\Sigma N \\cup \\Sigma \\mathcal{X}_i,\\mathcal{Y}_3 \\mathcal{X}_i,\\mathcal{Y}_3 Other Greek letters (N\\cup\\Sigma)^* (N\\cup\\Sigma)^* \\alpha, \\gamma \\alpha, \\gamma A production rule is written: \u200b A \\rightarrow \\alpha A \\rightarrow \\alpha or A \\rightarrow \\mathcal{X_1...X_m} A \\rightarrow \\mathcal{X_1...X_m} depending on the RHS. A production rule with multiple RHS is written with | \\begin{align*} A \\rightarrow \\alpha \\\\ |\\space\\space \\beta \\\\ ... \\space \\space \\\\ |\\space\\space \\beta \\end{align*} \\begin{align*} A \\rightarrow \\alpha \\\\ |\\space\\space \\beta \\\\ ... \\space \\space \\\\ |\\space\\space \\beta \\end{align*} Derivation: \u200b \\alpha A \\beta \\Rightarrow \\alpha \\gamma\\beta \\alpha A \\beta \\Rightarrow \\alpha \\gamma\\beta is one step of derivation if A\\rightarrow \\gamma A\\rightarrow \\gamma is a production rule. \u200b \\Rightarrow^+ \\Rightarrow^+ derives in one or more steps. \u200b \\Rightarrow^* \\Rightarrow^* derives in zero or more steps. Sentential form: \u200b S\\Rightarrow^*\\beta:\\beta S\\Rightarrow^*\\beta:\\beta is a sentential form of the CFG \u200b SF(G): SF(G): the set of sentential forms of G thus: \u200b L(G)=\\{w\\in\\Sigma^* \\mid S\\Rightarrow^+w\\} L(G)=\\{w\\in\\Sigma^* \\mid S\\Rightarrow^+w\\} \u200b also \u200b L(G)=SF(G)\\cap\\Sigma^* L(G)=SF(G)\\cap\\Sigma^* Derivation \u00b6 Two conventions for rewriting nonterminals in a systematic order: Leftmost derivation (expands from left to right) Rightmost derivation (expands from right to left) Leftmost Derivation \u00b6 A derivation that always chooses the leftmost possible nonterminal at each step. Denoted with \\Rightarrow_{lm} \\Rightarrow_{lm} Example \u00b6 Derivation \u00b6 f ( v + v ) Rightmost Derivation \u00b6 The rightmost possible terminal is always expanded Denoted with \\Rightarrow_{rm} \\Rightarrow_{rm} Parse Trees \u00b6 A graphical representation of a derivation Root: start symbol S Each node: either grammar symbol or \\lambda \\lambda or \\varepsilon \\varepsilon Interior nodes: nonterminals An interior node and its children: production Example \u00b6 BNF Form \u00b6 Backus-Naur Form (BNF) Formal grammar for expressing CFG Extended BNF (EBNF) \u00b6 An extended form of BNF. Three additional postfix operators +, ?, and * are introduced. R+ indicates the occurrence of one or more Rs to express repetition R~opt~ is sometimes used. R? indicates the occurrence of zero or one Rs to express optionality [R] is sometimes used. R* indicates the ocurrence of zero or more Rs to express repetition {R} is sometimes used. EBNF can be rewritten to BNF Example: 1 expression -> term (+ term)* Rewritten to: 1 2 3 expression -> term term_tmp term_tmp -> + term term_tmp | \u03bb Algorithm to rewrite \u00b6 Properties of grammars \u00b6 A non-terminal N is left-recursive if starting with at sentential form N, we can produce another sentential form starting with N 1 expression -> expression \u2018+\u2019 factor | factor Right-recursion also exists, is less important 1 expression -> term '+' expression A non-terminal N is nullable , if starting with a sentential form N, we can produce an empty sentential form. 1 expression -> \u03bb A non-terminal is useless , if it can never produce a string of terminal symbols. 1 2 expression -> + expression | - expression Grammar Transformations \u00b6 We can rewrite the rules without changing the output of the rules. This can create less readable code grammars. Left factorization \u00b6 Example: Elimintaion of Left Recursion \u00b6 Example: Substitution \u00b6 Dangling Else Problem \u00b6 We have this rule: 1 2 3 4 single-Command ::= if Expression then single-Command | if Expression then single-Command else single-Command This parse tree? Or this parse tree? Rewrite the grammar: 1 2 sC ::= if E then sC endif | if E then sC else sC endif or 1 2 3 4 5 6 sC ::= CsC | OsC CsC ::= if E then CsC else CsC CsC ::= \u2026 OsC ::= if E then sC | if E then CsC else OsC","title":"Context Free Grammars"},{"location":"4-semester/SPO/05a-context-free-grammars/#context-free-grammar-cfg","text":"A finite set of terminal symbols A finite set of non-terminal symbols A start symbol A finite set of production rules","title":"Context Free Grammar (CFG)"},{"location":"4-semester/SPO/05a-context-free-grammars/#how-to-design-a-grammar","text":"Lets write a CFG for C-style function prototypes. Examples void myf1(int x, double y); int myf2(); int myf3(double z); double myf4(int, int w, int); void myf5(void);","title":"How to Design a Grammar"},{"location":"4-semester/SPO/05a-context-free-grammars/#one-possible-grammar","text":"","title":"One Possible Grammar"},{"location":"4-semester/SPO/05a-context-free-grammars/#examples","text":"1 2 3 4 5 void ident(int ident, double ident); int ident(); int ident(double ident); double ident(int, int ident, int); void ident(void);","title":"Examples"},{"location":"4-semester/SPO/05a-context-free-grammars/#another-possible-grammar","text":"","title":"Another Possible Grammar"},{"location":"4-semester/SPO/05a-context-free-grammars/#examples_1","text":"1 2 3 4 5 void ident(int ident, double ident); int ident(); int ident(double ident); int ident(int, int ident, int); void ident(void);","title":"Examples"},{"location":"4-semester/SPO/05a-context-free-grammars/#definition","text":"Components: G=(N,\\Sigma,P,S) G=(N,\\Sigma,P,S) A finite terminal alphabet \\Sigma \\Sigma The set of tokens produced by the scanner A finite nonterminal alphabet N variables of the grammar A start symbol S S \\in N S \\in N that initiates all derivations (also called goal symbol) A finite set of productions P: P:A \\rightarrow X_1...X_m, \\space \\text{where} \\space A \\in N, X_i \\in N \\cup \\Sigma, 1\\leq i \\leq m \\space \\text{and} \\space m \\geq 0 P:A \\rightarrow X_1...X_m, \\space \\text{where} \\space A \\in N, X_i \\in N \\cup \\Sigma, 1\\leq i \\leq m \\space \\text{and} \\space m \\geq 0 (also called rewriting rules) Vocabulary V=N\\cup \\Sigma V=N\\cup \\Sigma N \\cap \\Sigma = \\phi N \\cap \\Sigma = \\phi","title":"Definition"},{"location":"4-semester/SPO/05a-context-free-grammars/#notation","text":"The book Fisher et. al. uses the following notation: Names Beginning With Represent Symbols In Examples Uppercase N A, B, C, Prefix Lowercase and punctuation \\Sigma \\Sigma a, b, c, if, then, (, ; \\mathcal{X,Y} \\mathcal{X,Y} N \\cup \\Sigma N \\cup \\Sigma \\mathcal{X}_i,\\mathcal{Y}_3 \\mathcal{X}_i,\\mathcal{Y}_3 Other Greek letters (N\\cup\\Sigma)^* (N\\cup\\Sigma)^* \\alpha, \\gamma \\alpha, \\gamma A production rule is written: \u200b A \\rightarrow \\alpha A \\rightarrow \\alpha or A \\rightarrow \\mathcal{X_1...X_m} A \\rightarrow \\mathcal{X_1...X_m} depending on the RHS. A production rule with multiple RHS is written with | \\begin{align*} A \\rightarrow \\alpha \\\\ |\\space\\space \\beta \\\\ ... \\space \\space \\\\ |\\space\\space \\beta \\end{align*} \\begin{align*} A \\rightarrow \\alpha \\\\ |\\space\\space \\beta \\\\ ... \\space \\space \\\\ |\\space\\space \\beta \\end{align*} Derivation: \u200b \\alpha A \\beta \\Rightarrow \\alpha \\gamma\\beta \\alpha A \\beta \\Rightarrow \\alpha \\gamma\\beta is one step of derivation if A\\rightarrow \\gamma A\\rightarrow \\gamma is a production rule. \u200b \\Rightarrow^+ \\Rightarrow^+ derives in one or more steps. \u200b \\Rightarrow^* \\Rightarrow^* derives in zero or more steps. Sentential form: \u200b S\\Rightarrow^*\\beta:\\beta S\\Rightarrow^*\\beta:\\beta is a sentential form of the CFG \u200b SF(G): SF(G): the set of sentential forms of G thus: \u200b L(G)=\\{w\\in\\Sigma^* \\mid S\\Rightarrow^+w\\} L(G)=\\{w\\in\\Sigma^* \\mid S\\Rightarrow^+w\\} \u200b also \u200b L(G)=SF(G)\\cap\\Sigma^* L(G)=SF(G)\\cap\\Sigma^*","title":"Notation"},{"location":"4-semester/SPO/05a-context-free-grammars/#derivation","text":"Two conventions for rewriting nonterminals in a systematic order: Leftmost derivation (expands from left to right) Rightmost derivation (expands from right to left)","title":"Derivation"},{"location":"4-semester/SPO/05a-context-free-grammars/#leftmost-derivation","text":"A derivation that always chooses the leftmost possible nonterminal at each step. Denoted with \\Rightarrow_{lm} \\Rightarrow_{lm}","title":"Leftmost Derivation"},{"location":"4-semester/SPO/05a-context-free-grammars/#example","text":"","title":"Example"},{"location":"4-semester/SPO/05a-context-free-grammars/#derivation_1","text":"f ( v + v )","title":"Derivation"},{"location":"4-semester/SPO/05a-context-free-grammars/#rightmost-derivation","text":"The rightmost possible terminal is always expanded Denoted with \\Rightarrow_{rm} \\Rightarrow_{rm}","title":"Rightmost Derivation"},{"location":"4-semester/SPO/05a-context-free-grammars/#parse-trees","text":"A graphical representation of a derivation Root: start symbol S Each node: either grammar symbol or \\lambda \\lambda or \\varepsilon \\varepsilon Interior nodes: nonterminals An interior node and its children: production","title":"Parse Trees"},{"location":"4-semester/SPO/05a-context-free-grammars/#example_1","text":"","title":"Example"},{"location":"4-semester/SPO/05a-context-free-grammars/#bnf-form","text":"Backus-Naur Form (BNF) Formal grammar for expressing CFG","title":"BNF Form"},{"location":"4-semester/SPO/05a-context-free-grammars/#extended-bnf-ebnf","text":"An extended form of BNF. Three additional postfix operators +, ?, and * are introduced. R+ indicates the occurrence of one or more Rs to express repetition R~opt~ is sometimes used. R? indicates the occurrence of zero or one Rs to express optionality [R] is sometimes used. R* indicates the ocurrence of zero or more Rs to express repetition {R} is sometimes used. EBNF can be rewritten to BNF Example: 1 expression -> term (+ term)* Rewritten to: 1 2 3 expression -> term term_tmp term_tmp -> + term term_tmp | \u03bb","title":"Extended BNF (EBNF)"},{"location":"4-semester/SPO/05a-context-free-grammars/#algorithm-to-rewrite","text":"","title":"Algorithm to rewrite"},{"location":"4-semester/SPO/05a-context-free-grammars/#properties-of-grammars","text":"A non-terminal N is left-recursive if starting with at sentential form N, we can produce another sentential form starting with N 1 expression -> expression \u2018+\u2019 factor | factor Right-recursion also exists, is less important 1 expression -> term '+' expression A non-terminal N is nullable , if starting with a sentential form N, we can produce an empty sentential form. 1 expression -> \u03bb A non-terminal is useless , if it can never produce a string of terminal symbols. 1 2 expression -> + expression | - expression","title":"Properties of grammars"},{"location":"4-semester/SPO/05a-context-free-grammars/#grammar-transformations","text":"We can rewrite the rules without changing the output of the rules. This can create less readable code grammars.","title":"Grammar Transformations"},{"location":"4-semester/SPO/05a-context-free-grammars/#left-factorization","text":"Example:","title":"Left factorization"},{"location":"4-semester/SPO/05a-context-free-grammars/#elimintaion-of-left-recursion","text":"Example:","title":"Elimintaion of Left Recursion"},{"location":"4-semester/SPO/05a-context-free-grammars/#substitution","text":"","title":"Substitution"},{"location":"4-semester/SPO/05a-context-free-grammars/#dangling-else-problem","text":"We have this rule: 1 2 3 4 single-Command ::= if Expression then single-Command | if Expression then single-Command else single-Command This parse tree? Or this parse tree? Rewrite the grammar: 1 2 sC ::= if E then sC endif | if E then sC else sC endif or 1 2 3 4 5 6 sC ::= CsC | OsC CsC ::= if E then CsC else CsC CsC ::= \u2026 OsC ::= if E then sC | if E then CsC else OsC","title":"Dangling Else Problem"},{"location":"4-semester/SPO/05b-from-tokens-to-parse-trees/","text":"From Tokens to Parse Trees \u00b6 The process of finding the structure in the flat stream of tokens is called parsing , and the module that performs this task is called parser . Two well-known ways to parse. top-down L eft-scan, L eftmost derivation ( LL ) Constructs the parse tree in pre-order bottom-up L eft-scan, R ightmost derivation in reverse (LR) Constructs the parse tree in post-order Example Parsing of Micro-English: \u00b6 Top-down Parsers \u00b6 Tree is grown from the root (top) Corresponds to a left derivation Bottom-up parser \u00b6 Tree grows from the leaves (bottom) up to the root (top). Just read right derivations backwards. (Rightmost derivation in reverse) Top-Down vs. Bottom-Up parsing \u00b6 Hierachy \u00b6 Formal Definition of LL(1) \u00b6 Properties of the grammar that determines if it is LL(1) or not: A grammar G is LL(1) if for each set of productions M::=X_1|X_2|...|X_n: M::=X_1|X_2|...|X_n: \\text{first}[X_1], \\text{first}[X_2],...,\\text{first}[X_n] \\text{first}[X_1], \\text{first}[X_2],...,\\text{first}[X_n] are all pairwise disjoint If X_i\\Rightarrow^*\\lambda X_i\\Rightarrow^*\\lambda then \\text{first}[X_j]\\cap \\text{follow}[X]=\u00d8 \\text{first}[X_j]\\cap \\text{follow}[X]=\u00d8 , for 1\\leq j\\leq n. i\\neq j 1\\leq j\\leq n. i\\neq j If G is \\lambda \\lambda -free then (1) is sufficient An LL(1) grammar is a grammar which can be parsed with a top-down parser with a lookahead of one token . First Sets \u00b6 The set of all terminal symbols that can begin a sentential form derivable from the string \\alpha \\alpha \u200b First(\\alpha)=\\{a\\in\\Sigma \\mid \\alpha \\Rightarrow^*a\\beta\\} First(\\alpha)=\\{a\\in\\Sigma \\mid \\alpha \\Rightarrow^*a\\beta\\} We never include \\lambda \\lambda in First( \\alpha \\alpha ) even if \\alpha \\Rightarrow \\lambda \\alpha \\Rightarrow \\lambda Example \u00b6 1 2 3 First(Tail) = { + } First(Prefix) = { f } First(E) = { v, f, ( } Algorithm for Computing First-Set \u00b6 Follow Sets \u00b6 The set of terminals that can follow a nonterminal A in some sentential form. \u200b For A\\in N A\\in N \u200b Follow(A)=\\{b\\in \\Sigma \\mid S\\Rightarrow^+ \\alpha A b \\beta\\} Follow(A)=\\{b\\in \\Sigma \\mid S\\Rightarrow^+ \\alpha A b \\beta\\} The right context associated with A Example \u00b6 Using the same example as in First Sets 1 2 3 Follow(Tail) = { ) } Follow(Prefix) = { ( } Follow(E) = { $, ) } Algorithm for Computing Follow(A) \u00b6 Provable Facts About LL(1) Grammars \u00b6 No left-recursive grammar is LL(1) No ambiguous grammar is LL(1) Some languages have no LL(1) grammar A \\lambda \\lambda -free grammar, where each alternative X_j X_j for N::=X_j N::=X_j begins with a distinct terminal, is a simple LL(1) grammar LR Grammars \u00b6 A Grammar is an LR Grammar if it can be parsed by a LR parsing algorithm. Harder to implement LR than LL parsers. Tools exists though (JavaCUP, Yacc, C#CUP, SableCC) Can recognize LR(0) , LR(1) , SLR , LALR grammars. Bigger class of grammars than LL Can handle left recursion! Usually more convenient because less need to rewrite grammar Most commonly used for automatic tools today (LALR in particular) Tools for designing CFG's \u00b6 kfG Edit Contex Free Grammar tool (Grammophone) ACLA Udvid Kig p\u00e5 predictset! Side 176 i Fisher et. al. PDF","title":"From Tokens to Parse Trees"},{"location":"4-semester/SPO/05b-from-tokens-to-parse-trees/#from-tokens-to-parse-trees","text":"The process of finding the structure in the flat stream of tokens is called parsing , and the module that performs this task is called parser . Two well-known ways to parse. top-down L eft-scan, L eftmost derivation ( LL ) Constructs the parse tree in pre-order bottom-up L eft-scan, R ightmost derivation in reverse (LR) Constructs the parse tree in post-order","title":"From Tokens to Parse Trees"},{"location":"4-semester/SPO/05b-from-tokens-to-parse-trees/#example-parsing-of-micro-english","text":"","title":"Example Parsing of Micro-English:"},{"location":"4-semester/SPO/05b-from-tokens-to-parse-trees/#top-down-parsers","text":"Tree is grown from the root (top) Corresponds to a left derivation","title":"Top-down Parsers"},{"location":"4-semester/SPO/05b-from-tokens-to-parse-trees/#bottom-up-parser","text":"Tree grows from the leaves (bottom) up to the root (top). Just read right derivations backwards. (Rightmost derivation in reverse)","title":"Bottom-up parser"},{"location":"4-semester/SPO/05b-from-tokens-to-parse-trees/#top-down-vs-bottom-up-parsing","text":"","title":"Top-Down vs. Bottom-Up parsing"},{"location":"4-semester/SPO/05b-from-tokens-to-parse-trees/#hierachy","text":"","title":"Hierachy"},{"location":"4-semester/SPO/05b-from-tokens-to-parse-trees/#formal-definition-of-ll1","text":"Properties of the grammar that determines if it is LL(1) or not: A grammar G is LL(1) if for each set of productions M::=X_1|X_2|...|X_n: M::=X_1|X_2|...|X_n: \\text{first}[X_1], \\text{first}[X_2],...,\\text{first}[X_n] \\text{first}[X_1], \\text{first}[X_2],...,\\text{first}[X_n] are all pairwise disjoint If X_i\\Rightarrow^*\\lambda X_i\\Rightarrow^*\\lambda then \\text{first}[X_j]\\cap \\text{follow}[X]=\u00d8 \\text{first}[X_j]\\cap \\text{follow}[X]=\u00d8 , for 1\\leq j\\leq n. i\\neq j 1\\leq j\\leq n. i\\neq j If G is \\lambda \\lambda -free then (1) is sufficient An LL(1) grammar is a grammar which can be parsed with a top-down parser with a lookahead of one token .","title":"Formal Definition of LL(1)"},{"location":"4-semester/SPO/05b-from-tokens-to-parse-trees/#first-sets","text":"The set of all terminal symbols that can begin a sentential form derivable from the string \\alpha \\alpha \u200b First(\\alpha)=\\{a\\in\\Sigma \\mid \\alpha \\Rightarrow^*a\\beta\\} First(\\alpha)=\\{a\\in\\Sigma \\mid \\alpha \\Rightarrow^*a\\beta\\} We never include \\lambda \\lambda in First( \\alpha \\alpha ) even if \\alpha \\Rightarrow \\lambda \\alpha \\Rightarrow \\lambda","title":"First Sets"},{"location":"4-semester/SPO/05b-from-tokens-to-parse-trees/#example","text":"1 2 3 First(Tail) = { + } First(Prefix) = { f } First(E) = { v, f, ( }","title":"Example"},{"location":"4-semester/SPO/05b-from-tokens-to-parse-trees/#algorithm-for-computing-first-set","text":"","title":"Algorithm for Computing First-Set"},{"location":"4-semester/SPO/05b-from-tokens-to-parse-trees/#follow-sets","text":"The set of terminals that can follow a nonterminal A in some sentential form. \u200b For A\\in N A\\in N \u200b Follow(A)=\\{b\\in \\Sigma \\mid S\\Rightarrow^+ \\alpha A b \\beta\\} Follow(A)=\\{b\\in \\Sigma \\mid S\\Rightarrow^+ \\alpha A b \\beta\\} The right context associated with A","title":"Follow Sets"},{"location":"4-semester/SPO/05b-from-tokens-to-parse-trees/#example_1","text":"Using the same example as in First Sets 1 2 3 Follow(Tail) = { ) } Follow(Prefix) = { ( } Follow(E) = { $, ) }","title":"Example"},{"location":"4-semester/SPO/05b-from-tokens-to-parse-trees/#algorithm-for-computing-followa","text":"","title":"Algorithm for Computing Follow(A)"},{"location":"4-semester/SPO/05b-from-tokens-to-parse-trees/#provable-facts-about-ll1-grammars","text":"No left-recursive grammar is LL(1) No ambiguous grammar is LL(1) Some languages have no LL(1) grammar A \\lambda \\lambda -free grammar, where each alternative X_j X_j for N::=X_j N::=X_j begins with a distinct terminal, is a simple LL(1) grammar","title":"Provable Facts About LL(1) Grammars"},{"location":"4-semester/SPO/05b-from-tokens-to-parse-trees/#lr-grammars","text":"A Grammar is an LR Grammar if it can be parsed by a LR parsing algorithm. Harder to implement LR than LL parsers. Tools exists though (JavaCUP, Yacc, C#CUP, SableCC) Can recognize LR(0) , LR(1) , SLR , LALR grammars. Bigger class of grammars than LL Can handle left recursion! Usually more convenient because less need to rewrite grammar Most commonly used for automatic tools today (LALR in particular)","title":"LR Grammars"},{"location":"4-semester/SPO/05b-from-tokens-to-parse-trees/#tools-for-designing-cfgs","text":"kfG Edit Contex Free Grammar tool (Grammophone) ACLA Udvid Kig p\u00e5 predictset! Side 176 i Fisher et. al. PDF","title":"Tools for designing CFG's"},{"location":"4-semester/SPO/06-lexical-analysis/","text":"Lexical Analysis \u00b6 We are looking at the scanner Lexemes are \"words\" in the input, for example keywords, operators, identifiers, literals, etc. Tokens: a data structure for lexemes and additional information. Example ac source program: 1 2 3 4 5 f b i a a = 5 b = a + 3.2 p b Turned into tokens by the scanner. We may choose to contain different amount and types of operation of different types of lexemes. This can be done to reduce the amount of memory used. Lexical Elements \u00b6 You need to decide what to use Warning slide 8 Lexemes \u00b6 The Lexem structure can be more detailed and subtle than one might expext. Rule of thumb If the lexem structure is complex then examine the language for design flaws!! Simple Grammar for Identifiers \u00b6 Can be transformed to a regular expression: 1 [a-z]([a-z]|[0-9])* Regular Expressions \u00b6 Regular Expression Meaning \\varepsilon \\varepsilon The empty string t Generates only the string t X Y Generates any string xy such that x is generated by X and y is generated by Y X | Y Generates any string which generated either by X or by Y X* The concatenation of zero or more strings generated by X (X) Parentheses are used for grouping P+ positive-closure , strings consisting of one or more strings in P catenated together A meta-character is any punctuation char or regular expression operator. A meta-character must be quoted when used as an ordinary char to avoid ambiguity. Regular Grammars \u00b6 Warning slide 15 The ac Language \u00b6 Remember the ac language Warning slide 17-19 Implement Scanner Based on RE by Hand \u00b6 Warning slide 25 Developing a Scanner \u00b6 Lexical grammar of Mini-Triangle Warning slide 26- Developing a Scanner by Hand \u00b6 Hard and error prone. Can be automated Programming scanner generator is an example of declarative programming What to scan, not how to scan Most compilers are developed using a generated scanner Slide 35 - Finite Automata and implementation of Scanners Generating Scanners \u00b6 Based on: Regular Expressions To describe the tokens to be recognized Finite State Machines An execution model to which RE's are \"compiled\" Regular Expressions can be recognized by a finite state machine (FA) Finite State Automaton Converting a RE Into an NDFA-epsilon \u00b6 Implementing a DFA \u00b6 Slide 46-47 Algorithm on slide 48 Implementing a Scanner as a DFA \u00b6 Slide 49 JLex Lexical Analyzer Generator for Java \u00b6 Slide 51-52 JLex Regular Expressions \u00b6 Slide 53-55- ...","title":"Lexical Analysis"},{"location":"4-semester/SPO/06-lexical-analysis/#lexical-analysis","text":"We are looking at the scanner Lexemes are \"words\" in the input, for example keywords, operators, identifiers, literals, etc. Tokens: a data structure for lexemes and additional information. Example ac source program: 1 2 3 4 5 f b i a a = 5 b = a + 3.2 p b Turned into tokens by the scanner. We may choose to contain different amount and types of operation of different types of lexemes. This can be done to reduce the amount of memory used.","title":"Lexical Analysis"},{"location":"4-semester/SPO/06-lexical-analysis/#lexical-elements","text":"You need to decide what to use Warning slide 8","title":"Lexical Elements"},{"location":"4-semester/SPO/06-lexical-analysis/#lexemes","text":"The Lexem structure can be more detailed and subtle than one might expext. Rule of thumb If the lexem structure is complex then examine the language for design flaws!!","title":"Lexemes"},{"location":"4-semester/SPO/06-lexical-analysis/#simple-grammar-for-identifiers","text":"Can be transformed to a regular expression: 1 [a-z]([a-z]|[0-9])*","title":"Simple Grammar for Identifiers"},{"location":"4-semester/SPO/06-lexical-analysis/#regular-expressions","text":"Regular Expression Meaning \\varepsilon \\varepsilon The empty string t Generates only the string t X Y Generates any string xy such that x is generated by X and y is generated by Y X | Y Generates any string which generated either by X or by Y X* The concatenation of zero or more strings generated by X (X) Parentheses are used for grouping P+ positive-closure , strings consisting of one or more strings in P catenated together A meta-character is any punctuation char or regular expression operator. A meta-character must be quoted when used as an ordinary char to avoid ambiguity.","title":"Regular Expressions"},{"location":"4-semester/SPO/06-lexical-analysis/#regular-grammars","text":"Warning slide 15","title":"Regular Grammars"},{"location":"4-semester/SPO/06-lexical-analysis/#the-ac-language","text":"Remember the ac language Warning slide 17-19","title":"The ac Language"},{"location":"4-semester/SPO/06-lexical-analysis/#implement-scanner-based-on-re-by-hand","text":"Warning slide 25","title":"Implement Scanner Based on RE by Hand"},{"location":"4-semester/SPO/06-lexical-analysis/#developing-a-scanner","text":"Lexical grammar of Mini-Triangle Warning slide 26-","title":"Developing a Scanner"},{"location":"4-semester/SPO/06-lexical-analysis/#developing-a-scanner-by-hand","text":"Hard and error prone. Can be automated Programming scanner generator is an example of declarative programming What to scan, not how to scan Most compilers are developed using a generated scanner Slide 35 - Finite Automata and implementation of Scanners","title":"Developing a Scanner by Hand"},{"location":"4-semester/SPO/06-lexical-analysis/#generating-scanners","text":"Based on: Regular Expressions To describe the tokens to be recognized Finite State Machines An execution model to which RE's are \"compiled\" Regular Expressions can be recognized by a finite state machine (FA) Finite State Automaton","title":"Generating Scanners"},{"location":"4-semester/SPO/06-lexical-analysis/#converting-a-re-into-an-ndfa-epsilon","text":"","title":"Converting a RE Into an NDFA-epsilon"},{"location":"4-semester/SPO/06-lexical-analysis/#implementing-a-dfa","text":"Slide 46-47 Algorithm on slide 48","title":"Implementing a DFA"},{"location":"4-semester/SPO/06-lexical-analysis/#implementing-a-scanner-as-a-dfa","text":"Slide 49","title":"Implementing a Scanner as a DFA"},{"location":"4-semester/SPO/06-lexical-analysis/#jlex-lexical-analyzer-generator-for-java","text":"Slide 51-52","title":"JLex Lexical Analyzer Generator for Java"},{"location":"4-semester/SPO/06-lexical-analysis/#jlex-regular-expressions","text":"Slide 53-55- ...","title":"JLex Regular Expressions"},{"location":"4-semester/SPO/exam/","text":"SPO - Exam \u00b6","title":"Exam"},{"location":"4-semester/SPO/exam/#spo-exam","text":"","title":"SPO - Exam"},{"location":"4-semester/SPO/exam/1-language-design-and-control-structures/","text":"Language Design and Control Structures \u00b6 Language Design Criteria Evaluation of expressions Explicit sequence control vs. structured sequence control Loop constructs Subprograms Parameter mechanisms Design Criteria \u00b6 Orthogonality \u00b6 \u201cThe number of independent primitive concepts has been minimized in order that the language be easy to describe, to learn, and to implement. On the other hand, these concepts have been applied \u201corthogonally\u201d in order to maximize the expressive power of the language while trying to avoid deleterious superfluities\u201d -Adriaan van Wijngaarden et al., Revised Report on the Algorithmic Language ALGOL 68, section 0.1.2, Orthogonal design \u201cA precise definition is difficult to produce, but languages that are called orthogonal tend to have a small number of core concepts and a set of ways of uniformly combining these concepts. The semantics of the combinations are uniform; no special restrictions exist for specific instances of combinations.\u201d -David Schmidt Lack of Orthogonality \u00b6 Examples of exceptions from C: Structures (but not arrays) may be returned from a function. An array can be returned if it is inside a structure. A member of a structure can be any data type (except void, or the structure of the same type). An array element can be any data type (except void) Everything is passed by value (except arrays) Void can be used as a type in a structure, but a variable of this type cannot be declared in a function Sequence Control \u00b6 Implicit and explicit sequence control Expressions Precedence rules Associativity Statements Sequence Conditionals Loop Constructs Unstructured vs structured sequence control Expressions \u00b6 Operator evaluation order Operand evaluation order Operators Most operators are infix or prefix Order of evaluation determined by operator precedence and associativity The following grammar is ambiguous The grammar can express operator precedence: In LL(1) Gives us: Association can be expressed with grammar also: Statements \u00b6 Sequential Conditional Looping Construct Must have all three to provide full power of a Computing Machine! Sequential \u00b6 Skip Assignments Most languages treats assignments as a basic operation Some languages have derived assignment operators such as: += and *= in C I/O Some languages treat I/O as basic operations Others treat I/O as functions/methods Sequencing C;C Blocks begin...end , {...} , let...in...end Conditional \u00b6 Single-way IF...THEN... Controlled by boolean expression Two-way IF...THEN...ELSE... Controlled by boolean expression IF...THEN is usually treated as degenerate of IF...THEN...ELSE... Multi-way SWITCH Loops \u00b6 Counter-controlled iterators For-loops Logical-iterators Iterations based on data structures Recursion For-loops \u00b6 Controlled by loop variable of scalar type with bounds and increment size. Logic-Test \u00b6 While-loops Test performed before entry repeat...until and do...while Test performed at end Always executed at least once Subprograms \u00b6 A subprogram has a single entry point The caller is suspended during execution of the called subprogram Control always returns to the caller when the called subprogram's execution terminates Functions vs Procedures Procedures provide user-defined statements Abstractions over statements Functions provide user-defined operators Abstraction over expressions Methods used for both functions and procedures Specification : Name Signature Number and types of input arguments, number and types of output results Actions Direct function relating input values to output values Side effects on global state and subprogram internal state Local Referencing Envirionments \u00b6 Local variables can be stack-dynamic Support for recursion Storage for locals is shared among some subprograms But Allocation/de-allocation, initialization time Indirect addressing Subprograms cannot be history sensitive Local variables can be static Advantages and disadvantages are the opposite of those for stack-dynamic local variables Actual/Formal Parameter Correspondence \u00b6 Positional Binding is by position First actual is bound to first formal, and so forth Safe and effective Keyword Name of the formal to which the actual is to be bound, is specified with actual. Parameters can appear in any order, avoiding parameter correspondence errors BUT: User must know the formal parameters's names In some languages, formal parameters can have default values. Some languages allow variable number of parameters Attributes of variables are used to exchange information Name - Call-by-name Memory location - Call-by-reference Value Call-by-value (one way, from actual to formal parameter) Call-by-value-result (two ways between actual and formal parameter) Call-by-result (one way, from formal to actual parameter) out in C# Design Considerations \u00b6 Efficiency One-way or two-way These are in conflict Good programming \\rightarrow \\rightarrow Limited access to variables, meaning one-way whenever possible Efficiency \\rightarrow \\rightarrow Pass by reference is fastest way to pass structures of significant size Functions should not allow reference parameters Subprograms as Parameter \u00b6 First class functions Lamdas C and C++: functions cannot be passed as parameters but pointers to functions can be passed and their types include the types of the parameters, so parameters can be type checked Ada does not allow subprogram parameters; an alternative is provided via Ada\u2019s generic facility Java until Java 8 did not allow method names to be passed as parameters C# supports functions a parameters through delegates Delegates can now be anonymous or lambda expression We talk about first class functions Functional languages supports functions as first class functions Tennent's Language Design Principles \u00b6 The Principle of Abstraction All major syntactic categories should have abstractions defined over them. For example, functions are abstractions over expressions. The Principle of Correspondence Declarations \\approx \\approx Parameters The Principle of Data Type Completeness All data types should be first class without arbitrary restriction on their use - Originally defined by R.D.Tennent Abstraction \u00b6 Nearly all programming languages support process (or command) abstractions with subprograms (procedures) Many programming languages support expression abstraction with functions. Nearly all programming languages designed since 1980 have supported data abstraction Abstract data types Objects Modules Correspondence \u00b6","title":"1 - Language Design and Control Structures"},{"location":"4-semester/SPO/exam/1-language-design-and-control-structures/#language-design-and-control-structures","text":"Language Design Criteria Evaluation of expressions Explicit sequence control vs. structured sequence control Loop constructs Subprograms Parameter mechanisms","title":"Language Design and Control Structures"},{"location":"4-semester/SPO/exam/1-language-design-and-control-structures/#design-criteria","text":"","title":"Design Criteria"},{"location":"4-semester/SPO/exam/1-language-design-and-control-structures/#orthogonality","text":"\u201cThe number of independent primitive concepts has been minimized in order that the language be easy to describe, to learn, and to implement. On the other hand, these concepts have been applied \u201corthogonally\u201d in order to maximize the expressive power of the language while trying to avoid deleterious superfluities\u201d -Adriaan van Wijngaarden et al., Revised Report on the Algorithmic Language ALGOL 68, section 0.1.2, Orthogonal design \u201cA precise definition is difficult to produce, but languages that are called orthogonal tend to have a small number of core concepts and a set of ways of uniformly combining these concepts. The semantics of the combinations are uniform; no special restrictions exist for specific instances of combinations.\u201d -David Schmidt","title":"Orthogonality"},{"location":"4-semester/SPO/exam/1-language-design-and-control-structures/#lack-of-orthogonality","text":"Examples of exceptions from C: Structures (but not arrays) may be returned from a function. An array can be returned if it is inside a structure. A member of a structure can be any data type (except void, or the structure of the same type). An array element can be any data type (except void) Everything is passed by value (except arrays) Void can be used as a type in a structure, but a variable of this type cannot be declared in a function","title":"Lack of Orthogonality"},{"location":"4-semester/SPO/exam/1-language-design-and-control-structures/#sequence-control","text":"Implicit and explicit sequence control Expressions Precedence rules Associativity Statements Sequence Conditionals Loop Constructs Unstructured vs structured sequence control","title":"Sequence Control"},{"location":"4-semester/SPO/exam/1-language-design-and-control-structures/#expressions","text":"Operator evaluation order Operand evaluation order Operators Most operators are infix or prefix Order of evaluation determined by operator precedence and associativity The following grammar is ambiguous The grammar can express operator precedence: In LL(1) Gives us: Association can be expressed with grammar also:","title":"Expressions"},{"location":"4-semester/SPO/exam/1-language-design-and-control-structures/#statements","text":"Sequential Conditional Looping Construct Must have all three to provide full power of a Computing Machine!","title":"Statements"},{"location":"4-semester/SPO/exam/1-language-design-and-control-structures/#sequential","text":"Skip Assignments Most languages treats assignments as a basic operation Some languages have derived assignment operators such as: += and *= in C I/O Some languages treat I/O as basic operations Others treat I/O as functions/methods Sequencing C;C Blocks begin...end , {...} , let...in...end","title":"Sequential"},{"location":"4-semester/SPO/exam/1-language-design-and-control-structures/#conditional","text":"Single-way IF...THEN... Controlled by boolean expression Two-way IF...THEN...ELSE... Controlled by boolean expression IF...THEN is usually treated as degenerate of IF...THEN...ELSE... Multi-way SWITCH","title":"Conditional"},{"location":"4-semester/SPO/exam/1-language-design-and-control-structures/#loops","text":"Counter-controlled iterators For-loops Logical-iterators Iterations based on data structures Recursion","title":"Loops"},{"location":"4-semester/SPO/exam/1-language-design-and-control-structures/#for-loops","text":"Controlled by loop variable of scalar type with bounds and increment size.","title":"For-loops"},{"location":"4-semester/SPO/exam/1-language-design-and-control-structures/#logic-test","text":"While-loops Test performed before entry repeat...until and do...while Test performed at end Always executed at least once","title":"Logic-Test"},{"location":"4-semester/SPO/exam/1-language-design-and-control-structures/#subprograms","text":"A subprogram has a single entry point The caller is suspended during execution of the called subprogram Control always returns to the caller when the called subprogram's execution terminates Functions vs Procedures Procedures provide user-defined statements Abstractions over statements Functions provide user-defined operators Abstraction over expressions Methods used for both functions and procedures Specification : Name Signature Number and types of input arguments, number and types of output results Actions Direct function relating input values to output values Side effects on global state and subprogram internal state","title":"Subprograms"},{"location":"4-semester/SPO/exam/1-language-design-and-control-structures/#local-referencing-envirionments","text":"Local variables can be stack-dynamic Support for recursion Storage for locals is shared among some subprograms But Allocation/de-allocation, initialization time Indirect addressing Subprograms cannot be history sensitive Local variables can be static Advantages and disadvantages are the opposite of those for stack-dynamic local variables","title":"Local Referencing Envirionments"},{"location":"4-semester/SPO/exam/1-language-design-and-control-structures/#actualformal-parameter-correspondence","text":"Positional Binding is by position First actual is bound to first formal, and so forth Safe and effective Keyword Name of the formal to which the actual is to be bound, is specified with actual. Parameters can appear in any order, avoiding parameter correspondence errors BUT: User must know the formal parameters's names In some languages, formal parameters can have default values. Some languages allow variable number of parameters Attributes of variables are used to exchange information Name - Call-by-name Memory location - Call-by-reference Value Call-by-value (one way, from actual to formal parameter) Call-by-value-result (two ways between actual and formal parameter) Call-by-result (one way, from formal to actual parameter) out in C#","title":"Actual/Formal Parameter Correspondence"},{"location":"4-semester/SPO/exam/1-language-design-and-control-structures/#design-considerations","text":"Efficiency One-way or two-way These are in conflict Good programming \\rightarrow \\rightarrow Limited access to variables, meaning one-way whenever possible Efficiency \\rightarrow \\rightarrow Pass by reference is fastest way to pass structures of significant size Functions should not allow reference parameters","title":"Design Considerations"},{"location":"4-semester/SPO/exam/1-language-design-and-control-structures/#subprograms-as-parameter","text":"First class functions Lamdas C and C++: functions cannot be passed as parameters but pointers to functions can be passed and their types include the types of the parameters, so parameters can be type checked Ada does not allow subprogram parameters; an alternative is provided via Ada\u2019s generic facility Java until Java 8 did not allow method names to be passed as parameters C# supports functions a parameters through delegates Delegates can now be anonymous or lambda expression We talk about first class functions Functional languages supports functions as first class functions","title":"Subprograms as Parameter"},{"location":"4-semester/SPO/exam/1-language-design-and-control-structures/#tennents-language-design-principles","text":"The Principle of Abstraction All major syntactic categories should have abstractions defined over them. For example, functions are abstractions over expressions. The Principle of Correspondence Declarations \\approx \\approx Parameters The Principle of Data Type Completeness All data types should be first class without arbitrary restriction on their use - Originally defined by R.D.Tennent","title":"Tennent's Language Design Principles"},{"location":"4-semester/SPO/exam/1-language-design-and-control-structures/#abstraction","text":"Nearly all programming languages support process (or command) abstractions with subprograms (procedures) Many programming languages support expression abstraction with functions. Nearly all programming languages designed since 1980 have supported data abstraction Abstract data types Objects Modules","title":"Abstraction"},{"location":"4-semester/SPO/exam/1-language-design-and-control-structures/#correspondence","text":"","title":"Correspondence"},{"location":"4-semester/SPO/exam/2-structure-of-the-compiler/","text":"Structure of the Compiler \u00b6 Describe the phases of the compiler and give an overall description of what the purpose of each phase is and how the phases interface Single pass vs. multi pass compiler Issues in language design Issues in code generation Phases of a Compiler \u00b6 The different phases can be seen as different transformation steps to transform source code into object code. The different phases correspond roughly to the different parts of the language specification: Syntax analysis \\Leftrightarrow \\Leftrightarrow Syntax Lexical Analysis \\Leftrightarrow \\Leftrightarrow Regular Expressions Parsing \\Leftrightarrow \\Leftrightarrow Context Free Grammar Contextual Analysis \\Leftrightarrow \\Leftrightarrow Contextual Constraints Scope Checking \\Leftrightarrow \\Leftrightarrow Scope rules (static semantics) Type Checking \\Leftrightarrow \\Leftrightarrow Type rules (static semantics) Code Generation \\Leftrightarrow \\Leftrightarrow Semantics (dynamic semantics) Organization of the Compiler \u00b6 N\u00e6sten alle moderne compilere er syntax-directed . Compileringsprocessen er drevet af den syntaktiske struktur. De fleste compilere laver et AST (abstract syntax tree). Scanner \u00b6 Scanneren l\u00e6ser input texten, karakter efter karakter. Grupperer karakterer i tokens Som eks. identifiers, integers, reserved words, delimiters. Eliminerer un\u00f8dvendig information som comments. Laver en stream a tokens. Drevet af regular expressions Parser \u00b6 Parseren er baseret p\u00e5 en formel syntax specifikation, s\u00e5 som CFG'er. L\u00e6ser tokens og grupperer dem i phrases if\u00f8lge syntax specifikationen. Typisk drevet af tabeller lavet fra en CFG af en parser generator Parseren verificerer korrekt syntax. Hvis en fejl findes laver den en fejlmeddelelse Parsers genererer ofte et AST. Type Checker \u00b6 Tjekker static semantics for hver AST node. The construct is legal and meaningful Variabler er deklareret, typer er korrekte osv. Type checker decorates AST noden ved at tilf\u00f8je type information. Hvis en semantisk fejl findes, laves en fejlmeddelelse. Translator \u00b6 Hvis en AST node er semantisk korrekt, kan den translates til IR kode. I simple ikke-optimerende compilere, kan translatoren generere target code uden at bruge explicit IR. G\u00f8r retargeting mere besv\u00e6rligt. Symbol Tables \u00b6 En symbol table er en mekanisme der bruges til at associere information med identifiers delt mellem compiler-phases. Bruges gennem type checking, men kan ogs\u00e5 bruges i andre faser. Optimizer \u00b6 IR kode analyseres og laves til funktionel identisk men optimeret kode af optimizeren . Kan v\u00e6re kompleks og indeholde flere underfaser. De fleste compilere lader dig slukke for optimizer. Kan ogs\u00e5 g\u00f8res efter code generation. Eksempel: peep-hole optimization . Unders\u00f8ger kode f\u00e5 instruktioner af gangen. Kan optimere ting som: Eleminering af gange med 1 eller addition med 0. Lad af v\u00e6rdi til register n\u00e5r v\u00e6rdi er i andet register. Sekvens af instruktioner til single instruktion med samme effekt. Code Generator \u00b6 Mapper IR code til target code. Kr\u00e6ver maskin specifik information Laver maskinspecifik optimering Er normalt hand-coded da de er meget specifikke og komplekse. Example \u00b6 Example As an example see the notes on the ac language and compiler","title":"2 - Structure of the Compiler"},{"location":"4-semester/SPO/exam/2-structure-of-the-compiler/#structure-of-the-compiler","text":"Describe the phases of the compiler and give an overall description of what the purpose of each phase is and how the phases interface Single pass vs. multi pass compiler Issues in language design Issues in code generation","title":"Structure of the Compiler"},{"location":"4-semester/SPO/exam/2-structure-of-the-compiler/#phases-of-a-compiler","text":"The different phases can be seen as different transformation steps to transform source code into object code. The different phases correspond roughly to the different parts of the language specification: Syntax analysis \\Leftrightarrow \\Leftrightarrow Syntax Lexical Analysis \\Leftrightarrow \\Leftrightarrow Regular Expressions Parsing \\Leftrightarrow \\Leftrightarrow Context Free Grammar Contextual Analysis \\Leftrightarrow \\Leftrightarrow Contextual Constraints Scope Checking \\Leftrightarrow \\Leftrightarrow Scope rules (static semantics) Type Checking \\Leftrightarrow \\Leftrightarrow Type rules (static semantics) Code Generation \\Leftrightarrow \\Leftrightarrow Semantics (dynamic semantics)","title":"Phases of a Compiler"},{"location":"4-semester/SPO/exam/2-structure-of-the-compiler/#organization-of-the-compiler","text":"N\u00e6sten alle moderne compilere er syntax-directed . Compileringsprocessen er drevet af den syntaktiske struktur. De fleste compilere laver et AST (abstract syntax tree).","title":"Organization of the Compiler"},{"location":"4-semester/SPO/exam/2-structure-of-the-compiler/#scanner","text":"Scanneren l\u00e6ser input texten, karakter efter karakter. Grupperer karakterer i tokens Som eks. identifiers, integers, reserved words, delimiters. Eliminerer un\u00f8dvendig information som comments. Laver en stream a tokens. Drevet af regular expressions","title":"Scanner"},{"location":"4-semester/SPO/exam/2-structure-of-the-compiler/#parser","text":"Parseren er baseret p\u00e5 en formel syntax specifikation, s\u00e5 som CFG'er. L\u00e6ser tokens og grupperer dem i phrases if\u00f8lge syntax specifikationen. Typisk drevet af tabeller lavet fra en CFG af en parser generator Parseren verificerer korrekt syntax. Hvis en fejl findes laver den en fejlmeddelelse Parsers genererer ofte et AST.","title":"Parser"},{"location":"4-semester/SPO/exam/2-structure-of-the-compiler/#type-checker","text":"Tjekker static semantics for hver AST node. The construct is legal and meaningful Variabler er deklareret, typer er korrekte osv. Type checker decorates AST noden ved at tilf\u00f8je type information. Hvis en semantisk fejl findes, laves en fejlmeddelelse.","title":"Type Checker"},{"location":"4-semester/SPO/exam/2-structure-of-the-compiler/#translator","text":"Hvis en AST node er semantisk korrekt, kan den translates til IR kode. I simple ikke-optimerende compilere, kan translatoren generere target code uden at bruge explicit IR. G\u00f8r retargeting mere besv\u00e6rligt.","title":"Translator"},{"location":"4-semester/SPO/exam/2-structure-of-the-compiler/#symbol-tables","text":"En symbol table er en mekanisme der bruges til at associere information med identifiers delt mellem compiler-phases. Bruges gennem type checking, men kan ogs\u00e5 bruges i andre faser.","title":"Symbol Tables"},{"location":"4-semester/SPO/exam/2-structure-of-the-compiler/#optimizer","text":"IR kode analyseres og laves til funktionel identisk men optimeret kode af optimizeren . Kan v\u00e6re kompleks og indeholde flere underfaser. De fleste compilere lader dig slukke for optimizer. Kan ogs\u00e5 g\u00f8res efter code generation. Eksempel: peep-hole optimization . Unders\u00f8ger kode f\u00e5 instruktioner af gangen. Kan optimere ting som: Eleminering af gange med 1 eller addition med 0. Lad af v\u00e6rdi til register n\u00e5r v\u00e6rdi er i andet register. Sekvens af instruktioner til single instruktion med samme effekt.","title":"Optimizer"},{"location":"4-semester/SPO/exam/2-structure-of-the-compiler/#code-generator","text":"Mapper IR code til target code. Kr\u00e6ver maskin specifik information Laver maskinspecifik optimering Er normalt hand-coded da de er meget specifikke og komplekse.","title":"Code Generator"},{"location":"4-semester/SPO/exam/2-structure-of-the-compiler/#example","text":"Example As an example see the notes on the ac language and compiler","title":"Example"},{"location":"4-semester/SPO/exam/3-lexical-analysis/","text":"Lexical Analysis \u00b6 Describe the role of the lexical analysis phase Describe how a scanner can be implemented by hand or auto-generated Describe regular expressions and finite automata The Scanner \u00b6 The scanner is sometimes called lexical analyzer or lexer . Lexemes are \"words\" in the input, for example keywords, operators, identifiers, literals, etc. Tokens: a data structure for lexemes and additional information. Lexical Elements \u00b6 Character set ASCII vs Unicode Identifiers Operators +, -, /, *, ... Keywords if, then, while Noise words Elementary data Numbers (integers, floats) Strings Symbols Delimiters begin...end vs {...} Comments /*...*/ vs # vs ! vs // Whitespace Layout Lexemes \u00b6 Can be detailed and subtle String constants: Escape sequence: \\\", \\n, ... Null strings Rational constants: 0.1, 10.01 0.1, 10.01 $.1,\\ 10.,\\ $ vs 1..10 1..10 Rule of Thumb If the lexem structure is complex then examine the language for design flaws!! Regular Expressions \u00b6 Tokens can be \"scanned\" with regular expressions. Regular Expression Meaning \\varepsilon \\varepsilon The empty string t Generates only the string t X Y Generates any string xy such that x is generated by X and y is generated by Y X | Y Generates any string which generated either by X or by Y X* The concatenation of zero or more strings generated by X (X) Parentheses are used for grouping P+ positive-closure , strings consisting of one or more strings in P catenated together A meta-character is any punctuation char or regular expression operator. A meta-character must be quoted when used as an ordinary char to avoid ambiguity. Identifier Grammar to RE \u00b6 Elimination of Left Recursion 1 N ::= X | N Y => N ::= X Y* Left factorization: 1 X Y | X Z => X ( Y | Z ) Regular Grammars \u00b6 A grammar is regular if by substituting every nonterminal (except the root one) with its righthand side, you can reduce it down to a single production for the root, with only terminals and operators on the righthand side. This grammar is regular: 1 2 3 Identifier ::= Letter | Identifier Letter | Identifier Digit Because it can be reduced to: 1 Identifier ::= Letter (Letter | Digit)* Or rather 1 (a|b|c|...|z)((a|b|c|...|z) | (0|1|2|...|9))* Called a regular expression often reduced to: 1 [a-z]([a-z]|[0-9])* Implementing \u00b6 A DFA can be implemented with a transition table: Can be coded in one of two forms: Table-driven The table is explicitly represented in a runtime table, interpreted by the program Often used by scanner generators Token independent Explicit control The table appears implicitly through the control logic of the program. Pseudocode \u00b6 The DFA from above implemented: Table-driven \u00b6 Explicit Control \u00b6 Transducers \u00b6 It is smart to associate a semantic value with the token, such as the value of an integer constant. An FA that does this is called a transducer . Scanner Generator \u00b6 The scanner is often generated by a tool, such as Lex The tokens are specified in a scanner specification that is fed to the generator tool. Performance Considerations \u00b6","title":"3 - Lexical Analysis"},{"location":"4-semester/SPO/exam/3-lexical-analysis/#lexical-analysis","text":"Describe the role of the lexical analysis phase Describe how a scanner can be implemented by hand or auto-generated Describe regular expressions and finite automata","title":"Lexical Analysis"},{"location":"4-semester/SPO/exam/3-lexical-analysis/#the-scanner","text":"The scanner is sometimes called lexical analyzer or lexer . Lexemes are \"words\" in the input, for example keywords, operators, identifiers, literals, etc. Tokens: a data structure for lexemes and additional information.","title":"The Scanner"},{"location":"4-semester/SPO/exam/3-lexical-analysis/#lexical-elements","text":"Character set ASCII vs Unicode Identifiers Operators +, -, /, *, ... Keywords if, then, while Noise words Elementary data Numbers (integers, floats) Strings Symbols Delimiters begin...end vs {...} Comments /*...*/ vs # vs ! vs // Whitespace Layout","title":"Lexical Elements"},{"location":"4-semester/SPO/exam/3-lexical-analysis/#lexemes","text":"Can be detailed and subtle String constants: Escape sequence: \\\", \\n, ... Null strings Rational constants: 0.1, 10.01 0.1, 10.01 $.1,\\ 10.,\\ $ vs 1..10 1..10 Rule of Thumb If the lexem structure is complex then examine the language for design flaws!!","title":"Lexemes"},{"location":"4-semester/SPO/exam/3-lexical-analysis/#regular-expressions","text":"Tokens can be \"scanned\" with regular expressions. Regular Expression Meaning \\varepsilon \\varepsilon The empty string t Generates only the string t X Y Generates any string xy such that x is generated by X and y is generated by Y X | Y Generates any string which generated either by X or by Y X* The concatenation of zero or more strings generated by X (X) Parentheses are used for grouping P+ positive-closure , strings consisting of one or more strings in P catenated together A meta-character is any punctuation char or regular expression operator. A meta-character must be quoted when used as an ordinary char to avoid ambiguity.","title":"Regular Expressions"},{"location":"4-semester/SPO/exam/3-lexical-analysis/#identifier-grammar-to-re","text":"Elimination of Left Recursion 1 N ::= X | N Y => N ::= X Y* Left factorization: 1 X Y | X Z => X ( Y | Z )","title":"Identifier Grammar to RE"},{"location":"4-semester/SPO/exam/3-lexical-analysis/#regular-grammars","text":"A grammar is regular if by substituting every nonterminal (except the root one) with its righthand side, you can reduce it down to a single production for the root, with only terminals and operators on the righthand side. This grammar is regular: 1 2 3 Identifier ::= Letter | Identifier Letter | Identifier Digit Because it can be reduced to: 1 Identifier ::= Letter (Letter | Digit)* Or rather 1 (a|b|c|...|z)((a|b|c|...|z) | (0|1|2|...|9))* Called a regular expression often reduced to: 1 [a-z]([a-z]|[0-9])*","title":"Regular Grammars"},{"location":"4-semester/SPO/exam/3-lexical-analysis/#implementing","text":"A DFA can be implemented with a transition table: Can be coded in one of two forms: Table-driven The table is explicitly represented in a runtime table, interpreted by the program Often used by scanner generators Token independent Explicit control The table appears implicitly through the control logic of the program.","title":"Implementing"},{"location":"4-semester/SPO/exam/3-lexical-analysis/#pseudocode","text":"The DFA from above implemented:","title":"Pseudocode"},{"location":"4-semester/SPO/exam/3-lexical-analysis/#table-driven","text":"","title":"Table-driven"},{"location":"4-semester/SPO/exam/3-lexical-analysis/#explicit-control","text":"","title":"Explicit Control"},{"location":"4-semester/SPO/exam/3-lexical-analysis/#transducers","text":"It is smart to associate a semantic value with the token, such as the value of an integer constant. An FA that does this is called a transducer .","title":"Transducers"},{"location":"4-semester/SPO/exam/3-lexical-analysis/#scanner-generator","text":"The scanner is often generated by a tool, such as Lex The tokens are specified in a scanner specification that is fed to the generator tool.","title":"Scanner Generator"},{"location":"4-semester/SPO/exam/3-lexical-analysis/#performance-considerations","text":"","title":"Performance Considerations"},{"location":"4-semester/SPO/exam/4-parsing/","text":"Parsing \u00b6 Describe the purpose of the parser Discuss top down vs. bottom up parsing Explain necessary conditions for construction of recursive descent parsers Discuss the construction of an RD parser from a grammar Discuss bottom up / LR parsing To Read: Sebesta Chapter 4.4- Fischer Chapter 5, 6 Context Free Grammar \u00b6 See Context Free Grammars Parsers \u00b6 See From Tokens to Parse Trees Recursive Descent Parsing (Top-Down) \u00b6 LL(1) grammars can be parsed with a Recursive Descent Parser. Algorithm to Convert EBNF to RD Parser \u00b6 Express grammar in EBNF Grammar Transformations Left factorizations and Left recursion elemination Create a parser class with Private variable currentToken Methods to call the scanner accept and acceptIt Implement private parsing methods Add private parseN method for each non-terminal N N public parse method that gets the first token from the scanner calls parseS ( S S is the start symbol of the grammar) Bottom-Up Parsers \u00b6 LR grammars . LR(0) : Simplest algorithm Theoretically important but rather weak (not practical) SLR : An improved version of LR(0) More practical but still rather weak LR(1) : LR(0) algorithm with extra lookahead token Very powerful algorithm. Not used often because of large memory requirements (big parsing tables) Note: LR(0) and LR(1) use 1 lookahead token when operating 0 res. 1 refer to token used in table construction LR(k) for k>0 k>0 , k tokens are used for operation and table LALR : \"Watered down\" version of LR(1) Still very powerful, but much smaller parsing tables Most commonly used algorithm today Terminology \u00b6 \\alpha \\alpha is a right sentential form if S \\Rightarrow^*_{rm} \\alpha S \\Rightarrow^*_{rm} \\alpha with \\alpha=\\beta x \\alpha=\\beta x where x x is a string of terminals A handle of a right sentential form \\gamma \\ (=\\alpha \\beta w) \\gamma \\ (=\\alpha \\beta w) is a production A \\rightarrow \\beta A \\rightarrow \\beta and a position in \\gamma \\gamma where \\beta \\beta may be found and replaced by A A to produce the previous right-sentential form in a rightmost derivation of \\gamma: \\gamma: S \\Rightarrow^*_{rm} \\alpha A w \\Rightarrow_{rm} \\alpha \\beta w S \\Rightarrow^*_{rm} \\alpha A w \\Rightarrow_{rm} \\alpha \\beta w A handle is a production we can reverse without getting stuck If the handle is A \\rightarrow \\beta A \\rightarrow \\beta , we will also call \\beta \\beta the handle. Handles and Reductions \u00b6 Shift-Reduce \u00b6 Resembles Knitting \u00b6 Algorithm \u00b6 All bottom up parsers have similar algorithm A loop with these parts Try to find the leftmost node of the parse tree which has not yet been constructed, but all of whose children have been constructed. This sequence of children is called a handle The sequence of children is built by pushing, also called shifting , elements on a stack Construct a new parse tree node Called reducing The difference between different algorithms is only in the way they find the handle. Parse Table \u00b6 For every state and every terminal Either shift x Put next input-symbol on the stack and go to state x or reduce production On the stack we now have symbols to go backwards in the production - afterwards do a goto For every state and every non-terminal Goto x Tells us, in which state to be in after a reduce-operation Empty cells in the table indicate an error LR(0)-DFA \u00b6 To get parse table: Build DFA and encode it in a table. Every state is a set of items Transitions are labeled by symbols States must be closed New states are constructed from states and transitions Item :","title":"4 - Parsing"},{"location":"4-semester/SPO/exam/4-parsing/#parsing","text":"Describe the purpose of the parser Discuss top down vs. bottom up parsing Explain necessary conditions for construction of recursive descent parsers Discuss the construction of an RD parser from a grammar Discuss bottom up / LR parsing To Read: Sebesta Chapter 4.4- Fischer Chapter 5, 6","title":"Parsing"},{"location":"4-semester/SPO/exam/4-parsing/#context-free-grammar","text":"See Context Free Grammars","title":"Context Free Grammar"},{"location":"4-semester/SPO/exam/4-parsing/#parsers","text":"See From Tokens to Parse Trees","title":"Parsers"},{"location":"4-semester/SPO/exam/4-parsing/#recursive-descent-parsing-top-down","text":"LL(1) grammars can be parsed with a Recursive Descent Parser.","title":"Recursive Descent Parsing (Top-Down)"},{"location":"4-semester/SPO/exam/4-parsing/#algorithm-to-convert-ebnf-to-rd-parser","text":"Express grammar in EBNF Grammar Transformations Left factorizations and Left recursion elemination Create a parser class with Private variable currentToken Methods to call the scanner accept and acceptIt Implement private parsing methods Add private parseN method for each non-terminal N N public parse method that gets the first token from the scanner calls parseS ( S S is the start symbol of the grammar)","title":"Algorithm to Convert EBNF to RD Parser"},{"location":"4-semester/SPO/exam/4-parsing/#bottom-up-parsers","text":"LR grammars . LR(0) : Simplest algorithm Theoretically important but rather weak (not practical) SLR : An improved version of LR(0) More practical but still rather weak LR(1) : LR(0) algorithm with extra lookahead token Very powerful algorithm. Not used often because of large memory requirements (big parsing tables) Note: LR(0) and LR(1) use 1 lookahead token when operating 0 res. 1 refer to token used in table construction LR(k) for k>0 k>0 , k tokens are used for operation and table LALR : \"Watered down\" version of LR(1) Still very powerful, but much smaller parsing tables Most commonly used algorithm today","title":"Bottom-Up Parsers"},{"location":"4-semester/SPO/exam/4-parsing/#terminology","text":"\\alpha \\alpha is a right sentential form if S \\Rightarrow^*_{rm} \\alpha S \\Rightarrow^*_{rm} \\alpha with \\alpha=\\beta x \\alpha=\\beta x where x x is a string of terminals A handle of a right sentential form \\gamma \\ (=\\alpha \\beta w) \\gamma \\ (=\\alpha \\beta w) is a production A \\rightarrow \\beta A \\rightarrow \\beta and a position in \\gamma \\gamma where \\beta \\beta may be found and replaced by A A to produce the previous right-sentential form in a rightmost derivation of \\gamma: \\gamma: S \\Rightarrow^*_{rm} \\alpha A w \\Rightarrow_{rm} \\alpha \\beta w S \\Rightarrow^*_{rm} \\alpha A w \\Rightarrow_{rm} \\alpha \\beta w A handle is a production we can reverse without getting stuck If the handle is A \\rightarrow \\beta A \\rightarrow \\beta , we will also call \\beta \\beta the handle.","title":"Terminology"},{"location":"4-semester/SPO/exam/4-parsing/#handles-and-reductions","text":"","title":"Handles and Reductions"},{"location":"4-semester/SPO/exam/4-parsing/#shift-reduce","text":"","title":"Shift-Reduce"},{"location":"4-semester/SPO/exam/4-parsing/#resembles-knitting","text":"","title":"Resembles Knitting"},{"location":"4-semester/SPO/exam/4-parsing/#algorithm","text":"All bottom up parsers have similar algorithm A loop with these parts Try to find the leftmost node of the parse tree which has not yet been constructed, but all of whose children have been constructed. This sequence of children is called a handle The sequence of children is built by pushing, also called shifting , elements on a stack Construct a new parse tree node Called reducing The difference between different algorithms is only in the way they find the handle.","title":"Algorithm"},{"location":"4-semester/SPO/exam/4-parsing/#parse-table","text":"For every state and every terminal Either shift x Put next input-symbol on the stack and go to state x or reduce production On the stack we now have symbols to go backwards in the production - afterwards do a goto For every state and every non-terminal Goto x Tells us, in which state to be in after a reduce-operation Empty cells in the table indicate an error","title":"Parse Table"},{"location":"4-semester/SPO/exam/4-parsing/#lr0-dfa","text":"To get parse table: Build DFA and encode it in a table. Every state is a set of items Transitions are labeled by symbols States must be closed New states are constructed from states and transitions Item :","title":"LR(0)-DFA"},{"location":"4-semester/SPO/exam/5-semantic-analysis/","text":"Semantic Analysis \u00b6 Describe the purpose of the Semantic Analysis phase Discuss Identification and type checking Discuss scopes/block structure and implication for implementation of identification-/symbol tables Discuss implementation of semantic analysis","title":"5 - Semantic Analysis"},{"location":"4-semester/SPO/exam/5-semantic-analysis/#semantic-analysis","text":"Describe the purpose of the Semantic Analysis phase Discuss Identification and type checking Discuss scopes/block structure and implication for implementation of identification-/symbol tables Discuss implementation of semantic analysis","title":"Semantic Analysis"},{"location":"4-semester/SPO/exam/6-runtime-organization/","text":"Runtime Organization \u00b6 Data representation (direct vs. indirect) Storage allocation strategies: static vs. stack dynamic Activation records (sometimes called frames) Routines and Parameter passing \\newcommand{\\bits}{\\text{ bits}} \\newcommand{\\bit}{\\text{ bit}} \\newcommand{\\size}[1]{\\text{size}[\\mathtt{#1}]} \\newcommand{\\bits}{\\text{ bits}} \\newcommand{\\bit}{\\text{ bit}} \\newcommand{\\size}[1]{\\text{size}[\\mathtt{#1}]} Data Representation \u00b6 Data representation: how to represent values of the source language on the target machine. Important properties of a representation schema Non-confusion Different values of a given type should have different representations Uniqueness Each value should always have the same representation Important issues Constant-size representation The representation of all values of a given type should occupy the same amount of space Direct vs indirect representation Indirect Representation \u00b6 Why chose indirect representation? Make the representation \"constant size\" even if representation requires different amounts of memory for different values. Indirect vs Direct \u00b6 Direct representations are often preferable for efficiency More efficient access (no need to follow pointers) More efficient \"storage class\" (e.g stack- rather than heap allocation) For types with widely varying size of representations, it is almost a must to use indirect. Notation \u00b6 If T T is a type then: \\#[T] \\#[T] is the cardinality of the type (the number of possible values) \\text{size}[T] \\text{size}[T] is the size of the representation (in number of bits/bytes) In general: if \\#[T]=n \\#[T]=n then \\text{size}[T]=\\log_2n \\text{ bits} \\text{size}[T]=\\log_2n \\text{ bits} Primitive Types \u00b6 Types that cannot be decomposed into simpler types. For example: integer , boolean , char etc. Boolean \u00b6 2 values: true and false \\Rightarrow \\#[\\mathtt{boolean}]=2 \\Rightarrow \\#[\\mathtt{boolean}]=2 \\Rightarrow \\text{size}[\\mathtt{boolean}]=1 \\text{ bit} \\Rightarrow \\text{size}[\\mathtt{boolean}]=1 \\text{ bit} Possible representations Integer \u00b6 Typically uses one word (16, 32 or 64 bits). \\Rightarrow \\#[\\mathtt{integer}]=\\leq 2^{16}=65536 \\Rightarrow \\#[\\mathtt{integer}]=\\leq 2^{16}=65536 \\Rightarrow \\text{size}[\\mathtt{boolean}]=word\\ (=16 \\bits) \\Rightarrow \\text{size}[\\mathtt{boolean}]=word\\ (=16 \\bits) Modern processors use two's complement representation of integers: Composite Types \u00b6 Records Arrays Variant Records or Disjoint Unions Pointers or References (Objects) Functions Records \u00b6 Example: Triangle Record Representations 1 2 3 4 5 6 7 8 9 10 11 12 type Date = record y : Integer , m : Integer , d : Integer end ; type Details = record female : Boolean , dob : Date , status : Char end ; var today : Date ; var my : Details \\Rightarrow \\text{size}[\\mathtt{Date}]= 3*\\size{integer}=3\\ words \\Rightarrow \\text{size}[\\mathtt{Date}]= 3*\\size{integer}=3\\ words \\text{address}[\\mathtt{today.y}]=\\text{address}[\\mathtt{today}]+0\\\\ \\text{address}[\\mathtt{today.m}]=\\text{address}[\\mathtt{today}]+1\\\\ \\text{address}[\\mathtt{today.d}]=\\text{address}[\\mathtt{today}]+2 \\text{address}[\\mathtt{today.y}]=\\text{address}[\\mathtt{today}]+0\\\\ \\text{address}[\\mathtt{today.m}]=\\text{address}[\\mathtt{today}]+1\\\\ \\text{address}[\\mathtt{today.d}]=\\text{address}[\\mathtt{today}]+2 \\text{address}[\\mathtt{my.dob.m}]=\\text{address}[\\mathtt{my.dob}]+1=\\text{address}[\\mathtt{my}]+2 \\text{address}[\\mathtt{my.dob.m}]=\\text{address}[\\mathtt{my.dob}]+1=\\text{address}[\\mathtt{my}]+2 Disjoint Unions \u00b6 Example: Pascal variant records 1 2 3 4 5 6 type Number = record case discrete : Boolean of true : ( i : Integer ) ; false : ( r : Real ) end ; var num : Number Assuming \\size{Integer}=\\size{Boolean}=1 \\size{Integer}=\\size{Boolean}=1 and \\size{Real}=2 \\size{Real}=2 then \\size{Number}=\\size{Boolean}+ \\max{(\\size{Integer}, \\size{Real})}=1+\\max{(1,2)}=3 \\size{Number}=\\size{Boolean}+ \\max{(\\size{Integer}, \\size{Real})}=1+\\max{(1,2)}=3 Arrays \u00b6 Two kinds of arrays Static arrays: size is known at compile time Dynamic arrays: Number of elements is computed at run-time and sometimes may vary at run-time (Flex-arrays) Static Arrays \u00b6 1 2 3 type Name = array 6 of Char ; var me : Name ; var names : array 2 of Name 1 2 3 4 type Coding = record Char c , Integer n end var code : array 3 of Coding Dynamic Arrays \u00b6 Example: Java arrays 1 2 3 4 5 char [] buffer ; buffer = new char [ buffersize ] ; ... for ( int i = 0 ; i < buffer . length ; i ++ ) // can ask for size at run time buffer [ i ]= '' ; 1 2 char [] buffer ; buffer = new char [ 7 ] ; Possible representation: Another possible representation: Where to Put Data \u00b6 3 methods Static allocation Stack allocation Heap allocation Static Allocation \u00b6 Originally all data were global. All memory allocation was static . Data was placed at a fixed memory address during compilation, for the entire execution of a program. Static allocation can waste memory space. Fortran introduced equivalent statement that forces 2 variables to share memory location In modern languages, static allocation is used for global variables and literals (constants) that are fixed in size and accessible throughout program execution. Also used for static and extern variables in C/C++ and for static fields in C# and Java classes. Stack Allocation \u00b6 Recursive languages require dynamic memory allocation. Each time a recursive method is called, a new copy of local variables (frame) is pushed on a runtime stack. The number of allocations is unknown at compile-time. A frame (or activation record) contains space for all of the local variables in the method. When the method returns, its frame is popped and the space reclaimed. Thus, only the methods that are actually executing are allocated memory space in the runtime stack. This is called stack allocation . Frame for procedure p : Stack Storage Allocation \u00b6 Allocation of local variables Example: When do the variables \"exist\"?: Procedure activation behaves like a stack (LIFO) Local variables \"live\" as long as the procedure they are declared in. 1+2\\Rightarrow 1+2\\Rightarrow Allocation of locals on the \"call stack\" us a good model Recursion \u00b6 1 2 3 4 int fact ( int n ) { if ( n > 1 ) return n * fact ( n - 1 ); else return 1 ; } Dynamic Link \u00b6 Stack frames may vary in size and because the stack may contain more than just frames (e.g. registers saved across calls), dynamic link is used to point to the preceding frame: Nested Functions/Procedures \u00b6 1 2 3 4 5 6 int p ( int a ) { int q ( int b ) { if ( b < 0 ) q ( - b ) else return a + b ; } return q ( - 10 ); } Functions can nest in languages like Pascal, ML and Python. How to keep track of static block structure as above? A static link points to the frame of the method that statically encloses the current method: An alternative is the use of a display. We maintain a set of registers which comprise the display: Blocks \u00b6 1 2 3 4 5 void p ( int a ) { int b ; if ( a > 0 ) { float c , d ; } else { int e [ 10 ] ; } } We can view blocks as parameter-less procedures, and use procedure-level-frames to implement blocks. But because the then and else parts of the if statement above are mutually exclusive, variables in block 1 and 2 can overlay. This is called block-level frame as contrasted with procedure level frame High-order Functions \u00b6 Functions as values (first-class) Pass as arguments Return as values Stored into data structures Implementation: A code pointer (a code address + an environment pointer) Called a closure Routines \u00b6 The assembly language equivalent of procedures Not directly supported by language constructs, but modeled in terms of how to use the low-level machine to emulate procedures. Behavior to emulate: Calling a routine and returning to the caller after completion Passing arguments to a called routine Returning a result from a routine Local and non-local variables Transferring control to and from routine: Most low-level processors have CALL and RETURN for transferring control from caller to callee and back. Transmitting arguments and return values: Caller and callee must agree on a method to transfer argument and return values. Called routine protocol There are many possible ways. Often dictated by the operating system. Routine Protocol Examples \u00b6 Example 1 \u00b6 Passing arguments: First argument in R1, second in R2, etc. Passing return value: Return result (if any) in R0 This is simplistic What if more arguments than registers What if argument is larger than a register Example 2 \u00b6 Passing arguments: Pass argument on top of stack Passing of return value: Leave return value on stack top Puts no boundary on number of arguments, or size of arguments.","title":"6 - Runtime Organization"},{"location":"4-semester/SPO/exam/6-runtime-organization/#runtime-organization","text":"Data representation (direct vs. indirect) Storage allocation strategies: static vs. stack dynamic Activation records (sometimes called frames) Routines and Parameter passing \\newcommand{\\bits}{\\text{ bits}} \\newcommand{\\bit}{\\text{ bit}} \\newcommand{\\size}[1]{\\text{size}[\\mathtt{#1}]} \\newcommand{\\bits}{\\text{ bits}} \\newcommand{\\bit}{\\text{ bit}} \\newcommand{\\size}[1]{\\text{size}[\\mathtt{#1}]}","title":"Runtime Organization"},{"location":"4-semester/SPO/exam/6-runtime-organization/#data-representation","text":"Data representation: how to represent values of the source language on the target machine. Important properties of a representation schema Non-confusion Different values of a given type should have different representations Uniqueness Each value should always have the same representation Important issues Constant-size representation The representation of all values of a given type should occupy the same amount of space Direct vs indirect representation","title":"Data Representation"},{"location":"4-semester/SPO/exam/6-runtime-organization/#indirect-representation","text":"Why chose indirect representation? Make the representation \"constant size\" even if representation requires different amounts of memory for different values.","title":"Indirect Representation"},{"location":"4-semester/SPO/exam/6-runtime-organization/#indirect-vs-direct","text":"Direct representations are often preferable for efficiency More efficient access (no need to follow pointers) More efficient \"storage class\" (e.g stack- rather than heap allocation) For types with widely varying size of representations, it is almost a must to use indirect.","title":"Indirect vs Direct"},{"location":"4-semester/SPO/exam/6-runtime-organization/#notation","text":"If T T is a type then: \\#[T] \\#[T] is the cardinality of the type (the number of possible values) \\text{size}[T] \\text{size}[T] is the size of the representation (in number of bits/bytes) In general: if \\#[T]=n \\#[T]=n then \\text{size}[T]=\\log_2n \\text{ bits} \\text{size}[T]=\\log_2n \\text{ bits}","title":"Notation"},{"location":"4-semester/SPO/exam/6-runtime-organization/#primitive-types","text":"Types that cannot be decomposed into simpler types. For example: integer , boolean , char etc.","title":"Primitive Types"},{"location":"4-semester/SPO/exam/6-runtime-organization/#boolean","text":"2 values: true and false \\Rightarrow \\#[\\mathtt{boolean}]=2 \\Rightarrow \\#[\\mathtt{boolean}]=2 \\Rightarrow \\text{size}[\\mathtt{boolean}]=1 \\text{ bit} \\Rightarrow \\text{size}[\\mathtt{boolean}]=1 \\text{ bit} Possible representations","title":"Boolean"},{"location":"4-semester/SPO/exam/6-runtime-organization/#integer","text":"Typically uses one word (16, 32 or 64 bits). \\Rightarrow \\#[\\mathtt{integer}]=\\leq 2^{16}=65536 \\Rightarrow \\#[\\mathtt{integer}]=\\leq 2^{16}=65536 \\Rightarrow \\text{size}[\\mathtt{boolean}]=word\\ (=16 \\bits) \\Rightarrow \\text{size}[\\mathtt{boolean}]=word\\ (=16 \\bits) Modern processors use two's complement representation of integers:","title":"Integer"},{"location":"4-semester/SPO/exam/6-runtime-organization/#composite-types","text":"Records Arrays Variant Records or Disjoint Unions Pointers or References (Objects) Functions","title":"Composite  Types"},{"location":"4-semester/SPO/exam/6-runtime-organization/#records","text":"Example: Triangle Record Representations 1 2 3 4 5 6 7 8 9 10 11 12 type Date = record y : Integer , m : Integer , d : Integer end ; type Details = record female : Boolean , dob : Date , status : Char end ; var today : Date ; var my : Details \\Rightarrow \\text{size}[\\mathtt{Date}]= 3*\\size{integer}=3\\ words \\Rightarrow \\text{size}[\\mathtt{Date}]= 3*\\size{integer}=3\\ words \\text{address}[\\mathtt{today.y}]=\\text{address}[\\mathtt{today}]+0\\\\ \\text{address}[\\mathtt{today.m}]=\\text{address}[\\mathtt{today}]+1\\\\ \\text{address}[\\mathtt{today.d}]=\\text{address}[\\mathtt{today}]+2 \\text{address}[\\mathtt{today.y}]=\\text{address}[\\mathtt{today}]+0\\\\ \\text{address}[\\mathtt{today.m}]=\\text{address}[\\mathtt{today}]+1\\\\ \\text{address}[\\mathtt{today.d}]=\\text{address}[\\mathtt{today}]+2 \\text{address}[\\mathtt{my.dob.m}]=\\text{address}[\\mathtt{my.dob}]+1=\\text{address}[\\mathtt{my}]+2 \\text{address}[\\mathtt{my.dob.m}]=\\text{address}[\\mathtt{my.dob}]+1=\\text{address}[\\mathtt{my}]+2","title":"Records"},{"location":"4-semester/SPO/exam/6-runtime-organization/#disjoint-unions","text":"Example: Pascal variant records 1 2 3 4 5 6 type Number = record case discrete : Boolean of true : ( i : Integer ) ; false : ( r : Real ) end ; var num : Number Assuming \\size{Integer}=\\size{Boolean}=1 \\size{Integer}=\\size{Boolean}=1 and \\size{Real}=2 \\size{Real}=2 then \\size{Number}=\\size{Boolean}+ \\max{(\\size{Integer}, \\size{Real})}=1+\\max{(1,2)}=3 \\size{Number}=\\size{Boolean}+ \\max{(\\size{Integer}, \\size{Real})}=1+\\max{(1,2)}=3","title":"Disjoint Unions"},{"location":"4-semester/SPO/exam/6-runtime-organization/#arrays","text":"Two kinds of arrays Static arrays: size is known at compile time Dynamic arrays: Number of elements is computed at run-time and sometimes may vary at run-time (Flex-arrays)","title":"Arrays"},{"location":"4-semester/SPO/exam/6-runtime-organization/#static-arrays","text":"1 2 3 type Name = array 6 of Char ; var me : Name ; var names : array 2 of Name 1 2 3 4 type Coding = record Char c , Integer n end var code : array 3 of Coding","title":"Static Arrays"},{"location":"4-semester/SPO/exam/6-runtime-organization/#dynamic-arrays","text":"Example: Java arrays 1 2 3 4 5 char [] buffer ; buffer = new char [ buffersize ] ; ... for ( int i = 0 ; i < buffer . length ; i ++ ) // can ask for size at run time buffer [ i ]= '' ; 1 2 char [] buffer ; buffer = new char [ 7 ] ; Possible representation: Another possible representation:","title":"Dynamic Arrays"},{"location":"4-semester/SPO/exam/6-runtime-organization/#where-to-put-data","text":"3 methods Static allocation Stack allocation Heap allocation","title":"Where to Put Data"},{"location":"4-semester/SPO/exam/6-runtime-organization/#static-allocation","text":"Originally all data were global. All memory allocation was static . Data was placed at a fixed memory address during compilation, for the entire execution of a program. Static allocation can waste memory space. Fortran introduced equivalent statement that forces 2 variables to share memory location In modern languages, static allocation is used for global variables and literals (constants) that are fixed in size and accessible throughout program execution. Also used for static and extern variables in C/C++ and for static fields in C# and Java classes.","title":"Static Allocation"},{"location":"4-semester/SPO/exam/6-runtime-organization/#stack-allocation","text":"Recursive languages require dynamic memory allocation. Each time a recursive method is called, a new copy of local variables (frame) is pushed on a runtime stack. The number of allocations is unknown at compile-time. A frame (or activation record) contains space for all of the local variables in the method. When the method returns, its frame is popped and the space reclaimed. Thus, only the methods that are actually executing are allocated memory space in the runtime stack. This is called stack allocation . Frame for procedure p :","title":"Stack Allocation"},{"location":"4-semester/SPO/exam/6-runtime-organization/#stack-storage-allocation","text":"Allocation of local variables Example: When do the variables \"exist\"?: Procedure activation behaves like a stack (LIFO) Local variables \"live\" as long as the procedure they are declared in. 1+2\\Rightarrow 1+2\\Rightarrow Allocation of locals on the \"call stack\" us a good model","title":"Stack Storage Allocation"},{"location":"4-semester/SPO/exam/6-runtime-organization/#recursion","text":"1 2 3 4 int fact ( int n ) { if ( n > 1 ) return n * fact ( n - 1 ); else return 1 ; }","title":"Recursion"},{"location":"4-semester/SPO/exam/6-runtime-organization/#dynamic-link","text":"Stack frames may vary in size and because the stack may contain more than just frames (e.g. registers saved across calls), dynamic link is used to point to the preceding frame:","title":"Dynamic Link"},{"location":"4-semester/SPO/exam/6-runtime-organization/#nested-functionsprocedures","text":"1 2 3 4 5 6 int p ( int a ) { int q ( int b ) { if ( b < 0 ) q ( - b ) else return a + b ; } return q ( - 10 ); } Functions can nest in languages like Pascal, ML and Python. How to keep track of static block structure as above? A static link points to the frame of the method that statically encloses the current method: An alternative is the use of a display. We maintain a set of registers which comprise the display:","title":"Nested Functions/Procedures"},{"location":"4-semester/SPO/exam/6-runtime-organization/#blocks","text":"1 2 3 4 5 void p ( int a ) { int b ; if ( a > 0 ) { float c , d ; } else { int e [ 10 ] ; } } We can view blocks as parameter-less procedures, and use procedure-level-frames to implement blocks. But because the then and else parts of the if statement above are mutually exclusive, variables in block 1 and 2 can overlay. This is called block-level frame as contrasted with procedure level frame","title":"Blocks"},{"location":"4-semester/SPO/exam/6-runtime-organization/#high-order-functions","text":"Functions as values (first-class) Pass as arguments Return as values Stored into data structures Implementation: A code pointer (a code address + an environment pointer) Called a closure","title":"High-order Functions"},{"location":"4-semester/SPO/exam/6-runtime-organization/#routines","text":"The assembly language equivalent of procedures Not directly supported by language constructs, but modeled in terms of how to use the low-level machine to emulate procedures. Behavior to emulate: Calling a routine and returning to the caller after completion Passing arguments to a called routine Returning a result from a routine Local and non-local variables Transferring control to and from routine: Most low-level processors have CALL and RETURN for transferring control from caller to callee and back. Transmitting arguments and return values: Caller and callee must agree on a method to transfer argument and return values. Called routine protocol There are many possible ways. Often dictated by the operating system.","title":"Routines"},{"location":"4-semester/SPO/exam/6-runtime-organization/#routine-protocol-examples","text":"","title":"Routine Protocol Examples"},{"location":"4-semester/SPO/exam/6-runtime-organization/#example-1","text":"Passing arguments: First argument in R1, second in R2, etc. Passing return value: Return result (if any) in R0 This is simplistic What if more arguments than registers What if argument is larger than a register","title":"Example 1"},{"location":"4-semester/SPO/exam/6-runtime-organization/#example-2","text":"Passing arguments: Pass argument on top of stack Passing of return value: Leave return value on stack top Puts no boundary on number of arguments, or size of arguments.","title":"Example 2"},{"location":"4-semester/SPO/exam/7-heap-allocation-and-gc/","text":"Heap Allocation and Garbage Collection \u00b6 Why may we need heap allocation? Garbage collection strategies Types of GCs Heap Allocation \u00b6 Memory allocation under explicit programmatic control C malloc , C++ / Pascal / Java / C# new -operation Memory allocation implicit in language constructs Lisp, Scheme, Haskell, SML, ... most functional languages. Autoboxing/unboxing in Java 1.5 and C# Deallocation under explicit programmatic control C, C++, Pascal ( free , delete , dispose operations) Deallocation implicit Java, C#, Lisp, Scheme, Haskell, SML, ... Garbage \u00b6 1 2 3 4 int * p , * q ; ... p = malloc ( sizeof ( int )); p = q ; Created space becomes garbage. 1 2 3 4 for ( int i = 0 ; i < 10000 ; i ++ ) { SomeClass obj = new SomeClass ( i ); System . out . println ( obj ); } Creates 10 000 objects which becomes garbage just after the print. Problems with Explicit Heap Management \u00b6 Dangling Pointer \u00b6 1 2 3 4 5 int * p , * q ; ... p = malloc ( sizeof ( int )) q = p ; free ( p ); Dangling pointer in q now. Hard to Recognize \u00b6 1 2 3 4 float myArray [ 100 ]; p = myArray ; * ( p + i ) = ... //equivalent to myArray[i] Stacks and Dynamic Allocations are Incompatible \u00b6 We can't do dynamic allocation within the stack. Where to put the Heap? \u00b6 It may grow and shrink during runtime. It is not LIFO like the stack We will typically have both heap- and stack allocated memory coexisting. Implicit Memory Management \u00b6 A current trend of modern programming language development is to give only implicit means of memory management to the programmer. The constant increase of hardware memory justifies the policy of automatic memory management The explicit memory management distracts the programmer from his primary tasks The philosophy of high-level languages conforms to the implicit memory management. Garbage Collection \u00b6 Garbage collection provides the \"illusion of infinite memory\"! A garbage collector predicts the future! Types of garbage collectors The \"Classic\" algorithms Reference counting Mark and Sweep Copying garbage collection Generational garbage collection Incremental Tracing garbage collection Direct Garbage Collectors: a record is associated with each node in the heap. The record for node N N indicates how many other nodes or roots points to N N . Indirect/Tracing Garbage Collectors: usually invoked when a user's request for memory fails. The garbage collector visits all live nodes, and returns all other memory to the free list. If sufficient memory has been recovered from this process, the user's request for memory is satisfied. Terminology \u00b6 Roots Values that a program can manipulate directly (values held in registers, on the program stack, and global variables) Node/Cell/Object An individually allocated piece of data in the heap Children Nodes The list of pointers that a given node contains Live Node A node whose address is held in a root or is the child of a live node Garbage Nodes that are not live, but are not free either. Garbage Collection The task of recovering (freeing) garbage nodes Mutator The program running alongside the garbage collection system Reference Counting \u00b6 Every cell has a reference count field, containing the number of pointers to that cell from roots or heap cells. Initially all cells in the heap are placed in the free list When a cell is allocated from the free list , its reference count is set to 1 When a pointer is set to reference a cell, the cells reference count is incremented by 1. If a pointer to the cells is deleted, its reference count is decremented by 1. When a cell's reference count reaches 0, its pointers to its children are deleted and it is returned to the free list Advantages and Disadvantages \u00b6 Advantages: GC overhead is distributed Locality of reference is no worse than mutator Free memory is returned to free list quickly Disadvantages: High time cost (every time a pointer is changed, reference count must be updated) In place of a single assignment x.f = p : Storage overhead for reference counter can be high If the reference counter overflows, the object becomes permanent Unable to reclaim cyclic data structures Free Memory Management \u00b6 The Heap is not LIFO like the stack, how to manage free space in the \"middle\" of the heap? We can use a free list Which we can store in the free memory itself since it is not used by anything else Mark-Sweep \u00b6 The first tracing garbage collection algorithm Garbage cells are allowed to build up until heap space is exhausted (a user program requests a memory allocation, but there is insufficient free space on the heap) At this point, the mark-sweep algorithm is invoked, and garbage cells are returned to the free list Performed in two phases: Mark: identifies all live cells by setting a mark bit. Live cells are cells reachable from a root Sweep: returns garbage cells to the free list Pseudo Code \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 void garbageCollect() { mark all heap variables as free foreach frame in stack scan(frame) foreach heapvar marked as free add heapvar to freelist } void scan(region) { foreach pointer p in region{ if p points to region marked as free then{ mark region at p as reachable scan(region at p) } } } Advantages and Disadvantages \u00b6 Advantages: Cyclic data structures can be recovered Tends to be faster than reference counting Disadvantages: Computation must be halted while GC is being performed Every live cell must be visited in the mark phase, and every cell in the heap must be visited in the sweep phase. GC becomes more frequent as residency of a program increases May fragment memory Mark-Sweep-Compact \u00b6 Advantages: The contiguous free area eliminates fragmentation problem Allocating objects of various sizes is simple The garbage space is \"squeezed out\", without disturbing the original ordering of objects. This improves locality. Disadvantages: Requires several passes over the data. \"Sliding compactors\" takes two, three or more passes over the live objects. One pass computes the new location Subsequent passes update the pointers to refer to new locations, and actually move the objects Copying Garbage Collection \u00b6 Cheney's Algorithm Like mark-compact, copying garbage collection, but does not really \"collect\" garbage. The heap is subdivided into two contiguous subspaces FromSpace and ToSpace During normal program execution, only one of these semispaces is in use. When the garbage collector is called, all the live data are copied from the current semispace (FromSpace) to the other semispace (ToSpace), so that objects need only be traversed once. The work needed is proportional to the amount of live data (all of which must be copied). Advantages and Disadvantages \u00b6 Advantages: Allocation is extremely cheap. Excellent asymptotic complexity. Fragmentation is eliminated. Only one pass through the data is required. Disadvantages: The use of two semi-spaces doubles memory requirement Poor locality. Using virtual memory will cause excessive paging. Generational Garbage Collection \u00b6 Attempts to address weaknesses of simple tracing collectors such as mark-sweep and copying collectors All active data must be marked or copied For copying collectors, each page of the heap is touched every two collection cycles, even though the user program is only using half the heap, leading to poor cache behavior and page faults Long-lived objects are handled inefficiently Generational garbage collection is based on the generational hypothesis : Most objects die young As such, concentrate GC efforts on objects likely to be garbage: young objects Advantages and Disadvantages \u00b6 Advantages: Keeps youngest generation's size small Helps address mistakes made by the promotion policy by creating more intermediate generations that still get garbage collected fairly frequently. Disadvantages: Collections for intermediate generations may be disruptive. Tends to increase number of inter-generational pointers, increasing the size of the root set for younger generations. Performs poorly if any of the main assumptions are false: That objects tend to die young. That there are relatively few pointers from old objects to young ones. Incremental Tracing Collectors \u00b6 Program (Mutator) and Garbage Collector run concurrently. Can think of system as similar to two threads. One performs collection, and the other represents the regular program in execution. Can be used in systems with real-time requirements. For example, process control systems. Allow mutator to do its job without destroying collector\u2019s possibilities for keeping track of modifications of the object graph, and at the same time Allowing collector to do its job without interfering with mutator Summary \u00b6","title":"7 - Heap Allocation and Garbage Collection"},{"location":"4-semester/SPO/exam/7-heap-allocation-and-gc/#heap-allocation-and-garbage-collection","text":"Why may we need heap allocation? Garbage collection strategies Types of GCs","title":"Heap Allocation and Garbage Collection"},{"location":"4-semester/SPO/exam/7-heap-allocation-and-gc/#heap-allocation","text":"Memory allocation under explicit programmatic control C malloc , C++ / Pascal / Java / C# new -operation Memory allocation implicit in language constructs Lisp, Scheme, Haskell, SML, ... most functional languages. Autoboxing/unboxing in Java 1.5 and C# Deallocation under explicit programmatic control C, C++, Pascal ( free , delete , dispose operations) Deallocation implicit Java, C#, Lisp, Scheme, Haskell, SML, ...","title":"Heap Allocation"},{"location":"4-semester/SPO/exam/7-heap-allocation-and-gc/#garbage","text":"1 2 3 4 int * p , * q ; ... p = malloc ( sizeof ( int )); p = q ; Created space becomes garbage. 1 2 3 4 for ( int i = 0 ; i < 10000 ; i ++ ) { SomeClass obj = new SomeClass ( i ); System . out . println ( obj ); } Creates 10 000 objects which becomes garbage just after the print.","title":"Garbage"},{"location":"4-semester/SPO/exam/7-heap-allocation-and-gc/#problems-with-explicit-heap-management","text":"","title":"Problems with Explicit Heap Management"},{"location":"4-semester/SPO/exam/7-heap-allocation-and-gc/#dangling-pointer","text":"1 2 3 4 5 int * p , * q ; ... p = malloc ( sizeof ( int )) q = p ; free ( p ); Dangling pointer in q now.","title":"Dangling Pointer"},{"location":"4-semester/SPO/exam/7-heap-allocation-and-gc/#hard-to-recognize","text":"1 2 3 4 float myArray [ 100 ]; p = myArray ; * ( p + i ) = ... //equivalent to myArray[i]","title":"Hard to Recognize"},{"location":"4-semester/SPO/exam/7-heap-allocation-and-gc/#stacks-and-dynamic-allocations-are-incompatible","text":"We can't do dynamic allocation within the stack.","title":"Stacks and Dynamic Allocations are Incompatible"},{"location":"4-semester/SPO/exam/7-heap-allocation-and-gc/#where-to-put-the-heap","text":"It may grow and shrink during runtime. It is not LIFO like the stack We will typically have both heap- and stack allocated memory coexisting.","title":"Where to put the Heap?"},{"location":"4-semester/SPO/exam/7-heap-allocation-and-gc/#implicit-memory-management","text":"A current trend of modern programming language development is to give only implicit means of memory management to the programmer. The constant increase of hardware memory justifies the policy of automatic memory management The explicit memory management distracts the programmer from his primary tasks The philosophy of high-level languages conforms to the implicit memory management.","title":"Implicit Memory Management"},{"location":"4-semester/SPO/exam/7-heap-allocation-and-gc/#garbage-collection","text":"Garbage collection provides the \"illusion of infinite memory\"! A garbage collector predicts the future! Types of garbage collectors The \"Classic\" algorithms Reference counting Mark and Sweep Copying garbage collection Generational garbage collection Incremental Tracing garbage collection Direct Garbage Collectors: a record is associated with each node in the heap. The record for node N N indicates how many other nodes or roots points to N N . Indirect/Tracing Garbage Collectors: usually invoked when a user's request for memory fails. The garbage collector visits all live nodes, and returns all other memory to the free list. If sufficient memory has been recovered from this process, the user's request for memory is satisfied.","title":"Garbage Collection"},{"location":"4-semester/SPO/exam/7-heap-allocation-and-gc/#terminology","text":"Roots Values that a program can manipulate directly (values held in registers, on the program stack, and global variables) Node/Cell/Object An individually allocated piece of data in the heap Children Nodes The list of pointers that a given node contains Live Node A node whose address is held in a root or is the child of a live node Garbage Nodes that are not live, but are not free either. Garbage Collection The task of recovering (freeing) garbage nodes Mutator The program running alongside the garbage collection system","title":"Terminology"},{"location":"4-semester/SPO/exam/7-heap-allocation-and-gc/#reference-counting","text":"Every cell has a reference count field, containing the number of pointers to that cell from roots or heap cells. Initially all cells in the heap are placed in the free list When a cell is allocated from the free list , its reference count is set to 1 When a pointer is set to reference a cell, the cells reference count is incremented by 1. If a pointer to the cells is deleted, its reference count is decremented by 1. When a cell's reference count reaches 0, its pointers to its children are deleted and it is returned to the free list","title":"Reference Counting"},{"location":"4-semester/SPO/exam/7-heap-allocation-and-gc/#advantages-and-disadvantages","text":"Advantages: GC overhead is distributed Locality of reference is no worse than mutator Free memory is returned to free list quickly Disadvantages: High time cost (every time a pointer is changed, reference count must be updated) In place of a single assignment x.f = p : Storage overhead for reference counter can be high If the reference counter overflows, the object becomes permanent Unable to reclaim cyclic data structures","title":"Advantages and Disadvantages"},{"location":"4-semester/SPO/exam/7-heap-allocation-and-gc/#free-memory-management","text":"The Heap is not LIFO like the stack, how to manage free space in the \"middle\" of the heap? We can use a free list Which we can store in the free memory itself since it is not used by anything else","title":"Free Memory Management"},{"location":"4-semester/SPO/exam/7-heap-allocation-and-gc/#mark-sweep","text":"The first tracing garbage collection algorithm Garbage cells are allowed to build up until heap space is exhausted (a user program requests a memory allocation, but there is insufficient free space on the heap) At this point, the mark-sweep algorithm is invoked, and garbage cells are returned to the free list Performed in two phases: Mark: identifies all live cells by setting a mark bit. Live cells are cells reachable from a root Sweep: returns garbage cells to the free list","title":"Mark-Sweep"},{"location":"4-semester/SPO/exam/7-heap-allocation-and-gc/#pseudo-code","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 void garbageCollect() { mark all heap variables as free foreach frame in stack scan(frame) foreach heapvar marked as free add heapvar to freelist } void scan(region) { foreach pointer p in region{ if p points to region marked as free then{ mark region at p as reachable scan(region at p) } } }","title":"Pseudo Code"},{"location":"4-semester/SPO/exam/7-heap-allocation-and-gc/#advantages-and-disadvantages_1","text":"Advantages: Cyclic data structures can be recovered Tends to be faster than reference counting Disadvantages: Computation must be halted while GC is being performed Every live cell must be visited in the mark phase, and every cell in the heap must be visited in the sweep phase. GC becomes more frequent as residency of a program increases May fragment memory","title":"Advantages and Disadvantages"},{"location":"4-semester/SPO/exam/7-heap-allocation-and-gc/#mark-sweep-compact","text":"Advantages: The contiguous free area eliminates fragmentation problem Allocating objects of various sizes is simple The garbage space is \"squeezed out\", without disturbing the original ordering of objects. This improves locality. Disadvantages: Requires several passes over the data. \"Sliding compactors\" takes two, three or more passes over the live objects. One pass computes the new location Subsequent passes update the pointers to refer to new locations, and actually move the objects","title":"Mark-Sweep-Compact"},{"location":"4-semester/SPO/exam/7-heap-allocation-and-gc/#copying-garbage-collection","text":"Cheney's Algorithm Like mark-compact, copying garbage collection, but does not really \"collect\" garbage. The heap is subdivided into two contiguous subspaces FromSpace and ToSpace During normal program execution, only one of these semispaces is in use. When the garbage collector is called, all the live data are copied from the current semispace (FromSpace) to the other semispace (ToSpace), so that objects need only be traversed once. The work needed is proportional to the amount of live data (all of which must be copied).","title":"Copying Garbage Collection"},{"location":"4-semester/SPO/exam/7-heap-allocation-and-gc/#advantages-and-disadvantages_2","text":"Advantages: Allocation is extremely cheap. Excellent asymptotic complexity. Fragmentation is eliminated. Only one pass through the data is required. Disadvantages: The use of two semi-spaces doubles memory requirement Poor locality. Using virtual memory will cause excessive paging.","title":"Advantages and Disadvantages"},{"location":"4-semester/SPO/exam/7-heap-allocation-and-gc/#generational-garbage-collection","text":"Attempts to address weaknesses of simple tracing collectors such as mark-sweep and copying collectors All active data must be marked or copied For copying collectors, each page of the heap is touched every two collection cycles, even though the user program is only using half the heap, leading to poor cache behavior and page faults Long-lived objects are handled inefficiently Generational garbage collection is based on the generational hypothesis : Most objects die young As such, concentrate GC efforts on objects likely to be garbage: young objects","title":"Generational Garbage Collection"},{"location":"4-semester/SPO/exam/7-heap-allocation-and-gc/#advantages-and-disadvantages_3","text":"Advantages: Keeps youngest generation's size small Helps address mistakes made by the promotion policy by creating more intermediate generations that still get garbage collected fairly frequently. Disadvantages: Collections for intermediate generations may be disruptive. Tends to increase number of inter-generational pointers, increasing the size of the root set for younger generations. Performs poorly if any of the main assumptions are false: That objects tend to die young. That there are relatively few pointers from old objects to young ones.","title":"Advantages and Disadvantages"},{"location":"4-semester/SPO/exam/7-heap-allocation-and-gc/#incremental-tracing-collectors","text":"Program (Mutator) and Garbage Collector run concurrently. Can think of system as similar to two threads. One performs collection, and the other represents the regular program in execution. Can be used in systems with real-time requirements. For example, process control systems. Allow mutator to do its job without destroying collector\u2019s possibilities for keeping track of modifications of the object graph, and at the same time Allowing collector to do its job without interfering with mutator","title":"Incremental Tracing Collectors"},{"location":"4-semester/SPO/exam/7-heap-allocation-and-gc/#summary","text":"","title":"Summary"},{"location":"4-semester/SPO/exam/8-code-generation/","text":"Code Generation \u00b6 Describe the purpose of the code generator Discuss Intermediate representations Describe issues in code generation Code templates and implementations Back patching Implementation of functions/procedures/methods Register Allocation and Code Scheduling The Code Generation Phase \u00b6 Intermediate Representations \u00b6 \"Neutral\" architecture Easy to translate to native code Can abstract away complicated runtime issues Stack Frame Management Memory Management Register Allocation Challenges \u00b6 IL must be precisely defined Translators and processors must be crafted for an IL Connections must be made between levels so feedback from intermediate steps can be related to the source program Efficiency S*T vs S+T \u00b6 JVM \u00b6 Easy conversion between java types and JVM types The JVM is an abstract machine . Class Files \u00b6 Binary encodings of the data and instructions in a Java program Contains: Table of constants Tables describing the class Name, superclass, interfaces, attributes, constructors Tables describing fields and methods Name, type/signature attributes (private, public, etc) Code for methods Internal Architecure \u00b6 Runtime Data Areas \u00b6 Supports multi-threading. Two kinds of runtime data areas Shared between all threads Private to a single thread Stack Machine \u00b6 JVM is a stack machine. 1 ( a * b ) + ( 1 - ( c * 2 )) On the stack machine: Interpreter \u00b6 The core of a JVM interpreter is basically this: Instruction-set \u00b6 Typed instructions 1 2 3 4 5 iload integer load lload long load fload float load dload double load aload reference-type load Different operands: Code Generation Visitor \u00b6 TopVisitor Starts at root of AST Handles class/method declarations Calls others for specific needs MethodBodyVisitor Generates most of the actual code Calls others for specific needs Signature Visitor Handles AST subtrees for method definition or invocation Method name, parameter types, return type Used by MethodBodyVisitor for invocations LHSVisitor Generates code for LHS of assignments May call other visitors if LHS contains subexpressions Java example: a[x+y] = ... Remember that LHS of assignment use the address of a variable, whereas the RHS uses the value Code Emmision \u00b6 1 2 3 4 5 MethodBodyVisitor . visit ( Plus ){ visit ( E1 ); visit ( E2 ); emit ( \"iadd\\n\" ); } Postludes \u00b6 Sometimes a single emission isn't enough Assignments: Visit LHS to find storage location and type Visit RHS to compute value Re-visit LHS to emit storage operations Inefficient! Better: LHS visitor builds storage operation Saves in a Postlude Parent requests postlude emission Class Declarations \u00b6 Code Templates \u00b6 If-then-else: Template Invariants \u00b6 A statement and a void expression leaves the stack height unchanged A non-void expression increases the stack height by one This is a local property of each template The generated code must be verifiable This is not a local property, since the verifier performs a global static analysis Instruction Selection \u00b6 We define patterns for instructions. We can now match pattern to find the best instruction to use. Register Allocation \u00b6 Compiler generating code for register machine needs to pay attention to register allocation, since it is a limited resource. Routine protocol Allocate arg1 in R1, arg2 in R2, ... and result in R0 But what if there are more args than regs? On MIPS all calculations takes place in registers Reduces traffic between memory and regs Register Needs \u00b6 Code Scheduling \u00b6 Modern computers are pipelined Instructions are processed in stages Instructions take different time to execute If result from previous instruction is needed but not yet ready, then we have a stalled pipeline Delayed load Load from memory takes 2, 10 or 100 cycles Also FP instructions takes time Register Allocation and Code Scheduling \u00b6 Register allocations algorithms try to minimize number of regs used May conflict with pipeline architecture Using more regs than strictly necessary may avoid pipeline stalls Solution: Integrated register allocator and code scheduler As long as registers are available, they are used. When registers grow scarce, the algorithm switches emphasis and begins to schedule code to free registers. Peephole Optimizations \u00b6","title":"8 - Code Generation"},{"location":"4-semester/SPO/exam/8-code-generation/#code-generation","text":"Describe the purpose of the code generator Discuss Intermediate representations Describe issues in code generation Code templates and implementations Back patching Implementation of functions/procedures/methods Register Allocation and Code Scheduling","title":"Code Generation"},{"location":"4-semester/SPO/exam/8-code-generation/#the-code-generation-phase","text":"","title":"The Code Generation Phase"},{"location":"4-semester/SPO/exam/8-code-generation/#intermediate-representations","text":"\"Neutral\" architecture Easy to translate to native code Can abstract away complicated runtime issues Stack Frame Management Memory Management Register Allocation","title":"Intermediate Representations"},{"location":"4-semester/SPO/exam/8-code-generation/#challenges","text":"IL must be precisely defined Translators and processors must be crafted for an IL Connections must be made between levels so feedback from intermediate steps can be related to the source program Efficiency","title":"Challenges"},{"location":"4-semester/SPO/exam/8-code-generation/#st-vs-st","text":"","title":"S*T vs S+T"},{"location":"4-semester/SPO/exam/8-code-generation/#jvm","text":"Easy conversion between java types and JVM types The JVM is an abstract machine .","title":"JVM"},{"location":"4-semester/SPO/exam/8-code-generation/#class-files","text":"Binary encodings of the data and instructions in a Java program Contains: Table of constants Tables describing the class Name, superclass, interfaces, attributes, constructors Tables describing fields and methods Name, type/signature attributes (private, public, etc) Code for methods","title":"Class Files"},{"location":"4-semester/SPO/exam/8-code-generation/#internal-architecure","text":"","title":"Internal Architecure"},{"location":"4-semester/SPO/exam/8-code-generation/#runtime-data-areas","text":"Supports multi-threading. Two kinds of runtime data areas Shared between all threads Private to a single thread","title":"Runtime Data Areas"},{"location":"4-semester/SPO/exam/8-code-generation/#stack-machine","text":"JVM is a stack machine. 1 ( a * b ) + ( 1 - ( c * 2 )) On the stack machine:","title":"Stack Machine"},{"location":"4-semester/SPO/exam/8-code-generation/#interpreter","text":"The core of a JVM interpreter is basically this:","title":"Interpreter"},{"location":"4-semester/SPO/exam/8-code-generation/#instruction-set","text":"Typed instructions 1 2 3 4 5 iload integer load lload long load fload float load dload double load aload reference-type load Different operands:","title":"Instruction-set"},{"location":"4-semester/SPO/exam/8-code-generation/#code-generation-visitor","text":"TopVisitor Starts at root of AST Handles class/method declarations Calls others for specific needs MethodBodyVisitor Generates most of the actual code Calls others for specific needs Signature Visitor Handles AST subtrees for method definition or invocation Method name, parameter types, return type Used by MethodBodyVisitor for invocations LHSVisitor Generates code for LHS of assignments May call other visitors if LHS contains subexpressions Java example: a[x+y] = ... Remember that LHS of assignment use the address of a variable, whereas the RHS uses the value","title":"Code Generation Visitor"},{"location":"4-semester/SPO/exam/8-code-generation/#code-emmision","text":"1 2 3 4 5 MethodBodyVisitor . visit ( Plus ){ visit ( E1 ); visit ( E2 ); emit ( \"iadd\\n\" ); }","title":"Code Emmision"},{"location":"4-semester/SPO/exam/8-code-generation/#postludes","text":"Sometimes a single emission isn't enough Assignments: Visit LHS to find storage location and type Visit RHS to compute value Re-visit LHS to emit storage operations Inefficient! Better: LHS visitor builds storage operation Saves in a Postlude Parent requests postlude emission","title":"Postludes"},{"location":"4-semester/SPO/exam/8-code-generation/#class-declarations","text":"","title":"Class Declarations"},{"location":"4-semester/SPO/exam/8-code-generation/#code-templates","text":"If-then-else:","title":"Code Templates"},{"location":"4-semester/SPO/exam/8-code-generation/#template-invariants","text":"A statement and a void expression leaves the stack height unchanged A non-void expression increases the stack height by one This is a local property of each template The generated code must be verifiable This is not a local property, since the verifier performs a global static analysis","title":"Template Invariants"},{"location":"4-semester/SPO/exam/8-code-generation/#instruction-selection","text":"We define patterns for instructions. We can now match pattern to find the best instruction to use.","title":"Instruction Selection"},{"location":"4-semester/SPO/exam/8-code-generation/#register-allocation","text":"Compiler generating code for register machine needs to pay attention to register allocation, since it is a limited resource. Routine protocol Allocate arg1 in R1, arg2 in R2, ... and result in R0 But what if there are more args than regs? On MIPS all calculations takes place in registers Reduces traffic between memory and regs","title":"Register Allocation"},{"location":"4-semester/SPO/exam/8-code-generation/#register-needs","text":"","title":"Register Needs"},{"location":"4-semester/SPO/exam/8-code-generation/#code-scheduling","text":"Modern computers are pipelined Instructions are processed in stages Instructions take different time to execute If result from previous instruction is needed but not yet ready, then we have a stalled pipeline Delayed load Load from memory takes 2, 10 or 100 cycles Also FP instructions takes time","title":"Code Scheduling"},{"location":"4-semester/SPO/exam/8-code-generation/#register-allocation-and-code-scheduling","text":"Register allocations algorithms try to minimize number of regs used May conflict with pipeline architecture Using more regs than strictly necessary may avoid pipeline stalls Solution: Integrated register allocator and code scheduler As long as registers are available, they are used. When registers grow scarce, the algorithm switches emphasis and begins to schedule code to free registers.","title":"Register Allocation and Code Scheduling"},{"location":"4-semester/SPO/exam/8-code-generation/#peephole-optimizations","text":"","title":"Peephole Optimizations"},{"location":"4-semester/SS/","text":"SS - SYNTAX OG SEMANTIK \u00b6 Moodle side: https://www.moodle.aau.dk/course/view.php?id=28781","title":"Course"},{"location":"4-semester/SS/#ss-syntax-og-semantik","text":"Moodle side: https://www.moodle.aau.dk/course/view.php?id=28781","title":"SS - SYNTAX OG SEMANTIK"},{"location":"4-semester/SS/0-eksamen-fremgangsm%C3%A5der/","text":"Eksamen Fremgangsm\u00e5de \u00b6 Konvertering af NFA til DFA \u00b6 Husk at \\varepsilon \\varepsilon transitioner er \"gratis\" De m\u00e5 dog ikke tages \"alene\", de skal l\u00e6ses sammen med et andet symbol. Dette symbol kan godt g\u00e5 til samme tilstand, og s\u00e5 kan \\varepsilon \\varepsilon g\u00e5 videre. Pushdown Automater \u00b6 Brug epsilon-transitioner til at komme videre ( \\varepsilon, \\varepsilon \\to \\varepsilon \\varepsilon, \\varepsilon \\to \\varepsilon ) Eksempel \u00b6 L_1=\\{a^ib^jc^k\\mid i,j,k \\geq 0;\\ k=i+j\\} L_1=\\{a^ib^jc^k\\mid i,j,k \\geq 0;\\ k=i+j\\} Fra CFG til Pushdown \u00b6 Pumping Lemma for Regul\u00e6re Sprog \u00b6 Antag at L L er et regul\u00e6rt sprog, s\u00e5 ville Pumping Lemma v\u00e6re overholdt. (Se formelsamling s. 5) V\u00e6lg en streng s\\in L s\\in L der er defineret ud fra p p og kan \"pumpes op\" s\u00e5 s\\notin L s\\notin L Skriv: \" Jeg v\u00e6lger s= s= \"valgte streng\" og viser at ingen opsplitninger af s s vil kunne overholde alle tre betingelser i pumping lemma for regul\u00e6re sprog samtidigt. Hvis (3) skal g\u00e6lde s\u00e5, m\u00e5 xy xy best\u00e5 af \"...\" og dermed y=x^k y=x^k for k\\geq 0 k\\geq 0 . Hvis (2) ogs\u00e5 skal g\u00e6lde m\u00e5 y y ikke v\u00e6re den tomme streng. Men s\u00e5 overtr\u00e6des (1), for vi har at xy^iz \\notin L xy^iz \\notin L hvis i= i= \"...\", da \"...\". \" Semantik \u00b6 Husk at numeraler skal g\u00f8res til tal ved \\mathcal{N}[\\![n]\\!] \\mathcal{N}[\\![n]\\!] Eksempel: \\langle \\text{cutoff}(x, n), s \\rangle \u2192 s[x \\mapsto v] \\quad \\text{hvor } v = \\mathcal{N}[\\![n]\\!] \\quad \\text{hvis } s(x) > \\mathcal{N}[\\![n]\\!] \\langle \\text{cutoff}(x, n), s \\rangle \u2192 s[x \\mapsto v] \\quad \\text{hvor } v = \\mathcal{N}[\\![n]\\!] \\quad \\text{hvis } s(x) > \\mathcal{N}[\\![n]\\!] Sl\u00e5 op i tilstand s med s(x) s(x) Husk at stregen over ikke altid bruges Se Fejl \u00b6 Tjek at start og sluttilstande er korrekte Tjek efter s \\vdash s \\vdash Small-Step Semantik \u00b6 Brug de predefinerede kommandoer (s. 7 i formelsamling) Variabelbindinger \u00b6 Hvis du sl\u00e5r op i Env_p Env_p og f\u00e5r variabelenvironment med, er der tale om statisk variabelbinding. Det samme g\u00e6lder for procedurebindinger. Hvis du sl\u00e5r op, f\u00e5r du en gammel krop - Mads-Bo, 2019","title":"Eksamen - Fremgangsm\u00e5der"},{"location":"4-semester/SS/0-eksamen-fremgangsm%C3%A5der/#eksamen-fremgangsmade","text":"","title":"Eksamen Fremgangsm\u00e5de"},{"location":"4-semester/SS/0-eksamen-fremgangsm%C3%A5der/#konvertering-af-nfa-til-dfa","text":"Husk at \\varepsilon \\varepsilon transitioner er \"gratis\" De m\u00e5 dog ikke tages \"alene\", de skal l\u00e6ses sammen med et andet symbol. Dette symbol kan godt g\u00e5 til samme tilstand, og s\u00e5 kan \\varepsilon \\varepsilon g\u00e5 videre.","title":"Konvertering af NFA til DFA"},{"location":"4-semester/SS/0-eksamen-fremgangsm%C3%A5der/#pushdown-automater","text":"Brug epsilon-transitioner til at komme videre ( \\varepsilon, \\varepsilon \\to \\varepsilon \\varepsilon, \\varepsilon \\to \\varepsilon )","title":"Pushdown Automater"},{"location":"4-semester/SS/0-eksamen-fremgangsm%C3%A5der/#eksempel","text":"L_1=\\{a^ib^jc^k\\mid i,j,k \\geq 0;\\ k=i+j\\} L_1=\\{a^ib^jc^k\\mid i,j,k \\geq 0;\\ k=i+j\\}","title":"Eksempel"},{"location":"4-semester/SS/0-eksamen-fremgangsm%C3%A5der/#fra-cfg-til-pushdown","text":"","title":"Fra CFG til Pushdown"},{"location":"4-semester/SS/0-eksamen-fremgangsm%C3%A5der/#pumping-lemma-for-regulre-sprog","text":"Antag at L L er et regul\u00e6rt sprog, s\u00e5 ville Pumping Lemma v\u00e6re overholdt. (Se formelsamling s. 5) V\u00e6lg en streng s\\in L s\\in L der er defineret ud fra p p og kan \"pumpes op\" s\u00e5 s\\notin L s\\notin L Skriv: \" Jeg v\u00e6lger s= s= \"valgte streng\" og viser at ingen opsplitninger af s s vil kunne overholde alle tre betingelser i pumping lemma for regul\u00e6re sprog samtidigt. Hvis (3) skal g\u00e6lde s\u00e5, m\u00e5 xy xy best\u00e5 af \"...\" og dermed y=x^k y=x^k for k\\geq 0 k\\geq 0 . Hvis (2) ogs\u00e5 skal g\u00e6lde m\u00e5 y y ikke v\u00e6re den tomme streng. Men s\u00e5 overtr\u00e6des (1), for vi har at xy^iz \\notin L xy^iz \\notin L hvis i= i= \"...\", da \"...\". \"","title":"Pumping Lemma for Regul\u00e6re Sprog"},{"location":"4-semester/SS/0-eksamen-fremgangsm%C3%A5der/#semantik","text":"Husk at numeraler skal g\u00f8res til tal ved \\mathcal{N}[\\![n]\\!] \\mathcal{N}[\\![n]\\!] Eksempel: \\langle \\text{cutoff}(x, n), s \\rangle \u2192 s[x \\mapsto v] \\quad \\text{hvor } v = \\mathcal{N}[\\![n]\\!] \\quad \\text{hvis } s(x) > \\mathcal{N}[\\![n]\\!] \\langle \\text{cutoff}(x, n), s \\rangle \u2192 s[x \\mapsto v] \\quad \\text{hvor } v = \\mathcal{N}[\\![n]\\!] \\quad \\text{hvis } s(x) > \\mathcal{N}[\\![n]\\!] Sl\u00e5 op i tilstand s med s(x) s(x) Husk at stregen over ikke altid bruges","title":"Semantik"},{"location":"4-semester/SS/0-eksamen-fremgangsm%C3%A5der/#se-fejl","text":"Tjek at start og sluttilstande er korrekte Tjek efter s \\vdash s \\vdash","title":"Se Fejl"},{"location":"4-semester/SS/0-eksamen-fremgangsm%C3%A5der/#small-step-semantik","text":"Brug de predefinerede kommandoer (s. 7 i formelsamling)","title":"Small-Step Semantik"},{"location":"4-semester/SS/0-eksamen-fremgangsm%C3%A5der/#variabelbindinger","text":"Hvis du sl\u00e5r op i Env_p Env_p og f\u00e5r variabelenvironment med, er der tale om statisk variabelbinding. Det samme g\u00e6lder for procedurebindinger. Hvis du sl\u00e5r op, f\u00e5r du en gammel krop - Mads-Bo, 2019","title":"Variabelbindinger"},{"location":"4-semester/SS/01a-regul%C3%A6re-sprog/","text":"Grundl\u00e6ggende Sprogteori \u00b6 Et sprog er en m\u00e6ndge af strenge . Alfabet \u00b6 Definition: En endelig m\u00e6ngde af tegn. Notation: Skrives \\Sigma \\Sigma Eksempler \u00b6 \u200b A=\\{a,b,c,d,...,\u00e6,\u00f8,\u00e5\\} A=\\{a,b,c,d,...,\u00e6,\u00f8,\u00e5\\} (danske alfabet) \u200b B=\\{0,1\\} B=\\{0,1\\} (bin\u00e6re alfabet) \u200b ASCII-tegnene Streng \u00b6 Definition: Givet et alfabet \\Sigma \\Sigma , er en streng over \\Sigma \\Sigma er en endelig f\u00f8lge af tegn fra \\Sigma\u200b \\Sigma\u200b Notation: L\u00e6ngden af en streng s betegnes |s| |s| Eksempler: \u00b6 Streng over A \u200b |abe|=3\u200b |abe|=3\u200b \u200b |kpst|=4 |kpst|=4 Streng over B \u200b |01001|=5\u200b |01001|=5\u200b \u200b |100|=3 |100|=3 En lille streng: \u200b \\varepsilon\u200b \\varepsilon\u200b ( den tomme streng ) \u200b |\\varepsilon|=0 |\\varepsilon|=0 Sprog \u00b6 Definition: Givet et alfabet \\Sigma \\Sigma , er et sprog over \\Sigma \\Sigma en endelig m\u00e6ngde af strenge over \\Sigma \\Sigma Eksempler \u00b6 Alfabet A: \u200b L_1=\\{a,aa,bba\\} L_1=\\{a,aa,bba\\} \u200b L_2=\\{a,aa,aaa,...\\} L_2=\\{a,aa,aaa,...\\} \u200b L_3=\u00d8 L_3=\u00d8 (det tomme sprog) \\Sigma^* \\Sigma^* - Sigma-stjerne \u00b6 Definition: Lad \\Sigma \\Sigma v\u00e6re et alfabet. \\Sigma^*\u200b \\Sigma^*\u200b er sproget best\u00e5ende af alle strenge over \\Sigma\u200b \\Sigma\u200b Eksempel \u00b6 \\Sigma=\\{0,1\\} \\Sigma=\\{0,1\\} \\Sigma^*=\\{\\varepsilon,0,1,00,01,10,11,000,001,... \\}\u200b \\Sigma^*=\\{\\varepsilon,0,1,00,01,10,11,000,001,... \\}\u200b Konkatination \u00b6 Definition: Lad u og v v\u00e6re strenge over \\Sigma \\Sigma \u200b ( u\\in\\Sigma^*,v\\in\\Sigma^* u\\in\\Sigma^*,v\\in\\Sigma^* ) S\u00e5 er u\\circ v u\\circ v (skrives tit uv uv ) strengen best\u00e5ende af symbolerne i u efterfulgt af symbolerne i v Eksempler \u00b6 u=abc u=abc v=hat v=hat uv=abchat uv=abchat -- u=kat\u200b u=kat\u200b v=\\varepsilon v=\\varepsilon uv=kat uv=kat Husk! \u00b6 Et sprog er bare en m\u00e6ngde af strenge. S\u00e5 vi kan bruge alle de s\u00e6dvanlige m\u00e6ngdeoperation, s\u00e5 l\u00e6nge de igen giver os et sprog. L_1\\cup L_2 :\\{x|x\\in L_1\\lor x \\in L_2\\} L_1\\cup L_2 :\\{x|x\\in L_1\\lor x \\in L_2\\} L_1 \\cap L_2 : \\{x|x\\in L_1 \\land x\\in L_2\\} L_1 \\cap L_2 : \\{x|x\\in L_1 \\land x\\in L_2\\} L_1 -L_2: \\{x|x \\in L_1 \\land x \\notin L_2 \\} L_1 -L_2: \\{x|x \\in L_1 \\land x \\notin L_2 \\} Men f.eks.: L_1 \\times L_2 L_1 \\times L_2 giver ingen mening Regul\u00e6re Sprog \u00b6 L er regul\u00e6rt hvis \\exists \\space \\text{DFA} \\space A:L=L(A) \\exists \\space \\text{DFA} \\space A:L=L(A) \u200b L er regul\u00e6rt hvis der findes en DFA der l\u00e6ser L Regul\u00e6re Operationer \u00b6 Foreningsm\u00e6ngde: \u200b L_1 \\cup L_2 = \\{w \\mid w \\in L_1 \\text{eller} \\space w \\in L_2\\} L_1 \\cup L_2 = \\{w \\mid w \\in L_1 \\text{eller} \\space w \\in L_2\\} Konkatination: \u200b L_1 \\circ L_2 = \\{w \\mid \\exists u, \\exists v:u \\in L_1, v \\in L_2, w=uv\\} L_1 \\circ L_2 = \\{w \\mid \\exists u, \\exists v:u \\in L_1, v \\in L_2, w=uv\\} L-stjerne (Kleene-stjerne): \u200b {L_1}^* = \\{x_1...x_k \\mid k \\geq 0, x_i \\in L_1 \\space \\text{for} \\space 0 \\leq i \\leq k\\}\u200b {L_1}^* = \\{x_1...x_k \\mid k \\geq 0, x_i \\in L_1 \\space \\text{for} \\space 0 \\leq i \\leq k\\}\u200b Lukket Under Foreningsm\u00e6ngden \u00b6 S\u00e6tning: Hvis L_1 L_1 og L_2 L_2 er regul\u00e6re sprog, s\u00e5 er L_1 \\cup L_2 L_1 \\cup L_2 ogs\u00e5 et regul\u00e6rt sprog. Produktkonstruktionen \u00b6","title":"Regul\u00e6re Sprog"},{"location":"4-semester/SS/01a-regul%C3%A6re-sprog/#grundlggende-sprogteori","text":"Et sprog er en m\u00e6ndge af strenge .","title":"Grundl\u00e6ggende Sprogteori"},{"location":"4-semester/SS/01a-regul%C3%A6re-sprog/#alfabet","text":"Definition: En endelig m\u00e6ngde af tegn. Notation: Skrives \\Sigma \\Sigma","title":"Alfabet"},{"location":"4-semester/SS/01a-regul%C3%A6re-sprog/#eksempler","text":"\u200b A=\\{a,b,c,d,...,\u00e6,\u00f8,\u00e5\\} A=\\{a,b,c,d,...,\u00e6,\u00f8,\u00e5\\} (danske alfabet) \u200b B=\\{0,1\\} B=\\{0,1\\} (bin\u00e6re alfabet) \u200b ASCII-tegnene","title":"Eksempler"},{"location":"4-semester/SS/01a-regul%C3%A6re-sprog/#streng","text":"Definition: Givet et alfabet \\Sigma \\Sigma , er en streng over \\Sigma \\Sigma er en endelig f\u00f8lge af tegn fra \\Sigma\u200b \\Sigma\u200b Notation: L\u00e6ngden af en streng s betegnes |s| |s|","title":"Streng"},{"location":"4-semester/SS/01a-regul%C3%A6re-sprog/#eksempler_1","text":"Streng over A \u200b |abe|=3\u200b |abe|=3\u200b \u200b |kpst|=4 |kpst|=4 Streng over B \u200b |01001|=5\u200b |01001|=5\u200b \u200b |100|=3 |100|=3 En lille streng: \u200b \\varepsilon\u200b \\varepsilon\u200b ( den tomme streng ) \u200b |\\varepsilon|=0 |\\varepsilon|=0","title":"Eksempler:"},{"location":"4-semester/SS/01a-regul%C3%A6re-sprog/#sprog","text":"Definition: Givet et alfabet \\Sigma \\Sigma , er et sprog over \\Sigma \\Sigma en endelig m\u00e6ngde af strenge over \\Sigma \\Sigma","title":"Sprog"},{"location":"4-semester/SS/01a-regul%C3%A6re-sprog/#eksempler_2","text":"Alfabet A: \u200b L_1=\\{a,aa,bba\\} L_1=\\{a,aa,bba\\} \u200b L_2=\\{a,aa,aaa,...\\} L_2=\\{a,aa,aaa,...\\} \u200b L_3=\u00d8 L_3=\u00d8 (det tomme sprog)","title":"Eksempler"},{"location":"4-semester/SS/01a-regul%C3%A6re-sprog/#sigmasigma-sigma-stjerne","text":"Definition: Lad \\Sigma \\Sigma v\u00e6re et alfabet. \\Sigma^*\u200b \\Sigma^*\u200b er sproget best\u00e5ende af alle strenge over \\Sigma\u200b \\Sigma\u200b","title":"\\Sigma^*\\Sigma^* - Sigma-stjerne"},{"location":"4-semester/SS/01a-regul%C3%A6re-sprog/#eksempel","text":"\\Sigma=\\{0,1\\} \\Sigma=\\{0,1\\} \\Sigma^*=\\{\\varepsilon,0,1,00,01,10,11,000,001,... \\}\u200b \\Sigma^*=\\{\\varepsilon,0,1,00,01,10,11,000,001,... \\}\u200b","title":"Eksempel"},{"location":"4-semester/SS/01a-regul%C3%A6re-sprog/#konkatination","text":"Definition: Lad u og v v\u00e6re strenge over \\Sigma \\Sigma \u200b ( u\\in\\Sigma^*,v\\in\\Sigma^* u\\in\\Sigma^*,v\\in\\Sigma^* ) S\u00e5 er u\\circ v u\\circ v (skrives tit uv uv ) strengen best\u00e5ende af symbolerne i u efterfulgt af symbolerne i v","title":"Konkatination"},{"location":"4-semester/SS/01a-regul%C3%A6re-sprog/#eksempler_3","text":"u=abc u=abc v=hat v=hat uv=abchat uv=abchat -- u=kat\u200b u=kat\u200b v=\\varepsilon v=\\varepsilon uv=kat uv=kat","title":"Eksempler"},{"location":"4-semester/SS/01a-regul%C3%A6re-sprog/#husk","text":"Et sprog er bare en m\u00e6ngde af strenge. S\u00e5 vi kan bruge alle de s\u00e6dvanlige m\u00e6ngdeoperation, s\u00e5 l\u00e6nge de igen giver os et sprog. L_1\\cup L_2 :\\{x|x\\in L_1\\lor x \\in L_2\\} L_1\\cup L_2 :\\{x|x\\in L_1\\lor x \\in L_2\\} L_1 \\cap L_2 : \\{x|x\\in L_1 \\land x\\in L_2\\} L_1 \\cap L_2 : \\{x|x\\in L_1 \\land x\\in L_2\\} L_1 -L_2: \\{x|x \\in L_1 \\land x \\notin L_2 \\} L_1 -L_2: \\{x|x \\in L_1 \\land x \\notin L_2 \\} Men f.eks.: L_1 \\times L_2 L_1 \\times L_2 giver ingen mening","title":"Husk!"},{"location":"4-semester/SS/01a-regul%C3%A6re-sprog/#regulre-sprog","text":"L er regul\u00e6rt hvis \\exists \\space \\text{DFA} \\space A:L=L(A) \\exists \\space \\text{DFA} \\space A:L=L(A) \u200b L er regul\u00e6rt hvis der findes en DFA der l\u00e6ser L","title":"Regul\u00e6re Sprog"},{"location":"4-semester/SS/01a-regul%C3%A6re-sprog/#regulre-operationer","text":"Foreningsm\u00e6ngde: \u200b L_1 \\cup L_2 = \\{w \\mid w \\in L_1 \\text{eller} \\space w \\in L_2\\} L_1 \\cup L_2 = \\{w \\mid w \\in L_1 \\text{eller} \\space w \\in L_2\\} Konkatination: \u200b L_1 \\circ L_2 = \\{w \\mid \\exists u, \\exists v:u \\in L_1, v \\in L_2, w=uv\\} L_1 \\circ L_2 = \\{w \\mid \\exists u, \\exists v:u \\in L_1, v \\in L_2, w=uv\\} L-stjerne (Kleene-stjerne): \u200b {L_1}^* = \\{x_1...x_k \\mid k \\geq 0, x_i \\in L_1 \\space \\text{for} \\space 0 \\leq i \\leq k\\}\u200b {L_1}^* = \\{x_1...x_k \\mid k \\geq 0, x_i \\in L_1 \\space \\text{for} \\space 0 \\leq i \\leq k\\}\u200b","title":"Regul\u00e6re Operationer"},{"location":"4-semester/SS/01a-regul%C3%A6re-sprog/#lukket-under-foreningsmngden","text":"S\u00e6tning: Hvis L_1 L_1 og L_2 L_2 er regul\u00e6re sprog, s\u00e5 er L_1 \\cup L_2 L_1 \\cup L_2 ogs\u00e5 et regul\u00e6rt sprog.","title":"Lukket Under Foreningsm\u00e6ngden"},{"location":"4-semester/SS/01a-regul%C3%A6re-sprog/#produktkonstruktionen","text":"","title":"Produktkonstruktionen"},{"location":"4-semester/SS/01b-endelige-automater/","text":"Endelige automater \u00b6 Man ledte efter en model ag neuroner i hjernen. I datalogi anvendes endelige automater: I den fase af en compiler kaldet 'Leksikalske analyser' (lexer) I specifikation af systemer. Endelige automater er simple algoritmer til sproggendkendelse. \"Givet w, har vi w\\in L w\\in L ?\" Tilstand er markeret af en cirkel. Transition er markeret med en pil. Accepttilstand er markeret med en ekstra cirkel. Definition : En endelig automat (DFA) er en 5-tupel \u200b (Q,\\Sigma,q_0,\\delta,F) (Q,\\Sigma,q_0,\\delta,F) Q Q endelig m\u00e6ngde af tilf\u00e6lde \\Sigma \\Sigma input alfabet q_0 q_0 starttilstand q_0\\in Q q_0\\in Q \\delta \\delta overf\u00f8ringsfunktion F F m\u00e6ngden af accepttilstande F\\subseteq Q F\\subseteq Q Overf\u00f8ringsfunktion \u00b6 \\delta(q,a)=q_1 \\delta(q,a)=q_1 \\delta:Q\\times \\Sigma \\rightarrow Q \\delta:Q\\times \\Sigma \\rightarrow Q Eksempel \u00b6 Q=\\{q_0,q_1\\} Q=\\{q_0,q_1\\} \\Sigma =\\{0,1\\} \\Sigma =\\{0,1\\} q_0=q_0 q_0=q_0 F=\\{q_0\\} F=\\{q_0\\} \\delta \\delta 0 1 q_0 q_0 q_0 q_0 q_1 q_1 q_1 q_1 q_1 q_1 q_1 q_1 Andet \u00b6 Et lille v\u00e6rkt\u00f8j til at tegne endelige automater: http://madebyevan.com/fsm/","title":"Endelige Automater"},{"location":"4-semester/SS/01b-endelige-automater/#endelige-automater","text":"Man ledte efter en model ag neuroner i hjernen. I datalogi anvendes endelige automater: I den fase af en compiler kaldet 'Leksikalske analyser' (lexer) I specifikation af systemer. Endelige automater er simple algoritmer til sproggendkendelse. \"Givet w, har vi w\\in L w\\in L ?\" Tilstand er markeret af en cirkel. Transition er markeret med en pil. Accepttilstand er markeret med en ekstra cirkel. Definition : En endelig automat (DFA) er en 5-tupel \u200b (Q,\\Sigma,q_0,\\delta,F) (Q,\\Sigma,q_0,\\delta,F) Q Q endelig m\u00e6ngde af tilf\u00e6lde \\Sigma \\Sigma input alfabet q_0 q_0 starttilstand q_0\\in Q q_0\\in Q \\delta \\delta overf\u00f8ringsfunktion F F m\u00e6ngden af accepttilstande F\\subseteq Q F\\subseteq Q","title":"Endelige automater"},{"location":"4-semester/SS/01b-endelige-automater/#overfringsfunktion","text":"\\delta(q,a)=q_1 \\delta(q,a)=q_1 \\delta:Q\\times \\Sigma \\rightarrow Q \\delta:Q\\times \\Sigma \\rightarrow Q","title":"Overf\u00f8ringsfunktion"},{"location":"4-semester/SS/01b-endelige-automater/#eksempel","text":"Q=\\{q_0,q_1\\} Q=\\{q_0,q_1\\} \\Sigma =\\{0,1\\} \\Sigma =\\{0,1\\} q_0=q_0 q_0=q_0 F=\\{q_0\\} F=\\{q_0\\} \\delta \\delta 0 1 q_0 q_0 q_0 q_0 q_1 q_1 q_1 q_1 q_1 q_1 q_1 q_1","title":"Eksempel"},{"location":"4-semester/SS/01b-endelige-automater/#andet","text":"Et lille v\u00e6rkt\u00f8j til at tegne endelige automater: http://madebyevan.com/fsm/","title":"Andet"},{"location":"4-semester/SS/02-nondeterministiske-endelige-automater/","text":"Nondeterministiske Endelige Automater \u00b6 En NFA M accepterer et input w hvis M ved at l\u00e6se w KAN havne i en accepttilstand til sidst. Note Hvis en NFA ingen steder har at g\u00e5, \"crasher\" den, og l\u00e6ser dermed ikke strengen. Definition \u00b6 En nonderterministisk endelig automat (NFA) er en 5-tupel. $$ (Q,\\Sigma,\\delta,q_0,F) $$ Q Q : endelig m\u00e6ngde af tilstande \\Sigma \\Sigma : input alfabet \\delta \\delta : overf\u00f8ringsfunktion q_0 q_0 : starttilstand q_0 \\in Q q_0 \\in Q F F : m\u00e6ngde af accepttilstande F \\subseteq Q F \\subseteq Q Transitioner beskriver m\u00e6ngden af mulige efterf\u00f8lgertilstande for ethvert tegn. \\delta(q_1,a)=\u00d8 \\delta(q_1,a)=\u00d8 \\delta(q_2,a)=\\{q_1,q_2\\} \\delta(q_2,a)=\\{q_1,q_2\\} \\delta(q_1,\\varepsilon)=\\{q_2\\} \\delta(q_1,\\varepsilon)=\\{q_2\\} \\delta(q_2, \\varepsilon)=\u00d8 \\delta(q_2, \\varepsilon)=\u00d8 $$ \\delta:Q\\times\\Sigma_\\varepsilon \\rightarrow \\mathcal{P}(Q) $$ \u200b - m\u00e6ngden af alle delm\u00e6ngder af Q \u00c6kvivalens mellem DFA og NFA \u00b6 NFA'er og DFA'er er \u00e6kvivalente: For enhver NFA N kan vi konstruere en DFA M s\u00e5: L(N)=L(M) L(N)=L(M) Eksempel 1: Eksempel 2: Lukning Under de Regul\u00e6re Operationer \u00b6 Foreningsm\u00e6ngden \u00b6 Konkatination \u00b6 Stjerneoperationen \u00b6","title":"Nondeterministiske Endelige Automater"},{"location":"4-semester/SS/02-nondeterministiske-endelige-automater/#nondeterministiske-endelige-automater","text":"En NFA M accepterer et input w hvis M ved at l\u00e6se w KAN havne i en accepttilstand til sidst. Note Hvis en NFA ingen steder har at g\u00e5, \"crasher\" den, og l\u00e6ser dermed ikke strengen.","title":"Nondeterministiske Endelige Automater"},{"location":"4-semester/SS/02-nondeterministiske-endelige-automater/#definition","text":"En nonderterministisk endelig automat (NFA) er en 5-tupel. $$ (Q,\\Sigma,\\delta,q_0,F) $$ Q Q : endelig m\u00e6ngde af tilstande \\Sigma \\Sigma : input alfabet \\delta \\delta : overf\u00f8ringsfunktion q_0 q_0 : starttilstand q_0 \\in Q q_0 \\in Q F F : m\u00e6ngde af accepttilstande F \\subseteq Q F \\subseteq Q Transitioner beskriver m\u00e6ngden af mulige efterf\u00f8lgertilstande for ethvert tegn. \\delta(q_1,a)=\u00d8 \\delta(q_1,a)=\u00d8 \\delta(q_2,a)=\\{q_1,q_2\\} \\delta(q_2,a)=\\{q_1,q_2\\} \\delta(q_1,\\varepsilon)=\\{q_2\\} \\delta(q_1,\\varepsilon)=\\{q_2\\} \\delta(q_2, \\varepsilon)=\u00d8 \\delta(q_2, \\varepsilon)=\u00d8 $$ \\delta:Q\\times\\Sigma_\\varepsilon \\rightarrow \\mathcal{P}(Q) $$ \u200b - m\u00e6ngden af alle delm\u00e6ngder af Q","title":"Definition"},{"location":"4-semester/SS/02-nondeterministiske-endelige-automater/#kvivalens-mellem-dfa-og-nfa","text":"NFA'er og DFA'er er \u00e6kvivalente: For enhver NFA N kan vi konstruere en DFA M s\u00e5: L(N)=L(M) L(N)=L(M) Eksempel 1: Eksempel 2:","title":"\u00c6kvivalens mellem DFA og NFA"},{"location":"4-semester/SS/02-nondeterministiske-endelige-automater/#lukning-under-de-regulre-operationer","text":"","title":"Lukning Under de Regul\u00e6re Operationer"},{"location":"4-semester/SS/02-nondeterministiske-endelige-automater/#foreningsmngden","text":"","title":"Foreningsm\u00e6ngden"},{"location":"4-semester/SS/02-nondeterministiske-endelige-automater/#konkatination","text":"","title":"Konkatination"},{"location":"4-semester/SS/02-nondeterministiske-endelige-automater/#stjerneoperationen","text":"","title":"Stjerneoperationen"},{"location":"4-semester/SS/03-regul%C3%A6re-udtryk/","text":"Regul\u00e6re Udtryk \u00b6 Regul\u00e6re udtryk og NFA'er er \u00e6kvivalente. Man taler om regul\u00e6re udtryk over et alfabet. Basis-udtryk Regul\u00e6rt udtryk R Sproget betegnet af R, L(R) a [a\\in \\Sigma] [a\\in \\Sigma] {a} \u00d8 {} \\varepsilon \\varepsilon { \\varepsilon \\varepsilon } Sammensatte udtryk Regul\u00e6rt udtryk R Sproget betegnet af R, L(R) (R_1\\cup R_2) (R_1\\cup R_2) L(R_1) \\cup L(R_2) L(R_1) \\cup L(R_2) (R_1 \\circ R_2) (R_1 \\circ R_2) L(R_1) \\circ L(R_2) L(R_1) \\circ L(R_2) R^* R^* L(R)^* L(R)^* Regul\u00e6re Udtryk er \u00c6kvivalente med NFA'er \u00b6 Eksempel: $$ (aa \\space \\cup \\space b)^* $$ Kan tegnes som NFA: \u03b5 \u03b5 \u03b5 a \u03b5 a \u03b5 b \u03b5 Og som vi s\u00e5 i Lektion 2 , s\u00e5 kan en NFA skrives om til en DFA. Generaliseret NFA (GNFA) \u00b6 Definition : En GNFA er en 5-tupel \u200b (Q, \\Sigma,q_{start},q_{accept},\\delta) (Q, \\Sigma,q_{start},q_{accept},\\delta) Q: m\u00e6ngde af tilstande \\Sigma \\Sigma : input alfabet q_{start} q_{start} : starttilstand q_{start} \\in Q q_{start} \\in Q q_{accept} q_{accept} : accepttilstand q_{accept} \\in Q q_{accept} \\in Q \\delta \\delta : Skal over en \" bordskik \" Bordskik : \u00c9n transition mellem hvert par af tilstande, dog Ingen transitioner fra q_{accept} q_{accept} Ingen transition til q_{start} q_{start} Regul\u00e6re udtryk p\u00e5 transitionerne. Eksempel \u00b6 Konstruer Regul\u00e6rt Udtryk ud fra GNFA \u00b6 Fjern tilstande i G \u00e9n efter \u00e9n og opdater. Til sidst har vi 2 tilstande med et regul\u00e6rt udtryk mellem. Fjerne tilstande \u00b6 M\u00e5 ikke v\u00e6re q_{start} q_{start} eller q_{accept} q_{accept} Kald den tilstand vi fjerne q_{rip} q_{rip} Opdatere transitioner \u00b6 F\u00f8r og efter q_i q_j q_rip q_i q_j R\u2081 R\u2083 R\u2084 R\u2082 R\u2084 U R\u2081R\u2082*R\u2083 For hvert par q_i,q_j \\neq q_{rip} q_i,q_j \\neq q_{rip} lav denne opdatering Algoritme \u00b6 \\text{CONVERT}(G)= \\text{CONVERT}(G)= Lad m\u00e6ngden af tilstande i G G v\u00e6re Q_G Q_G . Hvis Q_G =\\{q_{start}, q_{accept}\\} Q_G =\\{q_{start}, q_{accept}\\} s\u00e5 stop. Returner R R , hvor: Ellers v\u00e6lg q_{rip} \\notin \\{q_{start}, q_{accept}\\} q_{rip} \\notin \\{q_{start}, q_{accept}\\} For hvert par (q_i, q_j) (q_i, q_j) opdat\u00e9r transitioner som vist ovenfor . Kald ny GNFA for G' G' . Q_{G'}=Q_G \\smallsetminus \\{q_{rip}\\} Q_{G'}=Q_G \\smallsetminus \\{q_{rip}\\} \\text{CONVERT}(G') \\text{CONVERT}(G')","title":"Regul\u00e6re Udtryk"},{"location":"4-semester/SS/03-regul%C3%A6re-udtryk/#regulre-udtryk","text":"Regul\u00e6re udtryk og NFA'er er \u00e6kvivalente. Man taler om regul\u00e6re udtryk over et alfabet. Basis-udtryk Regul\u00e6rt udtryk R Sproget betegnet af R, L(R) a [a\\in \\Sigma] [a\\in \\Sigma] {a} \u00d8 {} \\varepsilon \\varepsilon { \\varepsilon \\varepsilon } Sammensatte udtryk Regul\u00e6rt udtryk R Sproget betegnet af R, L(R) (R_1\\cup R_2) (R_1\\cup R_2) L(R_1) \\cup L(R_2) L(R_1) \\cup L(R_2) (R_1 \\circ R_2) (R_1 \\circ R_2) L(R_1) \\circ L(R_2) L(R_1) \\circ L(R_2) R^* R^* L(R)^* L(R)^*","title":"Regul\u00e6re Udtryk"},{"location":"4-semester/SS/03-regul%C3%A6re-udtryk/#regulre-udtryk-er-kvivalente-med-nfaer","text":"Eksempel: $$ (aa \\space \\cup \\space b)^* $$ Kan tegnes som NFA: \u03b5 \u03b5 \u03b5 a \u03b5 a \u03b5 b \u03b5 Og som vi s\u00e5 i Lektion 2 , s\u00e5 kan en NFA skrives om til en DFA.","title":"Regul\u00e6re Udtryk er \u00c6kvivalente med NFA'er"},{"location":"4-semester/SS/03-regul%C3%A6re-udtryk/#generaliseret-nfa-gnfa","text":"Definition : En GNFA er en 5-tupel \u200b (Q, \\Sigma,q_{start},q_{accept},\\delta) (Q, \\Sigma,q_{start},q_{accept},\\delta) Q: m\u00e6ngde af tilstande \\Sigma \\Sigma : input alfabet q_{start} q_{start} : starttilstand q_{start} \\in Q q_{start} \\in Q q_{accept} q_{accept} : accepttilstand q_{accept} \\in Q q_{accept} \\in Q \\delta \\delta : Skal over en \" bordskik \" Bordskik : \u00c9n transition mellem hvert par af tilstande, dog Ingen transitioner fra q_{accept} q_{accept} Ingen transition til q_{start} q_{start} Regul\u00e6re udtryk p\u00e5 transitionerne.","title":"Generaliseret NFA (GNFA)"},{"location":"4-semester/SS/03-regul%C3%A6re-udtryk/#eksempel","text":"","title":"Eksempel"},{"location":"4-semester/SS/03-regul%C3%A6re-udtryk/#konstruer-regulrt-udtryk-ud-fra-gnfa","text":"Fjern tilstande i G \u00e9n efter \u00e9n og opdater. Til sidst har vi 2 tilstande med et regul\u00e6rt udtryk mellem.","title":"Konstruer Regul\u00e6rt Udtryk ud fra GNFA"},{"location":"4-semester/SS/03-regul%C3%A6re-udtryk/#fjerne-tilstande","text":"M\u00e5 ikke v\u00e6re q_{start} q_{start} eller q_{accept} q_{accept} Kald den tilstand vi fjerne q_{rip} q_{rip}","title":"Fjerne tilstande"},{"location":"4-semester/SS/03-regul%C3%A6re-udtryk/#opdatere-transitioner","text":"F\u00f8r og efter q_i q_j q_rip q_i q_j R\u2081 R\u2083 R\u2084 R\u2082 R\u2084 U R\u2081R\u2082*R\u2083 For hvert par q_i,q_j \\neq q_{rip} q_i,q_j \\neq q_{rip} lav denne opdatering","title":"Opdatere transitioner"},{"location":"4-semester/SS/03-regul%C3%A6re-udtryk/#algoritme","text":"\\text{CONVERT}(G)= \\text{CONVERT}(G)= Lad m\u00e6ngden af tilstande i G G v\u00e6re Q_G Q_G . Hvis Q_G =\\{q_{start}, q_{accept}\\} Q_G =\\{q_{start}, q_{accept}\\} s\u00e5 stop. Returner R R , hvor: Ellers v\u00e6lg q_{rip} \\notin \\{q_{start}, q_{accept}\\} q_{rip} \\notin \\{q_{start}, q_{accept}\\} For hvert par (q_i, q_j) (q_i, q_j) opdat\u00e9r transitioner som vist ovenfor . Kald ny GNFA for G' G' . Q_{G'}=Q_G \\smallsetminus \\{q_{rip}\\} Q_{G'}=Q_G \\smallsetminus \\{q_{rip}\\} \\text{CONVERT}(G') \\text{CONVERT}(G')","title":"Algoritme"},{"location":"4-semester/SS/04-pumping-lemma/","text":"Pumping Lemma \u00b6 DFA'er kan ikke t\u00e6lle! Hvordan beviser man at et sprog L ikke er regul\u00e6rt? En mulig strategi: Pumping Lemma Primtal \u00b6 Hvordan viser jeg at et naturligt tal ikke er et primtal? S\u00e6tning : Hvis p er et primtal og p>2 p>2 s\u00e5 er p et ulige tal. $$ 2174354654664 $$ Dette ender p\u00e5 4, er dermed et lige tal, og er derfor ikke et primtal. Pumping Lemma for regul\u00e6re sprog \u00b6 Hvis L er et regul\u00e6rt sprog, s\u00e5 findes der et tal p>0 p>0 s\u00e5ledes at enhver s\\in L s\\in L hvor |s| \\geq p |s| \\geq p kan splittes op som s=xyz s=xyz der opfylder: xy^iz \\in L \\space\\text{for alle}\\space i \\geq0 xy^iz \\in L \\space\\text{for alle}\\space i \\geq0 |y|>0 |y|>0 (y er ikke tom) |xy|\\leq p |xy|\\leq p (x og y findes blandt de p f\u00f8rste tegn) Eksempel 1 \u00b6 https://www.youtube.com/watch?v=FJq3NkqyYac&list=PLA8H0-CuqhGFb1yAl_z3XNCZXPrI21dal&index=4 Udvid udvid noterne med eksemplet fra videoen. Eksempel 2 \u00b6 https://www.youtube.com/watch?v=i8JPnM3rFrM&index=5&list=PLA8H0-CuqhGFb1yAl_z3XNCZXPrI21dal Udvid udvid noterne med eksemplet fra videoen.","title":"Pumping Lemma"},{"location":"4-semester/SS/04-pumping-lemma/#pumping-lemma","text":"DFA'er kan ikke t\u00e6lle! Hvordan beviser man at et sprog L ikke er regul\u00e6rt? En mulig strategi: Pumping Lemma","title":"Pumping Lemma"},{"location":"4-semester/SS/04-pumping-lemma/#primtal","text":"Hvordan viser jeg at et naturligt tal ikke er et primtal? S\u00e6tning : Hvis p er et primtal og p>2 p>2 s\u00e5 er p et ulige tal. $$ 2174354654664 $$ Dette ender p\u00e5 4, er dermed et lige tal, og er derfor ikke et primtal.","title":"Primtal"},{"location":"4-semester/SS/04-pumping-lemma/#pumping-lemma-for-regulre-sprog","text":"Hvis L er et regul\u00e6rt sprog, s\u00e5 findes der et tal p>0 p>0 s\u00e5ledes at enhver s\\in L s\\in L hvor |s| \\geq p |s| \\geq p kan splittes op som s=xyz s=xyz der opfylder: xy^iz \\in L \\space\\text{for alle}\\space i \\geq0 xy^iz \\in L \\space\\text{for alle}\\space i \\geq0 |y|>0 |y|>0 (y er ikke tom) |xy|\\leq p |xy|\\leq p (x og y findes blandt de p f\u00f8rste tegn)","title":"Pumping Lemma for regul\u00e6re sprog"},{"location":"4-semester/SS/04-pumping-lemma/#eksempel-1","text":"https://www.youtube.com/watch?v=FJq3NkqyYac&list=PLA8H0-CuqhGFb1yAl_z3XNCZXPrI21dal&index=4 Udvid udvid noterne med eksemplet fra videoen.","title":"Eksempel 1"},{"location":"4-semester/SS/04-pumping-lemma/#eksempel-2","text":"https://www.youtube.com/watch?v=i8JPnM3rFrM&index=5&list=PLA8H0-CuqhGFb1yAl_z3XNCZXPrI21dal Udvid udvid noterne med eksemplet fra videoen.","title":"Eksempel 2"},{"location":"4-semester/SS/05-kontekstfrie-grammatikker/","text":"Kontekstfrie Grammatikker \u00b6 Beskrive Genkende Regul\u00e6re sprog Regul\u00e6re udtryk Endelige automater - NFA og DFA (\u00e6kv.) Kontekstfrie sprog Kontekstfrie grammatikker Pushdown-automater - NDPDA og DPDA (ikke \u00e6kv.) Kontekstfri Grammatik - CFG \u00b6 Definition : En kontekstfri grammatik er en 4-tupel $$ (V,\\Sigma,R,S) $$ Symbol Betydning V Endelig m\u00e6ngde af variable (aka. nonterminaler) \\Sigma \\Sigma Endelig m\u00e6ngde af terminaler R Endelig m\u00e6ngde af regler S Startvariabel S\\in V S\\in V (aka. startsymbol) Regler i R er p\u00e5 formen: $$ A \\longrightarrow w, \\quad w \\in (V\\cup\\Sigma)^* $$ Eksempel \u00b6 Sprog: $$ L={a^nb^n | n \\geq0} $$ CFG: $$ \\text{S} \\longrightarrow \\varepsilon \\ |\\ \\text{aSb} $$ \u200b svarer til: \\begin{align*} \\text{S} &\\longrightarrow \\varepsilon \\\\ \\text{S} &\\longrightarrow \\text{aSb} \\end{align*} \\begin{align*} \\text{S} &\\longrightarrow \\varepsilon \\\\ \\text{S} &\\longrightarrow \\text{aSb} \\end{align*} Byggelse af en streng \u00b6 Strengen aaabbb \\begin{align*} \\text{S} &\\Rightarrow \\text{aSb} \\\\ & \\Rightarrow \\text{aaSbb} \\\\ & \\Rightarrow \\text{aaaSbbb} \\\\ & \\Rightarrow \\text{aaabbb} \\end{align*} \\begin{align*} \\text{S} &\\Rightarrow \\text{aSb} \\\\ & \\Rightarrow \\text{aaSbb} \\\\ & \\Rightarrow \\text{aaaSbbb} \\\\ & \\Rightarrow \\text{aaabbb} \\end{align*} Derivition \u00b6 Definition: Lad G v\u00e6re en CFG hvor G=(V,\\Sigma,R,S) G=(V,\\Sigma,R,S) og lad u,v \\in (V\\cup\\Sigma)^* u,v \\in (V\\cup\\Sigma)^* Hvis \\begin{align*} u&=u_1Au_2 \\quad og \\quad A\\longrightarrow w \\\\ og \\quad v &=u_1wu_2 \\end{align*} \\begin{align*} u&=u_1Au_2 \\quad og \\quad A\\longrightarrow w \\\\ og \\quad v &=u_1wu_2 \\end{align*} s\u00e5 skriver vi u \\Rightarrow v u \\Rightarrow v \u200b u deriverer i et skridt til v $$ u \\Rightarrow^* v $$ \u200b u deriverer i 0 eller flere et skridt til v Eksempel \u00b6 Sprog L_2=\\{w \\in \\{a,b\\}^* \\ | \\ \\text{w er et palindrom}\\} L_2=\\{w \\in \\{a,b\\}^* \\ | \\ \\text{w er et palindrom}\\} \u200b Eksempler: $$ aaa \\in L_2 \\quad abba \\in L_2 \\quad ab \\notin L_2 $$ Regler: $$ S \\longrightarrow \\varepsilon \\ | \\ a \\ | \\ b \\ | \\ aSa \\ | \\ bSb $$ Derivation af abba $$ S \\Rightarrow aSa \\Rightarrow abSba \\Rightarrow abba $$ Kontekstfrie sprog \u00b6 Definition: Lad G=(V,\\Sigma,R,S) G=(V,\\Sigma,R,S) v\u00e6re en CFG Sproget defineret af G er L(G)=\\{w\\in\\Sigma^* \\ | \\ S \\Rightarrow^*w\\} L(G)=\\{w\\in\\Sigma^* \\ | \\ S \\Rightarrow^*w\\} Et sprog L kalder vi kontekstfrit hvis der findes en CFG G s\u00e5 L=L(G) L=L(G) Kontekstfrie sprog vs Regul\u00e6re sprog \u00b6 S\u00e6tning Hvis M er en DFA, kan vi konstruere en CFG G s\u00e5 L(G)=L(M) L(G)=L(M) Eksempel: \u200b M: Ide i konstruktion: Til hver tilstand svarer en variabel i vores CFG \\begin{align*} A_1 &\\longrightarrow aA_1 \\ | \\ bA_2 \\ | \\ b\\\\ A_2 &\\longrightarrow vA_2 \\ | \\ aA_1 \\ | \\ b \\ | \\ \\varepsilon \\end{align*} \\begin{align*} A_1 &\\longrightarrow aA_1 \\ | \\ bA_2 \\ | \\ b\\\\ A_2 &\\longrightarrow vA_2 \\ | \\ aA_1 \\ | \\ b \\ | \\ \\varepsilon \\end{align*} Generelt: Hvis \\delta(q,a)=q' \\delta(q,a)=q' lav reglen: \u200b A_q\\longrightarrow aA_{q'} A_q\\longrightarrow aA_{q'} Hvis q'\\in F: \\quad A_q\\longrightarrow a q'\\in F: \\quad A_q\\longrightarrow a \u200b Hvis q\\in F : \\quad A_q\\longrightarrow \\varepsilon q\\in F : \\quad A_q\\longrightarrow \\varepsilon Verdenskort (Venn diagram) \u00b6 Derivitionstr\u00e6er / Parsetr\u00e6er \u00b6 Et parsetr\u00e6 er en tr\u00e6repr\u00e6sentation af en derivition. Eksempel \u00b6 Aritmetiske udtryk V=\\{E\\},\\Sigma=\\{+,*,a\\} \\\\ E \\longrightarrow E+E \\ | \\ E*E \\ | \\ a V=\\{E\\},\\Sigma=\\{+,*,a\\} \\\\ E \\longrightarrow E+E \\ | \\ E*E \\ | \\ a \"a+a*a\" E\\Rightarrow E+E \\Rightarrow a+E \\Rightarrow a+E*E \\Rightarrow a+a*E \\Rightarrow a+a*a E\\Rightarrow E+E \\Rightarrow a+E \\Rightarrow a+E*E \\Rightarrow a+a*E \\Rightarrow a+a*a Parsetr\u00e6 \u00b6 MEN \u00b6 Kan ogs\u00e5 deriveres p\u00e5 en anden m\u00e5de: E\\Rightarrow E*E \\Rightarrow E+E*E \\Rightarrow a+E*E \\Rightarrow a+a*E \\Rightarrow a+a*a E\\Rightarrow E*E \\Rightarrow E+E*E \\Rightarrow a+E*E \\Rightarrow a+a*E \\Rightarrow a+a*a Grammatikken er tvetydig (ambigous). Begge derivitioner er venstrederivitioner , 2 forskellige venstrederivitioner. Tvetydighed \u00b6 Definition : En CFG G er tvetydig hvis der findes en w\\in L(G) w\\in L(G) s\u00e5 S\\Rightarrow^*w S\\Rightarrow^*w med to (eller flere) forskellige venstrederivitioner S\u00e6tning: En CFG G er tvetydig hvis og kun hvis der findes en w\\in L(G) w\\in L(G) som har mindst to forskellige parsetr\u00e6er. Indbygget Tvetydighed \u00b6 Der findes kontekstfrie sprog L s\u00e5 enhver CFG G s\u00e5 L(G)=L L(G)=L vil v\u00e6re tvetydig. Den slags sprog kaldes indbygget tvetydig Bestem Tvetydighed \u00b6 S\u00e6tning Der findes ikke nogen algoritme, der altid kan fort\u00e6lle os om en CFG er tvetydig. (umulighedsresultat) Chomsky Normalform (CNF) \u00b6 Definition En CFG er p\u00e5 Chomsky-Normalform (CNF) hvis reglerne overholder: Eneste tilladte \\varepsilon \\varepsilon -regel er S\\longrightarrow\\varepsilon S\\longrightarrow\\varepsilon Alle andre regler er p\u00e5 formerne: A\\longrightarrow BC A\\longrightarrow BC A\\longrightarrow a A\\longrightarrow a S fremkommer ikke p\u00e5 nogen h\u00f8jreside En CFG p\u00e5 CNF vil altid g\u00f8re at parstr\u00e6er for strenge \"n\u00e6sten bin\u00e6re tr\u00e6er\" Eksempel: S\u00e6tning: For enhver CFG G kan vi konstruere en CFG G' p\u00e5 CNF s\u00e5 L(G')=L(G) L(G')=L(G) Bevis (Algoritme): Eksempel: \\begin{align*} S &\\longrightarrow aSSb \\ | \\ \\varepsilon \\end{align*} \\begin{align*} S &\\longrightarrow aSSb \\ | \\ \\varepsilon \\end{align*} 1) (ny startvariabel) Lav ny startvariabel S_1 S_1 og regel: $$ S_1 \\longrightarrow S $$ Eksempel: \\begin{align*} S_1 &\\longrightarrow S \\\\ S &\\longrightarrow aSSb \\ | \\ \\varepsilon \\end{align*} \\begin{align*} S_1 &\\longrightarrow S \\\\ S &\\longrightarrow aSSb \\ | \\ \\varepsilon \\end{align*} 2) (fix \\epsilon \\epsilon -regler) Fjern reglerne A \\longrightarrow \\varepsilon A \\longrightarrow \\varepsilon en ad gangen (epsilonregler) og Tilf\u00f8j nye regler hvor 0 eller flere forekomster af A er blevet fjernet. Eksempel: \\begin{align*} S_1 &\\longrightarrow S \\\\ S &\\longrightarrow aSSb \\ | \\ \\varepsilon\\\\ &\\Downarrow \\\\ S_1 &\\longrightarrow S \\ | \\ \\varepsilon\\\\ S &\\longrightarrow aSSb \\ | \\ aSb \\ | \\ ab\\\\ \\end{align*} \\begin{align*} S_1 &\\longrightarrow S \\\\ S &\\longrightarrow aSSb \\ | \\ \\varepsilon\\\\ &\\Downarrow \\\\ S_1 &\\longrightarrow S \\ | \\ \\varepsilon\\\\ S &\\longrightarrow aSSb \\ | \\ aSb \\ | \\ ab\\\\ \\end{align*} Vi fjerner at S kan blive til epsilon. Dette betyder at alle steder hvor der st\u00e5r et S, skal vi kunne skrive epsilon $$ S \\longrightarrow aSSb \\ | \\ aS \\varepsilon b \\ | \\ a \\varepsilon \\varepsilon b\\ $$ 3) (forl\u00e6ngelse af for korte regler) For hver regel der sender \u00e9n variabel over i \u00e9n anden variabel ( A\\longrightarrow B A\\longrightarrow B ): fjern reglen og erstat med A\\longrightarrow w A\\longrightarrow w for hver regel B\\longrightarrow w B\\longrightarrow w (der skriver B om til w) Eksempel: \\begin{align*} S_1 &\\longrightarrow S \\ | \\ \\varepsilon\\\\ S &\\longrightarrow aSSb \\ | \\ aSb \\ | \\ ab\\\\ &\\Downarrow \\\\ S_1 &\\longrightarrow aSSb \\ | \\ aSb \\ | \\ ab \\ | \\ \\varepsilon\\\\ S &\\longrightarrow aSSb \\ | \\ aSb \\ | \\ ab \\end{align*} \\begin{align*} S_1 &\\longrightarrow S \\ | \\ \\varepsilon\\\\ S &\\longrightarrow aSSb \\ | \\ aSb \\ | \\ ab\\\\ &\\Downarrow \\\\ S_1 &\\longrightarrow aSSb \\ | \\ aSb \\ | \\ ab \\ | \\ \\varepsilon\\\\ S &\\longrightarrow aSSb \\ | \\ aSb \\ | \\ ab \\end{align*} 4) (forkortelse af for lange regler) For hver regel A\\longrightarrow u_1u_2...u_k,\\quad k>2 A\\longrightarrow u_1u_2...u_k,\\quad k>2 ( u_i u_i kan v\u00e6re variabler eller terminaler), lav nye regler og nye variabler. \\begin{align*} A &\\longrightarrow u_1W_1 \\\\ W_1 &\\longrightarrow u_2W_2 \\\\ &\\quad \\vdots \\\\ W_{k-2} &\\longrightarrow u_{k-1}u_k \\end{align*} \\begin{align*} A &\\longrightarrow u_1W_1 \\\\ W_1 &\\longrightarrow u_2W_2 \\\\ &\\quad \\vdots \\\\ W_{k-2} &\\longrightarrow u_{k-1}u_k \\end{align*} Eksempel: \\begin{align*} S_1 &\\longrightarrow aSSb \\ | \\ aSb \\ | \\ ab \\ | \\ \\varepsilon\\\\ S &\\longrightarrow aSSb \\ | \\ aSb \\ | \\ ab \\\\ &\\;\\Downarrow \\\\ S_1 &\\longrightarrow aU_1 \\ | \\ aU_3 \\ | \\ ab \\ | \\ \\varepsilon \\\\ U_1 &\\longrightarrow SU_2 \\\\ U_2 &\\longrightarrow Sb \\\\ U_3 &\\longrightarrow Sb \\\\ S &\\longrightarrow aU_4 \\ | \\ aU_6 \\ | \\ ab \\ \\\\ U_4 &\\longrightarrow SU_5 \\\\ U_5 &\\longrightarrow Sb \\\\ U_6 &\\longrightarrow Sb \\\\ \\end{align*} \\begin{align*} S_1 &\\longrightarrow aSSb \\ | \\ aSb \\ | \\ ab \\ | \\ \\varepsilon\\\\ S &\\longrightarrow aSSb \\ | \\ aSb \\ | \\ ab \\\\ &\\;\\Downarrow \\\\ S_1 &\\longrightarrow aU_1 \\ | \\ aU_3 \\ | \\ ab \\ | \\ \\varepsilon \\\\ U_1 &\\longrightarrow SU_2 \\\\ U_2 &\\longrightarrow Sb \\\\ U_3 &\\longrightarrow Sb \\\\ S &\\longrightarrow aU_4 \\ | \\ aU_6 \\ | \\ ab \\ \\\\ U_4 &\\longrightarrow SU_5 \\\\ U_5 &\\longrightarrow Sb \\\\ U_6 &\\longrightarrow Sb \\\\ \\end{align*} 5) For hver terminal a som ikke er alene p\u00e5 en h\u00f8jreside: lav ny regel U_a U_a U_a\\longrightarrow a U_a\\longrightarrow a Eksempel: \\begin{align*} S_1 &\\longrightarrow aU_1 \\ | \\ aU_3 \\ | \\ ab \\ | \\ \\varepsilon \\\\ U_1 &\\longrightarrow SU_2 \\\\ U_2 &\\longrightarrow Sb \\\\ U_3 &\\longrightarrow Sb \\\\ S &\\longrightarrow aU_4 \\ | \\ aU_6 \\ | \\ ab \\ \\\\ U_4 &\\longrightarrow SU_5 \\\\ U_5 &\\longrightarrow Sb \\\\ U_6 &\\longrightarrow Sb \\\\ &\\;\\Downarrow \\\\ S_1 &\\longrightarrow U_aU_1 \\ | \\ U_aU_3 \\ | \\ U_AU_b \\ | \\ \\varepsilon \\\\ U_a &\\longrightarrow a \\\\ U_b &\\longrightarrow b \\\\ U_1 &\\longrightarrow SU_2 \\\\ U_2 &\\longrightarrow SU_b \\\\ U_3 &\\longrightarrow SU_b \\\\ S &\\longrightarrow U_aU_4 \\ | \\ U_aU_6 \\ | \\ U_aU_b \\ \\\\ U_4 &\\longrightarrow SU_5 \\\\ U_5 &\\longrightarrow SU_b \\\\ U_6 &\\longrightarrow SU_b \\\\ \\end{align*} \\begin{align*} S_1 &\\longrightarrow aU_1 \\ | \\ aU_3 \\ | \\ ab \\ | \\ \\varepsilon \\\\ U_1 &\\longrightarrow SU_2 \\\\ U_2 &\\longrightarrow Sb \\\\ U_3 &\\longrightarrow Sb \\\\ S &\\longrightarrow aU_4 \\ | \\ aU_6 \\ | \\ ab \\ \\\\ U_4 &\\longrightarrow SU_5 \\\\ U_5 &\\longrightarrow Sb \\\\ U_6 &\\longrightarrow Sb \\\\ &\\;\\Downarrow \\\\ S_1 &\\longrightarrow U_aU_1 \\ | \\ U_aU_3 \\ | \\ U_AU_b \\ | \\ \\varepsilon \\\\ U_a &\\longrightarrow a \\\\ U_b &\\longrightarrow b \\\\ U_1 &\\longrightarrow SU_2 \\\\ U_2 &\\longrightarrow SU_b \\\\ U_3 &\\longrightarrow SU_b \\\\ S &\\longrightarrow U_aU_4 \\ | \\ U_aU_6 \\ | \\ U_aU_b \\ \\\\ U_4 &\\longrightarrow SU_5 \\\\ U_5 &\\longrightarrow SU_b \\\\ U_6 &\\longrightarrow SU_b \\\\ \\end{align*}","title":"Kontekstfrie Grammatikker"},{"location":"4-semester/SS/05-kontekstfrie-grammatikker/#kontekstfrie-grammatikker","text":"Beskrive Genkende Regul\u00e6re sprog Regul\u00e6re udtryk Endelige automater - NFA og DFA (\u00e6kv.) Kontekstfrie sprog Kontekstfrie grammatikker Pushdown-automater - NDPDA og DPDA (ikke \u00e6kv.)","title":"Kontekstfrie Grammatikker"},{"location":"4-semester/SS/05-kontekstfrie-grammatikker/#kontekstfri-grammatik-cfg","text":"Definition : En kontekstfri grammatik er en 4-tupel $$ (V,\\Sigma,R,S) $$ Symbol Betydning V Endelig m\u00e6ngde af variable (aka. nonterminaler) \\Sigma \\Sigma Endelig m\u00e6ngde af terminaler R Endelig m\u00e6ngde af regler S Startvariabel S\\in V S\\in V (aka. startsymbol) Regler i R er p\u00e5 formen: $$ A \\longrightarrow w, \\quad w \\in (V\\cup\\Sigma)^* $$","title":"Kontekstfri Grammatik - CFG"},{"location":"4-semester/SS/05-kontekstfrie-grammatikker/#eksempel","text":"Sprog: $$ L={a^nb^n | n \\geq0} $$ CFG: $$ \\text{S} \\longrightarrow \\varepsilon \\ |\\ \\text{aSb} $$ \u200b svarer til: \\begin{align*} \\text{S} &\\longrightarrow \\varepsilon \\\\ \\text{S} &\\longrightarrow \\text{aSb} \\end{align*} \\begin{align*} \\text{S} &\\longrightarrow \\varepsilon \\\\ \\text{S} &\\longrightarrow \\text{aSb} \\end{align*}","title":"Eksempel"},{"location":"4-semester/SS/05-kontekstfrie-grammatikker/#byggelse-af-en-streng","text":"Strengen aaabbb \\begin{align*} \\text{S} &\\Rightarrow \\text{aSb} \\\\ & \\Rightarrow \\text{aaSbb} \\\\ & \\Rightarrow \\text{aaaSbbb} \\\\ & \\Rightarrow \\text{aaabbb} \\end{align*} \\begin{align*} \\text{S} &\\Rightarrow \\text{aSb} \\\\ & \\Rightarrow \\text{aaSbb} \\\\ & \\Rightarrow \\text{aaaSbbb} \\\\ & \\Rightarrow \\text{aaabbb} \\end{align*}","title":"Byggelse af en streng"},{"location":"4-semester/SS/05-kontekstfrie-grammatikker/#derivition","text":"Definition: Lad G v\u00e6re en CFG hvor G=(V,\\Sigma,R,S) G=(V,\\Sigma,R,S) og lad u,v \\in (V\\cup\\Sigma)^* u,v \\in (V\\cup\\Sigma)^* Hvis \\begin{align*} u&=u_1Au_2 \\quad og \\quad A\\longrightarrow w \\\\ og \\quad v &=u_1wu_2 \\end{align*} \\begin{align*} u&=u_1Au_2 \\quad og \\quad A\\longrightarrow w \\\\ og \\quad v &=u_1wu_2 \\end{align*} s\u00e5 skriver vi u \\Rightarrow v u \\Rightarrow v \u200b u deriverer i et skridt til v $$ u \\Rightarrow^* v $$ \u200b u deriverer i 0 eller flere et skridt til v","title":"Derivition"},{"location":"4-semester/SS/05-kontekstfrie-grammatikker/#eksempel_1","text":"Sprog L_2=\\{w \\in \\{a,b\\}^* \\ | \\ \\text{w er et palindrom}\\} L_2=\\{w \\in \\{a,b\\}^* \\ | \\ \\text{w er et palindrom}\\} \u200b Eksempler: $$ aaa \\in L_2 \\quad abba \\in L_2 \\quad ab \\notin L_2 $$ Regler: $$ S \\longrightarrow \\varepsilon \\ | \\ a \\ | \\ b \\ | \\ aSa \\ | \\ bSb $$ Derivation af abba $$ S \\Rightarrow aSa \\Rightarrow abSba \\Rightarrow abba $$","title":"Eksempel"},{"location":"4-semester/SS/05-kontekstfrie-grammatikker/#kontekstfrie-sprog","text":"Definition: Lad G=(V,\\Sigma,R,S) G=(V,\\Sigma,R,S) v\u00e6re en CFG Sproget defineret af G er L(G)=\\{w\\in\\Sigma^* \\ | \\ S \\Rightarrow^*w\\} L(G)=\\{w\\in\\Sigma^* \\ | \\ S \\Rightarrow^*w\\} Et sprog L kalder vi kontekstfrit hvis der findes en CFG G s\u00e5 L=L(G) L=L(G)","title":"Kontekstfrie sprog"},{"location":"4-semester/SS/05-kontekstfrie-grammatikker/#kontekstfrie-sprog-vs-regulre-sprog","text":"S\u00e6tning Hvis M er en DFA, kan vi konstruere en CFG G s\u00e5 L(G)=L(M) L(G)=L(M) Eksempel: \u200b M: Ide i konstruktion: Til hver tilstand svarer en variabel i vores CFG \\begin{align*} A_1 &\\longrightarrow aA_1 \\ | \\ bA_2 \\ | \\ b\\\\ A_2 &\\longrightarrow vA_2 \\ | \\ aA_1 \\ | \\ b \\ | \\ \\varepsilon \\end{align*} \\begin{align*} A_1 &\\longrightarrow aA_1 \\ | \\ bA_2 \\ | \\ b\\\\ A_2 &\\longrightarrow vA_2 \\ | \\ aA_1 \\ | \\ b \\ | \\ \\varepsilon \\end{align*} Generelt: Hvis \\delta(q,a)=q' \\delta(q,a)=q' lav reglen: \u200b A_q\\longrightarrow aA_{q'} A_q\\longrightarrow aA_{q'} Hvis q'\\in F: \\quad A_q\\longrightarrow a q'\\in F: \\quad A_q\\longrightarrow a \u200b Hvis q\\in F : \\quad A_q\\longrightarrow \\varepsilon q\\in F : \\quad A_q\\longrightarrow \\varepsilon","title":"Kontekstfrie sprog vs Regul\u00e6re sprog"},{"location":"4-semester/SS/05-kontekstfrie-grammatikker/#verdenskort-venn-diagram","text":"","title":"Verdenskort (Venn diagram)"},{"location":"4-semester/SS/05-kontekstfrie-grammatikker/#derivitionstrer-parsetrer","text":"Et parsetr\u00e6 er en tr\u00e6repr\u00e6sentation af en derivition.","title":"Derivitionstr\u00e6er / Parsetr\u00e6er"},{"location":"4-semester/SS/05-kontekstfrie-grammatikker/#eksempel_2","text":"Aritmetiske udtryk V=\\{E\\},\\Sigma=\\{+,*,a\\} \\\\ E \\longrightarrow E+E \\ | \\ E*E \\ | \\ a V=\\{E\\},\\Sigma=\\{+,*,a\\} \\\\ E \\longrightarrow E+E \\ | \\ E*E \\ | \\ a \"a+a*a\" E\\Rightarrow E+E \\Rightarrow a+E \\Rightarrow a+E*E \\Rightarrow a+a*E \\Rightarrow a+a*a E\\Rightarrow E+E \\Rightarrow a+E \\Rightarrow a+E*E \\Rightarrow a+a*E \\Rightarrow a+a*a","title":"Eksempel"},{"location":"4-semester/SS/05-kontekstfrie-grammatikker/#parsetr","text":"","title":"Parsetr\u00e6"},{"location":"4-semester/SS/05-kontekstfrie-grammatikker/#men","text":"Kan ogs\u00e5 deriveres p\u00e5 en anden m\u00e5de: E\\Rightarrow E*E \\Rightarrow E+E*E \\Rightarrow a+E*E \\Rightarrow a+a*E \\Rightarrow a+a*a E\\Rightarrow E*E \\Rightarrow E+E*E \\Rightarrow a+E*E \\Rightarrow a+a*E \\Rightarrow a+a*a Grammatikken er tvetydig (ambigous). Begge derivitioner er venstrederivitioner , 2 forskellige venstrederivitioner.","title":"MEN"},{"location":"4-semester/SS/05-kontekstfrie-grammatikker/#tvetydighed","text":"Definition : En CFG G er tvetydig hvis der findes en w\\in L(G) w\\in L(G) s\u00e5 S\\Rightarrow^*w S\\Rightarrow^*w med to (eller flere) forskellige venstrederivitioner S\u00e6tning: En CFG G er tvetydig hvis og kun hvis der findes en w\\in L(G) w\\in L(G) som har mindst to forskellige parsetr\u00e6er.","title":"Tvetydighed"},{"location":"4-semester/SS/05-kontekstfrie-grammatikker/#indbygget-tvetydighed","text":"Der findes kontekstfrie sprog L s\u00e5 enhver CFG G s\u00e5 L(G)=L L(G)=L vil v\u00e6re tvetydig. Den slags sprog kaldes indbygget tvetydig","title":"Indbygget Tvetydighed"},{"location":"4-semester/SS/05-kontekstfrie-grammatikker/#bestem-tvetydighed","text":"S\u00e6tning Der findes ikke nogen algoritme, der altid kan fort\u00e6lle os om en CFG er tvetydig. (umulighedsresultat)","title":"Bestem Tvetydighed"},{"location":"4-semester/SS/05-kontekstfrie-grammatikker/#chomsky-normalform-cnf","text":"Definition En CFG er p\u00e5 Chomsky-Normalform (CNF) hvis reglerne overholder: Eneste tilladte \\varepsilon \\varepsilon -regel er S\\longrightarrow\\varepsilon S\\longrightarrow\\varepsilon Alle andre regler er p\u00e5 formerne: A\\longrightarrow BC A\\longrightarrow BC A\\longrightarrow a A\\longrightarrow a S fremkommer ikke p\u00e5 nogen h\u00f8jreside En CFG p\u00e5 CNF vil altid g\u00f8re at parstr\u00e6er for strenge \"n\u00e6sten bin\u00e6re tr\u00e6er\" Eksempel: S\u00e6tning: For enhver CFG G kan vi konstruere en CFG G' p\u00e5 CNF s\u00e5 L(G')=L(G) L(G')=L(G) Bevis (Algoritme): Eksempel: \\begin{align*} S &\\longrightarrow aSSb \\ | \\ \\varepsilon \\end{align*} \\begin{align*} S &\\longrightarrow aSSb \\ | \\ \\varepsilon \\end{align*} 1) (ny startvariabel) Lav ny startvariabel S_1 S_1 og regel: $$ S_1 \\longrightarrow S $$ Eksempel: \\begin{align*} S_1 &\\longrightarrow S \\\\ S &\\longrightarrow aSSb \\ | \\ \\varepsilon \\end{align*} \\begin{align*} S_1 &\\longrightarrow S \\\\ S &\\longrightarrow aSSb \\ | \\ \\varepsilon \\end{align*} 2) (fix \\epsilon \\epsilon -regler) Fjern reglerne A \\longrightarrow \\varepsilon A \\longrightarrow \\varepsilon en ad gangen (epsilonregler) og Tilf\u00f8j nye regler hvor 0 eller flere forekomster af A er blevet fjernet. Eksempel: \\begin{align*} S_1 &\\longrightarrow S \\\\ S &\\longrightarrow aSSb \\ | \\ \\varepsilon\\\\ &\\Downarrow \\\\ S_1 &\\longrightarrow S \\ | \\ \\varepsilon\\\\ S &\\longrightarrow aSSb \\ | \\ aSb \\ | \\ ab\\\\ \\end{align*} \\begin{align*} S_1 &\\longrightarrow S \\\\ S &\\longrightarrow aSSb \\ | \\ \\varepsilon\\\\ &\\Downarrow \\\\ S_1 &\\longrightarrow S \\ | \\ \\varepsilon\\\\ S &\\longrightarrow aSSb \\ | \\ aSb \\ | \\ ab\\\\ \\end{align*} Vi fjerner at S kan blive til epsilon. Dette betyder at alle steder hvor der st\u00e5r et S, skal vi kunne skrive epsilon $$ S \\longrightarrow aSSb \\ | \\ aS \\varepsilon b \\ | \\ a \\varepsilon \\varepsilon b\\ $$ 3) (forl\u00e6ngelse af for korte regler) For hver regel der sender \u00e9n variabel over i \u00e9n anden variabel ( A\\longrightarrow B A\\longrightarrow B ): fjern reglen og erstat med A\\longrightarrow w A\\longrightarrow w for hver regel B\\longrightarrow w B\\longrightarrow w (der skriver B om til w) Eksempel: \\begin{align*} S_1 &\\longrightarrow S \\ | \\ \\varepsilon\\\\ S &\\longrightarrow aSSb \\ | \\ aSb \\ | \\ ab\\\\ &\\Downarrow \\\\ S_1 &\\longrightarrow aSSb \\ | \\ aSb \\ | \\ ab \\ | \\ \\varepsilon\\\\ S &\\longrightarrow aSSb \\ | \\ aSb \\ | \\ ab \\end{align*} \\begin{align*} S_1 &\\longrightarrow S \\ | \\ \\varepsilon\\\\ S &\\longrightarrow aSSb \\ | \\ aSb \\ | \\ ab\\\\ &\\Downarrow \\\\ S_1 &\\longrightarrow aSSb \\ | \\ aSb \\ | \\ ab \\ | \\ \\varepsilon\\\\ S &\\longrightarrow aSSb \\ | \\ aSb \\ | \\ ab \\end{align*} 4) (forkortelse af for lange regler) For hver regel A\\longrightarrow u_1u_2...u_k,\\quad k>2 A\\longrightarrow u_1u_2...u_k,\\quad k>2 ( u_i u_i kan v\u00e6re variabler eller terminaler), lav nye regler og nye variabler. \\begin{align*} A &\\longrightarrow u_1W_1 \\\\ W_1 &\\longrightarrow u_2W_2 \\\\ &\\quad \\vdots \\\\ W_{k-2} &\\longrightarrow u_{k-1}u_k \\end{align*} \\begin{align*} A &\\longrightarrow u_1W_1 \\\\ W_1 &\\longrightarrow u_2W_2 \\\\ &\\quad \\vdots \\\\ W_{k-2} &\\longrightarrow u_{k-1}u_k \\end{align*} Eksempel: \\begin{align*} S_1 &\\longrightarrow aSSb \\ | \\ aSb \\ | \\ ab \\ | \\ \\varepsilon\\\\ S &\\longrightarrow aSSb \\ | \\ aSb \\ | \\ ab \\\\ &\\;\\Downarrow \\\\ S_1 &\\longrightarrow aU_1 \\ | \\ aU_3 \\ | \\ ab \\ | \\ \\varepsilon \\\\ U_1 &\\longrightarrow SU_2 \\\\ U_2 &\\longrightarrow Sb \\\\ U_3 &\\longrightarrow Sb \\\\ S &\\longrightarrow aU_4 \\ | \\ aU_6 \\ | \\ ab \\ \\\\ U_4 &\\longrightarrow SU_5 \\\\ U_5 &\\longrightarrow Sb \\\\ U_6 &\\longrightarrow Sb \\\\ \\end{align*} \\begin{align*} S_1 &\\longrightarrow aSSb \\ | \\ aSb \\ | \\ ab \\ | \\ \\varepsilon\\\\ S &\\longrightarrow aSSb \\ | \\ aSb \\ | \\ ab \\\\ &\\;\\Downarrow \\\\ S_1 &\\longrightarrow aU_1 \\ | \\ aU_3 \\ | \\ ab \\ | \\ \\varepsilon \\\\ U_1 &\\longrightarrow SU_2 \\\\ U_2 &\\longrightarrow Sb \\\\ U_3 &\\longrightarrow Sb \\\\ S &\\longrightarrow aU_4 \\ | \\ aU_6 \\ | \\ ab \\ \\\\ U_4 &\\longrightarrow SU_5 \\\\ U_5 &\\longrightarrow Sb \\\\ U_6 &\\longrightarrow Sb \\\\ \\end{align*} 5) For hver terminal a som ikke er alene p\u00e5 en h\u00f8jreside: lav ny regel U_a U_a U_a\\longrightarrow a U_a\\longrightarrow a Eksempel: \\begin{align*} S_1 &\\longrightarrow aU_1 \\ | \\ aU_3 \\ | \\ ab \\ | \\ \\varepsilon \\\\ U_1 &\\longrightarrow SU_2 \\\\ U_2 &\\longrightarrow Sb \\\\ U_3 &\\longrightarrow Sb \\\\ S &\\longrightarrow aU_4 \\ | \\ aU_6 \\ | \\ ab \\ \\\\ U_4 &\\longrightarrow SU_5 \\\\ U_5 &\\longrightarrow Sb \\\\ U_6 &\\longrightarrow Sb \\\\ &\\;\\Downarrow \\\\ S_1 &\\longrightarrow U_aU_1 \\ | \\ U_aU_3 \\ | \\ U_AU_b \\ | \\ \\varepsilon \\\\ U_a &\\longrightarrow a \\\\ U_b &\\longrightarrow b \\\\ U_1 &\\longrightarrow SU_2 \\\\ U_2 &\\longrightarrow SU_b \\\\ U_3 &\\longrightarrow SU_b \\\\ S &\\longrightarrow U_aU_4 \\ | \\ U_aU_6 \\ | \\ U_aU_b \\ \\\\ U_4 &\\longrightarrow SU_5 \\\\ U_5 &\\longrightarrow SU_b \\\\ U_6 &\\longrightarrow SU_b \\\\ \\end{align*} \\begin{align*} S_1 &\\longrightarrow aU_1 \\ | \\ aU_3 \\ | \\ ab \\ | \\ \\varepsilon \\\\ U_1 &\\longrightarrow SU_2 \\\\ U_2 &\\longrightarrow Sb \\\\ U_3 &\\longrightarrow Sb \\\\ S &\\longrightarrow aU_4 \\ | \\ aU_6 \\ | \\ ab \\ \\\\ U_4 &\\longrightarrow SU_5 \\\\ U_5 &\\longrightarrow Sb \\\\ U_6 &\\longrightarrow Sb \\\\ &\\;\\Downarrow \\\\ S_1 &\\longrightarrow U_aU_1 \\ | \\ U_aU_3 \\ | \\ U_AU_b \\ | \\ \\varepsilon \\\\ U_a &\\longrightarrow a \\\\ U_b &\\longrightarrow b \\\\ U_1 &\\longrightarrow SU_2 \\\\ U_2 &\\longrightarrow SU_b \\\\ U_3 &\\longrightarrow SU_b \\\\ S &\\longrightarrow U_aU_4 \\ | \\ U_aU_6 \\ | \\ U_aU_b \\ \\\\ U_4 &\\longrightarrow SU_5 \\\\ U_5 &\\longrightarrow SU_b \\\\ U_6 &\\longrightarrow SU_b \\\\ \\end{align*}","title":"Chomsky Normalform (CNF)"},{"location":"4-semester/SS/06-pushdown-automater/","text":"Pushdown Automater \u00b6 PDA = en automat med en stak Udvid noter inds\u00e6t information om overf\u00f8ringsfunktion fra SS6 afsnit 2 https://www.youtube.com/watch?v=KUZwifJbGxE&list=PLA8H0-CuqhGGH6lTKmcgzzmK7kkwX75rF&index=2 Definition En pushdown-automat (PDA) er en 6-tupel $$ (Q,\\Sigma, \\Gamma, q_0, \\delta, F) $$ Symbol Betydning Q M\u00e6ngde af tilstande \\Sigma \\Sigma Input alfabet \\Gamma \\Gamma Stak alfabet q_0 q_0 Starttilstand q_0 \\in Q q_0 \\in Q \\delta \\delta Overf\u00f8rselsfunktion F Accepttilstande F\\subseteq Q F\\subseteq Q \\delta:\\quad Q \\times \\Sigma_{\\varepsilon} \\times \\Gamma_{\\varepsilon} \\longrightarrow \\mathcal{P}(Q \\times \\Gamma_{\\varepsilon}) \\delta:\\quad Q \\times \\Sigma_{\\varepsilon} \\times \\Gamma_{\\varepsilon} \\longrightarrow \\mathcal{P}(Q \\times \\Gamma_{\\varepsilon}) PDA Som en Orienteret Graf \u00b6 Betyder: N\u00e5r PDA'en er i tilstand q og ser a som n\u00e6ste input tegn og x er \u00f8verst p\u00e5 stakken, s\u00e5 skift til tilstand r og erstat x med y . $$ (r,y)\\in\\delta(q,a,x) $$ Eksempel \u00b6 L=\\{a^nb^n\\ | \\ n\\geq 0\\} L=\\{a^nb^n\\ | \\ n\\geq 0\\} $ bruges som bundmark\u00f8r i stakken. Inputstreng: aabb Stakindhold: Sprog Accepteret af PDA \u00b6 Lad M=(Q,\\Sigma, \\Gamma, q_0, \\delta, F) M=(Q,\\Sigma, \\Gamma, q_0, \\delta, F) v\u00e6re en PDA Lad w\\in\\Sigma^* w\\in\\Sigma^* , hvor w=u_1...u_k w=u_1...u_k , hvor u_i\\in\\Sigma_{\\varepsilon} \\quad (1\\leq i\\leq k) u_i\\in\\Sigma_{\\varepsilon} \\quad (1\\leq i\\leq k) M accepterer w hvis: Der findes en f\u00f8lge af tilstande r_0...r_k r_0...r_k Der findes en f\u00f8lge af stakindhold s_0...s_k s_0...s_k (s_i\\in\\Gamma^* \\ \\text{for} \\ 0\\leq i \\leq k) (s_i\\in\\Gamma^* \\ \\text{for} \\ 0\\leq i \\leq k) Der g\u00e6lder om (1) og (2) at: r_0=q_0,\\quad s_0=\\varepsilon r_0=q_0,\\quad s_0=\\varepsilon \\text{for alle}\\ 0\\leq i \\leq k \\text{for alle}\\ 0\\leq i \\leq k g\u00e6lder: \\begin{align*} && s_i &=a_iS_i' \\\\ (r_{i+1}&,a_{i+1}) \\in \\delta(r_i,u_{i+1},a_i) & s_{i+1} &=a_{i+1}S_i' \\end{align*} \\begin{align*} && s_i &=a_iS_i' \\\\ (r_{i+1}&,a_{i+1}) \\in \\delta(r_i,u_{i+1},a_i) & s_{i+1} &=a_{i+1}S_i' \\end{align*} r_k\\in F r_k\\in F Eksempel 2-3 \u00b6 Udvid noter inds\u00e6t eksempler fra SS6 afsnit 4 [https://www.youtube.com/watch?v=eBgtVhYUf50&list=PLA8H0-CuqhGGH6lTKmcgzzmK7kkwX75rF&index=4] Lav Pushdown Automat ud fra CFG \u00b6 Placer markersymbol $, og startvariablen p\u00e5 stakken. Gentag forevigt: Hvis toppen af stakken er en variabel A, nondeterministisk v\u00e6lg en af reglerne for A og substituer A med strengen p\u00e5 h\u00f8jre side af reglen. Hvis toppen af stakken er en terminal a, l\u00e6s n\u00e6ste symbol fra input og sammenlign med a. Hvis de matcher, gentag. Hvis de ikke matcher, afvis p\u00e5 denne gren. Hvis toppen af stakken er $, g\u00e5 ind i accept tilstand.","title":"Pushdown Automater"},{"location":"4-semester/SS/06-pushdown-automater/#pushdown-automater","text":"PDA = en automat med en stak Udvid noter inds\u00e6t information om overf\u00f8ringsfunktion fra SS6 afsnit 2 https://www.youtube.com/watch?v=KUZwifJbGxE&list=PLA8H0-CuqhGGH6lTKmcgzzmK7kkwX75rF&index=2 Definition En pushdown-automat (PDA) er en 6-tupel $$ (Q,\\Sigma, \\Gamma, q_0, \\delta, F) $$ Symbol Betydning Q M\u00e6ngde af tilstande \\Sigma \\Sigma Input alfabet \\Gamma \\Gamma Stak alfabet q_0 q_0 Starttilstand q_0 \\in Q q_0 \\in Q \\delta \\delta Overf\u00f8rselsfunktion F Accepttilstande F\\subseteq Q F\\subseteq Q \\delta:\\quad Q \\times \\Sigma_{\\varepsilon} \\times \\Gamma_{\\varepsilon} \\longrightarrow \\mathcal{P}(Q \\times \\Gamma_{\\varepsilon}) \\delta:\\quad Q \\times \\Sigma_{\\varepsilon} \\times \\Gamma_{\\varepsilon} \\longrightarrow \\mathcal{P}(Q \\times \\Gamma_{\\varepsilon})","title":"Pushdown Automater"},{"location":"4-semester/SS/06-pushdown-automater/#pda-som-en-orienteret-graf","text":"Betyder: N\u00e5r PDA'en er i tilstand q og ser a som n\u00e6ste input tegn og x er \u00f8verst p\u00e5 stakken, s\u00e5 skift til tilstand r og erstat x med y . $$ (r,y)\\in\\delta(q,a,x) $$","title":"PDA Som en Orienteret Graf"},{"location":"4-semester/SS/06-pushdown-automater/#eksempel","text":"L=\\{a^nb^n\\ | \\ n\\geq 0\\} L=\\{a^nb^n\\ | \\ n\\geq 0\\} $ bruges som bundmark\u00f8r i stakken. Inputstreng: aabb Stakindhold:","title":"Eksempel"},{"location":"4-semester/SS/06-pushdown-automater/#sprog-accepteret-af-pda","text":"Lad M=(Q,\\Sigma, \\Gamma, q_0, \\delta, F) M=(Q,\\Sigma, \\Gamma, q_0, \\delta, F) v\u00e6re en PDA Lad w\\in\\Sigma^* w\\in\\Sigma^* , hvor w=u_1...u_k w=u_1...u_k , hvor u_i\\in\\Sigma_{\\varepsilon} \\quad (1\\leq i\\leq k) u_i\\in\\Sigma_{\\varepsilon} \\quad (1\\leq i\\leq k) M accepterer w hvis: Der findes en f\u00f8lge af tilstande r_0...r_k r_0...r_k Der findes en f\u00f8lge af stakindhold s_0...s_k s_0...s_k (s_i\\in\\Gamma^* \\ \\text{for} \\ 0\\leq i \\leq k) (s_i\\in\\Gamma^* \\ \\text{for} \\ 0\\leq i \\leq k) Der g\u00e6lder om (1) og (2) at: r_0=q_0,\\quad s_0=\\varepsilon r_0=q_0,\\quad s_0=\\varepsilon \\text{for alle}\\ 0\\leq i \\leq k \\text{for alle}\\ 0\\leq i \\leq k g\u00e6lder: \\begin{align*} && s_i &=a_iS_i' \\\\ (r_{i+1}&,a_{i+1}) \\in \\delta(r_i,u_{i+1},a_i) & s_{i+1} &=a_{i+1}S_i' \\end{align*} \\begin{align*} && s_i &=a_iS_i' \\\\ (r_{i+1}&,a_{i+1}) \\in \\delta(r_i,u_{i+1},a_i) & s_{i+1} &=a_{i+1}S_i' \\end{align*} r_k\\in F r_k\\in F","title":"Sprog Accepteret af PDA"},{"location":"4-semester/SS/06-pushdown-automater/#eksempel-2-3","text":"Udvid noter inds\u00e6t eksempler fra SS6 afsnit 4 [https://www.youtube.com/watch?v=eBgtVhYUf50&list=PLA8H0-CuqhGGH6lTKmcgzzmK7kkwX75rF&index=4]","title":"Eksempel 2-3"},{"location":"4-semester/SS/06-pushdown-automater/#lav-pushdown-automat-ud-fra-cfg","text":"Placer markersymbol $, og startvariablen p\u00e5 stakken. Gentag forevigt: Hvis toppen af stakken er en variabel A, nondeterministisk v\u00e6lg en af reglerne for A og substituer A med strengen p\u00e5 h\u00f8jre side af reglen. Hvis toppen af stakken er en terminal a, l\u00e6s n\u00e6ste symbol fra input og sammenlign med a. Hvis de matcher, gentag. Hvis de ikke matcher, afvis p\u00e5 denne gren. Hvis toppen af stakken er $, g\u00e5 ind i accept tilstand.","title":"Lav Pushdown Automat ud fra CFG"},{"location":"4-semester/SS/07-pumping-lemma-for-kontekstfrie-sprog/","text":"Pumping Lemma for Kontekstfrie Sprog \u00b6 S\u00e6tning Hvis L er et kontekstfrit sprog, s\u00e5 findes der et p>0 p>0 s\u00e5ledes at for alle s\\in L s\\in L hvor |s|\\geq p |s|\\geq p , findes der en opsplitning s=uvxyz s=uvxyz s\u00e5ledes at: uv^ixy^iz\\in L uv^ixy^iz\\in L for alle i\\geq0 i\\geq0 |vy|>0 |vy|>0 v og y er ikke begge tomme |vxy|\\le p |vxy|\\le p v og y skal findes inden for et vindue p\u00e5 p tegn Anvendelse Vise at L ikke er kontekstfrit (modstridsbevis): Antag L var kontekstfrit, s\u00e5 var der et p>0\u200b p>0\u200b s\u00e5 alle s\\in L\u200b s\\in L\u200b hvor |s|\\geq p\u200b |s|\\geq p\u200b har en opsplitning s\u00e5 (1), (2) og (3) er overholdt. Men s\u00e5 finder vi en s\\in L, \\quad |s|\\geq p\u200b s\\in L, \\quad |s|\\geq p\u200b , s\u00e5 ingen opsplitning kan overholde (1), (2) og (3) Eksempel \u00b6 L=\\{a^nb^nc^n\\ | \\ n \\geq 0\\} L=\\{a^nb^nc^n\\ | \\ n \\geq 0\\} (Kongeeksemplet p\u00e5 et ikke kontekstfrit sprog) Antag at L var kontekstfrit. S\u00e5 var der et p>0 p>0 s\u00e5 for alle s\\in L s\\in L hvor |s|\\geq p |s|\\geq p findes en opsplitning s\u00e5 (1), (2) og (3) er overholdt. MEN, s\u00e5 finder vi en s\\in L, \\quad |s|\\geq p s\\in L, \\quad |s|\\geq p , s\u00e5 ingen opsplitning kan overholde (1), (2) og (3) \u200b V\u00e6lg s=a^pb^pc^p s=a^pb^pc^p , klart at |s|=3p\\geq p |s|=3p\\geq p Unders\u00f8g nu alle mulige opsplitninger. $$ \\underbrace{a...a}_p\\ \\underbrace{b...b}_p \\ \\underbrace{c...c}_p $$ Hvor kan v og y ligge henne? v eller y indeholder flere slags tegn. S\u00e5 bliver (1) overtr\u00e5dt; uv^ixy^i z\u200b uv^ixy^i z\u200b har tegn i forkert r\u00e6kkef\u00f8lge v best\u00e5r af a'er. Men hvis (3) skal g\u00e6lde, s\u00e5 m\u00e5 y best\u00e5 af a'er eller af b'er. Og s\u00e5 bliver (1) overtr\u00e5dt: uv^ixy^i z uv^ixy^i z har for f\u00e5 c'er v best\u00e5r af b'er. Men s\u00e5 skal y best\u00e5 af b'er eller af c'er, hvis (3) skal overholdes. Men s\u00e5 vil der v\u00e6re for f\u00e5 a'er. v best\u00e5r af c'er. S\u00e5 skal y ogs\u00e5 kun best\u00e5 af c'er. S\u00e5 vil der v\u00e6re for mange c'er Ingen opsplitning er gyldig, DVS: L er ikke kontekstfrit.","title":"Pumping Lemma for Kontekstfrie Sprog"},{"location":"4-semester/SS/07-pumping-lemma-for-kontekstfrie-sprog/#pumping-lemma-for-kontekstfrie-sprog","text":"S\u00e6tning Hvis L er et kontekstfrit sprog, s\u00e5 findes der et p>0 p>0 s\u00e5ledes at for alle s\\in L s\\in L hvor |s|\\geq p |s|\\geq p , findes der en opsplitning s=uvxyz s=uvxyz s\u00e5ledes at: uv^ixy^iz\\in L uv^ixy^iz\\in L for alle i\\geq0 i\\geq0 |vy|>0 |vy|>0 v og y er ikke begge tomme |vxy|\\le p |vxy|\\le p v og y skal findes inden for et vindue p\u00e5 p tegn Anvendelse Vise at L ikke er kontekstfrit (modstridsbevis): Antag L var kontekstfrit, s\u00e5 var der et p>0\u200b p>0\u200b s\u00e5 alle s\\in L\u200b s\\in L\u200b hvor |s|\\geq p\u200b |s|\\geq p\u200b har en opsplitning s\u00e5 (1), (2) og (3) er overholdt. Men s\u00e5 finder vi en s\\in L, \\quad |s|\\geq p\u200b s\\in L, \\quad |s|\\geq p\u200b , s\u00e5 ingen opsplitning kan overholde (1), (2) og (3)","title":"Pumping Lemma for Kontekstfrie Sprog"},{"location":"4-semester/SS/07-pumping-lemma-for-kontekstfrie-sprog/#eksempel","text":"L=\\{a^nb^nc^n\\ | \\ n \\geq 0\\} L=\\{a^nb^nc^n\\ | \\ n \\geq 0\\} (Kongeeksemplet p\u00e5 et ikke kontekstfrit sprog) Antag at L var kontekstfrit. S\u00e5 var der et p>0 p>0 s\u00e5 for alle s\\in L s\\in L hvor |s|\\geq p |s|\\geq p findes en opsplitning s\u00e5 (1), (2) og (3) er overholdt. MEN, s\u00e5 finder vi en s\\in L, \\quad |s|\\geq p s\\in L, \\quad |s|\\geq p , s\u00e5 ingen opsplitning kan overholde (1), (2) og (3) \u200b V\u00e6lg s=a^pb^pc^p s=a^pb^pc^p , klart at |s|=3p\\geq p |s|=3p\\geq p Unders\u00f8g nu alle mulige opsplitninger. $$ \\underbrace{a...a}_p\\ \\underbrace{b...b}_p \\ \\underbrace{c...c}_p $$ Hvor kan v og y ligge henne? v eller y indeholder flere slags tegn. S\u00e5 bliver (1) overtr\u00e5dt; uv^ixy^i z\u200b uv^ixy^i z\u200b har tegn i forkert r\u00e6kkef\u00f8lge v best\u00e5r af a'er. Men hvis (3) skal g\u00e6lde, s\u00e5 m\u00e5 y best\u00e5 af a'er eller af b'er. Og s\u00e5 bliver (1) overtr\u00e5dt: uv^ixy^i z uv^ixy^i z har for f\u00e5 c'er v best\u00e5r af b'er. Men s\u00e5 skal y best\u00e5 af b'er eller af c'er, hvis (3) skal overholdes. Men s\u00e5 vil der v\u00e6re for f\u00e5 a'er. v best\u00e5r af c'er. S\u00e5 skal y ogs\u00e5 kun best\u00e5 af c'er. S\u00e5 vil der v\u00e6re for mange c'er Ingen opsplitning er gyldig, DVS: L er ikke kontekstfrit.","title":"Eksempel"},{"location":"4-semester/SS/15-rekursive-definitioner/","text":"Rekursive Definitioner \u00b6 En rekursiv definition giver anledning til en h\u00f8jresidefunktion f f Definitionen har en \"l\u00f8sning\" hvis f f har et fikspunkt $$ L_S={a}\\ L_S\\ {b} \\cup {c} \\cup L_s $$ Lav en h\u00f8jresidefunktion: f(X)=\\{a\\}\\ X\\ \\{b\\} \\cup \\{c\\} \\cup X f(X)=\\{a\\}\\ X\\ \\{b\\} \\cup \\{c\\} \\cup X f f er en funktion over sprog - Eks: \\begin{align*} f(\\{aa,b\\}) \\\\ &= \\{a\\}\\{aa,b\\}\\{b\\} \\cup \\{c\\} \\cup \\{aa,b\\} \\\\ &= \\{aaab,abb\\} \\cup\\{c\\} \\cup \\{aa,b\\} \\\\ &= \\{aaab,abb,c,aa,b\\} \\end{align*} \\begin{align*} f(\\{aa,b\\}) \\\\ &= \\{a\\}\\{aa,b\\}\\{b\\} \\cup \\{c\\} \\cup \\{aa,b\\} \\\\ &= \\{aaab,abb\\} \\cup\\{c\\} \\cup \\{aa,b\\} \\\\ &= \\{aaab,abb,c,aa,b\\} \\end{align*} Vi vil gerne have et L_s L_s s\u00e5 f(L_s)=L_s f(L_s)=L_s \u200b L_s L_s er et fikspunkt for funktionen f f Fikspunkt \u00b6 Lad f:A\\to A f:A\\to A x x er et fikspunkt for f f hvis f(x)=x f(x)=x Jagten p\u00e5 en \"l\u00f8sning\" til en rekursiv definition er faktisk jagten p\u00e5 et fikspunkt for h\u00f8jresidefunktionen. F\u00f8lge af tiln\u00e6rmelser: \\begin{align*} f^0(\\varnothing) &= \\varnothing \\\\ f^1(\\varnothing) &= \\{a\\} \\varnothing \\{b\\} \\cup \\{c\\} \\cup \\varnothing &&= \\{c\\} \\\\ f^2(\\varnothing) &= f(f(\\varnothing)) &&= \\{acb,c\\} \\\\ f^3(\\varnothing) &= f(f^2(\\varnothing)) &&= \\{aacbb, acb,c\\} \\\\ f^4(\\varnothing) &= f(f^3(\\varnothing)) &&= \\{aaacbbb, aacbb, acb,c\\} \\\\ f^n(\\varnothing) &&&= \\{a^icb^i\\ |\\ 0 \\leq i < n\\} \\\\ \\end{align*} \\begin{align*} f^0(\\varnothing) &= \\varnothing \\\\ f^1(\\varnothing) &= \\{a\\} \\varnothing \\{b\\} \\cup \\{c\\} \\cup \\varnothing &&= \\{c\\} \\\\ f^2(\\varnothing) &= f(f(\\varnothing)) &&= \\{acb,c\\} \\\\ f^3(\\varnothing) &= f(f^2(\\varnothing)) &&= \\{aacbb, acb,c\\} \\\\ f^4(\\varnothing) &= f(f^3(\\varnothing)) &&= \\{aaacbbb, aacbb, acb,c\\} \\\\ f^n(\\varnothing) &&&= \\{a^icb^i\\ |\\ 0 \\leq i < n\\} \\\\ \\end{align*} Gr\u00e6nsev\u00e6rdien for f\u00f8lgen: \\{a^icb^i\\ |\\ i \\geq 0\\} \\{a^icb^i\\ |\\ i \\geq 0\\} Partiel Ordning \u00b6 Lad D D v\u00e6re en m\u00e6ngde og \\sqsubseteq \\sqsubseteq v\u00e6re en bin\u00e6r relation over D D , (dvs. relaterer par af elementer). (D,\\sqsubseteq) (D,\\sqsubseteq) er en partiel ordning hvis: \\begin{align*} &(1) &&\\text{For alle }d \\in D : &&d \\sqsubseteq d \\\\ &(2) &&\\text{For alle }d_1d_2 \\in D : &&\\text{hvis } d_1 \\sqsubseteq d_2 \\text{ og } d_2 \\sqsubseteq d_1 \\text{ s\u00e5 } d_1=d_2 \\\\ &(3) &&\\text{For alle } d_1,d_2,d_3 \\in D: &&\\text{hvis } d_1 \\sqsubseteq d_2 \\text{ og } d_2 \\sqsubseteq d_3 \\text{ s\u00e5 } d_1 \\sqsubseteq d_3 \\end{align*} \\begin{align*} &(1) &&\\text{For alle }d \\in D : &&d \\sqsubseteq d \\\\ &(2) &&\\text{For alle }d_1d_2 \\in D : &&\\text{hvis } d_1 \\sqsubseteq d_2 \\text{ og } d_2 \\sqsubseteq d_1 \\text{ s\u00e5 } d_1=d_2 \\\\ &(3) &&\\text{For alle } d_1,d_2,d_3 \\in D: &&\\text{hvis } d_1 \\sqsubseteq d_2 \\text{ og } d_2 \\sqsubseteq d_3 \\text{ s\u00e5 } d_1 \\sqsubseteq d_3 \\end{align*} En partiel ordning er alts\u00e5 bare en ordnet m\u00e6ngde der opf\u00f8rer sig ligesom \\N \\N (de naturlige tal) ordnet under \\leq \\leq (\\N, \\leq) (\\N, \\leq) er en partiel ordning Eksempel \u00b6 Lad S S v\u00e6re en m\u00e6ngde. (\\mathcal{P}(S),\\subseteq) (\\mathcal{P}(S),\\subseteq) er en partiel ordning. \u200b S=\\{1,2,3\\} S=\\{1,2,3\\} \u00d8vre Gr\u00e6nse \u00b6 Lad (D, \\sqsubseteq) (D, \\sqsubseteq) v\u00e6re en po (partiel ordning), og lad Y\\subseteq D Y\\subseteq D Lad x\\in D x\\in D . x x er en \u00f8vre gr\u00e6nse for y y hvis: \u200b for alle y \\in Y:y \\sqsubseteq x y \\in Y:y \\sqsubseteq x Hvis x er det mindste s\u00e5danne, kalder vi x for den mindste \u00f8vre gr\u00e6nse for y. Eksempel \u00b6 D=\\{1,2,3,4,5,6\\} D=\\{1,2,3,4,5,6\\} \\sqsubseteq \\sqsubseteq defineres ved x \\sqsubseteq y x \\sqsubseteq y hvis x \\leq y x \\leq y \u200b (D, \\sqsubseteq) (D, \\sqsubseteq) er en po. Lad Y=\\{1,2,3\\} Y=\\{1,2,3\\} \u200b 4 er en \u00f8vre gr\u00e6nse for Y Y . \u200b 3 er den mindste \u00f8vre gr\u00e6nse for Y Y . Notation \u00b6 Hvis x x er den mindste \u00f8vre gr\u00e6nse for y y kalder vi x x for \\lim{Y} \\lim{Y} K\u00e6de \u00b6 Lad (D, \\sqsubseteq) (D, \\sqsubseteq) v\u00e6re en po. En k\u00e6de i D D er en voksende f\u00f8lge af elementer i D D \u200b Y=\\{y_0,y_1,...\\} Y=\\{y_0,y_1,...\\} hvor \u200b y_0 \\sqsubseteq y_1 \\sqsubseteq y_2 \\ ... y_0 \\sqsubseteq y_1 \\sqsubseteq y_2 \\ ... Fuldst\u00e6ndig Partiel Ordning (Dom\u00e6ne) \u00b6 Lad (D,\\sqsubseteq) (D,\\sqsubseteq) v\u00e6re en po. Den kaldes fuldst\u00e6ndig partial ordning (fpo) hvis det g\u00e6lder at: For enhver k\u00e6de i D,Y D,Y , har vi at \\lim{Y} \\lim{Y} findes (enhver voksende f\u00f8lge har en mindste \u00f8vre gr\u00e6nse) Der findes et mindste element, \\perp \\perp s\u00e5: \\perp \\sqsubseteq x \\perp \\sqsubseteq x for alle x \\in D x \\in D ( \\perp \\perp er mindst af alle) Monoton \u00b6 Lad (D,\\sqsubseteq) (D,\\sqsubseteq) v\u00e6re en po og f:D\\to D f:D\\to D v\u00e6re en funktion. f f er monoton hvis: \u200b for alle x,y \\in D x,y \\in D : hvis x \\sqsubseteq y x \\sqsubseteq y s\u00e5 f(x) \\sqsubseteq f(y) f(x) \\sqsubseteq f(y) \u200b (voksende mht. \\sqsubseteq \\sqsubseteq ) Kontinuert Funktion \u00b6 Lad (D, \\sqsubseteq) (D, \\sqsubseteq) v\u00e6re en fpo. f: D \\to D f: D \\to D er monoton. f f er kontinuert hvis for enhver k\u00e6de Y= \\{y_0,y_1,...\\} Y= \\{y_0,y_1,...\\} har vi at \u200b f(\\lim{Y})= \\lim{f(y_0)} f(\\lim{Y})= \\lim{f(y_0)} \u200b ( f f bevarer gr\u00e6nsev\u00e6rdi; f f kan flyttes ind under \\lim \\lim ) Eksistens af Mindste Fikspunkt for Funktion \u00b6 Lad (D, \\sqsubseteq) (D, \\sqsubseteq) v\u00e6re en fpo, og lad f:D \\to D f:D \\to D v\u00e6re kontinuert. S\u00e5 har f f et mindste fikspunkt (mht \\sqsubseteq \\sqsubseteq ) som er givet ved: \\begin{align*} x^* &= \\lim_{i \\geq 0}{\\{f^0(\\perp), f^1(\\perp), f^2(\\perp),\\ ...\\}} \\\\ &= \\lim{\\{f^i(\\perp)\\ |\\ i \\geq 0\\}} \\end{align*} \\begin{align*} x^* &= \\lim_{i \\geq 0}{\\{f^0(\\perp), f^1(\\perp), f^2(\\perp),\\ ...\\}} \\\\ &= \\lim{\\{f^i(\\perp)\\ |\\ i \\geq 0\\}} \\end{align*}","title":"Rekursive Definitioner"},{"location":"4-semester/SS/15-rekursive-definitioner/#rekursive-definitioner","text":"En rekursiv definition giver anledning til en h\u00f8jresidefunktion f f Definitionen har en \"l\u00f8sning\" hvis f f har et fikspunkt $$ L_S={a}\\ L_S\\ {b} \\cup {c} \\cup L_s $$ Lav en h\u00f8jresidefunktion: f(X)=\\{a\\}\\ X\\ \\{b\\} \\cup \\{c\\} \\cup X f(X)=\\{a\\}\\ X\\ \\{b\\} \\cup \\{c\\} \\cup X f f er en funktion over sprog - Eks: \\begin{align*} f(\\{aa,b\\}) \\\\ &= \\{a\\}\\{aa,b\\}\\{b\\} \\cup \\{c\\} \\cup \\{aa,b\\} \\\\ &= \\{aaab,abb\\} \\cup\\{c\\} \\cup \\{aa,b\\} \\\\ &= \\{aaab,abb,c,aa,b\\} \\end{align*} \\begin{align*} f(\\{aa,b\\}) \\\\ &= \\{a\\}\\{aa,b\\}\\{b\\} \\cup \\{c\\} \\cup \\{aa,b\\} \\\\ &= \\{aaab,abb\\} \\cup\\{c\\} \\cup \\{aa,b\\} \\\\ &= \\{aaab,abb,c,aa,b\\} \\end{align*} Vi vil gerne have et L_s L_s s\u00e5 f(L_s)=L_s f(L_s)=L_s \u200b L_s L_s er et fikspunkt for funktionen f f","title":"Rekursive Definitioner"},{"location":"4-semester/SS/15-rekursive-definitioner/#fikspunkt","text":"Lad f:A\\to A f:A\\to A x x er et fikspunkt for f f hvis f(x)=x f(x)=x Jagten p\u00e5 en \"l\u00f8sning\" til en rekursiv definition er faktisk jagten p\u00e5 et fikspunkt for h\u00f8jresidefunktionen. F\u00f8lge af tiln\u00e6rmelser: \\begin{align*} f^0(\\varnothing) &= \\varnothing \\\\ f^1(\\varnothing) &= \\{a\\} \\varnothing \\{b\\} \\cup \\{c\\} \\cup \\varnothing &&= \\{c\\} \\\\ f^2(\\varnothing) &= f(f(\\varnothing)) &&= \\{acb,c\\} \\\\ f^3(\\varnothing) &= f(f^2(\\varnothing)) &&= \\{aacbb, acb,c\\} \\\\ f^4(\\varnothing) &= f(f^3(\\varnothing)) &&= \\{aaacbbb, aacbb, acb,c\\} \\\\ f^n(\\varnothing) &&&= \\{a^icb^i\\ |\\ 0 \\leq i < n\\} \\\\ \\end{align*} \\begin{align*} f^0(\\varnothing) &= \\varnothing \\\\ f^1(\\varnothing) &= \\{a\\} \\varnothing \\{b\\} \\cup \\{c\\} \\cup \\varnothing &&= \\{c\\} \\\\ f^2(\\varnothing) &= f(f(\\varnothing)) &&= \\{acb,c\\} \\\\ f^3(\\varnothing) &= f(f^2(\\varnothing)) &&= \\{aacbb, acb,c\\} \\\\ f^4(\\varnothing) &= f(f^3(\\varnothing)) &&= \\{aaacbbb, aacbb, acb,c\\} \\\\ f^n(\\varnothing) &&&= \\{a^icb^i\\ |\\ 0 \\leq i < n\\} \\\\ \\end{align*} Gr\u00e6nsev\u00e6rdien for f\u00f8lgen: \\{a^icb^i\\ |\\ i \\geq 0\\} \\{a^icb^i\\ |\\ i \\geq 0\\}","title":"Fikspunkt"},{"location":"4-semester/SS/15-rekursive-definitioner/#partiel-ordning","text":"Lad D D v\u00e6re en m\u00e6ngde og \\sqsubseteq \\sqsubseteq v\u00e6re en bin\u00e6r relation over D D , (dvs. relaterer par af elementer). (D,\\sqsubseteq) (D,\\sqsubseteq) er en partiel ordning hvis: \\begin{align*} &(1) &&\\text{For alle }d \\in D : &&d \\sqsubseteq d \\\\ &(2) &&\\text{For alle }d_1d_2 \\in D : &&\\text{hvis } d_1 \\sqsubseteq d_2 \\text{ og } d_2 \\sqsubseteq d_1 \\text{ s\u00e5 } d_1=d_2 \\\\ &(3) &&\\text{For alle } d_1,d_2,d_3 \\in D: &&\\text{hvis } d_1 \\sqsubseteq d_2 \\text{ og } d_2 \\sqsubseteq d_3 \\text{ s\u00e5 } d_1 \\sqsubseteq d_3 \\end{align*} \\begin{align*} &(1) &&\\text{For alle }d \\in D : &&d \\sqsubseteq d \\\\ &(2) &&\\text{For alle }d_1d_2 \\in D : &&\\text{hvis } d_1 \\sqsubseteq d_2 \\text{ og } d_2 \\sqsubseteq d_1 \\text{ s\u00e5 } d_1=d_2 \\\\ &(3) &&\\text{For alle } d_1,d_2,d_3 \\in D: &&\\text{hvis } d_1 \\sqsubseteq d_2 \\text{ og } d_2 \\sqsubseteq d_3 \\text{ s\u00e5 } d_1 \\sqsubseteq d_3 \\end{align*} En partiel ordning er alts\u00e5 bare en ordnet m\u00e6ngde der opf\u00f8rer sig ligesom \\N \\N (de naturlige tal) ordnet under \\leq \\leq (\\N, \\leq) (\\N, \\leq) er en partiel ordning","title":"Partiel Ordning"},{"location":"4-semester/SS/15-rekursive-definitioner/#eksempel","text":"Lad S S v\u00e6re en m\u00e6ngde. (\\mathcal{P}(S),\\subseteq) (\\mathcal{P}(S),\\subseteq) er en partiel ordning. \u200b S=\\{1,2,3\\} S=\\{1,2,3\\}","title":"Eksempel"},{"location":"4-semester/SS/15-rekursive-definitioner/#vre-grnse","text":"Lad (D, \\sqsubseteq) (D, \\sqsubseteq) v\u00e6re en po (partiel ordning), og lad Y\\subseteq D Y\\subseteq D Lad x\\in D x\\in D . x x er en \u00f8vre gr\u00e6nse for y y hvis: \u200b for alle y \\in Y:y \\sqsubseteq x y \\in Y:y \\sqsubseteq x Hvis x er det mindste s\u00e5danne, kalder vi x for den mindste \u00f8vre gr\u00e6nse for y.","title":"\u00d8vre Gr\u00e6nse"},{"location":"4-semester/SS/15-rekursive-definitioner/#eksempel_1","text":"D=\\{1,2,3,4,5,6\\} D=\\{1,2,3,4,5,6\\} \\sqsubseteq \\sqsubseteq defineres ved x \\sqsubseteq y x \\sqsubseteq y hvis x \\leq y x \\leq y \u200b (D, \\sqsubseteq) (D, \\sqsubseteq) er en po. Lad Y=\\{1,2,3\\} Y=\\{1,2,3\\} \u200b 4 er en \u00f8vre gr\u00e6nse for Y Y . \u200b 3 er den mindste \u00f8vre gr\u00e6nse for Y Y .","title":"Eksempel"},{"location":"4-semester/SS/15-rekursive-definitioner/#notation","text":"Hvis x x er den mindste \u00f8vre gr\u00e6nse for y y kalder vi x x for \\lim{Y} \\lim{Y}","title":"Notation"},{"location":"4-semester/SS/15-rekursive-definitioner/#kde","text":"Lad (D, \\sqsubseteq) (D, \\sqsubseteq) v\u00e6re en po. En k\u00e6de i D D er en voksende f\u00f8lge af elementer i D D \u200b Y=\\{y_0,y_1,...\\} Y=\\{y_0,y_1,...\\} hvor \u200b y_0 \\sqsubseteq y_1 \\sqsubseteq y_2 \\ ... y_0 \\sqsubseteq y_1 \\sqsubseteq y_2 \\ ...","title":"K\u00e6de"},{"location":"4-semester/SS/15-rekursive-definitioner/#fuldstndig-partiel-ordning-domne","text":"Lad (D,\\sqsubseteq) (D,\\sqsubseteq) v\u00e6re en po. Den kaldes fuldst\u00e6ndig partial ordning (fpo) hvis det g\u00e6lder at: For enhver k\u00e6de i D,Y D,Y , har vi at \\lim{Y} \\lim{Y} findes (enhver voksende f\u00f8lge har en mindste \u00f8vre gr\u00e6nse) Der findes et mindste element, \\perp \\perp s\u00e5: \\perp \\sqsubseteq x \\perp \\sqsubseteq x for alle x \\in D x \\in D ( \\perp \\perp er mindst af alle)","title":"Fuldst\u00e6ndig Partiel Ordning (Dom\u00e6ne)"},{"location":"4-semester/SS/15-rekursive-definitioner/#monoton","text":"Lad (D,\\sqsubseteq) (D,\\sqsubseteq) v\u00e6re en po og f:D\\to D f:D\\to D v\u00e6re en funktion. f f er monoton hvis: \u200b for alle x,y \\in D x,y \\in D : hvis x \\sqsubseteq y x \\sqsubseteq y s\u00e5 f(x) \\sqsubseteq f(y) f(x) \\sqsubseteq f(y) \u200b (voksende mht. \\sqsubseteq \\sqsubseteq )","title":"Monoton"},{"location":"4-semester/SS/15-rekursive-definitioner/#kontinuert-funktion","text":"Lad (D, \\sqsubseteq) (D, \\sqsubseteq) v\u00e6re en fpo. f: D \\to D f: D \\to D er monoton. f f er kontinuert hvis for enhver k\u00e6de Y= \\{y_0,y_1,...\\} Y= \\{y_0,y_1,...\\} har vi at \u200b f(\\lim{Y})= \\lim{f(y_0)} f(\\lim{Y})= \\lim{f(y_0)} \u200b ( f f bevarer gr\u00e6nsev\u00e6rdi; f f kan flyttes ind under \\lim \\lim )","title":"Kontinuert Funktion"},{"location":"4-semester/SS/15-rekursive-definitioner/#eksistens-af-mindste-fikspunkt-for-funktion","text":"Lad (D, \\sqsubseteq) (D, \\sqsubseteq) v\u00e6re en fpo, og lad f:D \\to D f:D \\to D v\u00e6re kontinuert. S\u00e5 har f f et mindste fikspunkt (mht \\sqsubseteq \\sqsubseteq ) som er givet ved: \\begin{align*} x^* &= \\lim_{i \\geq 0}{\\{f^0(\\perp), f^1(\\perp), f^2(\\perp),\\ ...\\}} \\\\ &= \\lim{\\{f^i(\\perp)\\ |\\ i \\geq 0\\}} \\end{align*} \\begin{align*} x^* &= \\lim_{i \\geq 0}{\\{f^0(\\perp), f^1(\\perp), f^2(\\perp),\\ ...\\}} \\\\ &= \\lim{\\{f^i(\\perp)\\ |\\ i \\geq 0\\}} \\end{align*}","title":"Eksistens af Mindste Fikspunkt for Funktion"},{"location":"5-semester/","text":"Indhold \u00b6 Kurser: CC - Beregnelighed og Kompleksitet MI - Machine Intelligence SOE - Software Engineering Moodle side: https://www.moodle.aau.dk/course/view.php?id=31415","title":"Index"},{"location":"5-semester/#indhold","text":"Kurser: CC - Beregnelighed og Kompleksitet MI - Machine Intelligence SOE - Software Engineering Moodle side: https://www.moodle.aau.dk/course/view.php?id=31415","title":"Indhold"},{"location":"5-semester/CC/","text":"CC - Beregnelighed og Kompleksitet \u00b6 Moodle side: https://www.moodle.aau.dk/course/view.php?id=31439","title":"Course"},{"location":"5-semester/CC/#cc-beregnelighed-og-kompleksitet","text":"Moodle side: https://www.moodle.aau.dk/course/view.php?id=31439","title":"CC - Beregnelighed og Kompleksitet"},{"location":"5-semester/CC/01-turing-maskiner/","text":"\\newcommand{\\TM}{(Q,\\Gamma, \\Sigma, \\delta, q_0, q_{accept}, q_{reject})} \\newcommand{\\TM}{(Q,\\Gamma, \\Sigma, \\delta, q_0, q_{accept}, q_{reject})} Turing Maskiner \u00b6 Turing Maskine \u00b6 Definition En 7-tuppel \\TM \\TM Q Q endelig m\u00e6ngde af tilstande \\Gamma \\Gamma b\u00e5nd-alfabet \\Sigma \\Sigma input alfabet ( \\Sigma \\subseteq \\Gamma \\Sigma \\subseteq \\Gamma ) \\delta \\delta overf\u00f8ringsfuntion q_0 q_0 starttilstand ( q_0 \\in Q q_0 \\in Q ) q_{accept} q_{accept} accepttilstand ( q_{accept} \\in Q q_{accept} \\in Q ) q_{reject} q_{reject} forkasttilstand ( q_{reject} \\in Q q_{reject} \\in Q ) q_{accept} \\neq q_{reject} q_{accept} \\neq q_{reject} $$ \\delta: (Q \\setminus {q_{accept},q_{reject}}) \\times \\Gamma \\to Q \\times \\Gamma \\times {L,R} $$ L L : Venstre R R : H\u00f8jre Example Se eksempel Lektion 1, Video 2 Konfiguration \u00b6 Definition Givet en TM TM M = \\TM M = \\TM er en konfiguration en streng $$ uqav, $$ hvor u u er del af b\u00e5nd til venstre for hoved (u\\in\\Gamma^*) (u\\in\\Gamma^*) , og q\\in Q q\\in Q , og a a indhold af det felt M M ser nu (a \\in \\Gamma) (a \\in \\Gamma) , og v v ikke-tomme del af b\u00e5nd til h\u00f8jre for hoved (v \\in \\Gamma^*) (v \\in \\Gamma^*) Beregning \u00b6 Definition Standsende Beregning \u00b6 En turing maskine kan g\u00e5 i en uendelig l\u00f8kke! Accept af Streng \u00b6 Definition En TM TM M M accepterer input w w , hvis: M M har en accepterende beregning p\u00e5 input w w En TM TM M M afviser input w w hvis: M M har en afvisende beregning p\u00e5 input w w Bem\u00e6rk: At M M ikke accepterer w w er ikke det samme som at M M afviser w w , da en TM TM kan g\u00e5 i en uendelig l\u00f8kke! Turing Genkendelig Sprog \u00b6 Med andre ord: Afg\u00f8rbart \u00b6 Note Inds\u00e6t fra afsnit 4 og frem","title":"Turing Maskiner"},{"location":"5-semester/CC/01-turing-maskiner/#turing-maskiner","text":"","title":"Turing Maskiner"},{"location":"5-semester/CC/01-turing-maskiner/#turing-maskine","text":"Definition En 7-tuppel \\TM \\TM Q Q endelig m\u00e6ngde af tilstande \\Gamma \\Gamma b\u00e5nd-alfabet \\Sigma \\Sigma input alfabet ( \\Sigma \\subseteq \\Gamma \\Sigma \\subseteq \\Gamma ) \\delta \\delta overf\u00f8ringsfuntion q_0 q_0 starttilstand ( q_0 \\in Q q_0 \\in Q ) q_{accept} q_{accept} accepttilstand ( q_{accept} \\in Q q_{accept} \\in Q ) q_{reject} q_{reject} forkasttilstand ( q_{reject} \\in Q q_{reject} \\in Q ) q_{accept} \\neq q_{reject} q_{accept} \\neq q_{reject} $$ \\delta: (Q \\setminus {q_{accept},q_{reject}}) \\times \\Gamma \\to Q \\times \\Gamma \\times {L,R} $$ L L : Venstre R R : H\u00f8jre Example Se eksempel Lektion 1, Video 2","title":"Turing Maskine"},{"location":"5-semester/CC/01-turing-maskiner/#konfiguration","text":"Definition Givet en TM TM M = \\TM M = \\TM er en konfiguration en streng $$ uqav, $$ hvor u u er del af b\u00e5nd til venstre for hoved (u\\in\\Gamma^*) (u\\in\\Gamma^*) , og q\\in Q q\\in Q , og a a indhold af det felt M M ser nu (a \\in \\Gamma) (a \\in \\Gamma) , og v v ikke-tomme del af b\u00e5nd til h\u00f8jre for hoved (v \\in \\Gamma^*) (v \\in \\Gamma^*)","title":"Konfiguration"},{"location":"5-semester/CC/01-turing-maskiner/#beregning","text":"Definition","title":"Beregning"},{"location":"5-semester/CC/01-turing-maskiner/#standsende-beregning","text":"En turing maskine kan g\u00e5 i en uendelig l\u00f8kke!","title":"Standsende Beregning"},{"location":"5-semester/CC/01-turing-maskiner/#accept-af-streng","text":"Definition En TM TM M M accepterer input w w , hvis: M M har en accepterende beregning p\u00e5 input w w En TM TM M M afviser input w w hvis: M M har en afvisende beregning p\u00e5 input w w Bem\u00e6rk: At M M ikke accepterer w w er ikke det samme som at M M afviser w w , da en TM TM kan g\u00e5 i en uendelig l\u00f8kke!","title":"Accept af Streng"},{"location":"5-semester/CC/01-turing-maskiner/#turing-genkendelig-sprog","text":"Med andre ord:","title":"Turing Genkendelig Sprog"},{"location":"5-semester/CC/01-turing-maskiner/#afgrbart","text":"Note Inds\u00e6t fra afsnit 4 og frem","title":"Afg\u00f8rbart"},{"location":"5-semester/CC/02-church-turing-tesen/","text":"\\newcommand{\\TM}{(Q,\\Gamma, \\Sigma, \\delta, q_0, q_{accept}, q_{reject})} \\newcommand{\\TM}{(Q,\\Gamma, \\Sigma, \\delta, q_0, q_{accept}, q_{reject})} Chuch-Turing-Tesen \u00b6 L\u00e6ringsm\u00e5l \u00b6 Forst\u00e5 \u00e6kvivalensen mellem de udvidede turing-maskin-modeller og den oprindelige Turing-maskin-model Forst\u00e5 Church-Turing-tesen og hvad dens foruds\u00e6tninger er. Forst\u00e5 at der er en dualitet mellem beslutningsproblemer og medlemskab af sprog Andvende dualiteten mellem beslutningsproblemer og sprog til at overs\u00e6tte instanser af det ene begreb til instanser af det andet. Forst\u00e5 strengbeskrivelses-notation og dens rolle og kunne anvende den til at overs\u00e6tte beslutningsproblemer til sprog. Flere B\u00e5nd? \u00b6 Overf\u00f8ringsfunktion: $$ \\delta: Q \\times \\Gamma^k \\longrightarrow Q \\times (\\Gamma \\times {L,R})^k $$ Simulering af en k-b\u00e5nds TM med \u00e9t B\u00e5nd \u00b6 Prikkerne markerer l\u00e6sehovedets placering \\Gamma_{\\text{ny}} = \\Gamma \\cup \\{\\dot{a} \\mid a \\in \\Gamma\\} \\cup \\{\\#\\} \\Gamma_{\\text{ny}} = \\Gamma \\cup \\{\\dot{a} \\mid a \\in \\Gamma\\} \\cup \\{\\#\\} Simulering af et skridt: Scan den ikke-blanke del af b\u00e5ndet, og find ud af hvad prikkerne peger p\u00e5 (Husk det vhja. tilstand) Flytte prikker og erstat tegn s\u00e5dan som k-b\u00e5nds-maskinen ville kr\u00e6ve det. Skub indhold mod h\u00f8jre, om n\u00f8dvendigt S\u00e6tning \u00b6 Hvis et sprog L L kan genkendes af en k-b\u00e5nds-TM, kan L L ogs\u00e5 genkendes af en 1-b\u00e5nds-TM. Flere b\u00e5nd betyder ikke noget for regnekraft! Nondeterminisme \u00b6 DFA \\sim NFA\\\\ DPDA \\nsim PDA\\\\ \\scriptstyle (DPDA \\to PDA)\\\\ \\scriptstyle (DPDA \\nleftarrow PDA)\\\\ DFA \\sim NFA\\\\ DPDA \\nsim PDA\\\\ \\scriptstyle (DPDA \\to PDA)\\\\ \\scriptstyle (DPDA \\nleftarrow PDA)\\\\ TM \\sim NTM TM \\sim NTM Overf\u00f8ringsfunktion for NTM: $$ \\delta : Q \\times \\Gamma \\longrightarrow \\mathcal{P}(Q \\times \\Gamma) $$ Eksempel \u00b6 $$ \\delta(q_8, a)={(q_9,b,R),(q_7, a,R)} $$ Accept \u00b6 Definition En NTM M M accepterer en streng, hvis der i beregningstr\u00e6et for strengen forekommer en konfiguration hvis tilstand er q_{accept} q_{accept} Simulering af NTM \u00b6 Sker ved at gennemvandre beregningstr\u00e6et og lede efter en accepterende konfiguration Problem: NTM'er kan g\u00e5 i uendelig l\u00f8kke, s\u00e5 grene i tr\u00e6et KAN v\u00e6re uendeligt lange! L\u00f8sning: Id\u00e9: Brug breddes\u00f8gning P\u00e5 input w w Kopier w w over p\u00e5 b\u00e5nd 1 For k=0,1,2,... k=0,1,2,... Kopier input fra b\u00e5nd 1 til b\u00e5nd 2 Generer n\u00e6ste indexf\u00f8lge af l\u00e6ngde k k Simuler en beregning p\u00e5 b\u00e5nd 2 svarende til indexf\u00f8lgen p\u00e5 b\u00e5nd 3 Hvis beregningen bes\u00f8ger q_{accept} q_{accept} : accepter! S\u00e6tning \u00b6 Hvis L L kan genkendes af en NTM, kan L L genkendes af en TM! Vigtigt af flere grunde: Nondeterminisme giver ikke \u00f8get regnekraft Vi m\u00e5 godt g\u00f8re brug af nondeterminisme! Enumerator \u00b6 TM'er som sproggenkendere: L_{input}(M)=\\{ w \\in \\Sigma^* \\mid M\\ \\text{accepterer}\\ w \\}0 L_{input}(M)=\\{ w \\in \\Sigma^* \\mid M\\ \\text{accepterer}\\ w \\}0 TM'er som sproggeneratorer ( enumerator ): L_{output}(M)=\\{ w \\in \\Gamma^* \\mid M\\ \\text{udskriver}\\ w\\ \\text{p\u00e5 tomt input} \\} L_{output}(M)=\\{ w \\in \\Gamma^* \\mid M\\ \\text{udskriver}\\ w\\ \\text{p\u00e5 tomt input} \\} S\u00e6tning \u00b6 L L er et genkendeligt \u200b \\Updownarrow \\Updownarrow \\exists \\exists enumerator E E s\u00e5 E E outputter L L ( L_{output}(E)=L L_{output}(E)=L ) Se bevis i Lektion 2, afsnit 4 Andre Modeller for Beregnelighed \u00b6 Alle modeller for beregnelighed vi kender i dag, er h\u00f8jest lige s\u00e5 st\u00e6rke som Turing Maskiner. Inklusive programmeringssprog Church-Turing-tesen \u00b6 Enhver model for beregnelighed vil v\u00e6re h\u00f8jest lige s\u00e5 st\u00e6rk som TM-modellen, hvis ellers den er rimelig. Kan ikke bevises (ikke en matematisk s\u00e6tning) Kan potentielt modbevises. Konsekvenser \u00b6 Alle varianter af TM'er er nyttige, n\u00e5r vi skal vise at et sprog er genkendeligt Pseudokode er nok til at beskrive en TM's adf\u00e6rd Beslutningsproblemer og Sprog \u00b6 Beslutningsproblemer: Ja-nej-sp\u00f8rgsm\u00e5l Eksempler: Givet en graf G , er G sammenh\u00e6ngende? Givet et tal n , er n et primtal? Givet en TM M og et input w , vil M acceptere w? De understregede elementer er parametre / input i problemet. Sprog \\leftrightarrow \\leftrightarrow Beslutningsproblem Et hvert beslutningsproblem kan formuleres som et sprog. Og til hvert sprog er der et beslutningsproblem: Givet w w , er w\\in L w\\in L ? Eksempel \u00b6 \"Givet G G , er G G da en sammenh\u00e6ngende graf?\" <G> <G> : Strengrepr\u00e6sentationen af G G Problem svarer til: $$ CONN={ \\ \\mid G\\ \\text{er en sammenh\u00e6ngende graf}} $$ Alts\u00e5 Et beslutningsproblem kan afg\u00f8res med en algoritme hvis, det tilsvarende sprog er afg\u00f8rbart!","title":"Church-Turing-Tesen"},{"location":"5-semester/CC/02-church-turing-tesen/#chuch-turing-tesen","text":"","title":"Chuch-Turing-Tesen"},{"location":"5-semester/CC/02-church-turing-tesen/#lringsmal","text":"Forst\u00e5 \u00e6kvivalensen mellem de udvidede turing-maskin-modeller og den oprindelige Turing-maskin-model Forst\u00e5 Church-Turing-tesen og hvad dens foruds\u00e6tninger er. Forst\u00e5 at der er en dualitet mellem beslutningsproblemer og medlemskab af sprog Andvende dualiteten mellem beslutningsproblemer og sprog til at overs\u00e6tte instanser af det ene begreb til instanser af det andet. Forst\u00e5 strengbeskrivelses-notation og dens rolle og kunne anvende den til at overs\u00e6tte beslutningsproblemer til sprog.","title":"L\u00e6ringsm\u00e5l"},{"location":"5-semester/CC/02-church-turing-tesen/#flere-band","text":"Overf\u00f8ringsfunktion: $$ \\delta: Q \\times \\Gamma^k \\longrightarrow Q \\times (\\Gamma \\times {L,R})^k $$","title":"Flere B\u00e5nd?"},{"location":"5-semester/CC/02-church-turing-tesen/#simulering-af-en-k-bands-tm-med-et-band","text":"Prikkerne markerer l\u00e6sehovedets placering \\Gamma_{\\text{ny}} = \\Gamma \\cup \\{\\dot{a} \\mid a \\in \\Gamma\\} \\cup \\{\\#\\} \\Gamma_{\\text{ny}} = \\Gamma \\cup \\{\\dot{a} \\mid a \\in \\Gamma\\} \\cup \\{\\#\\} Simulering af et skridt: Scan den ikke-blanke del af b\u00e5ndet, og find ud af hvad prikkerne peger p\u00e5 (Husk det vhja. tilstand) Flytte prikker og erstat tegn s\u00e5dan som k-b\u00e5nds-maskinen ville kr\u00e6ve det. Skub indhold mod h\u00f8jre, om n\u00f8dvendigt","title":"Simulering af en k-b\u00e5nds TM med \u00e9t B\u00e5nd"},{"location":"5-semester/CC/02-church-turing-tesen/#stning","text":"Hvis et sprog L L kan genkendes af en k-b\u00e5nds-TM, kan L L ogs\u00e5 genkendes af en 1-b\u00e5nds-TM. Flere b\u00e5nd betyder ikke noget for regnekraft!","title":"S\u00e6tning"},{"location":"5-semester/CC/02-church-turing-tesen/#nondeterminisme","text":"DFA \\sim NFA\\\\ DPDA \\nsim PDA\\\\ \\scriptstyle (DPDA \\to PDA)\\\\ \\scriptstyle (DPDA \\nleftarrow PDA)\\\\ DFA \\sim NFA\\\\ DPDA \\nsim PDA\\\\ \\scriptstyle (DPDA \\to PDA)\\\\ \\scriptstyle (DPDA \\nleftarrow PDA)\\\\ TM \\sim NTM TM \\sim NTM Overf\u00f8ringsfunktion for NTM: $$ \\delta : Q \\times \\Gamma \\longrightarrow \\mathcal{P}(Q \\times \\Gamma) $$","title":"Nondeterminisme"},{"location":"5-semester/CC/02-church-turing-tesen/#eksempel","text":"$$ \\delta(q_8, a)={(q_9,b,R),(q_7, a,R)} $$","title":"Eksempel"},{"location":"5-semester/CC/02-church-turing-tesen/#accept","text":"Definition En NTM M M accepterer en streng, hvis der i beregningstr\u00e6et for strengen forekommer en konfiguration hvis tilstand er q_{accept} q_{accept}","title":"Accept"},{"location":"5-semester/CC/02-church-turing-tesen/#simulering-af-ntm","text":"Sker ved at gennemvandre beregningstr\u00e6et og lede efter en accepterende konfiguration Problem: NTM'er kan g\u00e5 i uendelig l\u00f8kke, s\u00e5 grene i tr\u00e6et KAN v\u00e6re uendeligt lange! L\u00f8sning: Id\u00e9: Brug breddes\u00f8gning P\u00e5 input w w Kopier w w over p\u00e5 b\u00e5nd 1 For k=0,1,2,... k=0,1,2,... Kopier input fra b\u00e5nd 1 til b\u00e5nd 2 Generer n\u00e6ste indexf\u00f8lge af l\u00e6ngde k k Simuler en beregning p\u00e5 b\u00e5nd 2 svarende til indexf\u00f8lgen p\u00e5 b\u00e5nd 3 Hvis beregningen bes\u00f8ger q_{accept} q_{accept} : accepter!","title":"Simulering af NTM"},{"location":"5-semester/CC/02-church-turing-tesen/#stning_1","text":"Hvis L L kan genkendes af en NTM, kan L L genkendes af en TM! Vigtigt af flere grunde: Nondeterminisme giver ikke \u00f8get regnekraft Vi m\u00e5 godt g\u00f8re brug af nondeterminisme!","title":"S\u00e6tning"},{"location":"5-semester/CC/02-church-turing-tesen/#enumerator","text":"TM'er som sproggenkendere: L_{input}(M)=\\{ w \\in \\Sigma^* \\mid M\\ \\text{accepterer}\\ w \\}0 L_{input}(M)=\\{ w \\in \\Sigma^* \\mid M\\ \\text{accepterer}\\ w \\}0 TM'er som sproggeneratorer ( enumerator ): L_{output}(M)=\\{ w \\in \\Gamma^* \\mid M\\ \\text{udskriver}\\ w\\ \\text{p\u00e5 tomt input} \\} L_{output}(M)=\\{ w \\in \\Gamma^* \\mid M\\ \\text{udskriver}\\ w\\ \\text{p\u00e5 tomt input} \\}","title":"Enumerator"},{"location":"5-semester/CC/02-church-turing-tesen/#stning_2","text":"L L er et genkendeligt \u200b \\Updownarrow \\Updownarrow \\exists \\exists enumerator E E s\u00e5 E E outputter L L ( L_{output}(E)=L L_{output}(E)=L ) Se bevis i Lektion 2, afsnit 4","title":"S\u00e6tning"},{"location":"5-semester/CC/02-church-turing-tesen/#andre-modeller-for-beregnelighed","text":"Alle modeller for beregnelighed vi kender i dag, er h\u00f8jest lige s\u00e5 st\u00e6rke som Turing Maskiner. Inklusive programmeringssprog","title":"Andre Modeller for Beregnelighed"},{"location":"5-semester/CC/02-church-turing-tesen/#church-turing-tesen","text":"Enhver model for beregnelighed vil v\u00e6re h\u00f8jest lige s\u00e5 st\u00e6rk som TM-modellen, hvis ellers den er rimelig. Kan ikke bevises (ikke en matematisk s\u00e6tning) Kan potentielt modbevises.","title":"Church-Turing-tesen"},{"location":"5-semester/CC/02-church-turing-tesen/#konsekvenser","text":"Alle varianter af TM'er er nyttige, n\u00e5r vi skal vise at et sprog er genkendeligt Pseudokode er nok til at beskrive en TM's adf\u00e6rd","title":"Konsekvenser"},{"location":"5-semester/CC/02-church-turing-tesen/#beslutningsproblemer-og-sprog","text":"Beslutningsproblemer: Ja-nej-sp\u00f8rgsm\u00e5l Eksempler: Givet en graf G , er G sammenh\u00e6ngende? Givet et tal n , er n et primtal? Givet en TM M og et input w , vil M acceptere w? De understregede elementer er parametre / input i problemet. Sprog \\leftrightarrow \\leftrightarrow Beslutningsproblem Et hvert beslutningsproblem kan formuleres som et sprog. Og til hvert sprog er der et beslutningsproblem: Givet w w , er w\\in L w\\in L ?","title":"Beslutningsproblemer og Sprog"},{"location":"5-semester/CC/02-church-turing-tesen/#eksempel_1","text":"\"Givet G G , er G G da en sammenh\u00e6ngende graf?\" <G> <G> : Strengrepr\u00e6sentationen af G G Problem svarer til: $$ CONN={ \\ \\mid G\\ \\text{er en sammenh\u00e6ngende graf}} $$ Alts\u00e5 Et beslutningsproblem kan afg\u00f8res med en algoritme hvis, det tilsvarende sprog er afg\u00f8rbart!","title":"Eksempel"},{"location":"5-semester/CC/03-acceptproblemet-for-tm/","text":"Acceptproblemet for Turing-maskiner \u00b6 Theorem 4.11 A_{TM} A_{TM} er uafg\u00f8rbart Theorem 4.2.2 Et sprog L L er afg\u00f8rbart hvis og kun hvis L L er genkendeligt og \\overline L \\overline L er genkendeligt. \u200b Alts\u00e5: Et sprog er afg\u00f8rbart pr\u00e6cis n\u00e5r b\u00e5de det og dets komplement er genkendeligt. Corollary 4.23 \\overline {A_{TM}} \\overline {A_{TM}} er ikke genkendeligt","title":"Acceptproblemet for Turing-maskiner"},{"location":"5-semester/CC/03-acceptproblemet-for-tm/#acceptproblemet-for-turing-maskiner","text":"Theorem 4.11 A_{TM} A_{TM} er uafg\u00f8rbart Theorem 4.2.2 Et sprog L L er afg\u00f8rbart hvis og kun hvis L L er genkendeligt og \\overline L \\overline L er genkendeligt. \u200b Alts\u00e5: Et sprog er afg\u00f8rbart pr\u00e6cis n\u00e5r b\u00e5de det og dets komplement er genkendeligt. Corollary 4.23 \\overline {A_{TM}} \\overline {A_{TM}} er ikke genkendeligt","title":"Acceptproblemet for Turing-maskiner"},{"location":"5-semester/CC/04-more-problems-than-algorithms/","text":"Er der Flere Problemer end Algoritmer? Og er Problemer Besl\u00e6gtede? \u00b6 L\u00e6ringsm\u00e5l Forst\u00e5 begreberne \"samme st\u00f8rrelse\" for m\u00e6ngder og de begreber, der er n\u00f8dvendige for at definere det (herunder bjektioner) Forst\u00e5 og kunne anvende teknikken til diagonalisering Forst\u00e5 hvordan diagonaliseringsteknikken og beviset for uafg\u00f8rbarhed er relateret Forst\u00e5 s\u00e6tningen om uafg\u00f8rbarhed for HALT_{TM} HALT_{TM} og dens bevis ved reduktion M\u00e6ngdens St\u00f8rrelse \u00b6 Enentydig (1-1) Lad A,B A,B v\u00e6re m\u00e6ngder og f:A\\to B f:A\\to B f f er 1-1 eller enentydig hvis: $$ \\forall a_1,a_2\\in A: f(a_1)\\neq f(a_2) $$ hvis a_1 \\neq a_2 a_1 \\neq a_2 (Alts\u00e5 hvis 2 forskellige v\u00e6rdier, giver 2 forskellige outputs i funktionen) P\u00e5 Lad A,B A,B v\u00e6re m\u00e6ngder og f:A\\to B f:A\\to B f f er p\u00e5 hvis: $$ \\forall b \\in B : \\exists a \\in A: f(a) = b $$ Bijektion Lad A,B A,B v\u00e6re m\u00e6ngder og f:A\\to B f:A\\to B f f er en bijektion hvis f f er 1-1 og p\u00e5. Samme St\u00f8rrelse Lad A,B A,B v\u00e6re m\u00e6ngder. Vi siger at A A og B B har samme st\u00f8rrelse ( kardinalitet ) hvis: $$ \\exists f: A\\to B, \\text{s\u00e5 } f \\text{ er en bijektion} $$ T\u00e6llelig M\u00e6ngde Lad A A v\u00e6re en m\u00e6ngde. A A er t\u00e6llelig hvis A A har samme st\u00f8rrelse som de naturlige tal ( \\N \\N ). T\u00e6llelige M\u00e6ngder \u00b6 Positive Rationelle Tal \u00b6 \\Q_+ \\Q_+ er en t\u00e6llelig m\u00e6ngde! (Positive rationelle tal) Alle Strenge over Sigma \u00b6 \\Sigma^* \\Sigma^* er en t\u00e6llelig m\u00e6ngde! Konsekvens : Der er kun t\u00e6lleligt uendeligt mange TM'er Da hver TM M M beskrives med en strengkonstruktion \\langle M\\rangle \\langle M\\rangle Overt\u00e6llelige M\u00e6ngder \u00b6 Lad A A v\u00e6re en uendelig m\u00e6ngde. Hvis A A ikke er t\u00e6llelig, kalder vi A A for overt\u00e6llelig. Positive Reelle Tal \u00b6 \\R_+ \\R_+ er overt\u00e6llelig. Bevis: Numberphile - Diagonalisering . Potensm\u00e6ngden af alle Strenge \u00b6 \\mathcal{P}(\\Sigma^*) \\mathcal{P}(\\Sigma^*) er en overt\u00e6llelig m\u00e6ngde. Bevis ved diagonalisering: Observation: Ethvert sprog svarer til en uendelig 0-1-f\u00f8lge (og omvendt) Uafg\u00f8rbare Sprog \u00b6 HALT \u00b6 Theorem 5.1 HALT_{TM} HALT_{TM} er uafg\u00f8rbart. Bevis \u00b6 Lad os antage af TM R R afg\u00f8r HALT_{TM} HALT_{TM} , s\u00e5 kan vi konstruerer TM S S til at afg\u00f8re A_{TM} A_{TM} : S= S= \"P\u00e5 input \\langle M, w\\rangle \\langle M, w\\rangle \": K\u00f8r TM R R p\u00e5 input \\langle M, w\\rangle \\langle M, w\\rangle Hvis R R afviser, afvis Hvis R R accepterer, simuler M M p\u00e5 w w indtil den halter Hvis M M har accepteret, accepter , ellers afvis Ergo: Hvis R R afg\u00f8r HALT_{TM} HALT_{TM} , s\u00e5 afg\u00f8r S S , A_{TM} A_{TM} . Men da A_{TM} A_{TM} er uafg\u00f8rbart, da m\u00e5 HALT_{TM} HALT_{TM} ogs\u00e5 v\u00e6re uafg\u00f8rbart. \\square \\square","title":"Er der Flere Problemer end Algoritmer? Og er Problemer Besl\u00e6gtede?"},{"location":"5-semester/CC/04-more-problems-than-algorithms/#er-der-flere-problemer-end-algoritmer-og-er-problemer-beslgtede","text":"L\u00e6ringsm\u00e5l Forst\u00e5 begreberne \"samme st\u00f8rrelse\" for m\u00e6ngder og de begreber, der er n\u00f8dvendige for at definere det (herunder bjektioner) Forst\u00e5 og kunne anvende teknikken til diagonalisering Forst\u00e5 hvordan diagonaliseringsteknikken og beviset for uafg\u00f8rbarhed er relateret Forst\u00e5 s\u00e6tningen om uafg\u00f8rbarhed for HALT_{TM} HALT_{TM} og dens bevis ved reduktion","title":"Er der Flere Problemer end Algoritmer? Og er Problemer Besl\u00e6gtede?"},{"location":"5-semester/CC/04-more-problems-than-algorithms/#mngdens-strrelse","text":"Enentydig (1-1) Lad A,B A,B v\u00e6re m\u00e6ngder og f:A\\to B f:A\\to B f f er 1-1 eller enentydig hvis: $$ \\forall a_1,a_2\\in A: f(a_1)\\neq f(a_2) $$ hvis a_1 \\neq a_2 a_1 \\neq a_2 (Alts\u00e5 hvis 2 forskellige v\u00e6rdier, giver 2 forskellige outputs i funktionen) P\u00e5 Lad A,B A,B v\u00e6re m\u00e6ngder og f:A\\to B f:A\\to B f f er p\u00e5 hvis: $$ \\forall b \\in B : \\exists a \\in A: f(a) = b $$ Bijektion Lad A,B A,B v\u00e6re m\u00e6ngder og f:A\\to B f:A\\to B f f er en bijektion hvis f f er 1-1 og p\u00e5. Samme St\u00f8rrelse Lad A,B A,B v\u00e6re m\u00e6ngder. Vi siger at A A og B B har samme st\u00f8rrelse ( kardinalitet ) hvis: $$ \\exists f: A\\to B, \\text{s\u00e5 } f \\text{ er en bijektion} $$ T\u00e6llelig M\u00e6ngde Lad A A v\u00e6re en m\u00e6ngde. A A er t\u00e6llelig hvis A A har samme st\u00f8rrelse som de naturlige tal ( \\N \\N ).","title":"M\u00e6ngdens St\u00f8rrelse"},{"location":"5-semester/CC/04-more-problems-than-algorithms/#tllelige-mngder","text":"","title":"T\u00e6llelige M\u00e6ngder"},{"location":"5-semester/CC/04-more-problems-than-algorithms/#positive-rationelle-tal","text":"\\Q_+ \\Q_+ er en t\u00e6llelig m\u00e6ngde! (Positive rationelle tal)","title":"Positive Rationelle Tal"},{"location":"5-semester/CC/04-more-problems-than-algorithms/#alle-strenge-over-sigma","text":"\\Sigma^* \\Sigma^* er en t\u00e6llelig m\u00e6ngde! Konsekvens : Der er kun t\u00e6lleligt uendeligt mange TM'er Da hver TM M M beskrives med en strengkonstruktion \\langle M\\rangle \\langle M\\rangle","title":"Alle Strenge over Sigma"},{"location":"5-semester/CC/04-more-problems-than-algorithms/#overtllelige-mngder","text":"Lad A A v\u00e6re en uendelig m\u00e6ngde. Hvis A A ikke er t\u00e6llelig, kalder vi A A for overt\u00e6llelig.","title":"Overt\u00e6llelige M\u00e6ngder"},{"location":"5-semester/CC/04-more-problems-than-algorithms/#positive-reelle-tal","text":"\\R_+ \\R_+ er overt\u00e6llelig. Bevis: Numberphile - Diagonalisering .","title":"Positive Reelle Tal"},{"location":"5-semester/CC/04-more-problems-than-algorithms/#potensmngden-af-alle-strenge","text":"\\mathcal{P}(\\Sigma^*) \\mathcal{P}(\\Sigma^*) er en overt\u00e6llelig m\u00e6ngde. Bevis ved diagonalisering: Observation: Ethvert sprog svarer til en uendelig 0-1-f\u00f8lge (og omvendt)","title":"Potensm\u00e6ngden af alle Strenge"},{"location":"5-semester/CC/04-more-problems-than-algorithms/#uafgrbare-sprog","text":"","title":"Uafg\u00f8rbare Sprog"},{"location":"5-semester/CC/04-more-problems-than-algorithms/#halt","text":"Theorem 5.1 HALT_{TM} HALT_{TM} er uafg\u00f8rbart.","title":"HALT"},{"location":"5-semester/CC/04-more-problems-than-algorithms/#bevis","text":"Lad os antage af TM R R afg\u00f8r HALT_{TM} HALT_{TM} , s\u00e5 kan vi konstruerer TM S S til at afg\u00f8re A_{TM} A_{TM} : S= S= \"P\u00e5 input \\langle M, w\\rangle \\langle M, w\\rangle \": K\u00f8r TM R R p\u00e5 input \\langle M, w\\rangle \\langle M, w\\rangle Hvis R R afviser, afvis Hvis R R accepterer, simuler M M p\u00e5 w w indtil den halter Hvis M M har accepteret, accepter , ellers afvis Ergo: Hvis R R afg\u00f8r HALT_{TM} HALT_{TM} , s\u00e5 afg\u00f8r S S , A_{TM} A_{TM} . Men da A_{TM} A_{TM} er uafg\u00f8rbart, da m\u00e5 HALT_{TM} HALT_{TM} ogs\u00e5 v\u00e6re uafg\u00f8rbart. \\square \\square","title":"Bevis"},{"location":"5-semester/CC/05-beregningshistorier/","text":"Uafg\u00f8rbare Problemer om Beregningshistorier \u00b6 Konfigurationer \u00b6 En TM M M accepter input w w hvis der findes en f\u00f8lge af konfigurationer $$ C_1,\\dots,C_k $$ s\u00e5 C_1=q_0w C_1=q_0w ( q_0 q_0 er M M 's starttilstand') C_i\\to C_{i+1} C_i\\to C_{i+1} (for 1 \\leq i \\leq k-1 1 \\leq i \\leq k-1 ) C_k C_k er en accepterende konfiguration (indeholder q_{accept} q_{accept} ) Line\u00e6rt Begr\u00e6nsede Automater (LBA) \u00b6 En Line\u00e6rt begr\u00e6nset automat (LBA) er en TM der aldrig anvender flere b\u00e5ndceller end dem input st\u00e5r p\u00e5. Klassen af sprog som kan genkendes med LBA'er er pr\u00e6cis de kontekst-sensitive sprog Defineret med CSG'er, hvor regler er p\u00e5 formen uAv\\to u\\alpha v uAv\\to u\\alpha v Klassen af sprog som kan genkendes af LBA'er er pr\u00e6cis de sprog som kan genkendes af algoritmer med line\u00e6r pladskompleksitet. Acceptproblemet for LBA'er \u00b6 \"Givet en LBA M M og input w w , vil M M acceptere w w ?\" A_{LBA}=\\left\\{\\langle M,w\\rangle \\mid M \\text{ er en LBA, der accepterer } w \\right\\} A_{LBA}=\\left\\{\\langle M,w\\rangle \\mid M \\text{ er en LBA, der accepterer } w \\right\\} S\u00e6tning A_{LBA} A_{LBA} er afg\u00f8rbart! Bevis \u00b6 Tomhedsproblemet for LBA'er \u00b6 \"Givet en LBA M M , g\u00e6lder det at L(M)=\\empty L(M)=\\empty \" E_{LBA}=\\{\\langle M\\rangle\\mid M \\text{ er en LBA, } L(M)=\\empty\\} E_{LBA}=\\{\\langle M\\rangle\\mid M \\text{ er en LBA, } L(M)=\\empty\\} S\u00e6tning E_{LBA} E_{LBA} er uafg\u00f8rbart Totalitetsproblemet for CFG'er \u00b6 \"Givet en CFG G G , g\u00e6lder det at L(G)=\\Sigma^* L(G)=\\Sigma^* ?\" ALL_{CFG}=\\{ \\langle G\\rangle \\mid G \\text{ er en CFG, } L(G)=\\Sigma^*\\} ALL_{CFG}=\\{ \\langle G\\rangle \\mid G \\text{ er en CFG, } L(G)=\\Sigma^*\\} S\u00e6tning ALL_{CFG} ALL_{CFG} er uafg\u00f8rbart!","title":"Uafg\u00f8rbare Problemer om Beregningshistorier"},{"location":"5-semester/CC/05-beregningshistorier/#uafgrbare-problemer-om-beregningshistorier","text":"","title":"Uafg\u00f8rbare Problemer om Beregningshistorier"},{"location":"5-semester/CC/05-beregningshistorier/#konfigurationer","text":"En TM M M accepter input w w hvis der findes en f\u00f8lge af konfigurationer $$ C_1,\\dots,C_k $$ s\u00e5 C_1=q_0w C_1=q_0w ( q_0 q_0 er M M 's starttilstand') C_i\\to C_{i+1} C_i\\to C_{i+1} (for 1 \\leq i \\leq k-1 1 \\leq i \\leq k-1 ) C_k C_k er en accepterende konfiguration (indeholder q_{accept} q_{accept} )","title":"Konfigurationer"},{"location":"5-semester/CC/05-beregningshistorier/#linert-begrnsede-automater-lba","text":"En Line\u00e6rt begr\u00e6nset automat (LBA) er en TM der aldrig anvender flere b\u00e5ndceller end dem input st\u00e5r p\u00e5. Klassen af sprog som kan genkendes med LBA'er er pr\u00e6cis de kontekst-sensitive sprog Defineret med CSG'er, hvor regler er p\u00e5 formen uAv\\to u\\alpha v uAv\\to u\\alpha v Klassen af sprog som kan genkendes af LBA'er er pr\u00e6cis de sprog som kan genkendes af algoritmer med line\u00e6r pladskompleksitet.","title":"Line\u00e6rt Begr\u00e6nsede Automater (LBA)"},{"location":"5-semester/CC/05-beregningshistorier/#acceptproblemet-for-lbaer","text":"\"Givet en LBA M M og input w w , vil M M acceptere w w ?\" A_{LBA}=\\left\\{\\langle M,w\\rangle \\mid M \\text{ er en LBA, der accepterer } w \\right\\} A_{LBA}=\\left\\{\\langle M,w\\rangle \\mid M \\text{ er en LBA, der accepterer } w \\right\\} S\u00e6tning A_{LBA} A_{LBA} er afg\u00f8rbart!","title":"Acceptproblemet for LBA'er"},{"location":"5-semester/CC/05-beregningshistorier/#bevis","text":"","title":"Bevis"},{"location":"5-semester/CC/05-beregningshistorier/#tomhedsproblemet-for-lbaer","text":"\"Givet en LBA M M , g\u00e6lder det at L(M)=\\empty L(M)=\\empty \" E_{LBA}=\\{\\langle M\\rangle\\mid M \\text{ er en LBA, } L(M)=\\empty\\} E_{LBA}=\\{\\langle M\\rangle\\mid M \\text{ er en LBA, } L(M)=\\empty\\} S\u00e6tning E_{LBA} E_{LBA} er uafg\u00f8rbart","title":"Tomhedsproblemet for LBA'er"},{"location":"5-semester/CC/05-beregningshistorier/#totalitetsproblemet-for-cfger","text":"\"Givet en CFG G G , g\u00e6lder det at L(G)=\\Sigma^* L(G)=\\Sigma^* ?\" ALL_{CFG}=\\{ \\langle G\\rangle \\mid G \\text{ er en CFG, } L(G)=\\Sigma^*\\} ALL_{CFG}=\\{ \\langle G\\rangle \\mid G \\text{ er en CFG, } L(G)=\\Sigma^*\\} S\u00e6tning ALL_{CFG} ALL_{CFG} er uafg\u00f8rbart!","title":"Totalitetsproblemet for CFG'er"},{"location":"5-semester/CC/06-posts-korrespondanceproblem/","text":"Posts Korrospondanceproblem ; Teorien for Reduktioner \u00b6 Posts Korrespondanceproblem \u00b6 En samling af Post-brikker over alfabetet \\Sigma \\Sigma er en endelig m\u00e6ngde af par: P=\\left\\{ \\left[\\frac{t_1}{b_1}\\right],\\dots,\\left[\\frac{t_k}{b_k}\\right]\\right\\} P=\\left\\{ \\left[\\frac{t_1}{b_1}\\right],\\dots,\\left[\\frac{t_k}{b_k}\\right]\\right\\} hvor \\quad t_i,b_i\\in \\Sigma^*, \\quad (1\\leq i \\leq k) \\quad t_i,b_i\\in \\Sigma^*, \\quad (1\\leq i \\leq k) En match for P P er en f\u00f8lge af indekser i_1,\\dots,i_n i_1,\\dots,i_n s\u00e5 t_{i_1},\\dots,t_{i_n} = b_{i_1},\\dots,b_{i_n} t_{i_1},\\dots,t_{i_n} = b_{i_1},\\dots,b_{i_n} \u200b (Samme indeks m\u00e5 bruges 0 eller flere gange) Eksempel : Generalt: Hvis P P har \u00e9n match, s\u00e5 har P P uendeligt mange matches. Da brikker m\u00e5 bruges 0 eller flere gange er der et uendeligt stort s\u00f8gerum. Problemet \u00b6 Givet en samling Post-brikker P P , har P P en match? PCP= \\biggl\\{ <P>\\ \\mathrel{\\Big|} \\begin{array}{} P \\text{ er en samling Post-brikker. }\\\\ P \\text{ har en match} \\end{array} \\biggr\\} PCP= \\biggl\\{ <P>\\ \\mathrel{\\Big|} \\begin{array}{} P \\text{ er en samling Post-brikker. }\\\\ P \\text{ har en match} \\end{array} \\biggr\\} PCP PCP er uafg\u00f8rbart. Bevis \u00b6 Reduktion. To skridt: A_{TM} \\leadsto MPCP (\\textcolor{red}{**}) A_{TM} \\leadsto MPCP (\\textcolor{red}{**}) MPCP= \\biggl\\{ <P>\\ \\mathrel{\\Big|} \\begin{array}{} P \\text{ er en samling Post-brikker. }\\\\ P \\text{ har en match, der starter med brik 1} \\end{array} \\biggr\\} MPCP= \\biggl\\{ <P>\\ \\mathrel{\\Big|} \\begin{array}{} P \\text{ er en samling Post-brikker. }\\\\ P \\text{ har en match, der starter med brik 1} \\end{array} \\biggr\\} MPCP \\leadsto PCP (\\textcolor{red}{*}) MPCP \\leadsto PCP (\\textcolor{red}{*}) Bevis del 1 Bevis del 2 Reduktion \u00b6 Overs\u00e6ttelse mellem sprog Skal v\u00e6re trofast Skal v\u00e6re beregnbar Beregnbar Lad f: \\Sigma^* \\to \\Sigma^* f: \\Sigma^* \\to \\Sigma^* f f er en beregnbar hvis der findes en TM M_f M_f s\u00e5 at n\u00e5r f(x)=y f(x)=y , s\u00e5 vil M_f M_f med input x standse med y p\u00e5 b\u00e5ndet. Trofast Lad A, B A, B v\u00e6re sprog over alfabet \\Sigma \\Sigma \u200b f: \\Sigma^* \\to \\Sigma^* f: \\Sigma^* \\to \\Sigma^* er trofast hvis for alle x\\in\\Sigma^* x\\in\\Sigma^* \u200b x \\in A\\Longleftrightarrow f(x)\\in B x \\in A\\Longleftrightarrow f(x)\\in B Reduktion Lad A, B A, B v\u00e6re sprog over alfabet \\Sigma \\Sigma Vi siger at A A reducerer til B B , A\\leq_m B A\\leq_m B , hvis der findes en f: \\Sigma^* \\to \\Sigma^* f: \\Sigma^* \\to \\Sigma^* som er beregnbar og trofast mht. A A og B B T\u00e6nk p\u00e5 \\leq_m \\leq_m som \" ikke sv\u00e6rerer end \" Afg\u00f8rbarhed \u00b6 S\u00e6tning Hvis A \\leq_m B A \\leq_m B og B B er afg\u00f8rbart, s\u00e5 er A A ogs\u00e5 afg\u00f8rbart. Korollar Hvis A \\leq_m B A \\leq_m B og A A er uafg\u00f8rbar, s\u00e5 er B B ogs\u00e5 uafg\u00f8rbar Genkendelighed \u00b6 S\u00e6tning Hvis A \\leq_m B A \\leq_m B og B B er genkendeligt, s\u00e5 er A A ogs\u00e5 genkendeligt. Korollar Hvis A\\leq_m B A\\leq_m B og A A er u-genkendeligt, ss\u00e5 er B B heller ikke genkendeligt Komplement \u00b6 S\u00e6tning Hvis A\\leq_m B A\\leq_m B , s\u00e5 $ \\overline A \\leq_m \\overline B$","title":"Posts Korrospondanceproblem ; Teorien for Reduktioner"},{"location":"5-semester/CC/06-posts-korrespondanceproblem/#posts-korrospondanceproblem-teorien-for-reduktioner","text":"","title":"Posts Korrospondanceproblem ; Teorien for Reduktioner"},{"location":"5-semester/CC/06-posts-korrespondanceproblem/#posts-korrespondanceproblem","text":"En samling af Post-brikker over alfabetet \\Sigma \\Sigma er en endelig m\u00e6ngde af par: P=\\left\\{ \\left[\\frac{t_1}{b_1}\\right],\\dots,\\left[\\frac{t_k}{b_k}\\right]\\right\\} P=\\left\\{ \\left[\\frac{t_1}{b_1}\\right],\\dots,\\left[\\frac{t_k}{b_k}\\right]\\right\\} hvor \\quad t_i,b_i\\in \\Sigma^*, \\quad (1\\leq i \\leq k) \\quad t_i,b_i\\in \\Sigma^*, \\quad (1\\leq i \\leq k) En match for P P er en f\u00f8lge af indekser i_1,\\dots,i_n i_1,\\dots,i_n s\u00e5 t_{i_1},\\dots,t_{i_n} = b_{i_1},\\dots,b_{i_n} t_{i_1},\\dots,t_{i_n} = b_{i_1},\\dots,b_{i_n} \u200b (Samme indeks m\u00e5 bruges 0 eller flere gange) Eksempel : Generalt: Hvis P P har \u00e9n match, s\u00e5 har P P uendeligt mange matches. Da brikker m\u00e5 bruges 0 eller flere gange er der et uendeligt stort s\u00f8gerum.","title":"Posts Korrespondanceproblem"},{"location":"5-semester/CC/06-posts-korrespondanceproblem/#problemet","text":"Givet en samling Post-brikker P P , har P P en match? PCP= \\biggl\\{ <P>\\ \\mathrel{\\Big|} \\begin{array}{} P \\text{ er en samling Post-brikker. }\\\\ P \\text{ har en match} \\end{array} \\biggr\\} PCP= \\biggl\\{ <P>\\ \\mathrel{\\Big|} \\begin{array}{} P \\text{ er en samling Post-brikker. }\\\\ P \\text{ har en match} \\end{array} \\biggr\\} PCP PCP er uafg\u00f8rbart.","title":"Problemet"},{"location":"5-semester/CC/06-posts-korrespondanceproblem/#bevis","text":"Reduktion. To skridt: A_{TM} \\leadsto MPCP (\\textcolor{red}{**}) A_{TM} \\leadsto MPCP (\\textcolor{red}{**}) MPCP= \\biggl\\{ <P>\\ \\mathrel{\\Big|} \\begin{array}{} P \\text{ er en samling Post-brikker. }\\\\ P \\text{ har en match, der starter med brik 1} \\end{array} \\biggr\\} MPCP= \\biggl\\{ <P>\\ \\mathrel{\\Big|} \\begin{array}{} P \\text{ er en samling Post-brikker. }\\\\ P \\text{ har en match, der starter med brik 1} \\end{array} \\biggr\\} MPCP \\leadsto PCP (\\textcolor{red}{*}) MPCP \\leadsto PCP (\\textcolor{red}{*}) Bevis del 1 Bevis del 2","title":"Bevis"},{"location":"5-semester/CC/06-posts-korrespondanceproblem/#reduktion","text":"Overs\u00e6ttelse mellem sprog Skal v\u00e6re trofast Skal v\u00e6re beregnbar Beregnbar Lad f: \\Sigma^* \\to \\Sigma^* f: \\Sigma^* \\to \\Sigma^* f f er en beregnbar hvis der findes en TM M_f M_f s\u00e5 at n\u00e5r f(x)=y f(x)=y , s\u00e5 vil M_f M_f med input x standse med y p\u00e5 b\u00e5ndet. Trofast Lad A, B A, B v\u00e6re sprog over alfabet \\Sigma \\Sigma \u200b f: \\Sigma^* \\to \\Sigma^* f: \\Sigma^* \\to \\Sigma^* er trofast hvis for alle x\\in\\Sigma^* x\\in\\Sigma^* \u200b x \\in A\\Longleftrightarrow f(x)\\in B x \\in A\\Longleftrightarrow f(x)\\in B Reduktion Lad A, B A, B v\u00e6re sprog over alfabet \\Sigma \\Sigma Vi siger at A A reducerer til B B , A\\leq_m B A\\leq_m B , hvis der findes en f: \\Sigma^* \\to \\Sigma^* f: \\Sigma^* \\to \\Sigma^* som er beregnbar og trofast mht. A A og B B T\u00e6nk p\u00e5 \\leq_m \\leq_m som \" ikke sv\u00e6rerer end \"","title":"Reduktion"},{"location":"5-semester/CC/06-posts-korrespondanceproblem/#afgrbarhed","text":"S\u00e6tning Hvis A \\leq_m B A \\leq_m B og B B er afg\u00f8rbart, s\u00e5 er A A ogs\u00e5 afg\u00f8rbart. Korollar Hvis A \\leq_m B A \\leq_m B og A A er uafg\u00f8rbar, s\u00e5 er B B ogs\u00e5 uafg\u00f8rbar","title":"Afg\u00f8rbarhed"},{"location":"5-semester/CC/06-posts-korrespondanceproblem/#genkendelighed","text":"S\u00e6tning Hvis A \\leq_m B A \\leq_m B og B B er genkendeligt, s\u00e5 er A A ogs\u00e5 genkendeligt. Korollar Hvis A\\leq_m B A\\leq_m B og A A er u-genkendeligt, ss\u00e5 er B B heller ikke genkendeligt","title":"Genkendelighed"},{"location":"5-semester/CC/06-posts-korrespondanceproblem/#komplement","text":"S\u00e6tning Hvis A\\leq_m B A\\leq_m B , s\u00e5 $ \\overline A \\leq_m \\overline B$","title":"Komplement"},{"location":"5-semester/CC/07-rice/","text":"Rice's S\u00e6tning \u00b6 \\newcommand{\\egsk}{\\mathscr{S}}\\nonumber \\newcommand{\\egsk}{\\mathscr{S}}\\nonumber Egenskab \u00b6 Definition En egenskab er en klasse af genkendelige sprog. En egenskab \\egsk \\egsk er ikke-triviel hvis der er genkendelige sprog L_1,L_2 L_1,L_2 s\u00e5 \u200b L_1 \\in \\egsk L_1 \\in \\egsk men L_2 \\notin \\egsk L_2 \\notin \\egsk Der findes kun to trivielle egenskaber: \\egsk_{ALT} \\egsk_{ALT} \\egsk_{INTET}=\\{\\} \\egsk_{INTET}=\\{\\} Sprog \u00b6 Givet en egenskab \\egsk \\egsk , definerer vi: \\egsk_{TM}=\\left\\{ <M> \\mid M \\text{ er en TM},\\ L(M)\\in\\egsk \\right\\} \\egsk_{TM}=\\left\\{ <M> \\mid M \\text{ er en TM},\\ L(M)\\in\\egsk \\right\\} Rice's S\u00e6tning \u00b6 S\u00e6tning \u00b6 Hvis \\egsk \\egsk er en ikke-triviel egenskab, s\u00e5 er \\egsk_{TM} \\egsk_{TM} uafg\u00f8rbart . Bevis \u00b6 Reduktion A_{TM} \\leq_m \\egsk_{TM}\\\\ A_{TM} \\leq_m \\egsk_{TM}\\\\ \\langle M,w \\rangle \\leadsto \\langle M' \\rangle \\langle M,w \\rangle \\leadsto \\langle M' \\rangle s\u00e5 $$ M \\text{ accepterer } w \\Leftrightarrow L(M')\\in \\egsk $$ Antag at \\empty \\notin \\egsk \\empty \\notin \\egsk Det er nok, da: \\begin{align} \\egsk_{TM}&=\\{\\langle M \\rangle\\mid M \\text{ er en TM, } L(M)\\in \\egsk \\} &&\\text{ er afg\u00f8rbart}\\\\ &\\Updownarrow\\nonumber\\\\ \\overline\\egsk_{TM}&=\\{\\langle M \\rangle\\mid M \\text{ er en TM, } L(M)\\notin \\egsk \\}&&\\text{ er afg\u00f8rbart}\\\\ &=\\{\\langle M \\rangle\\mid M \\text{ er en TM, } L(M)\\in \\overline\\egsk \\} \\end{align} \\begin{align} \\egsk_{TM}&=\\{\\langle M \\rangle\\mid M \\text{ er en TM, } L(M)\\in \\egsk \\} &&\\text{ er afg\u00f8rbart}\\\\ &\\Updownarrow\\nonumber\\\\ \\overline\\egsk_{TM}&=\\{\\langle M \\rangle\\mid M \\text{ er en TM, } L(M)\\notin \\egsk \\}&&\\text{ er afg\u00f8rbart}\\\\ &=\\{\\langle M \\rangle\\mid M \\text{ er en TM, } L(M)\\in \\overline\\egsk \\} \\end{align} Vi har at \\empty \\notin \\egsk \\empty \\notin \\egsk Der m\u00e5 ogs\u00e5 v\u00e6re et L \\in \\egsk L \\in \\egsk (da \\egsk \\egsk er ikke-triviel), hvor L L er genkendeligt S\u00e5 er der en TM M_L M_L som genkender L L Vi bygger, givet \\langle M,w\\rangle \\langle M,w\\rangle , en M' M' s\u00e5 M M acc. w w \\Rightarrow \\Rightarrow L(M')=L \\in \\egsk L(M')=L \\in \\egsk M acc. ikke w w \\Rightarrow \\Rightarrow L(M')= \\empty \\notin \\egsk L(M')= \\empty \\notin \\egsk M'= M'= \" P\u00e5 input x. Simuler M M p\u00e5 w w . Hvis M acc. w w , s\u00e5 simuler M_L M_L p\u00e5 x x og svar hvad M_L M_L svarede. Ellers afvis \" M acc. w \\Rightarrow \\Rightarrow De x'er som M_L M_L acc., bliver accepteret \\Rightarrow \\Rightarrow L(M')=L\\quad \\in \\egsk L(M')=L\\quad \\in \\egsk M acc. ikke w \\Rightarrow \\Rightarrow Intet x bliver acceptereret \\Rightarrow \\Rightarrow L(M')=\\empty\\quad \\notin \\egsk L(M')=\\empty\\quad \\notin \\egsk Korollar \u00b6 E_{TM} E_{TM} er uafg\u00f8rbart REGULAR_{TM} REGULAR_{TM} er uafg\u00f8rbart ENDELIG_{TM} ENDELIG_{TM} er uafg\u00f8rbart Bevis \u00b6 Opskrift \u00b6 Er der tale om et problem om TM'er? Beslutningsproblem \\to \\to sprogudgave Sprogudgave \\to \\to Egenskab (hvilken sprogklasse \\egsk \\egsk er der tale om?) [er det overhovedet tilf\u00e6ldet?] Er \\egsk \\egsk en klasse af genkendelige sprog? (Hvis ja) Find et genkendeligt sprog L_1 \\in \\egsk L_1 \\in \\egsk og et genkendeligt L_2 \\notin \\egsk L_2 \\notin \\egsk , for s\u00e5 ved vi at \\egsk \\egsk er ikke-triviel","title":"Rice's S\u00e6tning"},{"location":"5-semester/CC/07-rice/#rices-stning","text":"\\newcommand{\\egsk}{\\mathscr{S}}\\nonumber \\newcommand{\\egsk}{\\mathscr{S}}\\nonumber","title":"Rice's S\u00e6tning"},{"location":"5-semester/CC/07-rice/#egenskab","text":"Definition En egenskab er en klasse af genkendelige sprog. En egenskab \\egsk \\egsk er ikke-triviel hvis der er genkendelige sprog L_1,L_2 L_1,L_2 s\u00e5 \u200b L_1 \\in \\egsk L_1 \\in \\egsk men L_2 \\notin \\egsk L_2 \\notin \\egsk Der findes kun to trivielle egenskaber: \\egsk_{ALT} \\egsk_{ALT} \\egsk_{INTET}=\\{\\} \\egsk_{INTET}=\\{\\}","title":"Egenskab"},{"location":"5-semester/CC/07-rice/#sprog","text":"Givet en egenskab \\egsk \\egsk , definerer vi: \\egsk_{TM}=\\left\\{ <M> \\mid M \\text{ er en TM},\\ L(M)\\in\\egsk \\right\\} \\egsk_{TM}=\\left\\{ <M> \\mid M \\text{ er en TM},\\ L(M)\\in\\egsk \\right\\}","title":"Sprog"},{"location":"5-semester/CC/07-rice/#rices-stning_1","text":"","title":"Rice's S\u00e6tning"},{"location":"5-semester/CC/07-rice/#stning","text":"Hvis \\egsk \\egsk er en ikke-triviel egenskab, s\u00e5 er \\egsk_{TM} \\egsk_{TM} uafg\u00f8rbart .","title":"S\u00e6tning"},{"location":"5-semester/CC/07-rice/#bevis","text":"Reduktion A_{TM} \\leq_m \\egsk_{TM}\\\\ A_{TM} \\leq_m \\egsk_{TM}\\\\ \\langle M,w \\rangle \\leadsto \\langle M' \\rangle \\langle M,w \\rangle \\leadsto \\langle M' \\rangle s\u00e5 $$ M \\text{ accepterer } w \\Leftrightarrow L(M')\\in \\egsk $$ Antag at \\empty \\notin \\egsk \\empty \\notin \\egsk Det er nok, da: \\begin{align} \\egsk_{TM}&=\\{\\langle M \\rangle\\mid M \\text{ er en TM, } L(M)\\in \\egsk \\} &&\\text{ er afg\u00f8rbart}\\\\ &\\Updownarrow\\nonumber\\\\ \\overline\\egsk_{TM}&=\\{\\langle M \\rangle\\mid M \\text{ er en TM, } L(M)\\notin \\egsk \\}&&\\text{ er afg\u00f8rbart}\\\\ &=\\{\\langle M \\rangle\\mid M \\text{ er en TM, } L(M)\\in \\overline\\egsk \\} \\end{align} \\begin{align} \\egsk_{TM}&=\\{\\langle M \\rangle\\mid M \\text{ er en TM, } L(M)\\in \\egsk \\} &&\\text{ er afg\u00f8rbart}\\\\ &\\Updownarrow\\nonumber\\\\ \\overline\\egsk_{TM}&=\\{\\langle M \\rangle\\mid M \\text{ er en TM, } L(M)\\notin \\egsk \\}&&\\text{ er afg\u00f8rbart}\\\\ &=\\{\\langle M \\rangle\\mid M \\text{ er en TM, } L(M)\\in \\overline\\egsk \\} \\end{align} Vi har at \\empty \\notin \\egsk \\empty \\notin \\egsk Der m\u00e5 ogs\u00e5 v\u00e6re et L \\in \\egsk L \\in \\egsk (da \\egsk \\egsk er ikke-triviel), hvor L L er genkendeligt S\u00e5 er der en TM M_L M_L som genkender L L Vi bygger, givet \\langle M,w\\rangle \\langle M,w\\rangle , en M' M' s\u00e5 M M acc. w w \\Rightarrow \\Rightarrow L(M')=L \\in \\egsk L(M')=L \\in \\egsk M acc. ikke w w \\Rightarrow \\Rightarrow L(M')= \\empty \\notin \\egsk L(M')= \\empty \\notin \\egsk M'= M'= \" P\u00e5 input x. Simuler M M p\u00e5 w w . Hvis M acc. w w , s\u00e5 simuler M_L M_L p\u00e5 x x og svar hvad M_L M_L svarede. Ellers afvis \" M acc. w \\Rightarrow \\Rightarrow De x'er som M_L M_L acc., bliver accepteret \\Rightarrow \\Rightarrow L(M')=L\\quad \\in \\egsk L(M')=L\\quad \\in \\egsk M acc. ikke w \\Rightarrow \\Rightarrow Intet x bliver acceptereret \\Rightarrow \\Rightarrow L(M')=\\empty\\quad \\notin \\egsk L(M')=\\empty\\quad \\notin \\egsk","title":"Bevis"},{"location":"5-semester/CC/07-rice/#korollar","text":"E_{TM} E_{TM} er uafg\u00f8rbart REGULAR_{TM} REGULAR_{TM} er uafg\u00f8rbart ENDELIG_{TM} ENDELIG_{TM} er uafg\u00f8rbart","title":"Korollar"},{"location":"5-semester/CC/07-rice/#bevis_1","text":"","title":"Bevis"},{"location":"5-semester/CC/07-rice/#opskrift","text":"Er der tale om et problem om TM'er? Beslutningsproblem \\to \\to sprogudgave Sprogudgave \\to \\to Egenskab (hvilken sprogklasse \\egsk \\egsk er der tale om?) [er det overhovedet tilf\u00e6ldet?] Er \\egsk \\egsk en klasse af genkendelige sprog? (Hvis ja) Find et genkendeligt sprog L_1 \\in \\egsk L_1 \\in \\egsk og et genkendeligt L_2 \\notin \\egsk L_2 \\notin \\egsk , for s\u00e5 ved vi at \\egsk \\egsk er ikke-triviel","title":"Opskrift"},{"location":"5-semester/CC/09-tidskompleksitet/","text":"Tidskompleksitet \u00b6 Definition (worst-case) Lad M M v\u00e6re en TM, der standser for ethvert input. Tidskompleksiteten af M M er en funktion $$ t_M:\\N\\to\\N $$ s\u00e5 \u200b t_M(n)=k t_M(n)=k \u200b hvis M M p\u00e5 et vilk\u00e5rligt input af l\u00e6ngde n h\u00f8jst bruger k skridt. Sammelign v\u00e6kstrater med O-notation Kompleksitetsklasser \u00b6 Kompleksitetsklasse = = famillie af sprog, der alle kan afg\u00f8res med afg\u00f8rer med bestemt tidskompleksitet. Definition Lad f: \\N \\to \\R_+ f: \\N \\to \\R_+ S\u00e5 er TIME(f(n)) TIME(f(n)) familien af sprog givet ved: TIME(f(n))=\\left\\{ L\\ \\mid \\begin{array}\\ L \\text{ kan afg\u00f8res af en TM M, }\\\\ \\text{der har tidskompleksitet } O(f(n) \\end{array} \\right\\} TIME(f(n))=\\left\\{ L\\ \\mid \\begin{array}\\ L \\text{ kan afg\u00f8res af en TM M, }\\\\ \\text{der har tidskompleksitet } O(f(n) \\end{array} \\right\\} Antallet af B\u00e5nd p\u00e5 TM \u00b6 Hvad betyder antallet af b\u00e5nd p\u00e5 en TM for tidskompleksiteten? S\u00e6tning Lad M v\u00e6re en k-b\u00e5nds-TM med tidskompleksiteten t(n)\\geq n t(n)\\geq n . S\u00e5 kan vi simulere M p\u00e5 en 1-b\u00e5nds TM med tidskompleksitet t^2(n) t^2(n) (kvadratisk langsommere) \u200b Afh\u00e6ngig ikke af antal b\u00e5nd! Tidskompleksitet i Nondeterminisme \u00b6 Definition Lad M M v\u00e6re en NTM der altid standser for ethvert input. M M har tidskompleksitet t: \\N\\to\\N t: \\N\\to\\N hvis det for enhver beregning p\u00e5 et vilk\u00e5rligt input af l\u00e6ngde n g\u00e6lder at M M h\u00f8jst bruger t(n) t(n) S\u00e6tning Lad M M v\u00e6re en NTM med tidskompleksitet t(n) t(n) . S\u00e5 kan M M simuleres af en DTM med tidskompleksitet 2^{O(t(n))} 2^{O(t(n))} (eksponentielt meget v\u00e6rre) Tidsklassen P \u00b6 Definition P=\\bigcup_{k\\geq 0} TIME(n^k)\\\\ P=\\bigcup_{k\\geq 0} TIME(n^k)\\\\ P=\\{L\\mid L \\text{ kan afg\u00f8res af en DTM med tidskompleksitet } O(n^k), k\\geq0\\} P=\\{L\\mid L \\text{ kan afg\u00f8res af en DTM med tidskompleksitet } O(n^k), k\\geq0\\} PATH \\in P PATH \\in P Kontekstfrie Sprog \u00b6 S\u00e6tning Alle kontekstfrie sprog er i P P Bevis En snedig algoritme, der givet en CFG G p\u00e5 Chomsky-normalform kan afg\u00f8re om et w\\in L(G) w\\in L(G) Tidskompleksitet er O(n^3),\\quad (n=|w|) O(n^3),\\quad (n=|w|) Kaldes CYK-algoritmen Eksempel p\u00e5 dynamisk programmering Bevis \u00b6 w=w_1,\\dots,w_n\\quad(|w|=n) \\nonumber w=w_1,\\dots,w_n\\quad(|w|=n) \\nonumber","title":"Tidskompleksitet"},{"location":"5-semester/CC/09-tidskompleksitet/#tidskompleksitet","text":"Definition (worst-case) Lad M M v\u00e6re en TM, der standser for ethvert input. Tidskompleksiteten af M M er en funktion $$ t_M:\\N\\to\\N $$ s\u00e5 \u200b t_M(n)=k t_M(n)=k \u200b hvis M M p\u00e5 et vilk\u00e5rligt input af l\u00e6ngde n h\u00f8jst bruger k skridt. Sammelign v\u00e6kstrater med O-notation","title":"Tidskompleksitet"},{"location":"5-semester/CC/09-tidskompleksitet/#kompleksitetsklasser","text":"Kompleksitetsklasse = = famillie af sprog, der alle kan afg\u00f8res med afg\u00f8rer med bestemt tidskompleksitet. Definition Lad f: \\N \\to \\R_+ f: \\N \\to \\R_+ S\u00e5 er TIME(f(n)) TIME(f(n)) familien af sprog givet ved: TIME(f(n))=\\left\\{ L\\ \\mid \\begin{array}\\ L \\text{ kan afg\u00f8res af en TM M, }\\\\ \\text{der har tidskompleksitet } O(f(n) \\end{array} \\right\\} TIME(f(n))=\\left\\{ L\\ \\mid \\begin{array}\\ L \\text{ kan afg\u00f8res af en TM M, }\\\\ \\text{der har tidskompleksitet } O(f(n) \\end{array} \\right\\}","title":"Kompleksitetsklasser"},{"location":"5-semester/CC/09-tidskompleksitet/#antallet-af-band-pa-tm","text":"Hvad betyder antallet af b\u00e5nd p\u00e5 en TM for tidskompleksiteten? S\u00e6tning Lad M v\u00e6re en k-b\u00e5nds-TM med tidskompleksiteten t(n)\\geq n t(n)\\geq n . S\u00e5 kan vi simulere M p\u00e5 en 1-b\u00e5nds TM med tidskompleksitet t^2(n) t^2(n) (kvadratisk langsommere) \u200b Afh\u00e6ngig ikke af antal b\u00e5nd!","title":"Antallet af B\u00e5nd p\u00e5 TM"},{"location":"5-semester/CC/09-tidskompleksitet/#tidskompleksitet-i-nondeterminisme","text":"Definition Lad M M v\u00e6re en NTM der altid standser for ethvert input. M M har tidskompleksitet t: \\N\\to\\N t: \\N\\to\\N hvis det for enhver beregning p\u00e5 et vilk\u00e5rligt input af l\u00e6ngde n g\u00e6lder at M M h\u00f8jst bruger t(n) t(n) S\u00e6tning Lad M M v\u00e6re en NTM med tidskompleksitet t(n) t(n) . S\u00e5 kan M M simuleres af en DTM med tidskompleksitet 2^{O(t(n))} 2^{O(t(n))} (eksponentielt meget v\u00e6rre)","title":"Tidskompleksitet i Nondeterminisme"},{"location":"5-semester/CC/09-tidskompleksitet/#tidsklassen-p","text":"Definition P=\\bigcup_{k\\geq 0} TIME(n^k)\\\\ P=\\bigcup_{k\\geq 0} TIME(n^k)\\\\ P=\\{L\\mid L \\text{ kan afg\u00f8res af en DTM med tidskompleksitet } O(n^k), k\\geq0\\} P=\\{L\\mid L \\text{ kan afg\u00f8res af en DTM med tidskompleksitet } O(n^k), k\\geq0\\} PATH \\in P PATH \\in P","title":"Tidsklassen P"},{"location":"5-semester/CC/09-tidskompleksitet/#kontekstfrie-sprog","text":"S\u00e6tning Alle kontekstfrie sprog er i P P Bevis En snedig algoritme, der givet en CFG G p\u00e5 Chomsky-normalform kan afg\u00f8re om et w\\in L(G) w\\in L(G) Tidskompleksitet er O(n^3),\\quad (n=|w|) O(n^3),\\quad (n=|w|) Kaldes CYK-algoritmen Eksempel p\u00e5 dynamisk programmering","title":"Kontekstfrie Sprog"},{"location":"5-semester/CC/09-tidskompleksitet/#bevis","text":"w=w_1,\\dots,w_n\\quad(|w|=n) \\nonumber w=w_1,\\dots,w_n\\quad(|w|=n) \\nonumber","title":"Bevis"},{"location":"5-semester/CC/10-np-fuldstaendighed/","text":"NP og NP-fuldst\u00e6ndighed \u00b6 Verifikator \u00b6 Lad A A v\u00e6re et sprog. En verifikator V V for sproget A A er en TM som opfylder at $$ A={w \\mid V \\text{ accepterer } \\langle w,c\\rangle} $$ hvor c c er et certifikat (bud p\u00e5 l\u00f8sning / vidne til medlemskab) En verifikator er polinomiel hvis den har polinomiel tidskompleksitet mht. |w| |w| c c kan kun have st\u00f8rrelser der er polinomiel i |w| |w| HAMPATH Eksempel \u00b6 Definition Givet en orienteret graf G G , er en sti i G G en Hamilton-sti, hvis stien bes\u00f8ger hver knude i G G pr\u00e6cis en gang. Problem \"Givet orienteret graf G G , startknude s s , slutknude t t , findes der da en Hamilton-sti i G G fra s s til t t ?\" HAMPATH=\\{\\langle G,s,t\\rangle \\mid G \\text{ er en orienteret graf med en Hamilton-sti fra knude } s \\text{ til } t\\} HAMPATH=\\{\\langle G,s,t\\rangle \\mid G \\text{ er en orienteret graf med en Hamilton-sti fra knude } s \\text{ til } t\\} Det er nemt (hurtigt) at tjekke om et bud p\u00e5 en Hamilton-sti faktisk er en s\u00e5dan. Eksempel: Men er det nemt at finde ud af om en G G har en Hamilton-sti? Verifikator for HAMPATH \u00b6 COMPOSITE Eksempel \u00b6 Et n\\in \\N n\\in \\N er sammensat hvis \\exists p,q \\in \\N \\exists p,q \\in \\N s\u00e5 p,q>1 p,q>1 og p\\cdot q=n p\\cdot q=n \"Givet n\\in \\N n\\in \\N er n sammensat?\" COMPOSITE=\\{\\langle n \\rangle \\mid n\\in \\N, n \\text{ sammensat}\\} COMPOSITE=\\{\\langle n \\rangle \\mid n\\in \\N, n \\text{ sammensat}\\} Verifikator for COMPOSITE \u00b6 Hvad er certifikat for COMPOSITE COMPOSITE ? \u200b Et vidne for at n\\in COMPOSITE n\\in COMPOSITE , dvs. p,q p,q Verifikator : \" \u200b Givet \\langle n, \\langle p,q \\rangle\\rangle \\langle n, \\langle p,q \\rangle\\rangle Hvis p<n p<n og q < n q < n s\u00e5 s\u00e5 hvis p\\cdot q = n p\\cdot q = n accepter ellers afvis \" NP \u00b6 Definition $$ NP = {L \\mid L \\text{ har en polynomiel verifikator}} $$ (sprog hvor det er nemt at tjekke om et bud p\u00e5 et vidne for medlemskab er korrekt, men ikke n\u00f8dvendigvis nemt at tjekke for medlemskab) nemt: lav (polynomiel tidskompleksitet) $$ NP = \\bigcup_{k\\geq 0} NTIME(n^k) $$ NP NP er klassen af sprog der kan afg\u00f8res i nondererminiistisk polynomiel tid. S\u00e6tning $$ \\begin{align} \\forall L:\\quad &L \\text{ har en polynomiel verifikator}\\nonumber\\ &\\Updownarrow\\nonumber\\ &L \\text{ kan afg\u00f8res i nondet. polynomiel tid}\\nonumber \\end{align} $$ NTIME \u00b6 Lad f: \\N \\to \\N f: \\N \\to \\N NTIME(f(n)) = \\{ L\\mid L \\text{ kan afg\u00f8res af en NTM med tidskompleksitet } O(f(n))\\} NTIME(f(n)) = \\{ L\\mid L \\text{ kan afg\u00f8res af en NTM med tidskompleksitet } O(f(n))\\} Bem\u00e6rk: TIME(f(n))\\subseteq NTIME(f(n)) TIME(f(n))\\subseteq NTIME(f(n)) Bevis for S\u00e6tning (6) \u00b6 \\Downarrow \\Downarrow ) Antag at L L har en polynomiel verifikator V V : V V har pol. tidskompleksitet mht. w w dvs O(n^k) O(n^k) (s\u00e5 |c| |c| kan h\u00f8jst v\u00e6re O(n^k) O(n^k) ) Vi vil lave en nondet. polynomiel afg\u00f8rer N N : \\Uparrow \\Uparrow ) Antag at L L kan afg\u00f8res i nondet. polynomiel tid med afg\u00f8rer N N . Vi vil lave en polynomiel verifikator V V for L L . Certifikat skal v\u00e6re et bud p\u00e5 en accepterende sti i beregningstr\u00e5et (dvs. en f\u00f8lge af konfigurationer) Verifikator V V : \\square \\square P NP \u00b6 Da TIME(f(n)) \\subseteq NTIME(f(n)) TIME(f(n)) \\subseteq NTIME(f(n)) har vi P \\subseteq NP P \\subseteq NP Reduktion \u00b6 F\u00f8r: \\leq_m \\approx \\leq_m \\approx er ikke sv\u00e6rere end Nu: \\leq_p \\approx \\leq_p \\approx er ikke sv\u00e6rere end (nu med tidskompleksitet) Definition Lad A, B A, B v\u00e6re sprog over \\Sigma^* \\Sigma^* A \\leq_p B A \\leq_p B ( A A polynomial reducerer til B B ), hvis \\exists f: \\Sigma^*\\to\\Sigma^* \\exists f: \\Sigma^*\\to\\Sigma^* s\u00e5 \\forall w \\quad w\\in A \\Leftrightarrow f(w)\\in B \\forall w \\quad w\\in A \\Leftrightarrow f(w)\\in B ( f f er trofast) og f f er beregnbar i polynomiel tid S\u00e6tning Hvis A \\leq_p B A \\leq_p B og B \\in P B \\in P , s\u00e5 A\\in P A\\in P NP Fuldst\u00e6ndighed \u00b6 Definition Et sprog L L kaldes NP-fuldst\u00e6ndigt hvis L\\in NP L\\in NP \\forall L' \\in NP.\\quad L'\\leq_p L \\forall L' \\in NP.\\quad L'\\leq_p L Hvis L_1, L_2 L_1, L_2 er NP-fuldst\u00e6ndige, har vi L_1\\in NP,\\quad L_2\\in NP L_1\\in NP,\\quad L_2\\in NP \\forall L' \\in NP. \\quad L'\\leq_p L_1, \\quad L' \\leq_p L_2 \\forall L' \\in NP. \\quad L'\\leq_p L_1, \\quad L' \\leq_p L_2 S\u00e5: L_1 \\leq_p L_2 \\quad L_2 \\leq_p L_1 L_1 \\leq_p L_2 \\quad L_2 \\leq_p L_1","title":"NP og NP-fuldst\u00e6ndighed"},{"location":"5-semester/CC/10-np-fuldstaendighed/#np-og-np-fuldstndighed","text":"","title":"NP og NP-fuldst\u00e6ndighed"},{"location":"5-semester/CC/10-np-fuldstaendighed/#verifikator","text":"Lad A A v\u00e6re et sprog. En verifikator V V for sproget A A er en TM som opfylder at $$ A={w \\mid V \\text{ accepterer } \\langle w,c\\rangle} $$ hvor c c er et certifikat (bud p\u00e5 l\u00f8sning / vidne til medlemskab) En verifikator er polinomiel hvis den har polinomiel tidskompleksitet mht. |w| |w| c c kan kun have st\u00f8rrelser der er polinomiel i |w| |w|","title":"Verifikator"},{"location":"5-semester/CC/10-np-fuldstaendighed/#hampath-eksempel","text":"Definition Givet en orienteret graf G G , er en sti i G G en Hamilton-sti, hvis stien bes\u00f8ger hver knude i G G pr\u00e6cis en gang. Problem \"Givet orienteret graf G G , startknude s s , slutknude t t , findes der da en Hamilton-sti i G G fra s s til t t ?\" HAMPATH=\\{\\langle G,s,t\\rangle \\mid G \\text{ er en orienteret graf med en Hamilton-sti fra knude } s \\text{ til } t\\} HAMPATH=\\{\\langle G,s,t\\rangle \\mid G \\text{ er en orienteret graf med en Hamilton-sti fra knude } s \\text{ til } t\\} Det er nemt (hurtigt) at tjekke om et bud p\u00e5 en Hamilton-sti faktisk er en s\u00e5dan. Eksempel: Men er det nemt at finde ud af om en G G har en Hamilton-sti?","title":"HAMPATH Eksempel"},{"location":"5-semester/CC/10-np-fuldstaendighed/#verifikator-for-hampath","text":"","title":"Verifikator for HAMPATH"},{"location":"5-semester/CC/10-np-fuldstaendighed/#composite-eksempel","text":"Et n\\in \\N n\\in \\N er sammensat hvis \\exists p,q \\in \\N \\exists p,q \\in \\N s\u00e5 p,q>1 p,q>1 og p\\cdot q=n p\\cdot q=n \"Givet n\\in \\N n\\in \\N er n sammensat?\" COMPOSITE=\\{\\langle n \\rangle \\mid n\\in \\N, n \\text{ sammensat}\\} COMPOSITE=\\{\\langle n \\rangle \\mid n\\in \\N, n \\text{ sammensat}\\}","title":"COMPOSITE Eksempel"},{"location":"5-semester/CC/10-np-fuldstaendighed/#verifikator-for-composite","text":"Hvad er certifikat for COMPOSITE COMPOSITE ? \u200b Et vidne for at n\\in COMPOSITE n\\in COMPOSITE , dvs. p,q p,q Verifikator : \" \u200b Givet \\langle n, \\langle p,q \\rangle\\rangle \\langle n, \\langle p,q \\rangle\\rangle Hvis p<n p<n og q < n q < n s\u00e5 s\u00e5 hvis p\\cdot q = n p\\cdot q = n accepter ellers afvis \"","title":"Verifikator for COMPOSITE"},{"location":"5-semester/CC/10-np-fuldstaendighed/#np","text":"Definition $$ NP = {L \\mid L \\text{ har en polynomiel verifikator}} $$ (sprog hvor det er nemt at tjekke om et bud p\u00e5 et vidne for medlemskab er korrekt, men ikke n\u00f8dvendigvis nemt at tjekke for medlemskab) nemt: lav (polynomiel tidskompleksitet) $$ NP = \\bigcup_{k\\geq 0} NTIME(n^k) $$ NP NP er klassen af sprog der kan afg\u00f8res i nondererminiistisk polynomiel tid. S\u00e6tning $$ \\begin{align} \\forall L:\\quad &L \\text{ har en polynomiel verifikator}\\nonumber\\ &\\Updownarrow\\nonumber\\ &L \\text{ kan afg\u00f8res i nondet. polynomiel tid}\\nonumber \\end{align} $$","title":"NP"},{"location":"5-semester/CC/10-np-fuldstaendighed/#ntime","text":"Lad f: \\N \\to \\N f: \\N \\to \\N NTIME(f(n)) = \\{ L\\mid L \\text{ kan afg\u00f8res af en NTM med tidskompleksitet } O(f(n))\\} NTIME(f(n)) = \\{ L\\mid L \\text{ kan afg\u00f8res af en NTM med tidskompleksitet } O(f(n))\\} Bem\u00e6rk: TIME(f(n))\\subseteq NTIME(f(n)) TIME(f(n))\\subseteq NTIME(f(n))","title":"NTIME"},{"location":"5-semester/CC/10-np-fuldstaendighed/#bevis-for-stning-6","text":"\\Downarrow \\Downarrow ) Antag at L L har en polynomiel verifikator V V : V V har pol. tidskompleksitet mht. w w dvs O(n^k) O(n^k) (s\u00e5 |c| |c| kan h\u00f8jst v\u00e6re O(n^k) O(n^k) ) Vi vil lave en nondet. polynomiel afg\u00f8rer N N : \\Uparrow \\Uparrow ) Antag at L L kan afg\u00f8res i nondet. polynomiel tid med afg\u00f8rer N N . Vi vil lave en polynomiel verifikator V V for L L . Certifikat skal v\u00e6re et bud p\u00e5 en accepterende sti i beregningstr\u00e5et (dvs. en f\u00f8lge af konfigurationer) Verifikator V V : \\square \\square","title":"Bevis for S\u00e6tning (6)"},{"location":"5-semester/CC/10-np-fuldstaendighed/#p-np","text":"Da TIME(f(n)) \\subseteq NTIME(f(n)) TIME(f(n)) \\subseteq NTIME(f(n)) har vi P \\subseteq NP P \\subseteq NP","title":"P NP"},{"location":"5-semester/CC/10-np-fuldstaendighed/#reduktion","text":"F\u00f8r: \\leq_m \\approx \\leq_m \\approx er ikke sv\u00e6rere end Nu: \\leq_p \\approx \\leq_p \\approx er ikke sv\u00e6rere end (nu med tidskompleksitet) Definition Lad A, B A, B v\u00e6re sprog over \\Sigma^* \\Sigma^* A \\leq_p B A \\leq_p B ( A A polynomial reducerer til B B ), hvis \\exists f: \\Sigma^*\\to\\Sigma^* \\exists f: \\Sigma^*\\to\\Sigma^* s\u00e5 \\forall w \\quad w\\in A \\Leftrightarrow f(w)\\in B \\forall w \\quad w\\in A \\Leftrightarrow f(w)\\in B ( f f er trofast) og f f er beregnbar i polynomiel tid S\u00e6tning Hvis A \\leq_p B A \\leq_p B og B \\in P B \\in P , s\u00e5 A\\in P A\\in P","title":"Reduktion"},{"location":"5-semester/CC/10-np-fuldstaendighed/#np-fuldstndighed","text":"Definition Et sprog L L kaldes NP-fuldst\u00e6ndigt hvis L\\in NP L\\in NP \\forall L' \\in NP.\\quad L'\\leq_p L \\forall L' \\in NP.\\quad L'\\leq_p L Hvis L_1, L_2 L_1, L_2 er NP-fuldst\u00e6ndige, har vi L_1\\in NP,\\quad L_2\\in NP L_1\\in NP,\\quad L_2\\in NP \\forall L' \\in NP. \\quad L'\\leq_p L_1, \\quad L' \\leq_p L_2 \\forall L' \\in NP. \\quad L'\\leq_p L_1, \\quad L' \\leq_p L_2 S\u00e5: L_1 \\leq_p L_2 \\quad L_2 \\leq_p L_1 L_1 \\leq_p L_2 \\quad L_2 \\leq_p L_1","title":"NP Fuldst\u00e6ndighed"},{"location":"5-semester/CC/12-np-complete-sprog/","text":"Flere NP-fuldst\u00e6ndige Sprog \u00b6 Lemma \\leq_p \\leq_p er transitiv Hvis L_1 \\leq_p L_2 L_1 \\leq_p L_2 og L_2\\leq_p L_3 L_2\\leq_p L_3 s\u00e5 L_1 \\leq_p L_3 L_1 \\leq_p L_3","title":"Flere NP-fuldst\u00e6ndige Sprog"},{"location":"5-semester/CC/12-np-complete-sprog/#flere-np-fuldstndige-sprog","text":"Lemma \\leq_p \\leq_p er transitiv Hvis L_1 \\leq_p L_2 L_1 \\leq_p L_2 og L_2\\leq_p L_3 L_2\\leq_p L_3 s\u00e5 L_1 \\leq_p L_3 L_1 \\leq_p L_3","title":"Flere NP-fuldst\u00e6ndige Sprog"},{"location":"5-semester/CC/13-pladskompleksitet/","text":"Pladskompleksitet \u00b6 Definition Deterministisk En DTM har pladskompleksitet f(n) f(n) , hvor f: \\N \\to \\N f: \\N \\to \\N hvis enhver beregning p\u00e5 et input af l\u00e6ngde n n h\u00f8jst bruger f(n) f(n) felter p\u00e5 b\u00e5ndet (for alle input af l\u00e6ngden n n ) Definition Nondeterministisk En NTM har pladskompleksitet f(n) f(n) , hvor f: \\N \\to \\N f: \\N \\to \\N hvis enhver beregning p\u00e5 et input af l\u00e6ngde n n h\u00f8jst bruger f(n) f(n) felter p\u00e5 b\u00e5ndet (for ethvert input af l\u00e6ngden n n og enhver mulig beregning p\u00e5 et s\u00e5dant input) Pladskompleksitetsklasser \u00b6 Definition Lad f:\\N\\to\\N f:\\N\\to\\N \\begin{align} SPACE(f(n))&=\\{L \\mid L \\text{ kan afg\u00f8res af en DTM med pladskompleksitet } O(f(n))\\}\\\\ NSPACE(f(n))&=\\{L \\mid L \\text{ kan afg\u00f8res af en NTM med pladskompleksitet } O(f(n))\\} \\end{align} \\begin{align} SPACE(f(n))&=\\{L \\mid L \\text{ kan afg\u00f8res af en DTM med pladskompleksitet } O(f(n))\\}\\\\ NSPACE(f(n))&=\\{L \\mid L \\text{ kan afg\u00f8res af en NTM med pladskompleksitet } O(f(n))\\} \\end{align} Klart at SPACE(f(n))\\subseteq NSPACE(f(n)) SPACE(f(n))\\subseteq NSPACE(f(n)) Eksempler \u00b6 SAT \u00b6 SAT\\in SPACE(n) SAT\\in SPACE(n) Afg\u00f8rer Sammenh\u00e6ng Mellem Tids- og Pladskompleksitet \u00b6 S\u00e6tning Hvis en TM har pladskompleksitet O(f(n)) O(f(n)) , s\u00e5 har den tidskompleksitet 2^{O(f(n))} 2^{O(f(n))} Bevis EXPTIME \u00b6 EXPTIME=\\bigcup TIME(2^{n^k}) EXPTIME=\\bigcup TIME(2^{n^k}) S\u00e6tning Hvis L \\in SPACE(n^k) L \\in SPACE(n^k) s\u00e5 L\\in TIME(2^{n^k}) L\\in TIME(2^{n^k}) , dvs L\\in EXPTIME L\\in EXPTIME Alts\u00e5 hvis L L kan afg\u00f8res i polynomiel plads, kan L L afg\u00f8res i eksponentiel tid. PSPACE \u00b6 \\begin{align} PSPACE&=\\bigcup_{k\\geq0} SPACE(n^k)\\\\ NPSPACE &= \\bigcup_{k\\geq0} NSPACE(n^k) \\end{align} \\begin{align} PSPACE&=\\bigcup_{k\\geq0} SPACE(n^k)\\\\ NPSPACE &= \\bigcup_{k\\geq0} NSPACE(n^k) \\end{align} Klart at $$ PSPACE\\subseteq NPSPAC $$ Det vides at PSPACE = NPSPAC PSPACE = NPSPAC Verdenskort \u00b6","title":"Pladskompleksitet"},{"location":"5-semester/CC/13-pladskompleksitet/#pladskompleksitet","text":"Definition Deterministisk En DTM har pladskompleksitet f(n) f(n) , hvor f: \\N \\to \\N f: \\N \\to \\N hvis enhver beregning p\u00e5 et input af l\u00e6ngde n n h\u00f8jst bruger f(n) f(n) felter p\u00e5 b\u00e5ndet (for alle input af l\u00e6ngden n n ) Definition Nondeterministisk En NTM har pladskompleksitet f(n) f(n) , hvor f: \\N \\to \\N f: \\N \\to \\N hvis enhver beregning p\u00e5 et input af l\u00e6ngde n n h\u00f8jst bruger f(n) f(n) felter p\u00e5 b\u00e5ndet (for ethvert input af l\u00e6ngden n n og enhver mulig beregning p\u00e5 et s\u00e5dant input)","title":"Pladskompleksitet"},{"location":"5-semester/CC/13-pladskompleksitet/#pladskompleksitetsklasser","text":"Definition Lad f:\\N\\to\\N f:\\N\\to\\N \\begin{align} SPACE(f(n))&=\\{L \\mid L \\text{ kan afg\u00f8res af en DTM med pladskompleksitet } O(f(n))\\}\\\\ NSPACE(f(n))&=\\{L \\mid L \\text{ kan afg\u00f8res af en NTM med pladskompleksitet } O(f(n))\\} \\end{align} \\begin{align} SPACE(f(n))&=\\{L \\mid L \\text{ kan afg\u00f8res af en DTM med pladskompleksitet } O(f(n))\\}\\\\ NSPACE(f(n))&=\\{L \\mid L \\text{ kan afg\u00f8res af en NTM med pladskompleksitet } O(f(n))\\} \\end{align} Klart at SPACE(f(n))\\subseteq NSPACE(f(n)) SPACE(f(n))\\subseteq NSPACE(f(n))","title":"Pladskompleksitetsklasser"},{"location":"5-semester/CC/13-pladskompleksitet/#eksempler","text":"","title":"Eksempler"},{"location":"5-semester/CC/13-pladskompleksitet/#sat","text":"SAT\\in SPACE(n) SAT\\in SPACE(n) Afg\u00f8rer","title":"SAT"},{"location":"5-semester/CC/13-pladskompleksitet/#sammenhng-mellem-tids-og-pladskompleksitet","text":"S\u00e6tning Hvis en TM har pladskompleksitet O(f(n)) O(f(n)) , s\u00e5 har den tidskompleksitet 2^{O(f(n))} 2^{O(f(n))} Bevis","title":"Sammenh\u00e6ng Mellem Tids- og Pladskompleksitet"},{"location":"5-semester/CC/13-pladskompleksitet/#exptime","text":"EXPTIME=\\bigcup TIME(2^{n^k}) EXPTIME=\\bigcup TIME(2^{n^k}) S\u00e6tning Hvis L \\in SPACE(n^k) L \\in SPACE(n^k) s\u00e5 L\\in TIME(2^{n^k}) L\\in TIME(2^{n^k}) , dvs L\\in EXPTIME L\\in EXPTIME Alts\u00e5 hvis L L kan afg\u00f8res i polynomiel plads, kan L L afg\u00f8res i eksponentiel tid.","title":"EXPTIME"},{"location":"5-semester/CC/13-pladskompleksitet/#pspace","text":"\\begin{align} PSPACE&=\\bigcup_{k\\geq0} SPACE(n^k)\\\\ NPSPACE &= \\bigcup_{k\\geq0} NSPACE(n^k) \\end{align} \\begin{align} PSPACE&=\\bigcup_{k\\geq0} SPACE(n^k)\\\\ NPSPACE &= \\bigcup_{k\\geq0} NSPACE(n^k) \\end{align} Klart at $$ PSPACE\\subseteq NPSPAC $$ Det vides at PSPACE = NPSPAC PSPACE = NPSPAC","title":"PSPACE"},{"location":"5-semester/CC/13-pladskompleksitet/#verdenskort","text":"","title":"Verdenskort"},{"location":"5-semester/CC/99-sprog-i-pensum/","text":"Sprogproblemer \u00b6 Afg\u00f8rbare Sprog \u00b6 A_{DFA}=\\{\\langle B, w\\rangle \\mid B \\text{ er en DFA der accepterer } w\\} A_{DFA}=\\{\\langle B, w\\rangle \\mid B \\text{ er en DFA der accepterer } w\\} A_{DFA} A_{DFA} er afg\u00f8rbart E_{DFA} = \\{\\langle A\\rangle \\mid A \\text{ er en DFA og } L(A) = \\empty\\} E_{DFA} = \\{\\langle A\\rangle \\mid A \\text{ er en DFA og } L(A) = \\empty\\} E_{DFA} E_{DFA} er afg\u00f8rbart A_{CFG}=\\{\\langle G, w\\rangle \\mid G \\text{ er en CFG der genererer } w\\} A_{CFG}=\\{\\langle G, w\\rangle \\mid G \\text{ er en CFG der genererer } w\\} A_{CFG} A_{CFG} er afg\u00f8rbart Uafg\u00f8rbare Sprog \u00b6 A_{TM}=\\{\\langle M,w \\rangle \\mid M \\text{ er en TM der accepterer } w\\} A_{TM}=\\{\\langle M,w \\rangle \\mid M \\text{ er en TM der accepterer } w\\} A_{TM} A_{TM} er uafg\u00f8rbart Genkendeligt Ikke ko-genkendeligt HALT_{TM} = \\{\\langle M, w\\rangle \\mid M \\text{ er en TM der standser p\u00e5 input } w\\} HALT_{TM} = \\{\\langle M, w\\rangle \\mid M \\text{ er en TM der standser p\u00e5 input } w\\} HALT_{TM} HALT_{TM} er uafg\u00f8rbart Genkendeligt Ikke ko-genkendeligt E_{TM} = \\{\\langle M\\rangle \\mid M \\text{ er en TM og } L(M) = \\empty\\} E_{TM} = \\{\\langle M\\rangle \\mid M \\text{ er en TM og } L(M) = \\empty\\} E_{TM} E_{TM} er uafg\u00f8rbart EQ_{TM}=\\{\\langle M_1, M_2\\rangle \\mid {M_1, M_2 \\text{ er TMs og } L(M_1)=L(M_2)}\\} EQ_{TM}=\\{\\langle M_1, M_2\\rangle \\mid {M_1, M_2 \\text{ er TMs og } L(M_1)=L(M_2)}\\} EQ_{TM} EQ_{TM} er uafg\u00f8rbart Ikke genkendeligt Ikke ko-genkendeligt Sprog i NP \u00b6 SAT=\\{\\langle \\phi \\rangle \\mid \\phi \\text{ er en opfyldelig Boolsk formel }\\} SAT=\\{\\langle \\phi \\rangle \\mid \\phi \\text{ er en opfyldelig Boolsk formel }\\} SAT SAT er afg\u00f8rbart SAT\\in NP SAT\\in NP HAMPATH=\\{\\langle G,s,t\\rangle \\mid G \\text{ er en orienteret graf med en Hamilton-sti fra knude } s \\text{ til } t\\} HAMPATH=\\{\\langle G,s,t\\rangle \\mid G \\text{ er en orienteret graf med en Hamilton-sti fra knude } s \\text{ til } t\\} HAMPATH HAMPATH er afg\u00f8rbart COMPOSITE=\\{\\langle n \\rangle \\mid n\\in \\N, n \\text{ sammensat}\\} COMPOSITE=\\{\\langle n \\rangle \\mid n\\in \\N, n \\text{ sammensat}\\} COMPOSITE COMPOSITE er afg\u00f8rbart 3SAT = \\{\\langle \\phi \\rangle\\mid \\phi \\text{ er en opfyldelig 3CNF-formel}\\} 3SAT = \\{\\langle \\phi \\rangle\\mid \\phi \\text{ er en opfyldelig 3CNF-formel}\\} 3SAT 3SAT er afg\u00f8rbart 3SAT 3SAT er NP-fuldst\u00e6ndig VERTEX-COVER=\\{ \\langle G,k\\rangle\\mid G \\text{ er en ikke-orienteret graf med en k-knudeoverd\u00e6kning} \\} VERTEX-COVER=\\{ \\langle G,k\\rangle\\mid G \\text{ er en ikke-orienteret graf med en k-knudeoverd\u00e6kning} \\} Afg\u00f8rbart NP-fuldst\u00e6ndigt UHAMPATH=\\{\\langle G,s,t\\rangle \\mid G \\text{ er en ikke-orienteret graf med en Hamilton-sti fra knude } s \\text{ til } t\\} UHAMPATH=\\{\\langle G,s,t\\rangle \\mid G \\text{ er en ikke-orienteret graf med en Hamilton-sti fra knude } s \\text{ til } t\\} Afg\u00f8rbart NP-fuldst\u00e6ndigt SUBSET-SUM=\\left\\{\\langle S, k\\rangle\\ \\left|\\ \\begin{array}{} S \\text{ er en endelig m\u00e6ngde } S\\subseteq \\N,\\\\ \\text{ og der findes } S'\\subseteq S \\text{ s\u00e5 } \\sum_{x\\in S'} x=k,\\quad k\\in \\N \\end{array} \\right. \\right\\} SUBSET-SUM=\\left\\{\\langle S, k\\rangle\\ \\left|\\ \\begin{array}{} S \\text{ er en endelig m\u00e6ngde } S\\subseteq \\N,\\\\ \\text{ og der findes } S'\\subseteq S \\text{ s\u00e5 } \\sum_{x\\in S'} x=k,\\quad k\\in \\N \\end{array} \\right. \\right\\} Afg\u00f8rbart NP-fuldst\u00e6ndigt","title":"Sprogproblemer"},{"location":"5-semester/CC/99-sprog-i-pensum/#sprogproblemer","text":"","title":"Sprogproblemer"},{"location":"5-semester/CC/99-sprog-i-pensum/#afgrbare-sprog","text":"A_{DFA}=\\{\\langle B, w\\rangle \\mid B \\text{ er en DFA der accepterer } w\\} A_{DFA}=\\{\\langle B, w\\rangle \\mid B \\text{ er en DFA der accepterer } w\\} A_{DFA} A_{DFA} er afg\u00f8rbart E_{DFA} = \\{\\langle A\\rangle \\mid A \\text{ er en DFA og } L(A) = \\empty\\} E_{DFA} = \\{\\langle A\\rangle \\mid A \\text{ er en DFA og } L(A) = \\empty\\} E_{DFA} E_{DFA} er afg\u00f8rbart A_{CFG}=\\{\\langle G, w\\rangle \\mid G \\text{ er en CFG der genererer } w\\} A_{CFG}=\\{\\langle G, w\\rangle \\mid G \\text{ er en CFG der genererer } w\\} A_{CFG} A_{CFG} er afg\u00f8rbart","title":"Afg\u00f8rbare Sprog"},{"location":"5-semester/CC/99-sprog-i-pensum/#uafgrbare-sprog","text":"A_{TM}=\\{\\langle M,w \\rangle \\mid M \\text{ er en TM der accepterer } w\\} A_{TM}=\\{\\langle M,w \\rangle \\mid M \\text{ er en TM der accepterer } w\\} A_{TM} A_{TM} er uafg\u00f8rbart Genkendeligt Ikke ko-genkendeligt HALT_{TM} = \\{\\langle M, w\\rangle \\mid M \\text{ er en TM der standser p\u00e5 input } w\\} HALT_{TM} = \\{\\langle M, w\\rangle \\mid M \\text{ er en TM der standser p\u00e5 input } w\\} HALT_{TM} HALT_{TM} er uafg\u00f8rbart Genkendeligt Ikke ko-genkendeligt E_{TM} = \\{\\langle M\\rangle \\mid M \\text{ er en TM og } L(M) = \\empty\\} E_{TM} = \\{\\langle M\\rangle \\mid M \\text{ er en TM og } L(M) = \\empty\\} E_{TM} E_{TM} er uafg\u00f8rbart EQ_{TM}=\\{\\langle M_1, M_2\\rangle \\mid {M_1, M_2 \\text{ er TMs og } L(M_1)=L(M_2)}\\} EQ_{TM}=\\{\\langle M_1, M_2\\rangle \\mid {M_1, M_2 \\text{ er TMs og } L(M_1)=L(M_2)}\\} EQ_{TM} EQ_{TM} er uafg\u00f8rbart Ikke genkendeligt Ikke ko-genkendeligt","title":"Uafg\u00f8rbare Sprog"},{"location":"5-semester/CC/99-sprog-i-pensum/#sprog-i-np","text":"SAT=\\{\\langle \\phi \\rangle \\mid \\phi \\text{ er en opfyldelig Boolsk formel }\\} SAT=\\{\\langle \\phi \\rangle \\mid \\phi \\text{ er en opfyldelig Boolsk formel }\\} SAT SAT er afg\u00f8rbart SAT\\in NP SAT\\in NP HAMPATH=\\{\\langle G,s,t\\rangle \\mid G \\text{ er en orienteret graf med en Hamilton-sti fra knude } s \\text{ til } t\\} HAMPATH=\\{\\langle G,s,t\\rangle \\mid G \\text{ er en orienteret graf med en Hamilton-sti fra knude } s \\text{ til } t\\} HAMPATH HAMPATH er afg\u00f8rbart COMPOSITE=\\{\\langle n \\rangle \\mid n\\in \\N, n \\text{ sammensat}\\} COMPOSITE=\\{\\langle n \\rangle \\mid n\\in \\N, n \\text{ sammensat}\\} COMPOSITE COMPOSITE er afg\u00f8rbart 3SAT = \\{\\langle \\phi \\rangle\\mid \\phi \\text{ er en opfyldelig 3CNF-formel}\\} 3SAT = \\{\\langle \\phi \\rangle\\mid \\phi \\text{ er en opfyldelig 3CNF-formel}\\} 3SAT 3SAT er afg\u00f8rbart 3SAT 3SAT er NP-fuldst\u00e6ndig VERTEX-COVER=\\{ \\langle G,k\\rangle\\mid G \\text{ er en ikke-orienteret graf med en k-knudeoverd\u00e6kning} \\} VERTEX-COVER=\\{ \\langle G,k\\rangle\\mid G \\text{ er en ikke-orienteret graf med en k-knudeoverd\u00e6kning} \\} Afg\u00f8rbart NP-fuldst\u00e6ndigt UHAMPATH=\\{\\langle G,s,t\\rangle \\mid G \\text{ er en ikke-orienteret graf med en Hamilton-sti fra knude } s \\text{ til } t\\} UHAMPATH=\\{\\langle G,s,t\\rangle \\mid G \\text{ er en ikke-orienteret graf med en Hamilton-sti fra knude } s \\text{ til } t\\} Afg\u00f8rbart NP-fuldst\u00e6ndigt SUBSET-SUM=\\left\\{\\langle S, k\\rangle\\ \\left|\\ \\begin{array}{} S \\text{ er en endelig m\u00e6ngde } S\\subseteq \\N,\\\\ \\text{ og der findes } S'\\subseteq S \\text{ s\u00e5 } \\sum_{x\\in S'} x=k,\\quad k\\in \\N \\end{array} \\right. \\right\\} SUBSET-SUM=\\left\\{\\langle S, k\\rangle\\ \\left|\\ \\begin{array}{} S \\text{ er en endelig m\u00e6ngde } S\\subseteq \\N,\\\\ \\text{ og der findes } S'\\subseteq S \\text{ s\u00e5 } \\sum_{x\\in S'} x=k,\\quad k\\in \\N \\end{array} \\right. \\right\\} Afg\u00f8rbart NP-fuldst\u00e6ndigt","title":"Sprog i NP"},{"location":"5-semester/MI/","text":"MI - Machine Intelligence \u00b6 Moodle side: https://www.moodle.aau.dk/course/view.php?id=31435","title":"Course"},{"location":"5-semester/MI/#mi-machine-intelligence","text":"Moodle side: https://www.moodle.aau.dk/course/view.php?id=31435","title":"MI - Machine Intelligence"},{"location":"5-semester/MI/09-04-introduction/","text":"Introduction \u00b6 Mondays 08:15-12: Exercises 8:15-10:00 (in the lecture room), lecture 10:15-12:00 Wednesdays 08:15-12:00 Extended Exercise sessions (group rooms) Written exam You can bring everything except computers The Book \u00b6 Online Version A foundational book Gives a basic understanding of the field Topics \u00b6 Introduction Problem solving as search Constrained satisfaction problems Logic-based knowledge representation Representing domains endowed with uncertainty Bayesian networks Machine Learning Planning Reinforced Learning Multi-agent systems","title":"Introduction"},{"location":"5-semester/MI/09-04-introduction/#introduction","text":"Mondays 08:15-12: Exercises 8:15-10:00 (in the lecture room), lecture 10:15-12:00 Wednesdays 08:15-12:00 Extended Exercise sessions (group rooms) Written exam You can bring everything except computers","title":"Introduction"},{"location":"5-semester/MI/09-04-introduction/#the-book","text":"Online Version A foundational book Gives a basic understanding of the field","title":"The Book"},{"location":"5-semester/MI/09-04-introduction/#topics","text":"Introduction Problem solving as search Constrained satisfaction problems Logic-based knowledge representation Representing domains endowed with uncertainty Bayesian networks Machine Learning Planning Reinforced Learning Multi-agent systems","title":"Topics"},{"location":"5-semester/MI/09-09-problem-solving-as-search/","text":"Problem Solving as Search \u00b6 Extra knowledge beyond the search space is called heuristic knowledge Problem Description \u00b6 We consider problems where an agent has a state-based representation of its environment can observe with certainty which state it is in has a certain goal it wants to achieve can execute actions that have definite effects (no uncertainty) Agent needs to find a sequence of actions leading to a goal state A state in which its goal is achieved Example \u00b6 Problem: re-arange tiles into goal configuration 362.880 states ( 9! 9! ) Actions: move_ up/down/left/right State-Space Problem \u00b6 Consists of: A set of states A subset of start states A set of actions (not all available at all states) An action function that for a given state s s and action a a returns the state reached when executing a a in s s A goal test that for any state s s returns the boolean value goal(s) goal(s) (true if s s is a goal state) (Optional) A cost function on actions (Optional) A value function on goal states A Solution consists of: For any given start state, a sequence of actions that lead to a goal state (optional) a sequence of actions with minimal cost (optional) a sequence of actions leading to a goal state with maximal value Graphs \u00b6 Se AD1 - Graph Theory Se PM, 3.3.1: Formalizing Graph Searching A directed graph consists of A set of nodes A set of arcs (ordered in pairs of nodes) Further terminology: n_2 n_2 is a neighbor of n_4 n_4 (not the other way round!) n_3, n_4, n_2, n_5 n_3, n_4, n_2, n_5 is a path from n_3 n_3 to n_5 n_5 n_2, n_5, n_4, n_2 n_2, n_5, n_4, n_2 is a path that is a cycle a graph is acyclic if it has no cycles Example 1 \u00b6 Nodes: states Arcs: possible state-transitions from actions (can be labeled with actions) Example 2 \u00b6 Graph Search \u00b6 A state-space problem can be solved by searching in the state-space graph for paths from start states to a goal state Does not require the whole graph at once Search may only locally generate neighbors of currently visited node From Graph to Search Tree \u00b6 Search tree represents how we can navigate the state-space graph Red nodes are nodes that we can explore from where we are now ( frontier or fringe ) Generic Search Algorithm Depth-first Search \u00b6 AD1 - Depth-First Search Properties Space used is linear in the length of the current path May not terminate if state-space graph has cycles With a forward branching factor bounded by b and depth n , the worst-case time complexity of a finite tree is b^n b^n Breadth-First Search \u00b6 AD1 - Breadth-First Search Properties Will always find a solution if one exists Size of frontier always increases during search up to order of magnitude of total size of search tree Can be adapted to find a minimum cost path Problem With Cost Function \u00b6 Assume that for each action at each state we have an associated cost The cost of a solution is the sum of the costs of all actions on the path from start to goal A minimum cost solution is a solution with minimal cost Example Breadth-first finds the shortests , but not the cheapest solution Depth-first may find either, depending on order of neighbor enumeration Lowest-Cost-First Search \u00b6 Simple modification of generic. With each path in frontier store the cost of path Modify one line of code select and remove path <n_0,\\dots,n_k> <n_0,\\dots,n_k> $\\color{red}\\text{with minimal cost} $ from Frontier Properties If all actions have non-zero cost, and solution exists, a minimal cost solution will be found Space requirement depends on cost structure, but usually similar to breadth-first Iterative Deepening Search \u00b6 Goal: Termination guarantee of breadth-first seach Space efficiency of depth-first search Algorithm Set a k k -value, do depth-first search till k k layers deep. Increase k k , repeat Properties : Has desired termination and space efficiency properties Duplicates computations (depth-bounded search k k repeats computations of depth-bounded search k-1 k-1 ). Not as problematic as it looks: constant overhead of ( b/(b-1) b/(b-1) ) Uninformed Search \u00b6 3.5 Uninformed Search Strategies Depth-first, Breadth-first and Iterative deepening are uninformed search strategies: They do not assume/use any knowledge of the search space exept the pure graph structure. Informed Search \u00b6 Actual Cost Given a cost function on actions, can define for any node n in the search tree: opt(n) = cost of optimal path from n to a goal state Infinite if no path to goal exists opt function can usually not be computed opt(n) only depends on the state at node n Heuristic Function A heuristic function h(n) h(n) takes a node n n and returns a non-negative real number. This number is an estimate of the cost of the least-cost path from node n n to a goal node h(n)\\leq opt(n) h(n)\\leq opt(n) h(n) h(n) is an admissible heuristic if h(n) h(n) is always less than or equal to the actual cost. An example could be that h(n) h(n) could be the cost if we could move through walls. Simple use of heuristic function is heuristic depth-first search . Selects the locally best path, but explores all paths from that before it selects another path. Often used, but suffers the problems of depth-first search. Another use is greedy best-first search Always select a path on the frontier with the lowest heuristic value. Sometimes work well, but sometimes uses paths that looks promising in the beginning. Example \u00b6 \" Consider the graph shown in Figure 3.9 , drawn to scale, where the cost of an arc is its length. The aim is to find the shortest path from s s to g g . Suppose the Euclidean straight line distance to the goal g g is used as the heuristic function. A heuristic depth-first search will select the node below s s and will never terminate. Similarly, because all of the nodes below s s look good, a greedy best-first search will cycle between them, never trying an alternate route from g g . \" A* Search \u00b6 3.6.1 A* Search Uses both path cost (as lowest-cost-first), and heuristic information (as greedy best-first search). For each path on the frontier, A* uses an estimate of the total path cost from the start node to a goal node constrained to follow that path initially. Uses cost(p) cost(p) , the cost of the path found As well as h(p) h(p) , the estimated cost from the end of p p to the goal. For any path p p on the frontier, define: $$ f(p)=cost(p)+h(p) $$ This is an estimate of the total path cost to follow path p p then go to a goal node. If n n is the node at the end of path p p , this can be depicted as: \\underbrace{\\underbrace{\\text{start}\\ \\underrightarrow{\\text{actual}}}_{cost(p)} \\ n\\ \\underbrace{\\underrightarrow{\\text{estimate}} \\ \\text{goal}}_{h(p)}}_{f(p)} \\underbrace{\\underbrace{\\text{start}\\ \\underrightarrow{\\text{actual}}}_{cost(p)} \\ n\\ \\underbrace{\\underrightarrow{\\text{estimate}} \\ \\text{goal}}_{h(p)}}_{f(p)} If h(n) h(n) is an admissible heuristic, and so never overestimates the cost from node n n to a goal node, then f(p) f(p) does not overestimate the path cost of going from the start node to a goal node via p p A^* A^* is implemented using the Generic search algorithm , treating the frontier as a priority queue ordered by f(p) f(p) Example 3.15 Admissible \u00b6 A search algorithm is admissible if, whenever a solution exists, it returns an optimal solution. A* Admissiblity \u00b6 Proposition 3.1 If there is a solution, A* A* using heuristic function h h always returns an optimal solution if: The branching factor is finite (each node has a bounded number of neighbors) All arc costs are greater than some \\epsilon > 0 \\epsilon > 0 h h is an admissible heuristic, meaning that h(n) h(n) is less than or equal to the actual cost of the lowest-cost path from node n n to a goal node. See link for proof. Dynamic Programming \u00b6 For statically stored graphs, build a table of dist(n) the actual distance of the shortest path from node n to a goal. This can be built backwards from the goal: \\begin{align*} dist(n)=\\left\\{ \\begin{array} \\ 0 &\\text{if } is\\_goal(n) \\\\ \\min_{\\langle n,m\\rangle\\in A}(|\\langle n,m\\rangle|+dist(m)) & \\text{otherwise} \\end{array} \\right. \\end{align*} \\begin{align*} dist(n)=\\left\\{ \\begin{array} \\ 0 &\\text{if } is\\_goal(n) \\\\ \\min_{\\langle n,m\\rangle\\in A}(|\\langle n,m\\rangle|+dist(m)) & \\text{otherwise} \\end{array} \\right. \\end{align*} Example Two main problems: You need enough space to store the graph The dist function needs to be recomputed for each goal Pruning the Search Space \u00b6 3.7.1 Cycle Pruning A seacher can prune a path that ends in a node already on the path, without removing an optimal solution Using depth-first methods, with the graph explicitly stored, this can be done in constant time. For other methods, the cost ins linear in path length","title":"Problem Solving as Search"},{"location":"5-semester/MI/09-09-problem-solving-as-search/#problem-solving-as-search","text":"Extra knowledge beyond the search space is called heuristic knowledge","title":"Problem Solving as Search"},{"location":"5-semester/MI/09-09-problem-solving-as-search/#problem-description","text":"We consider problems where an agent has a state-based representation of its environment can observe with certainty which state it is in has a certain goal it wants to achieve can execute actions that have definite effects (no uncertainty) Agent needs to find a sequence of actions leading to a goal state A state in which its goal is achieved","title":"Problem Description"},{"location":"5-semester/MI/09-09-problem-solving-as-search/#example","text":"Problem: re-arange tiles into goal configuration 362.880 states ( 9! 9! ) Actions: move_ up/down/left/right","title":"Example"},{"location":"5-semester/MI/09-09-problem-solving-as-search/#state-space-problem","text":"Consists of: A set of states A subset of start states A set of actions (not all available at all states) An action function that for a given state s s and action a a returns the state reached when executing a a in s s A goal test that for any state s s returns the boolean value goal(s) goal(s) (true if s s is a goal state) (Optional) A cost function on actions (Optional) A value function on goal states A Solution consists of: For any given start state, a sequence of actions that lead to a goal state (optional) a sequence of actions with minimal cost (optional) a sequence of actions leading to a goal state with maximal value","title":"State-Space Problem"},{"location":"5-semester/MI/09-09-problem-solving-as-search/#graphs","text":"Se AD1 - Graph Theory Se PM, 3.3.1: Formalizing Graph Searching A directed graph consists of A set of nodes A set of arcs (ordered in pairs of nodes) Further terminology: n_2 n_2 is a neighbor of n_4 n_4 (not the other way round!) n_3, n_4, n_2, n_5 n_3, n_4, n_2, n_5 is a path from n_3 n_3 to n_5 n_5 n_2, n_5, n_4, n_2 n_2, n_5, n_4, n_2 is a path that is a cycle a graph is acyclic if it has no cycles","title":"Graphs"},{"location":"5-semester/MI/09-09-problem-solving-as-search/#example-1","text":"Nodes: states Arcs: possible state-transitions from actions (can be labeled with actions)","title":"Example 1"},{"location":"5-semester/MI/09-09-problem-solving-as-search/#example-2","text":"","title":"Example 2"},{"location":"5-semester/MI/09-09-problem-solving-as-search/#graph-search","text":"A state-space problem can be solved by searching in the state-space graph for paths from start states to a goal state Does not require the whole graph at once Search may only locally generate neighbors of currently visited node","title":"Graph Search"},{"location":"5-semester/MI/09-09-problem-solving-as-search/#from-graph-to-search-tree","text":"Search tree represents how we can navigate the state-space graph Red nodes are nodes that we can explore from where we are now ( frontier or fringe ) Generic Search Algorithm","title":"From Graph to Search Tree"},{"location":"5-semester/MI/09-09-problem-solving-as-search/#depth-first-search","text":"AD1 - Depth-First Search Properties Space used is linear in the length of the current path May not terminate if state-space graph has cycles With a forward branching factor bounded by b and depth n , the worst-case time complexity of a finite tree is b^n b^n","title":"Depth-first Search"},{"location":"5-semester/MI/09-09-problem-solving-as-search/#breadth-first-search","text":"AD1 - Breadth-First Search Properties Will always find a solution if one exists Size of frontier always increases during search up to order of magnitude of total size of search tree Can be adapted to find a minimum cost path","title":"Breadth-First Search"},{"location":"5-semester/MI/09-09-problem-solving-as-search/#problem-with-cost-function","text":"Assume that for each action at each state we have an associated cost The cost of a solution is the sum of the costs of all actions on the path from start to goal A minimum cost solution is a solution with minimal cost Example Breadth-first finds the shortests , but not the cheapest solution Depth-first may find either, depending on order of neighbor enumeration","title":"Problem With Cost Function"},{"location":"5-semester/MI/09-09-problem-solving-as-search/#lowest-cost-first-search","text":"Simple modification of generic. With each path in frontier store the cost of path Modify one line of code select and remove path <n_0,\\dots,n_k> <n_0,\\dots,n_k> $\\color{red}\\text{with minimal cost} $ from Frontier Properties If all actions have non-zero cost, and solution exists, a minimal cost solution will be found Space requirement depends on cost structure, but usually similar to breadth-first","title":"Lowest-Cost-First Search"},{"location":"5-semester/MI/09-09-problem-solving-as-search/#iterative-deepening-search","text":"Goal: Termination guarantee of breadth-first seach Space efficiency of depth-first search Algorithm Set a k k -value, do depth-first search till k k layers deep. Increase k k , repeat Properties : Has desired termination and space efficiency properties Duplicates computations (depth-bounded search k k repeats computations of depth-bounded search k-1 k-1 ). Not as problematic as it looks: constant overhead of ( b/(b-1) b/(b-1) )","title":"Iterative Deepening Search"},{"location":"5-semester/MI/09-09-problem-solving-as-search/#uninformed-search","text":"3.5 Uninformed Search Strategies Depth-first, Breadth-first and Iterative deepening are uninformed search strategies: They do not assume/use any knowledge of the search space exept the pure graph structure.","title":"Uninformed Search"},{"location":"5-semester/MI/09-09-problem-solving-as-search/#informed-search","text":"Actual Cost Given a cost function on actions, can define for any node n in the search tree: opt(n) = cost of optimal path from n to a goal state Infinite if no path to goal exists opt function can usually not be computed opt(n) only depends on the state at node n Heuristic Function A heuristic function h(n) h(n) takes a node n n and returns a non-negative real number. This number is an estimate of the cost of the least-cost path from node n n to a goal node h(n)\\leq opt(n) h(n)\\leq opt(n) h(n) h(n) is an admissible heuristic if h(n) h(n) is always less than or equal to the actual cost. An example could be that h(n) h(n) could be the cost if we could move through walls. Simple use of heuristic function is heuristic depth-first search . Selects the locally best path, but explores all paths from that before it selects another path. Often used, but suffers the problems of depth-first search. Another use is greedy best-first search Always select a path on the frontier with the lowest heuristic value. Sometimes work well, but sometimes uses paths that looks promising in the beginning.","title":"Informed Search"},{"location":"5-semester/MI/09-09-problem-solving-as-search/#example_1","text":"\" Consider the graph shown in Figure 3.9 , drawn to scale, where the cost of an arc is its length. The aim is to find the shortest path from s s to g g . Suppose the Euclidean straight line distance to the goal g g is used as the heuristic function. A heuristic depth-first search will select the node below s s and will never terminate. Similarly, because all of the nodes below s s look good, a greedy best-first search will cycle between them, never trying an alternate route from g g . \"","title":"Example"},{"location":"5-semester/MI/09-09-problem-solving-as-search/#a-search","text":"3.6.1 A* Search Uses both path cost (as lowest-cost-first), and heuristic information (as greedy best-first search). For each path on the frontier, A* uses an estimate of the total path cost from the start node to a goal node constrained to follow that path initially. Uses cost(p) cost(p) , the cost of the path found As well as h(p) h(p) , the estimated cost from the end of p p to the goal. For any path p p on the frontier, define: $$ f(p)=cost(p)+h(p) $$ This is an estimate of the total path cost to follow path p p then go to a goal node. If n n is the node at the end of path p p , this can be depicted as: \\underbrace{\\underbrace{\\text{start}\\ \\underrightarrow{\\text{actual}}}_{cost(p)} \\ n\\ \\underbrace{\\underrightarrow{\\text{estimate}} \\ \\text{goal}}_{h(p)}}_{f(p)} \\underbrace{\\underbrace{\\text{start}\\ \\underrightarrow{\\text{actual}}}_{cost(p)} \\ n\\ \\underbrace{\\underrightarrow{\\text{estimate}} \\ \\text{goal}}_{h(p)}}_{f(p)} If h(n) h(n) is an admissible heuristic, and so never overestimates the cost from node n n to a goal node, then f(p) f(p) does not overestimate the path cost of going from the start node to a goal node via p p A^* A^* is implemented using the Generic search algorithm , treating the frontier as a priority queue ordered by f(p) f(p) Example 3.15","title":"A* Search"},{"location":"5-semester/MI/09-09-problem-solving-as-search/#admissible","text":"A search algorithm is admissible if, whenever a solution exists, it returns an optimal solution.","title":"Admissible"},{"location":"5-semester/MI/09-09-problem-solving-as-search/#a-admissiblity","text":"Proposition 3.1 If there is a solution, A* A* using heuristic function h h always returns an optimal solution if: The branching factor is finite (each node has a bounded number of neighbors) All arc costs are greater than some \\epsilon > 0 \\epsilon > 0 h h is an admissible heuristic, meaning that h(n) h(n) is less than or equal to the actual cost of the lowest-cost path from node n n to a goal node. See link for proof.","title":"A* Admissiblity"},{"location":"5-semester/MI/09-09-problem-solving-as-search/#dynamic-programming","text":"For statically stored graphs, build a table of dist(n) the actual distance of the shortest path from node n to a goal. This can be built backwards from the goal: \\begin{align*} dist(n)=\\left\\{ \\begin{array} \\ 0 &\\text{if } is\\_goal(n) \\\\ \\min_{\\langle n,m\\rangle\\in A}(|\\langle n,m\\rangle|+dist(m)) & \\text{otherwise} \\end{array} \\right. \\end{align*} \\begin{align*} dist(n)=\\left\\{ \\begin{array} \\ 0 &\\text{if } is\\_goal(n) \\\\ \\min_{\\langle n,m\\rangle\\in A}(|\\langle n,m\\rangle|+dist(m)) & \\text{otherwise} \\end{array} \\right. \\end{align*} Example Two main problems: You need enough space to store the graph The dist function needs to be recomputed for each goal","title":"Dynamic Programming"},{"location":"5-semester/MI/09-09-problem-solving-as-search/#pruning-the-search-space","text":"3.7.1 Cycle Pruning A seacher can prune a path that ends in a node already on the path, without removing an optimal solution Using depth-first methods, with the graph explicitly stored, this can be done in constant time. For other methods, the cost ins linear in path length","title":"Pruning the Search Space"},{"location":"5-semester/MI/09-16-constrains-satisfaction-problems/","text":"Constraints Satisfaction Problems \u00b6 Features and Variables \u00b6 Describing the world (environment) by features: A possible world for a set of variables is an assignment of a value to each variable. Example (Cooking) : Constraint Satisfaction Problems \u00b6 A constraint is a condition on the values of variables in a possible world. Can be specified with: Extensional Constraint Specification \u00b6 Explicitly list all allowed (or disallowed) combination of values: Not on the list of allowed possible worlds: Intensional Constraint Specification \u00b6 Use logical expressions: \\begin{align*} Teacher\\_AD=Teacher\\_MI \\to Time\\_AD \\neq Time\\_MI\\\\ Time\\_AD = Time\\_MI \\to Room\\_AD \\neq Room\\_MI \\end{align*} \\begin{align*} Teacher\\_AD=Teacher\\_MI \\to Time\\_AD \\neq Time\\_MI\\\\ Time\\_AD = Time\\_MI \\to Room\\_AD \\neq Room\\_MI \\end{align*} If teacher for AD and MI is the same, then the time of AD cannot be the same as time of MI. Example: Sudoku \u00b6 Constraints: Definition \u00b6 A Constraint Satisfaction Problem (CSP) is given by a set of variables a set of constraints (usually intensional) A solution to a CSP consists of a possible world that satisfies all the constraints (also called a model of the constraints) CSP as State Space Problem \u00b6 A CSP can be represented as a state space problem: States are all partial assignments of values to variables that are consistent with the constraints For a state s s : select some variable V V not assigned a value in s s , and let the neighbors of s s be all states that assign a value to V V (if any exist) The start state is the state that does not assign any values A goal state is a state that assigns values to all variables Solving the CSP \u00b6 A solution to the state space problem is a path with a goal state at the end: A solution to the CSP problem To solve the state space problem need only be able to: enumerate all partial assignments that assign a value to one or more variable than s s check whether a partial assignment is consistent with the constraints (That is sufficient to implement the get_neighbors and goal functions needed in the generic search algorithm) Example \u00b6 Consistency Algorithms \u00b6 Idea Constraint Network \u00b6 The constraint network for a CSP consists of: 1 (oval) node for each variable X X 1 (rectangular) node for each constraint c c An (undirected) arc \\langle X,c \\rangle \\langle X,c \\rangle between every constraint and every variable involved in the constraint With each variable node X X is associated a (reduced) domain D_X D_X : Initially the domain of the variable Reduced by successively deleting values that cannot be part of a solution Arc Consistency \u00b6 An arc \\langle X,c \\rangle \\langle X,c \\rangle is arc consistent , if For all x\\in D_X x\\in D_X there exists values y_i,...,y_k y_i,...,y_k for the other variables involved in c c , such that x,y_i,...,y_k x,y_i,...,y_k is consistent with c c A constraint network is arc consistent , if all its arcs are arc consistent Examples Algorithm Outline \u00b6 Example: Slide 18 ( Appendix of this page ) Algorithm Outcomes Algorithm is guaranteed to terminate. Result independent of order in which arcs are processed. Possible cases at termination: D_X=\\empty D_X=\\empty for some X: X: CSP has no solution D_X D_X contains exactly one value for each X: X: CSP has unique solution, given by the D_X D_X values. Other If the CSP has a solution, then the solution can only consist of current D_X D_X values Variable Elimination \u00b6 Simplify problem by eliminating variables Operates on extensional (table) representations of constraints Algorithm requires projection and join operations on tables Project \u00b6 Join \u00b6 Given two tables r_1, r_2 r_1, r_2 for variables vars_1,vars_2 vars_1,vars_2 . The join is the table r_3=r_1 \\bowtie r_2 r_3=r_1 \\bowtie r_2 for variables vars_1 \\cup vars_2 vars_1 \\cup vars_2 that contains all tuples, which restricted to vars_1 vars_1 are in r_1 r_1 , and restricted to vars_2 vars_2 are in r_2 r_2 Example \u00b6 Algorithm Outline \u00b6 Example slide 24 ( Appendix of this page ) Properties \u00b6 The algorithm terminates The CSP has a solution if and only if the final constraint is non-empty The set of all solutions can be generated by joining the final constraint with the intermediate \"summarizing\" constraint generated in line 5. Algorithm operates on extensional constraint representations, therefore constraints must not contain too many tuples (initial and constructed constraints) Worst case: VE is not more efficient than enumerating all possible worlds and checking whether they are solutions Constraint Graph \u00b6 Consider the graph where there is one node for each variable two variables are connected when they appear together in one constraint Then VE will work better if the constraint graph is sparsely connected! Local Search \u00b6 So far all methods systematically explored the state space (possible worlds) Problem: Time and space when search space is large Local Search approach: Explore state space without \"bookkeeping\" (where have we been, and what needs to be explored?) no success/terminatiuon guarantees in practice, often the only thing that works Another state space graph representation for CSPs: Nodes are possible worlds Neighbors are possible worlds that differ in the value of exactly one variable Algorithm Outline \u00b6 1 2 3 select some node in state space graph as current_state while current_state is not a solution current_state = some neighbor of current_state Random Search \u00b6 Make choices in line 1. and 3. completely random \"Random Walk\" Unlikely to find a solution if state space is large with only a few solutions Greedy Search \u00b6 AKA Hill Climbing. Use an evaluation function on states Example: number of constraints not satisfied by state Always choose neighbor with minimal evaluation function value Terminates when all neighbors have higher value than current state (Current state is a local minimum ) Possible greedy search paths starting from different states: Problem \u00b6 Search terminates with local minimum of evaluation function. This may not be a solution to the CSP Solution Approaches \u00b6 Random restarts repeat greedy search with several randomly chosen initial states Random moves combine greedy moves with random steps Examples a) Small number of random restarts will find global minimum b) Make random move when local minimum reached Local Seach \u00b6 Maintain an assignment of a value to each variable At each step, select a \"neighbor\" of the current assignment (e.g. one that improves some heuristic value) Stop when a satisfying assignement is found, or return the best assignment found Requires What is a neighbor? Which neighbor should be selected? Most Improving Step \u00b6 Select the variable-value pair that gives the highest improvement Maintain a priority queue with variable-value pairs not part of the current assignment Weight\\langle X,v \\rangle=eval(current\\ assignment)-eval(current\\ assignment\\ but\\ with\\ X=v) Weight\\langle X,v \\rangle=eval(current\\ assignment)-eval(current\\ assignment\\ but\\ with\\ X=v) If X X is given a new value, update the weight of all pairs participating in a changed constraint Two-Stage Choice \u00b6 Choose variable Choose state Data Structure Maintain priority queue of variables; weight is the number of participating conflicts After selecting a variable, pick the value minimizes the number of conflicts Update weights of variables that participate in a conflict that is changed Simulated Annealing \u00b6 Algorithm Pick a variable at random and a new value at random If it is an improvement, adopt it. If it isnt an improvement, adopt it probabilistically depending on a temperature paramenter, T T With current assignment n n and proposed assignment n' n' we move to n' n' with probability: e^{(h(n')-h(n))/T} e^{(h(n')-h(n))/T} Reduce the temperature Probability of Accepting a Change Propositional Logic Basics \u00b6 Provides a formal language for representing constraints on binary variables Syntax \u00b6 Atomic Propositions \u00b6 Convention: Start with lowercase letter Propositions \u00b6 A set of propositions is also called a Knowledge Base Example \u00b6 \"If it rains I'll take my umbrella , or I'll stay home \" rains\\to(umbrella \\or home) rains\\to(umbrella \\or home) Semantics \u00b6 Interpretation \u00b6 An interpretation \\pi \\pi for a set of atomic propositions a_1,a_2,...,a_n a_1,a_2,...,a_n is an assignment of a truth value to each proposition Equal to possible world when atomic propositions seen as boolean variables \\pi(a_i)\\in\\{true,false\\} \\pi(a_i)\\in\\{true,false\\} An interpretation defines a thruth value for all propositions Models \u00b6 A model of a proposition (knowledge base) is an interpretation in which the proposition is true Propositions as constraints: a model is a possible world that satisfies the constraint Logical Consequence \u00b6 A proposition g g is a logical consequence of a knowledge base KB, if every model of KB is a model of g g KB \\models g KB \\models g (Whenever KB is true, then g is also true) Example \u00b6 KB=\\{man\\to mortal,man\\} KB=\\{man\\to mortal,man\\} then: KB\\models mortal KB\\models mortal","title":"Constraints Satisfaction Problems"},{"location":"5-semester/MI/09-16-constrains-satisfaction-problems/#constraints-satisfaction-problems","text":"","title":"Constraints Satisfaction Problems"},{"location":"5-semester/MI/09-16-constrains-satisfaction-problems/#features-and-variables","text":"Describing the world (environment) by features: A possible world for a set of variables is an assignment of a value to each variable. Example (Cooking) :","title":"Features and Variables"},{"location":"5-semester/MI/09-16-constrains-satisfaction-problems/#constraint-satisfaction-problems","text":"A constraint is a condition on the values of variables in a possible world. Can be specified with:","title":"Constraint Satisfaction Problems"},{"location":"5-semester/MI/09-16-constrains-satisfaction-problems/#extensional-constraint-specification","text":"Explicitly list all allowed (or disallowed) combination of values: Not on the list of allowed possible worlds:","title":"Extensional Constraint Specification"},{"location":"5-semester/MI/09-16-constrains-satisfaction-problems/#intensional-constraint-specification","text":"Use logical expressions: \\begin{align*} Teacher\\_AD=Teacher\\_MI \\to Time\\_AD \\neq Time\\_MI\\\\ Time\\_AD = Time\\_MI \\to Room\\_AD \\neq Room\\_MI \\end{align*} \\begin{align*} Teacher\\_AD=Teacher\\_MI \\to Time\\_AD \\neq Time\\_MI\\\\ Time\\_AD = Time\\_MI \\to Room\\_AD \\neq Room\\_MI \\end{align*} If teacher for AD and MI is the same, then the time of AD cannot be the same as time of MI.","title":"Intensional Constraint Specification"},{"location":"5-semester/MI/09-16-constrains-satisfaction-problems/#example-sudoku","text":"Constraints:","title":"Example: Sudoku"},{"location":"5-semester/MI/09-16-constrains-satisfaction-problems/#definition","text":"A Constraint Satisfaction Problem (CSP) is given by a set of variables a set of constraints (usually intensional) A solution to a CSP consists of a possible world that satisfies all the constraints (also called a model of the constraints)","title":"Definition"},{"location":"5-semester/MI/09-16-constrains-satisfaction-problems/#csp-as-state-space-problem","text":"A CSP can be represented as a state space problem: States are all partial assignments of values to variables that are consistent with the constraints For a state s s : select some variable V V not assigned a value in s s , and let the neighbors of s s be all states that assign a value to V V (if any exist) The start state is the state that does not assign any values A goal state is a state that assigns values to all variables","title":"CSP as State Space Problem"},{"location":"5-semester/MI/09-16-constrains-satisfaction-problems/#solving-the-csp","text":"A solution to the state space problem is a path with a goal state at the end: A solution to the CSP problem To solve the state space problem need only be able to: enumerate all partial assignments that assign a value to one or more variable than s s check whether a partial assignment is consistent with the constraints (That is sufficient to implement the get_neighbors and goal functions needed in the generic search algorithm)","title":"Solving the CSP"},{"location":"5-semester/MI/09-16-constrains-satisfaction-problems/#example","text":"","title":"Example"},{"location":"5-semester/MI/09-16-constrains-satisfaction-problems/#consistency-algorithms","text":"Idea","title":"Consistency Algorithms"},{"location":"5-semester/MI/09-16-constrains-satisfaction-problems/#constraint-network","text":"The constraint network for a CSP consists of: 1 (oval) node for each variable X X 1 (rectangular) node for each constraint c c An (undirected) arc \\langle X,c \\rangle \\langle X,c \\rangle between every constraint and every variable involved in the constraint With each variable node X X is associated a (reduced) domain D_X D_X : Initially the domain of the variable Reduced by successively deleting values that cannot be part of a solution","title":"Constraint Network"},{"location":"5-semester/MI/09-16-constrains-satisfaction-problems/#arc-consistency","text":"An arc \\langle X,c \\rangle \\langle X,c \\rangle is arc consistent , if For all x\\in D_X x\\in D_X there exists values y_i,...,y_k y_i,...,y_k for the other variables involved in c c , such that x,y_i,...,y_k x,y_i,...,y_k is consistent with c c A constraint network is arc consistent , if all its arcs are arc consistent Examples","title":"Arc Consistency"},{"location":"5-semester/MI/09-16-constrains-satisfaction-problems/#algorithm-outline","text":"Example: Slide 18 ( Appendix of this page ) Algorithm Outcomes Algorithm is guaranteed to terminate. Result independent of order in which arcs are processed. Possible cases at termination: D_X=\\empty D_X=\\empty for some X: X: CSP has no solution D_X D_X contains exactly one value for each X: X: CSP has unique solution, given by the D_X D_X values. Other If the CSP has a solution, then the solution can only consist of current D_X D_X values","title":"Algorithm Outline"},{"location":"5-semester/MI/09-16-constrains-satisfaction-problems/#variable-elimination","text":"Simplify problem by eliminating variables Operates on extensional (table) representations of constraints Algorithm requires projection and join operations on tables","title":"Variable Elimination"},{"location":"5-semester/MI/09-16-constrains-satisfaction-problems/#project","text":"","title":"Project"},{"location":"5-semester/MI/09-16-constrains-satisfaction-problems/#join","text":"Given two tables r_1, r_2 r_1, r_2 for variables vars_1,vars_2 vars_1,vars_2 . The join is the table r_3=r_1 \\bowtie r_2 r_3=r_1 \\bowtie r_2 for variables vars_1 \\cup vars_2 vars_1 \\cup vars_2 that contains all tuples, which restricted to vars_1 vars_1 are in r_1 r_1 , and restricted to vars_2 vars_2 are in r_2 r_2","title":"Join"},{"location":"5-semester/MI/09-16-constrains-satisfaction-problems/#example_1","text":"","title":"Example"},{"location":"5-semester/MI/09-16-constrains-satisfaction-problems/#algorithm-outline_1","text":"Example slide 24 ( Appendix of this page )","title":"Algorithm Outline"},{"location":"5-semester/MI/09-16-constrains-satisfaction-problems/#properties","text":"The algorithm terminates The CSP has a solution if and only if the final constraint is non-empty The set of all solutions can be generated by joining the final constraint with the intermediate \"summarizing\" constraint generated in line 5. Algorithm operates on extensional constraint representations, therefore constraints must not contain too many tuples (initial and constructed constraints) Worst case: VE is not more efficient than enumerating all possible worlds and checking whether they are solutions","title":"Properties"},{"location":"5-semester/MI/09-16-constrains-satisfaction-problems/#constraint-graph","text":"Consider the graph where there is one node for each variable two variables are connected when they appear together in one constraint Then VE will work better if the constraint graph is sparsely connected!","title":"Constraint Graph"},{"location":"5-semester/MI/09-16-constrains-satisfaction-problems/#local-search","text":"So far all methods systematically explored the state space (possible worlds) Problem: Time and space when search space is large Local Search approach: Explore state space without \"bookkeeping\" (where have we been, and what needs to be explored?) no success/terminatiuon guarantees in practice, often the only thing that works Another state space graph representation for CSPs: Nodes are possible worlds Neighbors are possible worlds that differ in the value of exactly one variable","title":"Local Search"},{"location":"5-semester/MI/09-16-constrains-satisfaction-problems/#algorithm-outline_2","text":"1 2 3 select some node in state space graph as current_state while current_state is not a solution current_state = some neighbor of current_state","title":"Algorithm Outline"},{"location":"5-semester/MI/09-16-constrains-satisfaction-problems/#random-search","text":"Make choices in line 1. and 3. completely random \"Random Walk\" Unlikely to find a solution if state space is large with only a few solutions","title":"Random Search"},{"location":"5-semester/MI/09-16-constrains-satisfaction-problems/#greedy-search","text":"AKA Hill Climbing. Use an evaluation function on states Example: number of constraints not satisfied by state Always choose neighbor with minimal evaluation function value Terminates when all neighbors have higher value than current state (Current state is a local minimum ) Possible greedy search paths starting from different states:","title":"Greedy Search"},{"location":"5-semester/MI/09-16-constrains-satisfaction-problems/#problem","text":"Search terminates with local minimum of evaluation function. This may not be a solution to the CSP","title":"Problem"},{"location":"5-semester/MI/09-16-constrains-satisfaction-problems/#solution-approaches","text":"Random restarts repeat greedy search with several randomly chosen initial states Random moves combine greedy moves with random steps Examples a) Small number of random restarts will find global minimum b) Make random move when local minimum reached","title":"Solution Approaches"},{"location":"5-semester/MI/09-16-constrains-satisfaction-problems/#local-seach","text":"Maintain an assignment of a value to each variable At each step, select a \"neighbor\" of the current assignment (e.g. one that improves some heuristic value) Stop when a satisfying assignement is found, or return the best assignment found Requires What is a neighbor? Which neighbor should be selected?","title":"Local Seach"},{"location":"5-semester/MI/09-16-constrains-satisfaction-problems/#most-improving-step","text":"Select the variable-value pair that gives the highest improvement Maintain a priority queue with variable-value pairs not part of the current assignment Weight\\langle X,v \\rangle=eval(current\\ assignment)-eval(current\\ assignment\\ but\\ with\\ X=v) Weight\\langle X,v \\rangle=eval(current\\ assignment)-eval(current\\ assignment\\ but\\ with\\ X=v) If X X is given a new value, update the weight of all pairs participating in a changed constraint","title":"Most Improving Step"},{"location":"5-semester/MI/09-16-constrains-satisfaction-problems/#two-stage-choice","text":"Choose variable Choose state Data Structure Maintain priority queue of variables; weight is the number of participating conflicts After selecting a variable, pick the value minimizes the number of conflicts Update weights of variables that participate in a conflict that is changed","title":"Two-Stage Choice"},{"location":"5-semester/MI/09-16-constrains-satisfaction-problems/#simulated-annealing","text":"Algorithm Pick a variable at random and a new value at random If it is an improvement, adopt it. If it isnt an improvement, adopt it probabilistically depending on a temperature paramenter, T T With current assignment n n and proposed assignment n' n' we move to n' n' with probability: e^{(h(n')-h(n))/T} e^{(h(n')-h(n))/T} Reduce the temperature Probability of Accepting a Change","title":"Simulated Annealing"},{"location":"5-semester/MI/09-16-constrains-satisfaction-problems/#propositional-logic-basics","text":"Provides a formal language for representing constraints on binary variables","title":"Propositional Logic Basics"},{"location":"5-semester/MI/09-16-constrains-satisfaction-problems/#syntax","text":"","title":"Syntax"},{"location":"5-semester/MI/09-16-constrains-satisfaction-problems/#atomic-propositions","text":"Convention: Start with lowercase letter","title":"Atomic Propositions"},{"location":"5-semester/MI/09-16-constrains-satisfaction-problems/#propositions","text":"A set of propositions is also called a Knowledge Base","title":"Propositions"},{"location":"5-semester/MI/09-16-constrains-satisfaction-problems/#example_2","text":"\"If it rains I'll take my umbrella , or I'll stay home \" rains\\to(umbrella \\or home) rains\\to(umbrella \\or home)","title":"Example"},{"location":"5-semester/MI/09-16-constrains-satisfaction-problems/#semantics","text":"","title":"Semantics"},{"location":"5-semester/MI/09-16-constrains-satisfaction-problems/#interpretation","text":"An interpretation \\pi \\pi for a set of atomic propositions a_1,a_2,...,a_n a_1,a_2,...,a_n is an assignment of a truth value to each proposition Equal to possible world when atomic propositions seen as boolean variables \\pi(a_i)\\in\\{true,false\\} \\pi(a_i)\\in\\{true,false\\} An interpretation defines a thruth value for all propositions","title":"Interpretation"},{"location":"5-semester/MI/09-16-constrains-satisfaction-problems/#models","text":"A model of a proposition (knowledge base) is an interpretation in which the proposition is true Propositions as constraints: a model is a possible world that satisfies the constraint","title":"Models"},{"location":"5-semester/MI/09-16-constrains-satisfaction-problems/#logical-consequence","text":"A proposition g g is a logical consequence of a knowledge base KB, if every model of KB is a model of g g KB \\models g KB \\models g (Whenever KB is true, then g is also true)","title":"Logical Consequence"},{"location":"5-semester/MI/09-16-constrains-satisfaction-problems/#example_3","text":"KB=\\{man\\to mortal,man\\} KB=\\{man\\to mortal,man\\} then: KB\\models mortal KB\\models mortal","title":"Example"},{"location":"5-semester/MI/09-16appendix/","text":"Constraints Satisfaction Problems - Appendix \u00b6 Generalized Arc Consistency Algorithm Example \u00b6 Variable Elimination Example \u00b6","title":"Constraints Satisfaction Problems - Appendix"},{"location":"5-semester/MI/09-16appendix/#constraints-satisfaction-problems-appendix","text":"","title":"Constraints Satisfaction Problems - Appendix"},{"location":"5-semester/MI/09-16appendix/#generalized-arc-consistency-algorithm-example","text":"","title":"Generalized Arc Consistency Algorithm Example"},{"location":"5-semester/MI/09-16appendix/#variable-elimination-example","text":"","title":"Variable Elimination Example"},{"location":"5-semester/MI/09-23-reasoning-under-certainty/","text":"Reasoning Under Uncertainty \u00b6 Epistemological Pertaining to an agent\u2019s beliefs of the world Ontological How the world is. Semantics of Probability \u00b6 Probability theory is built on the foundation of worlds and variables . The variables in probability theory are referred to as random variables . Variables are written starting with an uppercase letter. Each variable has a domain . The set of values that it can take. A discrete variable has a domain that is a finite or countable set. A primitive proposition is an assignment of a value to a variable or an inequality between variables, or variables and values. Examples: A=true A=true X<7 X<7 Y>Z Y>Z Propositions are built from primitive propositions using logical connectives . Probability Measures \u00b6 A probability measure is a function P P from worlds into non-negative real numbers P(\\Omega')\\in[0,1] P(\\Omega')\\in[0,1] to subsets \\Omega'\\subseteq \\Omega \\Omega'\\subseteq \\Omega such that Axiom 1: P(\\Omega)=1 P(\\Omega)=1 Axiom 2: if \\Omega_1 \\cap \\Omega_2=\\empty \\Omega_1 \\cap \\Omega_2=\\empty , then P(\\Omega_1\\cup\\Omega_2)=P(\\Omega_1)+P(\\Omega_2) P(\\Omega_1\\cup\\Omega_2)=P(\\Omega_1)+P(\\Omega_2) If all variables have a finite domain, then \\Omega \\Omega is finite, and a probability distribution is defined by assigning a probability value P(\\omega) P(\\omega) to each individual possible world \\omega\\in\\Omega \\omega\\in\\Omega For any \\Omega' \\subseteq \\Omega \\Omega' \\subseteq \\Omega then P(\\Omega')=\\sum_{\\omega\\in\\Omega'}P(\\omega) P(\\Omega')=\\sum_{\\omega\\in\\Omega'}P(\\omega) Example P(\\Omega')=0.08+0.13+0.03+0.21=0.45 P(\\Omega')=0.08+0.13+0.03+0.21=0.45 Probability of Propositions \u00b6 The definition of P P is extended to cover propositions. The probability of proposition \\alpha \\alpha written P(\\alpha) P(\\alpha) , is the sum of the probabilities of possible worlds in which \\alpha \\alpha is true. P(a)= \\sum_{\\omega\\ :\\ \\alpha \\text{ is true in } \\omega}{P(\\omega)} P(a)= \\sum_{\\omega\\ :\\ \\alpha \\text{ is true in } \\omega}{P(\\omega)} Example P(Color=red)=0.08+0.13+0.03+0.21=0.45 P(Color=red)=0.08+0.13+0.03+0.21=0.45 Probability Distribution \u00b6 If X X is a random variable, a probability distribution P(X) P(X) over X X is a function from the domain of X X into the real numbers, such that given a value \u200b x\\in domain(X) x\\in domain(X) , P(x) P(x) is the probability of the proposition X=x X=x . A probability distribution over a set of variables is a function from the values of those variables into a probability. Example: \u200b P(X,Y) P(X,Y) is a probability distribution over X X and Y Y such that \u200b P(X=x,Y=y) P(X=x,Y=y) , where x\\in domain(X) x\\in domain(X) and y\\in domain(Y) y\\in domain(Y) has the value P(X=x \\and Y=y) P(X=x \\and Y=y) where X=x \\and Y=y X=x \\and Y=y is a proposition and P P is the function on propositions defined above. If X_1 \\dots X_n X_1 \\dots X_n are all of the random variables, then an assignment to all of the random variables corresponds to a world, and the probability of the proposition defining a world is equal to the probability of the world. The distribution over all worlds, P(X_1, \\dots,X_n) P(X_1, \\dots,X_n) is called the joint probability distribution . Axioms for Probability \u00b6 Axiom If A A and B B are disjoint, then P(A\\cup B)=P(A)+P(B) P(A\\cup B)=P(A)+P(B) \u200b Example Consider a deck with 52 cards. If A=\\{2,3,4,5\\} A=\\{2,3,4,5\\} and B=\\{7,8\\} B=\\{7,8\\} then P(A\\cup B)=P(A)+P(B)=4/13+2/13=\\frac{6}{13} P(A\\cup B)=P(A)+P(B)=4/13+2/13=\\frac{6}{13} More Generally If C C and D D are not disjoint, then P(C\\cup D)=P(C)+P(D)-P(C\\cap D) P(C\\cup D)=P(C)+P(D)-P(C\\cap D) \u200b Example If C=\\{2,3,4,5\\} C=\\{2,3,4,5\\} and D=\\{\\spadesuit\\} D=\\{\\spadesuit\\} then P(C\\cup D)=4/13+1/4-4/52= \\frac{25}{52} P(C\\cup D)=4/13+1/4-4/52= \\frac{25}{52} From The Book Suppose P P is a function from propositions into real numbers that satisfies the following three axioms of probability: Axiom 1 \u200b 0\\leq P(\\alpha) 0\\leq P(\\alpha) for any proposition \\alpha \\alpha . That is, the belief in any proposition cannot be negative. Axiom 2 \u200b P(\\tau)=1 P(\\tau)=1 if \\tau \\tau is a tautology. That is, if \\tau \\tau is true in all possible worlds, its probability is 1. Axiom 3 \u200b P(\\alpha \\or \\beta)=P(\\alpha)+P(\\beta) P(\\alpha \\or \\beta)=P(\\alpha)+P(\\beta) if \\alpha \\alpha and \\beta \\beta are contradictory propositions; That is, if \\neg(\\alpha \\or \\beta) \\neg(\\alpha \\or \\beta) is a tautology. In other words, if two propositions cannot both be true (mutually exclusive), the probability of their disjunction, is the sum of their probabilities. If a measure of belief follows these intuitive axioms, it is covered by probability theory. Proposition 8.1 : If there are a finite number of finite discrete random variables, Axioms 1, 2 and 3 are sound and complete with respect to the semantics Proposition 8.2 : The following holds for all propositions \\alpha \\alpha and \\beta \\beta : Negation of a proposition: P(\\neg\\alpha)=1-P(\\alpha) P(\\neg\\alpha)=1-P(\\alpha) If \\alpha \\leftrightarrow \\beta \\alpha \\leftrightarrow \\beta , then P(\\alpha)=P(\\beta) P(\\alpha)=P(\\beta) . That is, logically equivalent propositions have the same probability. Reasoning by cases : P(\\alpha)=P(\\alpha \\and \\beta)+P(\\alpha \\and \\neg \\beta) P(\\alpha)=P(\\alpha \\and \\beta)+P(\\alpha \\and \\neg \\beta) If V V is a random variable with domain D D , then, for all propositions \\alpha \\alpha P(\\alpha)=\\sum_{d\\in D}{P(\\alpha \\and V = d)} P(\\alpha)=\\sum_{d\\in D}{P(\\alpha \\and V = d)} Disjunction for non-exclusive propositions: P(\\alpha \\or \\beta)=P(\\alpha) + P(\\beta) - P(\\alpha \\and \\beta) P(\\alpha \\or \\beta)=P(\\alpha) + P(\\beta) - P(\\alpha \\and \\beta) Proof Updating Probability \u00b6 Given new information ( evidence ), degrees of belief change Evidence can be represented as the set of possible world \\Omega' \\Omega' not ruled out by the observation When we observe \\Omega' \\Omega' Worlds that are not consistent with evidence have probability 0 The probabilities of worlds consistent with evidence are proportional to their probability before observation, and must sum to 1 Conditional Probability \u00b6 The measure of belief in proposition p p given proposition e e is called the conditional probability of p p given e e . Written: P(p\\mid e) P(p\\mid e) A proposition e e representing the conjunction of all of the agent\u2019s observations of the world is called evidence . Given evidence e e , the conditional probability P(p\\mid e) P(p\\mid e) is the agents posterior probability of p p . The probability P(p) P(p) is the prior probability of p p and is the same as P(p\\mid true) P(p\\mid true) . The conditional probability of p p given e e is: P(p\\mid e)=\\frac{P(p\\and e)}{P(e)} P(p\\mid e)=\\frac{P(p\\and e)}{P(e)} Example (probability for each world is 0.1) $$ P(S=circle\\mid Fill=f)=\\frac{P(S=cicle\\and Fill=f)}{P(Fill=f)}=0.1/0.4=0.25 $$ Bayes Rule \u00b6 For propositions p,e p,e : P(p\\mid e)=\\frac{P(e\\and p)}{P(e)}=\\frac{P(e\\mid p)\\cdot P(p)}{P(e)}=\\frac{P(e\\mid p)\\cdot P(p)}{P(e\\and p)+P(e\\and \\neg p)} P(p\\mid e)=\\frac{P(e\\and p)}{P(e)}=\\frac{P(e\\mid p)\\cdot P(p)}{P(e)}=\\frac{P(e\\mid p)\\cdot P(p)}{P(e\\and p)+P(e\\and \\neg p)} Example A doctor observes symptoms and wishes to find the probability of a disease: P(disease\\mid symp.)=\\frac{P(symp.\\mid disease)\\cdot P(disease)}{P(sympt.)} P(disease\\mid symp.)=\\frac{P(symp.\\mid disease)\\cdot P(disease)}{P(sympt.)} Chain Rule \u00b6 For propositions p_1,...,p_n p_1,...,p_n : P(p_1\\and \\dots \\and p_n)=P(p_1)P(p_2\\mid p_1)\\cdots P(p_i\\mid p_1\\and\\dots\\and p_{i-1})\\cdots P(p_n\\mid p_1 \\and\\dots\\and p_{n-1}) P(p_1\\and \\dots \\and p_n)=P(p_1)P(p_2\\mid p_1)\\cdots P(p_i\\mid p_1\\and\\dots\\and p_{i-1})\\cdots P(p_n\\mid p_1 \\and\\dots\\and p_{n-1}) Semantics of Conditional Probability \u00b6 Evidence e e e where e e is a proposition, will rule out all possible worlds that are incompatible with e e . Evidence e e induces a new probability P(w\\mid e) P(w\\mid e) of world w w given e e . Any world where e e is false has conditional probability 0 0 , and remaining worlds are normalized so their probabilities sum to 1 1 : P(w\\mid e)= \\left\\{{ \\begin{array}{rcl} c \\cdot P(w) & \\text{if} & e \\text{ is true in world } w \\\\ 0 & \\text{if} & e \\text{ is false in world } w \\end{array}}\\right. P(w\\mid e)= \\left\\{{ \\begin{array}{rcl} c \\cdot P(w) & \\text{if} & e \\text{ is true in world } w \\\\ 0 & \\text{if} & e \\text{ is false in world } w \\end{array}}\\right. where c c is a constant (that depends on e e ) that ensures the posterior probability of all worlds sums to 1 1 . For P(w \\mid e) P(w \\mid e) to be a probability measure over worlds for each e e : Therefore, c=1/P(e) c=1/P(e) . Thus, the conditional probability is only defined if P(e)>0 P(e)>0 The conditional probability of proposition h h given evidence e e is the sum of the conditional probabilities of the possible worlds in which h h is true: A conditional probability distribution , written P(X \\mid Y) P(X \\mid Y) , where X X and Y Y are variables or sets of variables, is a function of the variables: Given a value x\\in domain(X) x\\in domain(X) for X X and a value y \\in domain(Y) y \\in domain(Y) for Y Y , it gives the value P(X=x \\mid Y= y) P(X=x \\mid Y= y) . Proposition 8.3 ( Chain rule ): For any propositions a_1,\\dots,a_n a_1,\\dots,a_n : \\begin{align*} P(a_1 \\and a_2 \\and \\dots \\and a_n) &= &&P(a_1)^*\\\\ & && P(a_2 \\mid a_1)^*\\\\ & && P(a_3 \\mid a_1 \\and a_2)^*\\\\ & && \\vdots \\\\ & && P(a_n \\mid a_1 \\and \\dots \\and a_n-1)\\\\ &= && \\prod^n_{i=1}{P(a_i \\mid a_1 \\and \\dots \\and a_i-1),} \\end{align*} \\begin{align*} P(a_1 \\and a_2 \\and \\dots \\and a_n) &= &&P(a_1)^*\\\\ & && P(a_2 \\mid a_1)^*\\\\ & && P(a_3 \\mid a_1 \\and a_2)^*\\\\ & && \\vdots \\\\ & && P(a_n \\mid a_1 \\and \\dots \\and a_n-1)\\\\ &= && \\prod^n_{i=1}{P(a_i \\mid a_1 \\and \\dots \\and a_i-1),} \\end{align*} \u200b where the right-hand side is assumed to be zero if any of the products are zero (even if some of them are undefined . Note Complete notes. From Bayes' Rule Random Variables and Distributions \u00b6 Random Variables Variables defining possible worlds on which probabilities are defined. Distributions For a random variable A A , and a\\in D_A a\\in D_A we have the probability P(A=a)=P(\\{ \\omega\\in\\Omega \\mid A=a \\text{ in } \\omega\\}) P(A=a)=P(\\{ \\omega\\in\\Omega \\mid A=a \\text{ in } \\omega\\}) The probability distribution of A A is the function on D_A D_A that maps a a to P(A=a) P(A=a) The distribution of A A is denoted P(A) P(A) Joint Distributions Extension to several random variables P(A_1,\\dots,A_k) P(A_1,\\dots,A_k) is the joint distribution of A_1,\\dots,A_k A_1,\\dots,A_k The joint distribution tuples (a_1,\\dots,a_k) (a_1,\\dots,a_k) with a_i\\in D_{A_i} a_i\\in D_{A_i} to the probability $$ P(A_1=a_1,\\dots,A_k=a_k) $$ Chain Rule for Distributions \u00b6 P(A_1,\\dots,A_n)=P(A_1)P(A_2\\mid A_1)\\cdots P(A_i\\mid A_1,\\dots,A_{i-1})\\cdots P(A_n\\mid A_1,\\dots,A_{n-1}) P(A_1,\\dots,A_n)=P(A_1)P(A_2\\mid A_1)\\cdots P(A_i\\mid A_1,\\dots,A_{i-1})\\cdots P(A_n\\mid A_1,\\dots,A_{n-1}) Note: Each P(p_i\\mid p_1\\and\\dots\\and p_{i-1}) P(p_i\\mid p_1\\and\\dots\\and p_{i-1}) was a number. Each P(A_i\\mid A_1,\\dots,A_{i-1}) P(A_i\\mid A_1,\\dots,A_{i-1}) is a function on tuples (a_1,\\dots,a_i) (a_1,\\dots,a_i) Bayes rules for Variables \u00b6 Independence \u00b6 The variables A_i,\\dots,A_k A_i,\\dots,A_k and B_1,\\dots,B_m B_1,\\dots,B_m are independent if: P(A_1,\\dots,A_k \\mid B_1,\\dots,B_m)=P(A_1,\\dots,A_k) P(A_1,\\dots,A_k \\mid B_1,\\dots,B_m)=P(A_1,\\dots,A_k) Which is equivalent to P(B_1,\\dots,B_m \\mid A_1,\\dots,A_k)=P(B_1,\\dots,B_m) P(B_1,\\dots,B_m \\mid A_1,\\dots,A_k)=P(B_1,\\dots,B_m) and P(A_1,\\dots,A_k,B_1,\\dots,B_m)=P(A_1,\\dots,A_k)\\cdot P(B_1,\\dots,B_m) P(A_1,\\dots,A_k,B_1,\\dots,B_m)=P(A_1,\\dots,A_k)\\cdot P(B_1,\\dots,B_m) Example \u00b6 Results for Bayern Munich and SC Freiburg in seasons 2001/02and 2003/04. (Not counting thematches Munich vs. Freiburg): D_{Munich}=D_{Freiburg}=\\{Win,Draw,Loss\\} D_{Munich}=D_{Freiburg}=\\{Win,Draw,Loss\\} Summary: Joint distribution Conditional Distribution \\color{blue}P(Munich\\mid Freiburg) \\color{blue}P(Munich\\mid Freiburg) We have (almost): P(Munich \\mid Freiburg)=P(Munich) P(Munich \\mid Freiburg)=P(Munich) The variables Munich and Freiburg are independant Independance can greatly simplify the specification of a joint distribution: The probability for each possible world is then defined e.g. P(M=D,F=L)=0.25\\cdot 0.4062 = 0.10155 P(M=D,F=L)=0.25\\cdot 0.4062 = 0.10155 Conditionally Independent Variables \u00b6 The variables A_1,\\dots,A_n A_1,\\dots,A_n are conditionally independent of the variables B_1,\\dots,B_m B_1,\\dots,B_m given C_1,\\dots,C_k C_1,\\dots,C_k if P(A_1,\\dots,A_n \\mid B_1,\\dots,B_m,C_1,\\dots,C_k)=P(A_1,\\dots,A_n\\mid C_1,\\dots,C_k) P(A_1,\\dots,A_n \\mid B_1,\\dots,B_m,C_1,\\dots,C_k)=P(A_1,\\dots,A_n\\mid C_1,\\dots,C_k)","title":"Reasoning Under Uncertainty"},{"location":"5-semester/MI/09-23-reasoning-under-certainty/#reasoning-under-uncertainty","text":"Epistemological Pertaining to an agent\u2019s beliefs of the world Ontological How the world is.","title":"Reasoning Under Uncertainty"},{"location":"5-semester/MI/09-23-reasoning-under-certainty/#semantics-of-probability","text":"Probability theory is built on the foundation of worlds and variables . The variables in probability theory are referred to as random variables . Variables are written starting with an uppercase letter. Each variable has a domain . The set of values that it can take. A discrete variable has a domain that is a finite or countable set. A primitive proposition is an assignment of a value to a variable or an inequality between variables, or variables and values. Examples: A=true A=true X<7 X<7 Y>Z Y>Z Propositions are built from primitive propositions using logical connectives .","title":"Semantics of Probability"},{"location":"5-semester/MI/09-23-reasoning-under-certainty/#probability-measures","text":"A probability measure is a function P P from worlds into non-negative real numbers P(\\Omega')\\in[0,1] P(\\Omega')\\in[0,1] to subsets \\Omega'\\subseteq \\Omega \\Omega'\\subseteq \\Omega such that Axiom 1: P(\\Omega)=1 P(\\Omega)=1 Axiom 2: if \\Omega_1 \\cap \\Omega_2=\\empty \\Omega_1 \\cap \\Omega_2=\\empty , then P(\\Omega_1\\cup\\Omega_2)=P(\\Omega_1)+P(\\Omega_2) P(\\Omega_1\\cup\\Omega_2)=P(\\Omega_1)+P(\\Omega_2) If all variables have a finite domain, then \\Omega \\Omega is finite, and a probability distribution is defined by assigning a probability value P(\\omega) P(\\omega) to each individual possible world \\omega\\in\\Omega \\omega\\in\\Omega For any \\Omega' \\subseteq \\Omega \\Omega' \\subseteq \\Omega then P(\\Omega')=\\sum_{\\omega\\in\\Omega'}P(\\omega) P(\\Omega')=\\sum_{\\omega\\in\\Omega'}P(\\omega) Example P(\\Omega')=0.08+0.13+0.03+0.21=0.45 P(\\Omega')=0.08+0.13+0.03+0.21=0.45","title":"Probability Measures"},{"location":"5-semester/MI/09-23-reasoning-under-certainty/#probability-of-propositions","text":"The definition of P P is extended to cover propositions. The probability of proposition \\alpha \\alpha written P(\\alpha) P(\\alpha) , is the sum of the probabilities of possible worlds in which \\alpha \\alpha is true. P(a)= \\sum_{\\omega\\ :\\ \\alpha \\text{ is true in } \\omega}{P(\\omega)} P(a)= \\sum_{\\omega\\ :\\ \\alpha \\text{ is true in } \\omega}{P(\\omega)} Example P(Color=red)=0.08+0.13+0.03+0.21=0.45 P(Color=red)=0.08+0.13+0.03+0.21=0.45","title":"Probability of Propositions"},{"location":"5-semester/MI/09-23-reasoning-under-certainty/#probability-distribution","text":"If X X is a random variable, a probability distribution P(X) P(X) over X X is a function from the domain of X X into the real numbers, such that given a value \u200b x\\in domain(X) x\\in domain(X) , P(x) P(x) is the probability of the proposition X=x X=x . A probability distribution over a set of variables is a function from the values of those variables into a probability. Example: \u200b P(X,Y) P(X,Y) is a probability distribution over X X and Y Y such that \u200b P(X=x,Y=y) P(X=x,Y=y) , where x\\in domain(X) x\\in domain(X) and y\\in domain(Y) y\\in domain(Y) has the value P(X=x \\and Y=y) P(X=x \\and Y=y) where X=x \\and Y=y X=x \\and Y=y is a proposition and P P is the function on propositions defined above. If X_1 \\dots X_n X_1 \\dots X_n are all of the random variables, then an assignment to all of the random variables corresponds to a world, and the probability of the proposition defining a world is equal to the probability of the world. The distribution over all worlds, P(X_1, \\dots,X_n) P(X_1, \\dots,X_n) is called the joint probability distribution .","title":"Probability Distribution"},{"location":"5-semester/MI/09-23-reasoning-under-certainty/#axioms-for-probability","text":"Axiom If A A and B B are disjoint, then P(A\\cup B)=P(A)+P(B) P(A\\cup B)=P(A)+P(B) \u200b Example Consider a deck with 52 cards. If A=\\{2,3,4,5\\} A=\\{2,3,4,5\\} and B=\\{7,8\\} B=\\{7,8\\} then P(A\\cup B)=P(A)+P(B)=4/13+2/13=\\frac{6}{13} P(A\\cup B)=P(A)+P(B)=4/13+2/13=\\frac{6}{13} More Generally If C C and D D are not disjoint, then P(C\\cup D)=P(C)+P(D)-P(C\\cap D) P(C\\cup D)=P(C)+P(D)-P(C\\cap D) \u200b Example If C=\\{2,3,4,5\\} C=\\{2,3,4,5\\} and D=\\{\\spadesuit\\} D=\\{\\spadesuit\\} then P(C\\cup D)=4/13+1/4-4/52= \\frac{25}{52} P(C\\cup D)=4/13+1/4-4/52= \\frac{25}{52} From The Book Suppose P P is a function from propositions into real numbers that satisfies the following three axioms of probability: Axiom 1 \u200b 0\\leq P(\\alpha) 0\\leq P(\\alpha) for any proposition \\alpha \\alpha . That is, the belief in any proposition cannot be negative. Axiom 2 \u200b P(\\tau)=1 P(\\tau)=1 if \\tau \\tau is a tautology. That is, if \\tau \\tau is true in all possible worlds, its probability is 1. Axiom 3 \u200b P(\\alpha \\or \\beta)=P(\\alpha)+P(\\beta) P(\\alpha \\or \\beta)=P(\\alpha)+P(\\beta) if \\alpha \\alpha and \\beta \\beta are contradictory propositions; That is, if \\neg(\\alpha \\or \\beta) \\neg(\\alpha \\or \\beta) is a tautology. In other words, if two propositions cannot both be true (mutually exclusive), the probability of their disjunction, is the sum of their probabilities. If a measure of belief follows these intuitive axioms, it is covered by probability theory. Proposition 8.1 : If there are a finite number of finite discrete random variables, Axioms 1, 2 and 3 are sound and complete with respect to the semantics Proposition 8.2 : The following holds for all propositions \\alpha \\alpha and \\beta \\beta : Negation of a proposition: P(\\neg\\alpha)=1-P(\\alpha) P(\\neg\\alpha)=1-P(\\alpha) If \\alpha \\leftrightarrow \\beta \\alpha \\leftrightarrow \\beta , then P(\\alpha)=P(\\beta) P(\\alpha)=P(\\beta) . That is, logically equivalent propositions have the same probability. Reasoning by cases : P(\\alpha)=P(\\alpha \\and \\beta)+P(\\alpha \\and \\neg \\beta) P(\\alpha)=P(\\alpha \\and \\beta)+P(\\alpha \\and \\neg \\beta) If V V is a random variable with domain D D , then, for all propositions \\alpha \\alpha P(\\alpha)=\\sum_{d\\in D}{P(\\alpha \\and V = d)} P(\\alpha)=\\sum_{d\\in D}{P(\\alpha \\and V = d)} Disjunction for non-exclusive propositions: P(\\alpha \\or \\beta)=P(\\alpha) + P(\\beta) - P(\\alpha \\and \\beta) P(\\alpha \\or \\beta)=P(\\alpha) + P(\\beta) - P(\\alpha \\and \\beta) Proof","title":"Axioms for Probability"},{"location":"5-semester/MI/09-23-reasoning-under-certainty/#updating-probability","text":"Given new information ( evidence ), degrees of belief change Evidence can be represented as the set of possible world \\Omega' \\Omega' not ruled out by the observation When we observe \\Omega' \\Omega' Worlds that are not consistent with evidence have probability 0 The probabilities of worlds consistent with evidence are proportional to their probability before observation, and must sum to 1","title":"Updating Probability"},{"location":"5-semester/MI/09-23-reasoning-under-certainty/#conditional-probability","text":"The measure of belief in proposition p p given proposition e e is called the conditional probability of p p given e e . Written: P(p\\mid e) P(p\\mid e) A proposition e e representing the conjunction of all of the agent\u2019s observations of the world is called evidence . Given evidence e e , the conditional probability P(p\\mid e) P(p\\mid e) is the agents posterior probability of p p . The probability P(p) P(p) is the prior probability of p p and is the same as P(p\\mid true) P(p\\mid true) . The conditional probability of p p given e e is: P(p\\mid e)=\\frac{P(p\\and e)}{P(e)} P(p\\mid e)=\\frac{P(p\\and e)}{P(e)} Example (probability for each world is 0.1) $$ P(S=circle\\mid Fill=f)=\\frac{P(S=cicle\\and Fill=f)}{P(Fill=f)}=0.1/0.4=0.25 $$","title":"Conditional Probability"},{"location":"5-semester/MI/09-23-reasoning-under-certainty/#bayes-rule","text":"For propositions p,e p,e : P(p\\mid e)=\\frac{P(e\\and p)}{P(e)}=\\frac{P(e\\mid p)\\cdot P(p)}{P(e)}=\\frac{P(e\\mid p)\\cdot P(p)}{P(e\\and p)+P(e\\and \\neg p)} P(p\\mid e)=\\frac{P(e\\and p)}{P(e)}=\\frac{P(e\\mid p)\\cdot P(p)}{P(e)}=\\frac{P(e\\mid p)\\cdot P(p)}{P(e\\and p)+P(e\\and \\neg p)} Example A doctor observes symptoms and wishes to find the probability of a disease: P(disease\\mid symp.)=\\frac{P(symp.\\mid disease)\\cdot P(disease)}{P(sympt.)} P(disease\\mid symp.)=\\frac{P(symp.\\mid disease)\\cdot P(disease)}{P(sympt.)}","title":"Bayes Rule"},{"location":"5-semester/MI/09-23-reasoning-under-certainty/#chain-rule","text":"For propositions p_1,...,p_n p_1,...,p_n : P(p_1\\and \\dots \\and p_n)=P(p_1)P(p_2\\mid p_1)\\cdots P(p_i\\mid p_1\\and\\dots\\and p_{i-1})\\cdots P(p_n\\mid p_1 \\and\\dots\\and p_{n-1}) P(p_1\\and \\dots \\and p_n)=P(p_1)P(p_2\\mid p_1)\\cdots P(p_i\\mid p_1\\and\\dots\\and p_{i-1})\\cdots P(p_n\\mid p_1 \\and\\dots\\and p_{n-1})","title":"Chain Rule"},{"location":"5-semester/MI/09-23-reasoning-under-certainty/#semantics-of-conditional-probability","text":"Evidence e e e where e e is a proposition, will rule out all possible worlds that are incompatible with e e . Evidence e e induces a new probability P(w\\mid e) P(w\\mid e) of world w w given e e . Any world where e e is false has conditional probability 0 0 , and remaining worlds are normalized so their probabilities sum to 1 1 : P(w\\mid e)= \\left\\{{ \\begin{array}{rcl} c \\cdot P(w) & \\text{if} & e \\text{ is true in world } w \\\\ 0 & \\text{if} & e \\text{ is false in world } w \\end{array}}\\right. P(w\\mid e)= \\left\\{{ \\begin{array}{rcl} c \\cdot P(w) & \\text{if} & e \\text{ is true in world } w \\\\ 0 & \\text{if} & e \\text{ is false in world } w \\end{array}}\\right. where c c is a constant (that depends on e e ) that ensures the posterior probability of all worlds sums to 1 1 . For P(w \\mid e) P(w \\mid e) to be a probability measure over worlds for each e e : Therefore, c=1/P(e) c=1/P(e) . Thus, the conditional probability is only defined if P(e)>0 P(e)>0 The conditional probability of proposition h h given evidence e e is the sum of the conditional probabilities of the possible worlds in which h h is true: A conditional probability distribution , written P(X \\mid Y) P(X \\mid Y) , where X X and Y Y are variables or sets of variables, is a function of the variables: Given a value x\\in domain(X) x\\in domain(X) for X X and a value y \\in domain(Y) y \\in domain(Y) for Y Y , it gives the value P(X=x \\mid Y= y) P(X=x \\mid Y= y) . Proposition 8.3 ( Chain rule ): For any propositions a_1,\\dots,a_n a_1,\\dots,a_n : \\begin{align*} P(a_1 \\and a_2 \\and \\dots \\and a_n) &= &&P(a_1)^*\\\\ & && P(a_2 \\mid a_1)^*\\\\ & && P(a_3 \\mid a_1 \\and a_2)^*\\\\ & && \\vdots \\\\ & && P(a_n \\mid a_1 \\and \\dots \\and a_n-1)\\\\ &= && \\prod^n_{i=1}{P(a_i \\mid a_1 \\and \\dots \\and a_i-1),} \\end{align*} \\begin{align*} P(a_1 \\and a_2 \\and \\dots \\and a_n) &= &&P(a_1)^*\\\\ & && P(a_2 \\mid a_1)^*\\\\ & && P(a_3 \\mid a_1 \\and a_2)^*\\\\ & && \\vdots \\\\ & && P(a_n \\mid a_1 \\and \\dots \\and a_n-1)\\\\ &= && \\prod^n_{i=1}{P(a_i \\mid a_1 \\and \\dots \\and a_i-1),} \\end{align*} \u200b where the right-hand side is assumed to be zero if any of the products are zero (even if some of them are undefined . Note Complete notes. From Bayes' Rule","title":"Semantics of Conditional Probability"},{"location":"5-semester/MI/09-23-reasoning-under-certainty/#random-variables-and-distributions","text":"Random Variables Variables defining possible worlds on which probabilities are defined. Distributions For a random variable A A , and a\\in D_A a\\in D_A we have the probability P(A=a)=P(\\{ \\omega\\in\\Omega \\mid A=a \\text{ in } \\omega\\}) P(A=a)=P(\\{ \\omega\\in\\Omega \\mid A=a \\text{ in } \\omega\\}) The probability distribution of A A is the function on D_A D_A that maps a a to P(A=a) P(A=a) The distribution of A A is denoted P(A) P(A) Joint Distributions Extension to several random variables P(A_1,\\dots,A_k) P(A_1,\\dots,A_k) is the joint distribution of A_1,\\dots,A_k A_1,\\dots,A_k The joint distribution tuples (a_1,\\dots,a_k) (a_1,\\dots,a_k) with a_i\\in D_{A_i} a_i\\in D_{A_i} to the probability $$ P(A_1=a_1,\\dots,A_k=a_k) $$","title":"Random Variables and Distributions"},{"location":"5-semester/MI/09-23-reasoning-under-certainty/#chain-rule-for-distributions","text":"P(A_1,\\dots,A_n)=P(A_1)P(A_2\\mid A_1)\\cdots P(A_i\\mid A_1,\\dots,A_{i-1})\\cdots P(A_n\\mid A_1,\\dots,A_{n-1}) P(A_1,\\dots,A_n)=P(A_1)P(A_2\\mid A_1)\\cdots P(A_i\\mid A_1,\\dots,A_{i-1})\\cdots P(A_n\\mid A_1,\\dots,A_{n-1}) Note: Each P(p_i\\mid p_1\\and\\dots\\and p_{i-1}) P(p_i\\mid p_1\\and\\dots\\and p_{i-1}) was a number. Each P(A_i\\mid A_1,\\dots,A_{i-1}) P(A_i\\mid A_1,\\dots,A_{i-1}) is a function on tuples (a_1,\\dots,a_i) (a_1,\\dots,a_i)","title":"Chain Rule for Distributions"},{"location":"5-semester/MI/09-23-reasoning-under-certainty/#bayes-rules-for-variables","text":"","title":"Bayes rules for Variables"},{"location":"5-semester/MI/09-23-reasoning-under-certainty/#independence","text":"The variables A_i,\\dots,A_k A_i,\\dots,A_k and B_1,\\dots,B_m B_1,\\dots,B_m are independent if: P(A_1,\\dots,A_k \\mid B_1,\\dots,B_m)=P(A_1,\\dots,A_k) P(A_1,\\dots,A_k \\mid B_1,\\dots,B_m)=P(A_1,\\dots,A_k) Which is equivalent to P(B_1,\\dots,B_m \\mid A_1,\\dots,A_k)=P(B_1,\\dots,B_m) P(B_1,\\dots,B_m \\mid A_1,\\dots,A_k)=P(B_1,\\dots,B_m) and P(A_1,\\dots,A_k,B_1,\\dots,B_m)=P(A_1,\\dots,A_k)\\cdot P(B_1,\\dots,B_m) P(A_1,\\dots,A_k,B_1,\\dots,B_m)=P(A_1,\\dots,A_k)\\cdot P(B_1,\\dots,B_m)","title":"Independence"},{"location":"5-semester/MI/09-23-reasoning-under-certainty/#example","text":"Results for Bayern Munich and SC Freiburg in seasons 2001/02and 2003/04. (Not counting thematches Munich vs. Freiburg): D_{Munich}=D_{Freiburg}=\\{Win,Draw,Loss\\} D_{Munich}=D_{Freiburg}=\\{Win,Draw,Loss\\} Summary: Joint distribution Conditional Distribution \\color{blue}P(Munich\\mid Freiburg) \\color{blue}P(Munich\\mid Freiburg) We have (almost): P(Munich \\mid Freiburg)=P(Munich) P(Munich \\mid Freiburg)=P(Munich) The variables Munich and Freiburg are independant Independance can greatly simplify the specification of a joint distribution: The probability for each possible world is then defined e.g. P(M=D,F=L)=0.25\\cdot 0.4062 = 0.10155 P(M=D,F=L)=0.25\\cdot 0.4062 = 0.10155","title":"Example"},{"location":"5-semester/MI/09-23-reasoning-under-certainty/#conditionally-independent-variables","text":"The variables A_1,\\dots,A_n A_1,\\dots,A_n are conditionally independent of the variables B_1,\\dots,B_m B_1,\\dots,B_m given C_1,\\dots,C_k C_1,\\dots,C_k if P(A_1,\\dots,A_n \\mid B_1,\\dots,B_m,C_1,\\dots,C_k)=P(A_1,\\dots,A_n\\mid C_1,\\dots,C_k) P(A_1,\\dots,A_n \\mid B_1,\\dots,B_m,C_1,\\dots,C_k)=P(A_1,\\dots,A_n\\mid C_1,\\dots,C_k)","title":"Conditionally Independent Variables"},{"location":"5-semester/MI/10-07-bayesian-networks/","text":"Bayesian Networks \u00b6 Reasoning Under Uncertainty \u00b6 Example: Car-start-problem: \u201cIn the morning, my car will not start. I can hear the starter turn, but nothing happens. There may be several reasons for my problem. I can hear the starter roll, so there must be power from the battery. Therefore, the most probable causes are that the fuel has been stolen overnight or that the spark plugs are dirty. It may also be due to dirt in the carburetor, a loose connection in the ignition system, or something more serious. To find out, I first look at the fuel meter. It shows half full, so I decide to clean the spark plugs.\u201d How can you make a computer do these assumptions, and make a choice what to check? For propositional logic we have Boolean logic as a framework. When we deal with uncertain events, it would be nice to have something similar. We can extend the truth values of propositional logic to \"certainties\" which are numbers between 1 and 0. Certainties: A certainty of 0 means \"certainly not true\" A certainty of 1 means \"certainly true\" Example: \u201cif I take a cup of coffee while on break, I will with certainty 0.5 stay awake during the next lecture\u201d or \u201cif I take a short walk during the break, I will with certainty 0.8 stay awake during the next lecture.\u201d Casual Perspective on Car Start Problem \u00b6 To simplify, we assume that we have ${yes, no} $ for Fuel? Fuel? \\{yes, no\\} \\{yes, no\\} for Clean\\ Spark\\ Plugs? Clean\\ Spark\\ Plugs? \\{full, 1/2, empty\\} \\{full, 1/2, empty\\} for Fuel\\ Meter Fuel\\ Meter \\{yes, no\\} \\{yes, no\\} for Start? Start? . AKA states . We know that the state of Fuel? Fuel? and Clean\\ Spark\\ Plugs? Clean\\ Spark\\ Plugs? have a casual impact on the state of Start? Start? . Also, the state of Fuel? Fuel? has an impact on the state of Fuel\\ Meter Fuel\\ Meter . This can be represented with a graph: If we add a direction from no no to yes yes in each variable, we can represent directions of the impact. (Fig. 2.2.) Casual Networks and d-Separation \u00b6 A Casual Network consists of A set of variables A set of directed links (aka arcs ) Mathematically a directed graph. If there is a link from A to B we say that B is a child of A , and A is a parent of B . Bayesian Network \u00b6 A Bayesian Network for variables A_1,\\dots,A_k A_1,\\dots,A_k consists of a directed acyclic graph with nodes A_1,...,A_k A_1,...,A_k for each node a conditional probability table specifying the conditional distribution P(A_i\\mid parents(A_i)) P(A_i\\mid parents(A_i)) parents(A_i) parents(A_i) denotes the parents of A_i A_i in the graph and through the chain rule provides a compact representation of a joint probability distribution Example \u00b6 To turn this graph into a Bayesian network, the following conditional probability tables must be specified: \\begin{align*} &P(A)\\\\&P(B)\\\\&P(C\\mid A,B)\\\\&P(D\\mid A,C)\\\\&P(E\\mid B,D,F)\\\\&P(F\\mid A) \\end{align*} \\begin{align*} &P(A)\\\\&P(B)\\\\&P(C\\mid A,B)\\\\&P(D\\mid A,C)\\\\&P(E\\mid B,D,F)\\\\&P(F\\mid A) \\end{align*} Constructing a Bayesian Network \u00b6 Via Chain Rule \u00b6 Put the random variables in some order Write the joint distribution using chain rule Simplify conditional probability factors by conditional independence assumptions. That determines the parents of each node i.e. the graph structure Specify the conditional probability tables Note: The structure of the resulting network strongly depends on the chosen order of the variables. Via Causality \u00b6 Draw an edge from variable A A to variable B B if A A has a direct casual influence on A A Note: This may not always be possible: Inflation \\to \\to salaries or salaries \\to \\to inflation ? Rain doesn't cause Sun , and Sun doesn't cause Rain , but they are not independent either. Transmission of Evidence \u00b6 Serial Connection \u00b6 Consider fig 2.3: Evidence about A will influence the certainty of B , which influences the certainty of C . Similarly, evidence about C will influence the certainty of A through B . If the state of B is known, the channel is \"blocked\" and we say that A and C are d-separated given B . When a state of a variable is known, we say that the variable is instantiated . We conclude that evidence may be transmitted through a serial connection unless the state of the variable in the connection is known Diverging Connection \u00b6 Influence can pass between all the children of A unless the state of A is known. B,C,...,E are d-separated given A Converging Connection \u00b6 If nothing is known about A except what may be inferred from knowledge of its parents, then the parents are independent. However, if anything is known about the consequences, then information on on possible cause may tell us something about the other causes. D-Separation \u00b6 Definition Two distinct variables A A and B B in a casual network are d-separated if for all paths between A A and B B , there is an intermediate variable V\\neq A \\and V\\neq B V\\neq A \\and V\\neq B such that either: the connection is serial or diverging and V V is instantiated, or the connection is converging, and neither V V nor any of V V 's descendants have received evidence. If A A and B B are not d-separated, we call them d-connected Theorem For all pairwise disjoint sets A,B,C A,B,C of nodes in a Bayesian network: If C C d-separates A A from B B then P(A\\mid B,C)=P(A\\mid C) P(A\\mid B,C)=P(A\\mid C) Note: N\u00e5r man tjekker efter d-seperation, s\u00e5 tjek hele ruten p\u00e5 en gang! Tjek efter d-seperated, og IKKE efter d-connection Probabilities in Bayesian Network \u00b6 Define parents of random variable X_i X_i written parents(X_i) parents(X_i) to be a minimal set of predecessors of X_i X_i in the total ordering such that the other predecessors fo X_i X_i asre conditionally independent of X_i X_i given parents(X_i) parents(X_i) . Thus X_i X_i probabilistically depends on each of its parents, but is independant of its other predecessors. $$ P(X_i\\mid X_1,\\dots,X_i-1)=P(X_i\\mid parents(X_i)) $$ Putting the chain rule and the definition of parents together gives. P(X_1,X_2,\\dots,X_n)=\\prod_{i=1}^n P(X_i \\mid parents(X_i)) P(X_1,X_2,\\dots,X_n)=\\prod_{i=1}^n P(X_i \\mid parents(X_i)) If network contains \\{A,B,C\\} \\{A,B,C\\} then P(A,C)=\\sum_{B}P(A,B,C) P(A,C)=\\sum_{B}P(A,B,C) and","title":"Bayesian Networks"},{"location":"5-semester/MI/10-07-bayesian-networks/#bayesian-networks","text":"","title":"Bayesian Networks"},{"location":"5-semester/MI/10-07-bayesian-networks/#reasoning-under-uncertainty","text":"Example: Car-start-problem: \u201cIn the morning, my car will not start. I can hear the starter turn, but nothing happens. There may be several reasons for my problem. I can hear the starter roll, so there must be power from the battery. Therefore, the most probable causes are that the fuel has been stolen overnight or that the spark plugs are dirty. It may also be due to dirt in the carburetor, a loose connection in the ignition system, or something more serious. To find out, I first look at the fuel meter. It shows half full, so I decide to clean the spark plugs.\u201d How can you make a computer do these assumptions, and make a choice what to check? For propositional logic we have Boolean logic as a framework. When we deal with uncertain events, it would be nice to have something similar. We can extend the truth values of propositional logic to \"certainties\" which are numbers between 1 and 0. Certainties: A certainty of 0 means \"certainly not true\" A certainty of 1 means \"certainly true\" Example: \u201cif I take a cup of coffee while on break, I will with certainty 0.5 stay awake during the next lecture\u201d or \u201cif I take a short walk during the break, I will with certainty 0.8 stay awake during the next lecture.\u201d","title":"Reasoning Under Uncertainty"},{"location":"5-semester/MI/10-07-bayesian-networks/#casual-perspective-on-car-start-problem","text":"To simplify, we assume that we have ${yes, no} $ for Fuel? Fuel? \\{yes, no\\} \\{yes, no\\} for Clean\\ Spark\\ Plugs? Clean\\ Spark\\ Plugs? \\{full, 1/2, empty\\} \\{full, 1/2, empty\\} for Fuel\\ Meter Fuel\\ Meter \\{yes, no\\} \\{yes, no\\} for Start? Start? . AKA states . We know that the state of Fuel? Fuel? and Clean\\ Spark\\ Plugs? Clean\\ Spark\\ Plugs? have a casual impact on the state of Start? Start? . Also, the state of Fuel? Fuel? has an impact on the state of Fuel\\ Meter Fuel\\ Meter . This can be represented with a graph: If we add a direction from no no to yes yes in each variable, we can represent directions of the impact. (Fig. 2.2.)","title":"Casual Perspective on Car Start Problem"},{"location":"5-semester/MI/10-07-bayesian-networks/#casual-networks-and-d-separation","text":"A Casual Network consists of A set of variables A set of directed links (aka arcs ) Mathematically a directed graph. If there is a link from A to B we say that B is a child of A , and A is a parent of B .","title":"Casual Networks and d-Separation"},{"location":"5-semester/MI/10-07-bayesian-networks/#bayesian-network","text":"A Bayesian Network for variables A_1,\\dots,A_k A_1,\\dots,A_k consists of a directed acyclic graph with nodes A_1,...,A_k A_1,...,A_k for each node a conditional probability table specifying the conditional distribution P(A_i\\mid parents(A_i)) P(A_i\\mid parents(A_i)) parents(A_i) parents(A_i) denotes the parents of A_i A_i in the graph and through the chain rule provides a compact representation of a joint probability distribution","title":"Bayesian Network"},{"location":"5-semester/MI/10-07-bayesian-networks/#example","text":"To turn this graph into a Bayesian network, the following conditional probability tables must be specified: \\begin{align*} &P(A)\\\\&P(B)\\\\&P(C\\mid A,B)\\\\&P(D\\mid A,C)\\\\&P(E\\mid B,D,F)\\\\&P(F\\mid A) \\end{align*} \\begin{align*} &P(A)\\\\&P(B)\\\\&P(C\\mid A,B)\\\\&P(D\\mid A,C)\\\\&P(E\\mid B,D,F)\\\\&P(F\\mid A) \\end{align*}","title":"Example"},{"location":"5-semester/MI/10-07-bayesian-networks/#constructing-a-bayesian-network","text":"","title":"Constructing a Bayesian Network"},{"location":"5-semester/MI/10-07-bayesian-networks/#via-chain-rule","text":"Put the random variables in some order Write the joint distribution using chain rule Simplify conditional probability factors by conditional independence assumptions. That determines the parents of each node i.e. the graph structure Specify the conditional probability tables Note: The structure of the resulting network strongly depends on the chosen order of the variables.","title":"Via Chain Rule"},{"location":"5-semester/MI/10-07-bayesian-networks/#via-causality","text":"Draw an edge from variable A A to variable B B if A A has a direct casual influence on A A Note: This may not always be possible: Inflation \\to \\to salaries or salaries \\to \\to inflation ? Rain doesn't cause Sun , and Sun doesn't cause Rain , but they are not independent either.","title":"Via Causality"},{"location":"5-semester/MI/10-07-bayesian-networks/#transmission-of-evidence","text":"","title":"Transmission of Evidence"},{"location":"5-semester/MI/10-07-bayesian-networks/#serial-connection","text":"Consider fig 2.3: Evidence about A will influence the certainty of B , which influences the certainty of C . Similarly, evidence about C will influence the certainty of A through B . If the state of B is known, the channel is \"blocked\" and we say that A and C are d-separated given B . When a state of a variable is known, we say that the variable is instantiated . We conclude that evidence may be transmitted through a serial connection unless the state of the variable in the connection is known","title":"Serial Connection"},{"location":"5-semester/MI/10-07-bayesian-networks/#diverging-connection","text":"Influence can pass between all the children of A unless the state of A is known. B,C,...,E are d-separated given A","title":"Diverging Connection"},{"location":"5-semester/MI/10-07-bayesian-networks/#converging-connection","text":"If nothing is known about A except what may be inferred from knowledge of its parents, then the parents are independent. However, if anything is known about the consequences, then information on on possible cause may tell us something about the other causes.","title":"Converging Connection"},{"location":"5-semester/MI/10-07-bayesian-networks/#d-separation","text":"Definition Two distinct variables A A and B B in a casual network are d-separated if for all paths between A A and B B , there is an intermediate variable V\\neq A \\and V\\neq B V\\neq A \\and V\\neq B such that either: the connection is serial or diverging and V V is instantiated, or the connection is converging, and neither V V nor any of V V 's descendants have received evidence. If A A and B B are not d-separated, we call them d-connected Theorem For all pairwise disjoint sets A,B,C A,B,C of nodes in a Bayesian network: If C C d-separates A A from B B then P(A\\mid B,C)=P(A\\mid C) P(A\\mid B,C)=P(A\\mid C) Note: N\u00e5r man tjekker efter d-seperation, s\u00e5 tjek hele ruten p\u00e5 en gang! Tjek efter d-seperated, og IKKE efter d-connection","title":"D-Separation"},{"location":"5-semester/MI/10-07-bayesian-networks/#probabilities-in-bayesian-network","text":"Define parents of random variable X_i X_i written parents(X_i) parents(X_i) to be a minimal set of predecessors of X_i X_i in the total ordering such that the other predecessors fo X_i X_i asre conditionally independent of X_i X_i given parents(X_i) parents(X_i) . Thus X_i X_i probabilistically depends on each of its parents, but is independant of its other predecessors. $$ P(X_i\\mid X_1,\\dots,X_i-1)=P(X_i\\mid parents(X_i)) $$ Putting the chain rule and the definition of parents together gives. P(X_1,X_2,\\dots,X_n)=\\prod_{i=1}^n P(X_i \\mid parents(X_i)) P(X_1,X_2,\\dots,X_n)=\\prod_{i=1}^n P(X_i \\mid parents(X_i)) If network contains \\{A,B,C\\} \\{A,B,C\\} then P(A,C)=\\sum_{B}P(A,B,C) P(A,C)=\\sum_{B}P(A,B,C) and","title":"Probabilities in Bayesian Network"},{"location":"5-semester/MI/10-14-inference-in-bayesian-networks/","text":"Inference in Bayesian Networks \u00b6 Inference \u00b6 Inference Problem: Given: a Bayesian network Given an assignment of values to some of the variables in the network: E_i=e_i(i=1,\\dots,l) E_i=e_i(i=1,\\dots,l) \"Instantiation of the nodes \\bold E \\bold E \" \"Evidence \\bold E=e \\bold E=e entered\" \"Findings entered\" ... Want: for variables A\\notin \\bold E A\\notin \\bold E the posterior margnial P(A\\mid \\bold E=e) P(A\\mid \\bold E=e) According to the definition of conditional probability, it is sufficient to compute for each A\\in D_A A\\in D_A the value P(A=a,\\bold E=e) P(A=a,\\bold E=e) Together with P(\\bold E=e)=\\sum_{a\\in D_A}P(A=a,\\bold E=e) P(\\bold E=e)=\\sum_{a\\in D_A}P(A=a,\\bold E=e) This gives the posterior distribution Inference as Summation \u00b6 Let A A be the variable of interest, \\bold E \\bold E the evidence variables, and \\bold Y=Y_1,\\dots,Y_l \\bold Y=Y_1,\\dots,Y_l the remaining variables in the network not belonging to A\\cup \\bold E A\\cup \\bold E . Then $$ P(A=a,\\bold E=e)=\\sum_{y_1\\in D_{Y_1}}\\cdots\\sum_{y_l\\in D_{Y_l}}P(A=a,\\bold E=e,Y_1=y_1,\\dots,Y_l=y_l) $$ Note: For each \\bold y \\bold y the probability P(A=a,\\bold E = \\bold e, \\bold Y = \\bold y) P(A=a,\\bold E = \\bold e, \\bold Y = \\bold y) can be computed from the network In time linear in the number of random variables The number of configurations over \\bold Y \\bold Y is exponential in l l Inference Problems \u00b6 First Problem \u00b6 Find P(B\\mid a,f,g,h)={P(B,a,f,g,h)\\over P(a,f,g,h)} P(B\\mid a,f,g,h)={P(B,a,f,g,h)\\over P(a,f,g,h)} We can if we have access to P(A,B,C,D,E,F,G,H) P(A,B,C,D,E,F,G,H) P(A,B,C,D,E,F,G,H)=P(A)P(B)P(C)P(D\\mid A,B)\\cdots P(H\\mid E) P(A,B,C,D,E,F,G,H)=P(A)P(B)P(C)P(D\\mid A,B)\\cdots P(H\\mid E) Inserting evidence we get: and Second Problem \u00b6 See naive solution in Lecture 14.10 Slides 8-11 Naive Solution Summary \u00b6 Variable Elimination \u00b6 Problem The joint probability distribution will contain exponentially many entries Idea We can use the form of the joint distribution P P , and the law of distributivity to make the computation of the sum more efficient Thus, we can adapt our elimination procedure so that: we marginalize out variables sequentially when marginalizing out a particular variable X X , we only need to consider the factors involving X X Example \u00b6 Factors \u00b6 Calculus of factors The procedure operates on factors: functions of subsets of variables Required operations on factors: multiplication marginalization (summing out selected variables) restriction (setting selected variables to specific values) Complexity See example in Lecture 14.10 Slides 16-17 Singly Connected Networks \u00b6 A singly connected network is a network in which any two nodes are connected by at most one path of undirected edges. For singly connected network: any elimination order that \"peels\" variables from outside will only create factors of one variable. The complexity of inference is therefore linear in the total size of the network ( = combined size of all conditional probability tables) Approximate Inference \u00b6 Sample Generator \u00b6 Observation: can use Bayesian network as random generator that produces states \\bold X = \\bold x \\bold X = \\bold x according to distribution P P defined by the network Example \u00b6 Approximate Inference from Samples \u00b6 To compute an approximation of P(\\bold E = \\bold e) P(\\bold E = \\bold e) ( \\bold E \\bold E is a subset of the variables in the Bayesian network): generate a (large) number of random states count the frequency of states in which \\bold E = \\bold e \\bold E = \\bold e Hoeffding Bound \u00b6 Required Sample Size \u00b6 Example \u00b6 Rejection Sampling \u00b6 The simplest approach: Rejection Sampling Problem Samples with \\bold E \\neq \\bold e \\bold E \\neq \\bold e are useless! Ideally: would draw samples directly from the conditional distribution P(\\bold A \\mid \\bold E = \\bold e) P(\\bold A \\mid \\bold E = \\bold e) Likelihood Weighting \u00b6 We would like to sample from So instead weigh each generated sample with a weight corresponding to Part 2","title":"Inference in Bayesian Networks"},{"location":"5-semester/MI/10-14-inference-in-bayesian-networks/#inference-in-bayesian-networks","text":"","title":"Inference in Bayesian Networks"},{"location":"5-semester/MI/10-14-inference-in-bayesian-networks/#inference","text":"Inference Problem: Given: a Bayesian network Given an assignment of values to some of the variables in the network: E_i=e_i(i=1,\\dots,l) E_i=e_i(i=1,\\dots,l) \"Instantiation of the nodes \\bold E \\bold E \" \"Evidence \\bold E=e \\bold E=e entered\" \"Findings entered\" ... Want: for variables A\\notin \\bold E A\\notin \\bold E the posterior margnial P(A\\mid \\bold E=e) P(A\\mid \\bold E=e) According to the definition of conditional probability, it is sufficient to compute for each A\\in D_A A\\in D_A the value P(A=a,\\bold E=e) P(A=a,\\bold E=e) Together with P(\\bold E=e)=\\sum_{a\\in D_A}P(A=a,\\bold E=e) P(\\bold E=e)=\\sum_{a\\in D_A}P(A=a,\\bold E=e) This gives the posterior distribution","title":"Inference"},{"location":"5-semester/MI/10-14-inference-in-bayesian-networks/#inference-as-summation","text":"Let A A be the variable of interest, \\bold E \\bold E the evidence variables, and \\bold Y=Y_1,\\dots,Y_l \\bold Y=Y_1,\\dots,Y_l the remaining variables in the network not belonging to A\\cup \\bold E A\\cup \\bold E . Then $$ P(A=a,\\bold E=e)=\\sum_{y_1\\in D_{Y_1}}\\cdots\\sum_{y_l\\in D_{Y_l}}P(A=a,\\bold E=e,Y_1=y_1,\\dots,Y_l=y_l) $$ Note: For each \\bold y \\bold y the probability P(A=a,\\bold E = \\bold e, \\bold Y = \\bold y) P(A=a,\\bold E = \\bold e, \\bold Y = \\bold y) can be computed from the network In time linear in the number of random variables The number of configurations over \\bold Y \\bold Y is exponential in l l","title":"Inference as Summation"},{"location":"5-semester/MI/10-14-inference-in-bayesian-networks/#inference-problems","text":"","title":"Inference Problems"},{"location":"5-semester/MI/10-14-inference-in-bayesian-networks/#first-problem","text":"Find P(B\\mid a,f,g,h)={P(B,a,f,g,h)\\over P(a,f,g,h)} P(B\\mid a,f,g,h)={P(B,a,f,g,h)\\over P(a,f,g,h)} We can if we have access to P(A,B,C,D,E,F,G,H) P(A,B,C,D,E,F,G,H) P(A,B,C,D,E,F,G,H)=P(A)P(B)P(C)P(D\\mid A,B)\\cdots P(H\\mid E) P(A,B,C,D,E,F,G,H)=P(A)P(B)P(C)P(D\\mid A,B)\\cdots P(H\\mid E) Inserting evidence we get: and","title":"First Problem"},{"location":"5-semester/MI/10-14-inference-in-bayesian-networks/#second-problem","text":"See naive solution in Lecture 14.10 Slides 8-11","title":"Second Problem"},{"location":"5-semester/MI/10-14-inference-in-bayesian-networks/#naive-solution-summary","text":"","title":"Naive Solution Summary"},{"location":"5-semester/MI/10-14-inference-in-bayesian-networks/#variable-elimination","text":"Problem The joint probability distribution will contain exponentially many entries Idea We can use the form of the joint distribution P P , and the law of distributivity to make the computation of the sum more efficient Thus, we can adapt our elimination procedure so that: we marginalize out variables sequentially when marginalizing out a particular variable X X , we only need to consider the factors involving X X","title":"Variable Elimination"},{"location":"5-semester/MI/10-14-inference-in-bayesian-networks/#example","text":"","title":"Example"},{"location":"5-semester/MI/10-14-inference-in-bayesian-networks/#factors","text":"Calculus of factors The procedure operates on factors: functions of subsets of variables Required operations on factors: multiplication marginalization (summing out selected variables) restriction (setting selected variables to specific values) Complexity See example in Lecture 14.10 Slides 16-17","title":"Factors"},{"location":"5-semester/MI/10-14-inference-in-bayesian-networks/#singly-connected-networks","text":"A singly connected network is a network in which any two nodes are connected by at most one path of undirected edges. For singly connected network: any elimination order that \"peels\" variables from outside will only create factors of one variable. The complexity of inference is therefore linear in the total size of the network ( = combined size of all conditional probability tables)","title":"Singly Connected Networks"},{"location":"5-semester/MI/10-14-inference-in-bayesian-networks/#approximate-inference","text":"","title":"Approximate Inference"},{"location":"5-semester/MI/10-14-inference-in-bayesian-networks/#sample-generator","text":"Observation: can use Bayesian network as random generator that produces states \\bold X = \\bold x \\bold X = \\bold x according to distribution P P defined by the network","title":"Sample Generator"},{"location":"5-semester/MI/10-14-inference-in-bayesian-networks/#example_1","text":"","title":"Example"},{"location":"5-semester/MI/10-14-inference-in-bayesian-networks/#approximate-inference-from-samples","text":"To compute an approximation of P(\\bold E = \\bold e) P(\\bold E = \\bold e) ( \\bold E \\bold E is a subset of the variables in the Bayesian network): generate a (large) number of random states count the frequency of states in which \\bold E = \\bold e \\bold E = \\bold e","title":"Approximate Inference from Samples"},{"location":"5-semester/MI/10-14-inference-in-bayesian-networks/#hoeffding-bound","text":"","title":"Hoeffding Bound"},{"location":"5-semester/MI/10-14-inference-in-bayesian-networks/#required-sample-size","text":"","title":"Required Sample Size"},{"location":"5-semester/MI/10-14-inference-in-bayesian-networks/#example_2","text":"","title":"Example"},{"location":"5-semester/MI/10-14-inference-in-bayesian-networks/#rejection-sampling","text":"The simplest approach: Rejection Sampling Problem Samples with \\bold E \\neq \\bold e \\bold E \\neq \\bold e are useless! Ideally: would draw samples directly from the conditional distribution P(\\bold A \\mid \\bold E = \\bold e) P(\\bold A \\mid \\bold E = \\bold e)","title":"Rejection Sampling"},{"location":"5-semester/MI/10-14-inference-in-bayesian-networks/#likelihood-weighting","text":"We would like to sample from So instead weigh each generated sample with a weight corresponding to Part 2","title":"Likelihood Weighting"},{"location":"5-semester/MI/10-21-decision-trees/","text":"Learning: Introduction and Decision Trees \u00b6 Learning: The ability of an agent to improve its behavior based on experience, e.g.: The range of behaviors is expanded; the agent can do more. The accuracy on tasks is improved; the agent can do things better. The speed is improved; the agent can do things faster. \\newcommand{pval}{\\widehat Y}\\nonumber \\newcommand{pval}{\\widehat Y}\\nonumber Learning Issues \u00b6 The following components are part of any learning problem: Task : The behavior or task that is being improved Data: The experiences that are used to improve performance in the task, usually in the form of a sequence of examples Measure of improvement: How the improvement is measured. Examples: New skills that were not present initially Increasing accuracy in prediction Improved speed Learning techniques face the following issues: Task \u00b6 7.1: Task The most commonly studied learning task is supervised learning: Given some input features, some target features, and a set of training examples where the input features and target features are specified, predict the value of target features for new examples given their values on the input features. This is called classification when the target features are discrete, And regression when the target features are continuous. Other learning tasks include: Unsupervised learning Without defined targets Reinforced learning Based on rewards and punishments Analytic learning Learning to reason faster Inductive logic programming Learning richer representation such as logic programs Feedback \u00b6 7.1: Feedback Learning tasks can be characterized by the feedback given to the learner. Supervised Learning : What has to be learned is specified for each training example. Unsupervised Learning: No classifications are given, and the learner must discover categories and regularities in the data. Feedback often falls between these extremes, such as: Reinforcement Learning: Feedback in terms of rewards and punishments occurs after a sequence of actions This leads to the credit assignment problem of determining which actions were responsible for the rewards or punishments. Representation \u00b6 7.1: Representation Experiences must affect the agents internal representation. This internal representation could be the raw experiences, but it is typically a compact representation that generalizes the data. The problem of inferring an internal representation based on examples is called induction . The problem of deriving consequences of a knowledge base is called deduction Hypothesizing what may be true about a particular case is called abduction Two principles are at odds: The richer the representation, the more useful it is for subsequent problem solving . The richer the representation, the more difficult it is to learn . Online and Offline \u00b6 Online Learning: Training examples arrive as the agent is acting. Offline Learning: All of the training examples are available to an agent before it needs to act. An agent that learns online requires some representation of its previously seen examples before it has seen all of its examples. As new examples are observed, the agent must update its representation. Active learning is a form of online learning in which the agent acts to acquire useful examples from which to learn. Measuring Success \u00b6 To know whether an agent has learned, we must define a measure of success. The measure is usually not how well the agent performs on the training data, but how well the agent performs for new data. Success in learning should not be judged on correctly classifying the training set, but on being able to correctly classify unseen examples. Thus, the learner must generalize: go beyond the specific given examples to classify unseen examples. To evaluate a learning procedure, we can divide the examples into training examples and test examples . Bias \u00b6 The tendency to prefer one hypothesis over another is called a bias . Without a bias, an agent will not be able to make any predictions on unseen examples. What constitutes a good bias is an empirical question about which biases work best in practice. Learning as Search \u00b6 Given a representation and a bias, the problem of learning can be reduced to one of search. Unfortunately, the search spaces are typically prohibitively large for systematic search. The definition of the learning algorithm then becomes one of defining the search space, evaluation function and search method. Noise \u00b6 In most real-world situations, the data are not perfect. There can be noise where the observed features are not adequate to predict the classification, missing data where the observations of some of the features for some or all of the examples are missing, and errors where some of the features have been assigned wrong values. One of the important properties of a learning algorithm is its ability to handle noisy data in all of its forms. Interpolation and Extrapolation \u00b6 Interpolation: Making a prediction between cases for which there are data. Extrapolation: Making a prediction that goes beyond the seen examples. Extrapolation is usually much less accurate than interpolation. Supervised Learning \u00b6 7.2 Supervised Learning A set of examples and a set of features, partitioned into input features and target features. The aim is to predict the values of the target features from the input features. A feature is a function from examples into a value. If e e is an example, and F F is a feature, then F(e) F(e) is the value of feature F F for example e e . The domain of a feature is the set of values it can return. In a supervised learning task, the learner is given: a set of input features, X_1,\\dots,X_n X_1,\\dots,X_n a set of target features, Y_1,\\dots,Y_n Y_1,\\dots,Y_n a set of training examples , where the values for the input and target features are given for each example a set of test examples , where only the values for the input features are given. The aim is to predict the values of the target features for the test examples and as-yet-unseen examples. Example 7.1 See the image above. Predicting the value of Y Y is a regression task, because Y Y is a real-valued feature. Example e_8 e_8 is an interpolation problem, because the value of X X is between the values of the training examples. Example e_9 e_9 is an extrapolation problem, because the value of X X is outside the range of the range of the training examples. Evaluating Predictions \u00b6 A point estimate for target feature Y Y on example e e is a prediction of the value of Y(e) Y(e) . Let \\widehat{Y}(e) \\widehat{Y}(e) be the predicted value for target feature Y Y on example e e . The error for this example on this feature is a measure of how close \\widehat Y(e) \\widehat Y(e) is to Y(e) Y(e) . For regression, both \\widehat Y(e) \\widehat Y(e) and Y(e) Y(e) are real numbers that can be compared arithmetically. For classification, when the target feature Y Y is a discrete function, there are a number of alternatives: When the domain of Y Y is binary, we can associate one value with 0 and the other with 1, and the prediction can be some real number. In a cardinal feature the values are mapped to real numbers. This is appropriate when the values in the domain are totally ordered, and the differences between the numbers are meaningful. Often, mapping values to the real line is not appropriate even if the values are totally ordered. Example: Values = short, medium, long short, medium, long . The prediction that the value is either short short or long long is very different from the prediction that the value is medium medium When the domain of a feature is totally ordered, but the differences between the values are not comparable, the features is called an ordinal feature For a totally ordered feature, either cardinal or ordinal, and for a given value v v , a Boolean feature can be constructed as a cut : A new feature that has value 1 1 when Y\\leq v Y\\leq v and 0 0 otherwise. Combining cuts allows for features that are true for intervals. When Y Y is discrete with domain \\{v_1,\\dots,v_k\\} \\{v_1,\\dots,v_k\\} , where k>2 k>2 , a separate prediction can be made for each v_i v_i . This can be modeled by having a binary indicator variable, Y_i Y_i , associated with each value v_i v_i , where Y_i(e)=1 Y_i(e)=1 if Y(e)=v_i Y(e)=v_i , and Y_i(e)=0 Y_i(e)=0 otherwise. Example 7.3 Prediction Error \u00b6 In the following measures of prediction error, \u200b E E is a set of examples, and \u200b T T is a set of target features. For target feature Y\\in T Y\\in T and example e\\in E e\\in E , the actual value is Y(e) Y(e) and the predicted value is \\pval \\pval . The 0/1 error on E E is the sum of the number of predictions that are wrong: $$ \\sum_{e\\in E} \\sum_{Y\\in T}Y(e) \\neq \\pval(e)\\ , $$ \u200b where Y(e)\\neq \\pval(e) Y(e)\\neq \\pval(e) is 0 when false and 1 when true. This is the number of incorrect predictions, not taking into account how wrong the predictions are. The absolute error on E E is the sum of the absolute differences between the actual and predicted values: $$ \\sum_{e\\in E}\\sum_{Y \\in T} \\left| Y(e)-\\pval(e) \\right| $$ This is always non-negative, and is only zero when all predictions exactly fit the observed values. Here close predictions are better than far-away predictions. The sum-of-squares error on E E is: \\sum_{e\\in E}\\sum_{Y \\in T}(Y(e)-\\pval(e))^2 \\sum_{e\\in E}\\sum_{Y \\in T}(Y(e)-\\pval(e))^2 This treats large errors as much worse than small errors. The worst-case error on E E is the maximum absolute difference: $$ \\max_{e\\in E}\\max_{Y\\in T} \\left| Y(e) - \\pval(e) \\right| $$ In this case, the learner is evaluated by how bad it can be. These are often described in terms of the norms of the differences between the predicted and actual values. The 0/1 error is the L_0 L_0 error , the absolute error is the L_1 L_1 error , the sum-of-squares error is the square of the L_2 L_2 error , and the worst-case error is the L_\\infty L_\\infty error . The sum-of-squares error is often written as L_2^2 L_2^2 as the L_2 L_2 norm takes the square root of the sum of squares. Example 7.4 For the special case where the domain of Y Y is \\{0,1\\} \\{0,1\\} , and the prediction is in the range [0,1] [0,1] (and so for Boolean domains where true true is treated as 1, and false false as 0), the following can also be used to evaluate predictions: likelihood of the data log-likelihood See https://artint.info/2e/html/ArtInt2e.Ch7.S2.SS1.html#p5 Types of Errors \u00b6 A false-positive error ( type I error ) A false-negative error (type II error) For a given predictor for a given set of examples, suppose tp tp is the number of true positives, fp fp is the number of false positives, fn fn is the number of false negatives, and tn tn is the number of true negatives The following measures are often used: The precision is \\frac{tp}{tp+fp} \\frac{tp}{tp+fp} the proportion of positive predictions that are actual positives. The recall or true-positive rate is \\frac{tp}{tp+fn} \\frac{tp}{tp+fn} the proportion of actual positives that are predicted to be positive. The false-positive rate is \\frac{fp}{fp+tn} \\frac{fp}{fp+tn} the proportion of actual negatives predicted to be positive. An agent should try to maximize precision and recall, and to minimize the false-positive rate. These goals are incompatible however. To compare predictors for a given set of examples, an ROC space or receiver operating characteristic space** plots the false-positive rate against the true-positive rate. Each predictor these examples becomes a point in the space. A precision-recall space plots the precision against the recall. Example 7.6 Point Estimates with No Input Features \u00b6 The simplest case for learning is when there are no input features, and where there is a single target feature. Best-case for many learning algorithms. Proposition 7.1 Suppose V V is the multiset of values of Y(e) Y(e) for e\\in E e\\in E A prediction that minimizes the 0/1 error is a mode ; one of the values that appears most often. When there are multiple modes, any can be chosen. The prediction that minimizes the sum-of-squares error on E E is the mean of V V (average value) The absolute error is minimized by any median of V V The value that minimizes the worst-case error is (max+min)/2 (max+min)/2 where max max is the maximum value and min min is the minimum value. Full Proposition + proof When the target has domain \\{0,1\\} \\{0,1\\} the training examples can be summarized in n_0 n_0 : the number of examples with the value 0 n_1 n_1 : the number of examples with the value 1. The prediction for each new case is the same number, p p The optimal prediction for p p : Learning Decision Trees \u00b6 Decision tree learning is one of the simplest useful techniques for supervised classification learning. For this section we assume there is a single discrete target feature called the classification . Each element of the domain of the classification is called a class A decision tree or a classification tree is a tree in which each internal (non-leaf) node is labeled with a condition, a Boolean function of examples each internal node has two children, one labeled with true true and the other with false false each leaf of the tree is labeled with a point estimate on the class Example 7.7 Algorithm \u00b6 Key question : Which X_i X_i to choose in line 4? Approach: Choose the feature that would provide the best classifier if construction would terminate with that feature. Showing: Number of examples with class labels skip, reads, belonging to different sub-trees \\color{green} \\text{Green}: \\color{green} \\text{Green}: Predicted class label (possibly a tie between two labels) Class Purity \u00b6 Principle: Prefer features that split the examples into class pure subsets Normalized to probabilities: Entropy \u00b6 Purity Measure : \u00b6 For a probability distribution (p,1-p) (p,1-p) of a two-valued class label, define ^log2 : h(p,1-p)=-p\\cdot\\log_2(p)-(1-p)\\cdot\\log_2(1-p) h(p,1-p)=-p\\cdot\\log_2(p)-(1-p)\\cdot\\log_2(1-p) High values for impure distributions Maximal for (0.5,0.5) (0.5,0.5) Zero for (1,0) (1,0) and (0,1) (0,1) Example Generalization to Larger Domain \u00b6 For probability distribution on domain with n n elements: \\bold p=(p_1,\\dots,p_n)\\quad (p_n=1-\\sum_{i=1}^{n-1}p_i) \\bold p=(p_1,\\dots,p_n)\\quad (p_n=1-\\sum_{i=1}^{n-1}p_i) define entropy ^log2 : h(\\bold p)=-\\sum_{i=1}^n p_i\\cdot\\log_2(p_i) h(\\bold p)=-\\sum_{i=1}^n p_i\\cdot\\log_2(p_i) Again: Maximal for \\bold p=(1/n,\\dots,1/n) \\bold p=(1/n,\\dots,1/n) Zero for \\bold p = (1,0,\\dots,0) \\bold p = (1,0,\\dots,0) and so on Entropy Example \u00b6 We prefer features that split into subsets with low entropy, but consider example for binary class variable (values c_1,c_2 c_1,c_2 with initial counts c_1:20,c_2:20 c_1:20,c_2:20 ), and two 3-valued features X_1,X_2 X_1,X_2 : X_2 X_2 provides a better division of examples than X_1 X_1 . It gives a lower expected entropy : (1/40)\u00b7 0+(1/40)\u00b7(38/40)\u00b71>(15/40)\u00b70 + (15/40)\u00b70 + (10/40)\u00b71 (1/40)\u00b7 0+(1/40)\u00b7(38/40)\u00b71>(15/40)\u00b70 + (15/40)\u00b70 + (10/40)\u00b71 Expected Entropy and Information Gain \u00b6 For feature X X with domain v_1,\\dots,v_n v_1,\\dots,v_n let: E_i E_i be the set of examples with X=v_i X=v_i q_i=|E_i|/|E| q_i=|E_i|/|E| h_i h_i the entropy of the class label distribution in E_i E_i The expected entropy from splitting on X X then is: $$ h(Class\\mid X)=\\sum_{i=1}^n q_i\\cdot h_i $$ Let h(Class) h(Class) : entropy of class label distribution before splitting The Information Gain from splitting on X X then is: $$ h(Class)-h(Class \\mid X) $$ Information Gain in Decision Tree Learning In line 4 of the algorithm choose feature X_i X_i that gives the highest information gain Continuous/Many-Valued Attributes \u00b6 The information gain measure favors attributes with many values For example, the attribute Date will have a very high information gain, but is unable to generalize One approach for avoiding this, is to select attributes based on GainRation: $$ GainRation(S,A)=\\frac{Gain(S,A)}{SplitInformation(S,A)}\\ SplitInformation(S,A)=-\\sum_{i=1}^c\\frac{|S_i|}{|S|}\\log_2 \\frac{|S_i|}{|S|} $$ where S_i S_i is the subset of examples produced by splitting on the i i 'th value of A A Note that SplitInformation is the entropy of S S with regard to the values of A A We require that the attributes being tested are discrete valued. So in order to test a continuous valued attribute we need to \"discretize\" it. Suppose that the training examples are associated with the attribute Temperature: Create a new boolean valued attribute by first testing the two candidate thresholds: (48+60)/2 (48+60)/2 (80+90)/2 (80+90)/2 Next pick the one with the highest information gain (i.e., Temperature_{\\gt 54} Temperature_{\\gt 54} ) Overfitting \u00b6 Noise in data may lead to a bad classifier. In particularly, if the decision tree fits the data perfectly. This is called overfitting . Definition A hypothesis h h is said to overfit the training data if there exists some alternative hypothesis h' h' , such that: h h has a smaller error than h' h' over the training data, but h' h' has a smaller error than h h over the entire distribution of instances","title":"Learning: Introduction and Decision Trees"},{"location":"5-semester/MI/10-21-decision-trees/#learning-introduction-and-decision-trees","text":"Learning: The ability of an agent to improve its behavior based on experience, e.g.: The range of behaviors is expanded; the agent can do more. The accuracy on tasks is improved; the agent can do things better. The speed is improved; the agent can do things faster. \\newcommand{pval}{\\widehat Y}\\nonumber \\newcommand{pval}{\\widehat Y}\\nonumber","title":"Learning: Introduction and Decision Trees"},{"location":"5-semester/MI/10-21-decision-trees/#learning-issues","text":"The following components are part of any learning problem: Task : The behavior or task that is being improved Data: The experiences that are used to improve performance in the task, usually in the form of a sequence of examples Measure of improvement: How the improvement is measured. Examples: New skills that were not present initially Increasing accuracy in prediction Improved speed Learning techniques face the following issues:","title":"Learning Issues"},{"location":"5-semester/MI/10-21-decision-trees/#task","text":"7.1: Task The most commonly studied learning task is supervised learning: Given some input features, some target features, and a set of training examples where the input features and target features are specified, predict the value of target features for new examples given their values on the input features. This is called classification when the target features are discrete, And regression when the target features are continuous. Other learning tasks include: Unsupervised learning Without defined targets Reinforced learning Based on rewards and punishments Analytic learning Learning to reason faster Inductive logic programming Learning richer representation such as logic programs","title":"Task"},{"location":"5-semester/MI/10-21-decision-trees/#feedback","text":"7.1: Feedback Learning tasks can be characterized by the feedback given to the learner. Supervised Learning : What has to be learned is specified for each training example. Unsupervised Learning: No classifications are given, and the learner must discover categories and regularities in the data. Feedback often falls between these extremes, such as: Reinforcement Learning: Feedback in terms of rewards and punishments occurs after a sequence of actions This leads to the credit assignment problem of determining which actions were responsible for the rewards or punishments.","title":"Feedback"},{"location":"5-semester/MI/10-21-decision-trees/#representation","text":"7.1: Representation Experiences must affect the agents internal representation. This internal representation could be the raw experiences, but it is typically a compact representation that generalizes the data. The problem of inferring an internal representation based on examples is called induction . The problem of deriving consequences of a knowledge base is called deduction Hypothesizing what may be true about a particular case is called abduction Two principles are at odds: The richer the representation, the more useful it is for subsequent problem solving . The richer the representation, the more difficult it is to learn .","title":"Representation"},{"location":"5-semester/MI/10-21-decision-trees/#online-and-offline","text":"Online Learning: Training examples arrive as the agent is acting. Offline Learning: All of the training examples are available to an agent before it needs to act. An agent that learns online requires some representation of its previously seen examples before it has seen all of its examples. As new examples are observed, the agent must update its representation. Active learning is a form of online learning in which the agent acts to acquire useful examples from which to learn.","title":"Online and Offline"},{"location":"5-semester/MI/10-21-decision-trees/#measuring-success","text":"To know whether an agent has learned, we must define a measure of success. The measure is usually not how well the agent performs on the training data, but how well the agent performs for new data. Success in learning should not be judged on correctly classifying the training set, but on being able to correctly classify unseen examples. Thus, the learner must generalize: go beyond the specific given examples to classify unseen examples. To evaluate a learning procedure, we can divide the examples into training examples and test examples .","title":"Measuring Success"},{"location":"5-semester/MI/10-21-decision-trees/#bias","text":"The tendency to prefer one hypothesis over another is called a bias . Without a bias, an agent will not be able to make any predictions on unseen examples. What constitutes a good bias is an empirical question about which biases work best in practice.","title":"Bias"},{"location":"5-semester/MI/10-21-decision-trees/#learning-as-search","text":"Given a representation and a bias, the problem of learning can be reduced to one of search. Unfortunately, the search spaces are typically prohibitively large for systematic search. The definition of the learning algorithm then becomes one of defining the search space, evaluation function and search method.","title":"Learning as Search"},{"location":"5-semester/MI/10-21-decision-trees/#noise","text":"In most real-world situations, the data are not perfect. There can be noise where the observed features are not adequate to predict the classification, missing data where the observations of some of the features for some or all of the examples are missing, and errors where some of the features have been assigned wrong values. One of the important properties of a learning algorithm is its ability to handle noisy data in all of its forms.","title":"Noise"},{"location":"5-semester/MI/10-21-decision-trees/#interpolation-and-extrapolation","text":"Interpolation: Making a prediction between cases for which there are data. Extrapolation: Making a prediction that goes beyond the seen examples. Extrapolation is usually much less accurate than interpolation.","title":"Interpolation and Extrapolation"},{"location":"5-semester/MI/10-21-decision-trees/#supervised-learning","text":"7.2 Supervised Learning A set of examples and a set of features, partitioned into input features and target features. The aim is to predict the values of the target features from the input features. A feature is a function from examples into a value. If e e is an example, and F F is a feature, then F(e) F(e) is the value of feature F F for example e e . The domain of a feature is the set of values it can return. In a supervised learning task, the learner is given: a set of input features, X_1,\\dots,X_n X_1,\\dots,X_n a set of target features, Y_1,\\dots,Y_n Y_1,\\dots,Y_n a set of training examples , where the values for the input and target features are given for each example a set of test examples , where only the values for the input features are given. The aim is to predict the values of the target features for the test examples and as-yet-unseen examples. Example 7.1 See the image above. Predicting the value of Y Y is a regression task, because Y Y is a real-valued feature. Example e_8 e_8 is an interpolation problem, because the value of X X is between the values of the training examples. Example e_9 e_9 is an extrapolation problem, because the value of X X is outside the range of the range of the training examples.","title":"Supervised Learning"},{"location":"5-semester/MI/10-21-decision-trees/#evaluating-predictions","text":"A point estimate for target feature Y Y on example e e is a prediction of the value of Y(e) Y(e) . Let \\widehat{Y}(e) \\widehat{Y}(e) be the predicted value for target feature Y Y on example e e . The error for this example on this feature is a measure of how close \\widehat Y(e) \\widehat Y(e) is to Y(e) Y(e) . For regression, both \\widehat Y(e) \\widehat Y(e) and Y(e) Y(e) are real numbers that can be compared arithmetically. For classification, when the target feature Y Y is a discrete function, there are a number of alternatives: When the domain of Y Y is binary, we can associate one value with 0 and the other with 1, and the prediction can be some real number. In a cardinal feature the values are mapped to real numbers. This is appropriate when the values in the domain are totally ordered, and the differences between the numbers are meaningful. Often, mapping values to the real line is not appropriate even if the values are totally ordered. Example: Values = short, medium, long short, medium, long . The prediction that the value is either short short or long long is very different from the prediction that the value is medium medium When the domain of a feature is totally ordered, but the differences between the values are not comparable, the features is called an ordinal feature For a totally ordered feature, either cardinal or ordinal, and for a given value v v , a Boolean feature can be constructed as a cut : A new feature that has value 1 1 when Y\\leq v Y\\leq v and 0 0 otherwise. Combining cuts allows for features that are true for intervals. When Y Y is discrete with domain \\{v_1,\\dots,v_k\\} \\{v_1,\\dots,v_k\\} , where k>2 k>2 , a separate prediction can be made for each v_i v_i . This can be modeled by having a binary indicator variable, Y_i Y_i , associated with each value v_i v_i , where Y_i(e)=1 Y_i(e)=1 if Y(e)=v_i Y(e)=v_i , and Y_i(e)=0 Y_i(e)=0 otherwise. Example 7.3","title":"Evaluating Predictions"},{"location":"5-semester/MI/10-21-decision-trees/#prediction-error","text":"In the following measures of prediction error, \u200b E E is a set of examples, and \u200b T T is a set of target features. For target feature Y\\in T Y\\in T and example e\\in E e\\in E , the actual value is Y(e) Y(e) and the predicted value is \\pval \\pval . The 0/1 error on E E is the sum of the number of predictions that are wrong: $$ \\sum_{e\\in E} \\sum_{Y\\in T}Y(e) \\neq \\pval(e)\\ , $$ \u200b where Y(e)\\neq \\pval(e) Y(e)\\neq \\pval(e) is 0 when false and 1 when true. This is the number of incorrect predictions, not taking into account how wrong the predictions are. The absolute error on E E is the sum of the absolute differences between the actual and predicted values: $$ \\sum_{e\\in E}\\sum_{Y \\in T} \\left| Y(e)-\\pval(e) \\right| $$ This is always non-negative, and is only zero when all predictions exactly fit the observed values. Here close predictions are better than far-away predictions. The sum-of-squares error on E E is: \\sum_{e\\in E}\\sum_{Y \\in T}(Y(e)-\\pval(e))^2 \\sum_{e\\in E}\\sum_{Y \\in T}(Y(e)-\\pval(e))^2 This treats large errors as much worse than small errors. The worst-case error on E E is the maximum absolute difference: $$ \\max_{e\\in E}\\max_{Y\\in T} \\left| Y(e) - \\pval(e) \\right| $$ In this case, the learner is evaluated by how bad it can be. These are often described in terms of the norms of the differences between the predicted and actual values. The 0/1 error is the L_0 L_0 error , the absolute error is the L_1 L_1 error , the sum-of-squares error is the square of the L_2 L_2 error , and the worst-case error is the L_\\infty L_\\infty error . The sum-of-squares error is often written as L_2^2 L_2^2 as the L_2 L_2 norm takes the square root of the sum of squares. Example 7.4 For the special case where the domain of Y Y is \\{0,1\\} \\{0,1\\} , and the prediction is in the range [0,1] [0,1] (and so for Boolean domains where true true is treated as 1, and false false as 0), the following can also be used to evaluate predictions: likelihood of the data log-likelihood See https://artint.info/2e/html/ArtInt2e.Ch7.S2.SS1.html#p5","title":"Prediction Error"},{"location":"5-semester/MI/10-21-decision-trees/#types-of-errors","text":"A false-positive error ( type I error ) A false-negative error (type II error) For a given predictor for a given set of examples, suppose tp tp is the number of true positives, fp fp is the number of false positives, fn fn is the number of false negatives, and tn tn is the number of true negatives The following measures are often used: The precision is \\frac{tp}{tp+fp} \\frac{tp}{tp+fp} the proportion of positive predictions that are actual positives. The recall or true-positive rate is \\frac{tp}{tp+fn} \\frac{tp}{tp+fn} the proportion of actual positives that are predicted to be positive. The false-positive rate is \\frac{fp}{fp+tn} \\frac{fp}{fp+tn} the proportion of actual negatives predicted to be positive. An agent should try to maximize precision and recall, and to minimize the false-positive rate. These goals are incompatible however. To compare predictors for a given set of examples, an ROC space or receiver operating characteristic space** plots the false-positive rate against the true-positive rate. Each predictor these examples becomes a point in the space. A precision-recall space plots the precision against the recall. Example 7.6","title":"Types of Errors"},{"location":"5-semester/MI/10-21-decision-trees/#point-estimates-with-no-input-features","text":"The simplest case for learning is when there are no input features, and where there is a single target feature. Best-case for many learning algorithms. Proposition 7.1 Suppose V V is the multiset of values of Y(e) Y(e) for e\\in E e\\in E A prediction that minimizes the 0/1 error is a mode ; one of the values that appears most often. When there are multiple modes, any can be chosen. The prediction that minimizes the sum-of-squares error on E E is the mean of V V (average value) The absolute error is minimized by any median of V V The value that minimizes the worst-case error is (max+min)/2 (max+min)/2 where max max is the maximum value and min min is the minimum value. Full Proposition + proof When the target has domain \\{0,1\\} \\{0,1\\} the training examples can be summarized in n_0 n_0 : the number of examples with the value 0 n_1 n_1 : the number of examples with the value 1. The prediction for each new case is the same number, p p The optimal prediction for p p :","title":"Point Estimates with No Input Features"},{"location":"5-semester/MI/10-21-decision-trees/#learning-decision-trees","text":"Decision tree learning is one of the simplest useful techniques for supervised classification learning. For this section we assume there is a single discrete target feature called the classification . Each element of the domain of the classification is called a class A decision tree or a classification tree is a tree in which each internal (non-leaf) node is labeled with a condition, a Boolean function of examples each internal node has two children, one labeled with true true and the other with false false each leaf of the tree is labeled with a point estimate on the class Example 7.7","title":"Learning Decision Trees"},{"location":"5-semester/MI/10-21-decision-trees/#algorithm","text":"Key question : Which X_i X_i to choose in line 4? Approach: Choose the feature that would provide the best classifier if construction would terminate with that feature. Showing: Number of examples with class labels skip, reads, belonging to different sub-trees \\color{green} \\text{Green}: \\color{green} \\text{Green}: Predicted class label (possibly a tie between two labels)","title":"Algorithm"},{"location":"5-semester/MI/10-21-decision-trees/#class-purity","text":"Principle: Prefer features that split the examples into class pure subsets Normalized to probabilities:","title":"Class Purity"},{"location":"5-semester/MI/10-21-decision-trees/#entropy","text":"","title":"Entropy"},{"location":"5-semester/MI/10-21-decision-trees/#purity-measure","text":"For a probability distribution (p,1-p) (p,1-p) of a two-valued class label, define ^log2 : h(p,1-p)=-p\\cdot\\log_2(p)-(1-p)\\cdot\\log_2(1-p) h(p,1-p)=-p\\cdot\\log_2(p)-(1-p)\\cdot\\log_2(1-p) High values for impure distributions Maximal for (0.5,0.5) (0.5,0.5) Zero for (1,0) (1,0) and (0,1) (0,1) Example","title":"Purity Measure:"},{"location":"5-semester/MI/10-21-decision-trees/#generalization-to-larger-domain","text":"For probability distribution on domain with n n elements: \\bold p=(p_1,\\dots,p_n)\\quad (p_n=1-\\sum_{i=1}^{n-1}p_i) \\bold p=(p_1,\\dots,p_n)\\quad (p_n=1-\\sum_{i=1}^{n-1}p_i) define entropy ^log2 : h(\\bold p)=-\\sum_{i=1}^n p_i\\cdot\\log_2(p_i) h(\\bold p)=-\\sum_{i=1}^n p_i\\cdot\\log_2(p_i) Again: Maximal for \\bold p=(1/n,\\dots,1/n) \\bold p=(1/n,\\dots,1/n) Zero for \\bold p = (1,0,\\dots,0) \\bold p = (1,0,\\dots,0) and so on","title":"Generalization to Larger Domain"},{"location":"5-semester/MI/10-21-decision-trees/#entropy-example","text":"We prefer features that split into subsets with low entropy, but consider example for binary class variable (values c_1,c_2 c_1,c_2 with initial counts c_1:20,c_2:20 c_1:20,c_2:20 ), and two 3-valued features X_1,X_2 X_1,X_2 : X_2 X_2 provides a better division of examples than X_1 X_1 . It gives a lower expected entropy : (1/40)\u00b7 0+(1/40)\u00b7(38/40)\u00b71>(15/40)\u00b70 + (15/40)\u00b70 + (10/40)\u00b71 (1/40)\u00b7 0+(1/40)\u00b7(38/40)\u00b71>(15/40)\u00b70 + (15/40)\u00b70 + (10/40)\u00b71","title":"Entropy Example"},{"location":"5-semester/MI/10-21-decision-trees/#expected-entropy-and-information-gain","text":"For feature X X with domain v_1,\\dots,v_n v_1,\\dots,v_n let: E_i E_i be the set of examples with X=v_i X=v_i q_i=|E_i|/|E| q_i=|E_i|/|E| h_i h_i the entropy of the class label distribution in E_i E_i The expected entropy from splitting on X X then is: $$ h(Class\\mid X)=\\sum_{i=1}^n q_i\\cdot h_i $$ Let h(Class) h(Class) : entropy of class label distribution before splitting The Information Gain from splitting on X X then is: $$ h(Class)-h(Class \\mid X) $$ Information Gain in Decision Tree Learning In line 4 of the algorithm choose feature X_i X_i that gives the highest information gain","title":"Expected Entropy and Information Gain"},{"location":"5-semester/MI/10-21-decision-trees/#continuousmany-valued-attributes","text":"The information gain measure favors attributes with many values For example, the attribute Date will have a very high information gain, but is unable to generalize One approach for avoiding this, is to select attributes based on GainRation: $$ GainRation(S,A)=\\frac{Gain(S,A)}{SplitInformation(S,A)}\\ SplitInformation(S,A)=-\\sum_{i=1}^c\\frac{|S_i|}{|S|}\\log_2 \\frac{|S_i|}{|S|} $$ where S_i S_i is the subset of examples produced by splitting on the i i 'th value of A A Note that SplitInformation is the entropy of S S with regard to the values of A A We require that the attributes being tested are discrete valued. So in order to test a continuous valued attribute we need to \"discretize\" it. Suppose that the training examples are associated with the attribute Temperature: Create a new boolean valued attribute by first testing the two candidate thresholds: (48+60)/2 (48+60)/2 (80+90)/2 (80+90)/2 Next pick the one with the highest information gain (i.e., Temperature_{\\gt 54} Temperature_{\\gt 54} )","title":"Continuous/Many-Valued Attributes"},{"location":"5-semester/MI/10-21-decision-trees/#overfitting","text":"Noise in data may lead to a bad classifier. In particularly, if the decision tree fits the data perfectly. This is called overfitting . Definition A hypothesis h h is said to overfit the training data if there exists some alternative hypothesis h' h' , such that: h h has a smaller error than h' h' over the training data, but h' h' has a smaller error than h h over the entire distribution of instances","title":"Overfitting"},{"location":"5-semester/MI/10-28-neural-networks/","text":"Learning: Neural Networks \u00b6 Tensorflow Playground \\newcommand{\\derr}[1]{\\frac{\\part}{\\part {#1}}}\\nonumber \\newcommand{\\derr}[1]{\\frac{\\part}{\\part {#1}}}\\nonumber Linear Regression and Classification \u00b6 Linear Regression: The problem of fitting a linear function to a set of training examples, in which the input and target features are numeric. \\begin{align*} \\hat Y^{\\overline w}(e) &= w_0+w_1 * X_1(e)+\\dots +w_n * X_n(e)\\\\ &= \\sum^n_{i=0}w_i*X_i(e) \\end{align*} \\begin{align*} \\hat Y^{\\overline w}(e) &= w_0+w_1 * X_1(e)+\\dots +w_n * X_n(e)\\\\ &= \\sum^n_{i=0}w_i*X_i(e) \\end{align*} where \\overline w=\\langle w_0,w_1,\\dots,w_n\\rangle \\overline w=\\langle w_0,w_1,\\dots,w_n\\rangle is a tuple of weights. To make w_0 w_0 not be a special case, new feature X_0 X_0 is always 1. Suppose E E is a set of examples. The sum-of-squares error on E E for Y Y is: \\begin{align*} error(E,\\overline w) &= \\sum_{e\\in E}\\left( Y(e)-\\hat Y^{\\overline w}(e) \\right)^2 \\\\ &= \\sum_{e\\in E}\\left( Y(e)- \\sum^n_{i=0}w_i*X_i(e) \\right)^2 \\end{align*} \\begin{align*} error(E,\\overline w) &= \\sum_{e\\in E}\\left( Y(e)-\\hat Y^{\\overline w}(e) \\right)^2 \\\\ &= \\sum_{e\\in E}\\left( Y(e)- \\sum^n_{i=0}w_i*X_i(e) \\right)^2 \\end{align*} The weights that minimize the error can be computed analytically. A more general approach, is to compute the weights iteratively. Gradient Descent \u00b6 An iterative method for finding the minimum of a function. Starts with an initial set of weights In each step, it decreases each weight in proportion to its partial derivative. w_i:=w_i-\\eta *\\frac{\\partial}{\\partial w_i}error(E, \\overline w) w_i:=w_i-\\eta *\\frac{\\partial}{\\partial w_i}error(E, \\overline w) where \\eta \\eta , the gradient descent step size, is called the learning rate . Given as the input to the learning algorithm Linear Learner \u00b6 Termination is usually after some number of steps, when the error is small or when the changes get small. Not strictly implementing gradient descent. Weights change while it iterates through the examples ( Incremental gradient descent ) If examples are selected at random, its called stochastic gradient descent Batched gradient descent updates the weights after a batch of examples. If batch is equal to all examples, its equivalent to gradient descent. Squashed Linear Functions \u00b6 Consider binary classification. (domain of target is \\{0,1\\} \\{0,1\\} ) The use of a linear function does not work well for such classification tasks; a learner should never make a prediction of greater than 1 or less than 0. A squashed linear function is of the form: where f f , an activation function , is a function from the real line [-\\infty,\\infty] [-\\infty,\\infty] into some subset of the real line, such as [0,1] [0,1] A prediction based on a squashed linear function is a linear classifier . A simple activation function is the step function, step_0(x) step_0(x) , defined by: step_0(x)= \\left \\{ \\begin{array}\\ 1 & \\text{if } x \\geq0 \\\\ 0 & \\text{if } x < 0 \\end{array} \\right . step_0(x)= \\left \\{ \\begin{array}\\ 1 & \\text{if } x \\geq0 \\\\ 0 & \\text{if } x < 0 \\end{array} \\right . One differentiable activation function is the sigmoid or logistic function : $$ sigmoid(x)=\\frac{1}{1+e^{-x}} $$ This function squashes the real line into the interval (0,1) (0,1) Appropriate for classification Differentiable \\frac{d}{dx}sigmoid(x)=sigmoid(x)*(1-sigmoid(x)) \\frac{d}{dx}sigmoid(x)=sigmoid(x)*(1-sigmoid(x)) The problem of determining weights for the sigmoid of a linear function that minimize an error on a set of examples is called logistic regression To optimize the log loss error for logistic regression, minimize the negative log-likelihood: where \\delta(e)=Y(e)-\\hat Y^{\\overline w}(e) \\delta(e)=Y(e)-\\hat Y^{\\overline w}(e) The Linear\\_learner Linear\\_learner algorithm can be modified to carry out logistic regression to minimize log loss. Example 7.11 Linearly Separable \u00b6 Consider each input feature as a dimension. If there are n n features, there will be n n dimensions. A hyperplane in an n n -dimensional space is a set of points that all satisfies a constraint that some linear function of the variable is zero. The hyperplane forms a (n-1) (n-1) -dimensional space. In 2D that's a line In 3D that's a plane A classification is linearly separable if there exists a hyperplane where the classification is true on one side of the hyperplane and false on the other side. Figure 7.11: Linear separators for Boolean functions: Example 7.12 + 7.13 Neural Networks \u00b6 There are many different types of neural networks. The book considers feed-forward neural networks . Hierarchy consisting of linear functions interleaved with activation functions Neural network can have multiple input and target features (real-valued) Discrete features can be transformed into indicator variables or ordinal features . Inputs feed into layers of hidden units Features never directly observed A simple function of the units in a lower layer. Figure 7.16: A deep neural network. Single Neuron \u00b6 Two step computation Combine inputs as weighted sum Compute output by activation function of combined inputs Activation Functions \u00b6 The most common activation functions are: If activation function is sigmoid, i.e. out=\\sigma(\\sum_j i_j\\cdot w_j) out=\\sigma(\\sum_j i_j\\cdot w_j) we also talk of squashed linear function For the output neuron also the identity function is used: af(x)=id(x)=x af(x)=id(x)=x Layers \u00b6 Each layer of units is a function of the previous layer. Each example has a value for each unit. We consider 3 kinds of layers: An input layer consists of a unit for each input feature. Gets its value for an example from the value for the corresponding input feature for that example. A complete linear layer , where each output o_j o_j is a linear function of the input values v_i v_i to the layer (and, as in linear regression , an extra constant input that has value \u201c1\u201d is added) defined by: \u200b o_j=\\sum_i w_{ji}v_i o_j=\\sum_i w_{ji}v_i for weights w_{ji} w_{ji} that are learned. There is a weight for every input-output pair of the layer. in the diagram of Figure 7.16 there is a weight for every arc for the linear functions. An activation layer , where each output o_i o_i is a function of the corresponding input value; thus o_i=f(v_i) o_i=f(v_i) for activation function f f . Typical activation functions are the sigmoid: \\quad f(x)=1/(1+e^{-x}) sigmoid: \\quad f(x)=1/(1+e^{-x}) , and the rectified linear unit ReLU:\\quad f(x)=\\max(0,x) ReLU:\\quad f(x)=\\max(0,x) For regression, where the prediction can be any real number, its typical for the last layer to be complete linear layer. For binary classification, where the output values can be mapped to \\{0,1\\} \\{0,1\\} it is typical for the output to be a sigmoid function of its input We never want to predict a value greater or less than zero. Discrete Inputs \u00b6 If the regression should also use discrete predictor attributes, e.g.: replace discrete attributes with 0-1-valued indicator variables for their possible values: Indicator Variables \u00b6 For each value x_i x_i of X X with domain \\{x_1,\\dots,x_k\\} \\{x_1,\\dots,x_k\\} introduce a binary feature X\\_is\\_x_i X\\_is\\_x_i with values 0,1 0,1 Encode input X=x_i X=x_i by inputs X\\_is\\_x_0=0,\\dots,X\\_is\\_x_{i-1}=0,X\\_is\\_x_i=1,X\\_is\\_x_{i+1},\\dots,X\\_is\\_x_k=0 X\\_is\\_x_0=0,\\dots,X\\_is\\_x_{i-1}=0,X\\_is\\_x_i=1,X\\_is\\_x_{i+1},\\dots,X\\_is\\_x_k=0 Numerical Encoding \u00b6 Translate values into numbers: true, false \\mapsto 0,1 true, false \\mapsto 0,1 low,medium,high\\mapsto 0,1,2 low,medium,high\\mapsto 0,1,2 Probably not sensible: red,green,blue \\mapsto 0,1,2 red,green,blue \\mapsto 0,1,2 blue blue is not \"two times green green \" Neural Networks for Classification \u00b6 Use one output node for each class label. Classify instance by class label associated with output node with highest output value. Propagation in Neural Networks \u00b6 The output of neuron H H is: $$ o_H=\\sigma(1\\cdot0.1+0\\cdot0.1+1\\cdot0.1)=\\color{blue} 0.5498 $$ The output of neuron O O is: o_O=\\sigma(\\color{blue}{0.5498} \\color{black} \\cdot 0.1+1\\cdot0.1)=\\color{red}{0.53867} o_O=\\sigma(\\color{blue}{0.5498} \\color{black} \\cdot 0.1+1\\cdot0.1)=\\color{red}{0.53867} The Perceptron \u00b6 No hidden layer One output neuron o o sign sign activation function Computed: O(x_1,\\dots,x_n)= \\left\\{\\begin{array}{}\\begin{align*} 1 \\quad &\\text{if } w_0x_0+w_1x_1+\\dots w_n x_n\\\\ -1 \\quad &\\text{otherwise} \\end{align*}\\end{array}\\right. O(x_1,\\dots,x_n)= \\left\\{\\begin{array}{}\\begin{align*} 1 \\quad &\\text{if } w_0x_0+w_1x_1+\\dots w_n x_n\\\\ -1 \\quad &\\text{otherwise} \\end{align*}\\end{array}\\right. Convention from now on assume that x_0 x_0 is an input neuron with constant input value 1. Then write \\bold w \\cdot \\bold x \\bold w \\cdot \\bold x for w_0 x_0+w_1x_1+\\dots w_n x_n w_0 x_0+w_1x_1+\\dots w_n x_n Expressive Power \u00b6 The decision surface of a two-input perceptron is given by a straight line, separating positive and negative examples Can represent x_1 \\and x_2 x_1 \\and x_2 Can represent x_1 \\or x_2 x_1 \\or x_2 Cannot! represent x_1 \\text{ xor } x_2 x_1 \\text{ xor } x_2 The examples are not linear separable Multiple Neurons \u00b6 Learning \u00b6 We have \\mathcal{D}=(x_1,x_2,x_3,x_4) \\mathcal{D}=(x_1,x_2,x_3,x_4) input vectors (cases) \\bold t=(-1,1,-1,-1) \\bold t=(-1,1,-1,-1) vector of target outputs \\bold w=(w_0,w_1,w_2) \\bold w=(w_0,w_1,w_2) vector of current parameters \\bold o=(o_1,o_2,o_3,o_4) \\bold o=(o_1,o_2,o_3,o_4) vector of current outputs We request \\bold w^* \\bold w^* parameters yielding \\bold o = \\bold t \\bold o = \\bold t Weight Updating Procedure E>0 \\Rightarrow o E>0 \\Rightarrow o shall be increased \\Rightarrow \\bold x \\cdot \\bold w \\Rightarrow \\bold x \\cdot \\bold w up \\Rightarrow \\bold w:= \\bold w+ \\alpha E x \\Rightarrow \\bold w:= \\bold w+ \\alpha E x E<0 \\Rightarrow o E<0 \\Rightarrow o shall be decreased \\Rightarrow \\bold x \\cdot \\bold w \\Rightarrow \\bold x \\cdot \\bold w down \\Rightarrow \\bold w:= \\bold w+ \\alpha E x \\Rightarrow \\bold w:= \\bold w+ \\alpha E x \\alpha \\alpha is called the learning rate With \\mathcal D \\mathcal D linearly separable and \\alpha \\alpha not too large, this procedure will converge to a correct set of parameters See example in slides 24 ( appendix of this page) Sum of Squared Errors \u00b6 Given data (training examples): (x_i,y_i)\\quad (i=1,\\dots,N) (x_i,y_i)\\quad (i=1,\\dots,N) with x_i x_i : value of input features \\bold X \\bold X y_i y_i : value of output feature Y Y a neural network that computes outputs o_i=out(x_i) o_i=out(x_i) define sum of squared error SSE=\\sum^N_{i=1}(y_i-o_i)^2 SSE=\\sum^N_{i=1}(y_i-o_i)^2 For a given dataset the SSE is a smooth, convex function of the weights Example for n=2 n=2 and a linear activation function: Weights \\bold w \\bold w that minimize E(\\bold w) E(\\bold w) can be found by gradient descent Gradient Descent Learning \u00b6 The gradient is the vector of partial derivatives: \\nabla E[\\bold w]=\\left(\\frac{\\partial E}{\\partial w_o},\\frac{\\partial E}{\\partial w_1},\\dots,\\frac{\\partial E}{\\partial w_n} \\right) \\nabla E[\\bold w]=\\left(\\frac{\\partial E}{\\partial w_o},\\frac{\\partial E}{\\partial w_1},\\dots,\\frac{\\partial E}{\\partial w_n} \\right) The partial derivatives are (with linear activation function): $$ \\frac{\\partial E}{\\partial w_k}=\\sum_{i=1}^N(t_i-\\bold w\\cdot x_i)(-x_{i,k})\\label{gradiant_descent} $$ Gradient descent rule: Initialize \\bold w \\bold w with random values repeat \\bold w := \\bold w - \\eta \\nabla E(\\bold w) \\bold w := \\bold w - \\eta \\nabla E(\\bold w) until \\nabla E(\\bold w) \\approx 0 \\nabla E(\\bold w) \\approx 0 \\eta \\eta is a small constant, the learning rate Properties The procedure converges to the weights \\bold w \\bold w that minimize the SSE (if \\eta \\eta is small enough) Stochastic Gradient Descent \u00b6 Variation of gradient descent: Instead of following the gradient computed from the whole dataset ( \\ref{gradiant_descent} \\ref{gradiant_descent} ) iterate through the data instances one by one, and in one iteration follow the gradient defined by a single data instance (x_k,t_k) (x_k,t_k) \\frac{\\partial E}{\\partial w_i}=(t_k-\\bold w\\cdot x_k)(-x_k,i) \\frac{\\partial E}{\\partial w_i}=(t_k-\\bold w\\cdot x_k)(-x_k,i) The Task of Learning \u00b6 Given : structure and activation functions. To be learned: weights. Goal: given the training examples Find the weights that minimize the sum of squared errors (SSE) $$ \\sum_{i=1}^N\\sum_{j=1}^m(t_{j,i}-o_{j,i})^2, $$ where o_{j,i} o_{j,i} is the value of the j j th output neuron for the i i th data instance. Back-Propagation \u00b6 Back-propagation implements stochastic gradient descent for all weights. There are two properties of differentiation used in back-propagation: Linear rule: the derivative of a linear function is given by; \\derr{w}(aw+b)=a \\derr{w}(aw+b)=a Chain rule: if g g is a function of w w and function f f , which does not depend on w w , is applied to g(w) g(w) , then \\derr w f(g(w))=f'(g(w))*\\derr w g(w) \\derr w f(g(w))=f'(g(w))*\\derr w g(w) Learning consists of two passes through the network for each example: Prediction: given the values on the inputs for each layer, compute a value for the outputs of the layer Back-propagation: go backwards through the layers to update all of the weights of the network. Calculate an error term \\delta_h \\delta_h for a hidden unit by taking the weighted sum of the error terms, \\delta_k \\delta_k for each output units it influences. Updating Rules \u00b6 When using a sigmoid activation function we can derive the following update rule: where $$ \\delta_j=\\begin{array}{}\\left{ \\begin{align } &o_j(1-o_j)(t-o_j) &&\\quad \\text{for output nodes}\\ &o_j(1-o_j) \\sum_{k=1}^m w_{jk}\\delta_k &&\\quad \\text{for hidden nodes} \\end{align }\\right. \\end{array} $$ See example in slides 34 ( appendix of this page)","title":"Learning: Neural Network"},{"location":"5-semester/MI/10-28-neural-networks/#learning-neural-networks","text":"Tensorflow Playground \\newcommand{\\derr}[1]{\\frac{\\part}{\\part {#1}}}\\nonumber \\newcommand{\\derr}[1]{\\frac{\\part}{\\part {#1}}}\\nonumber","title":"Learning: Neural Networks"},{"location":"5-semester/MI/10-28-neural-networks/#linear-regression-and-classification","text":"Linear Regression: The problem of fitting a linear function to a set of training examples, in which the input and target features are numeric. \\begin{align*} \\hat Y^{\\overline w}(e) &= w_0+w_1 * X_1(e)+\\dots +w_n * X_n(e)\\\\ &= \\sum^n_{i=0}w_i*X_i(e) \\end{align*} \\begin{align*} \\hat Y^{\\overline w}(e) &= w_0+w_1 * X_1(e)+\\dots +w_n * X_n(e)\\\\ &= \\sum^n_{i=0}w_i*X_i(e) \\end{align*} where \\overline w=\\langle w_0,w_1,\\dots,w_n\\rangle \\overline w=\\langle w_0,w_1,\\dots,w_n\\rangle is a tuple of weights. To make w_0 w_0 not be a special case, new feature X_0 X_0 is always 1. Suppose E E is a set of examples. The sum-of-squares error on E E for Y Y is: \\begin{align*} error(E,\\overline w) &= \\sum_{e\\in E}\\left( Y(e)-\\hat Y^{\\overline w}(e) \\right)^2 \\\\ &= \\sum_{e\\in E}\\left( Y(e)- \\sum^n_{i=0}w_i*X_i(e) \\right)^2 \\end{align*} \\begin{align*} error(E,\\overline w) &= \\sum_{e\\in E}\\left( Y(e)-\\hat Y^{\\overline w}(e) \\right)^2 \\\\ &= \\sum_{e\\in E}\\left( Y(e)- \\sum^n_{i=0}w_i*X_i(e) \\right)^2 \\end{align*} The weights that minimize the error can be computed analytically. A more general approach, is to compute the weights iteratively.","title":"Linear Regression and Classification"},{"location":"5-semester/MI/10-28-neural-networks/#gradient-descent","text":"An iterative method for finding the minimum of a function. Starts with an initial set of weights In each step, it decreases each weight in proportion to its partial derivative. w_i:=w_i-\\eta *\\frac{\\partial}{\\partial w_i}error(E, \\overline w) w_i:=w_i-\\eta *\\frac{\\partial}{\\partial w_i}error(E, \\overline w) where \\eta \\eta , the gradient descent step size, is called the learning rate . Given as the input to the learning algorithm","title":"Gradient Descent"},{"location":"5-semester/MI/10-28-neural-networks/#linear-learner","text":"Termination is usually after some number of steps, when the error is small or when the changes get small. Not strictly implementing gradient descent. Weights change while it iterates through the examples ( Incremental gradient descent ) If examples are selected at random, its called stochastic gradient descent Batched gradient descent updates the weights after a batch of examples. If batch is equal to all examples, its equivalent to gradient descent.","title":"Linear Learner"},{"location":"5-semester/MI/10-28-neural-networks/#squashed-linear-functions","text":"Consider binary classification. (domain of target is \\{0,1\\} \\{0,1\\} ) The use of a linear function does not work well for such classification tasks; a learner should never make a prediction of greater than 1 or less than 0. A squashed linear function is of the form: where f f , an activation function , is a function from the real line [-\\infty,\\infty] [-\\infty,\\infty] into some subset of the real line, such as [0,1] [0,1] A prediction based on a squashed linear function is a linear classifier . A simple activation function is the step function, step_0(x) step_0(x) , defined by: step_0(x)= \\left \\{ \\begin{array}\\ 1 & \\text{if } x \\geq0 \\\\ 0 & \\text{if } x < 0 \\end{array} \\right . step_0(x)= \\left \\{ \\begin{array}\\ 1 & \\text{if } x \\geq0 \\\\ 0 & \\text{if } x < 0 \\end{array} \\right . One differentiable activation function is the sigmoid or logistic function : $$ sigmoid(x)=\\frac{1}{1+e^{-x}} $$ This function squashes the real line into the interval (0,1) (0,1) Appropriate for classification Differentiable \\frac{d}{dx}sigmoid(x)=sigmoid(x)*(1-sigmoid(x)) \\frac{d}{dx}sigmoid(x)=sigmoid(x)*(1-sigmoid(x)) The problem of determining weights for the sigmoid of a linear function that minimize an error on a set of examples is called logistic regression To optimize the log loss error for logistic regression, minimize the negative log-likelihood: where \\delta(e)=Y(e)-\\hat Y^{\\overline w}(e) \\delta(e)=Y(e)-\\hat Y^{\\overline w}(e) The Linear\\_learner Linear\\_learner algorithm can be modified to carry out logistic regression to minimize log loss. Example 7.11","title":"Squashed Linear Functions"},{"location":"5-semester/MI/10-28-neural-networks/#linearly-separable","text":"Consider each input feature as a dimension. If there are n n features, there will be n n dimensions. A hyperplane in an n n -dimensional space is a set of points that all satisfies a constraint that some linear function of the variable is zero. The hyperplane forms a (n-1) (n-1) -dimensional space. In 2D that's a line In 3D that's a plane A classification is linearly separable if there exists a hyperplane where the classification is true on one side of the hyperplane and false on the other side. Figure 7.11: Linear separators for Boolean functions: Example 7.12 + 7.13","title":"Linearly Separable"},{"location":"5-semester/MI/10-28-neural-networks/#neural-networks","text":"There are many different types of neural networks. The book considers feed-forward neural networks . Hierarchy consisting of linear functions interleaved with activation functions Neural network can have multiple input and target features (real-valued) Discrete features can be transformed into indicator variables or ordinal features . Inputs feed into layers of hidden units Features never directly observed A simple function of the units in a lower layer. Figure 7.16: A deep neural network.","title":"Neural Networks"},{"location":"5-semester/MI/10-28-neural-networks/#single-neuron","text":"Two step computation Combine inputs as weighted sum Compute output by activation function of combined inputs","title":"Single Neuron"},{"location":"5-semester/MI/10-28-neural-networks/#activation-functions","text":"The most common activation functions are: If activation function is sigmoid, i.e. out=\\sigma(\\sum_j i_j\\cdot w_j) out=\\sigma(\\sum_j i_j\\cdot w_j) we also talk of squashed linear function For the output neuron also the identity function is used: af(x)=id(x)=x af(x)=id(x)=x","title":"Activation Functions"},{"location":"5-semester/MI/10-28-neural-networks/#layers","text":"Each layer of units is a function of the previous layer. Each example has a value for each unit. We consider 3 kinds of layers: An input layer consists of a unit for each input feature. Gets its value for an example from the value for the corresponding input feature for that example. A complete linear layer , where each output o_j o_j is a linear function of the input values v_i v_i to the layer (and, as in linear regression , an extra constant input that has value \u201c1\u201d is added) defined by: \u200b o_j=\\sum_i w_{ji}v_i o_j=\\sum_i w_{ji}v_i for weights w_{ji} w_{ji} that are learned. There is a weight for every input-output pair of the layer. in the diagram of Figure 7.16 there is a weight for every arc for the linear functions. An activation layer , where each output o_i o_i is a function of the corresponding input value; thus o_i=f(v_i) o_i=f(v_i) for activation function f f . Typical activation functions are the sigmoid: \\quad f(x)=1/(1+e^{-x}) sigmoid: \\quad f(x)=1/(1+e^{-x}) , and the rectified linear unit ReLU:\\quad f(x)=\\max(0,x) ReLU:\\quad f(x)=\\max(0,x) For regression, where the prediction can be any real number, its typical for the last layer to be complete linear layer. For binary classification, where the output values can be mapped to \\{0,1\\} \\{0,1\\} it is typical for the output to be a sigmoid function of its input We never want to predict a value greater or less than zero.","title":"Layers"},{"location":"5-semester/MI/10-28-neural-networks/#discrete-inputs","text":"If the regression should also use discrete predictor attributes, e.g.: replace discrete attributes with 0-1-valued indicator variables for their possible values:","title":"Discrete Inputs"},{"location":"5-semester/MI/10-28-neural-networks/#indicator-variables","text":"For each value x_i x_i of X X with domain \\{x_1,\\dots,x_k\\} \\{x_1,\\dots,x_k\\} introduce a binary feature X\\_is\\_x_i X\\_is\\_x_i with values 0,1 0,1 Encode input X=x_i X=x_i by inputs X\\_is\\_x_0=0,\\dots,X\\_is\\_x_{i-1}=0,X\\_is\\_x_i=1,X\\_is\\_x_{i+1},\\dots,X\\_is\\_x_k=0 X\\_is\\_x_0=0,\\dots,X\\_is\\_x_{i-1}=0,X\\_is\\_x_i=1,X\\_is\\_x_{i+1},\\dots,X\\_is\\_x_k=0","title":"Indicator Variables"},{"location":"5-semester/MI/10-28-neural-networks/#numerical-encoding","text":"Translate values into numbers: true, false \\mapsto 0,1 true, false \\mapsto 0,1 low,medium,high\\mapsto 0,1,2 low,medium,high\\mapsto 0,1,2 Probably not sensible: red,green,blue \\mapsto 0,1,2 red,green,blue \\mapsto 0,1,2 blue blue is not \"two times green green \"","title":"Numerical Encoding"},{"location":"5-semester/MI/10-28-neural-networks/#neural-networks-for-classification","text":"Use one output node for each class label. Classify instance by class label associated with output node with highest output value.","title":"Neural Networks for Classification"},{"location":"5-semester/MI/10-28-neural-networks/#propagation-in-neural-networks","text":"The output of neuron H H is: $$ o_H=\\sigma(1\\cdot0.1+0\\cdot0.1+1\\cdot0.1)=\\color{blue} 0.5498 $$ The output of neuron O O is: o_O=\\sigma(\\color{blue}{0.5498} \\color{black} \\cdot 0.1+1\\cdot0.1)=\\color{red}{0.53867} o_O=\\sigma(\\color{blue}{0.5498} \\color{black} \\cdot 0.1+1\\cdot0.1)=\\color{red}{0.53867}","title":"Propagation in Neural Networks"},{"location":"5-semester/MI/10-28-neural-networks/#the-perceptron","text":"No hidden layer One output neuron o o sign sign activation function Computed: O(x_1,\\dots,x_n)= \\left\\{\\begin{array}{}\\begin{align*} 1 \\quad &\\text{if } w_0x_0+w_1x_1+\\dots w_n x_n\\\\ -1 \\quad &\\text{otherwise} \\end{align*}\\end{array}\\right. O(x_1,\\dots,x_n)= \\left\\{\\begin{array}{}\\begin{align*} 1 \\quad &\\text{if } w_0x_0+w_1x_1+\\dots w_n x_n\\\\ -1 \\quad &\\text{otherwise} \\end{align*}\\end{array}\\right. Convention from now on assume that x_0 x_0 is an input neuron with constant input value 1. Then write \\bold w \\cdot \\bold x \\bold w \\cdot \\bold x for w_0 x_0+w_1x_1+\\dots w_n x_n w_0 x_0+w_1x_1+\\dots w_n x_n","title":"The Perceptron"},{"location":"5-semester/MI/10-28-neural-networks/#expressive-power","text":"The decision surface of a two-input perceptron is given by a straight line, separating positive and negative examples Can represent x_1 \\and x_2 x_1 \\and x_2 Can represent x_1 \\or x_2 x_1 \\or x_2 Cannot! represent x_1 \\text{ xor } x_2 x_1 \\text{ xor } x_2 The examples are not linear separable","title":"Expressive Power"},{"location":"5-semester/MI/10-28-neural-networks/#multiple-neurons","text":"","title":"Multiple Neurons"},{"location":"5-semester/MI/10-28-neural-networks/#learning","text":"We have \\mathcal{D}=(x_1,x_2,x_3,x_4) \\mathcal{D}=(x_1,x_2,x_3,x_4) input vectors (cases) \\bold t=(-1,1,-1,-1) \\bold t=(-1,1,-1,-1) vector of target outputs \\bold w=(w_0,w_1,w_2) \\bold w=(w_0,w_1,w_2) vector of current parameters \\bold o=(o_1,o_2,o_3,o_4) \\bold o=(o_1,o_2,o_3,o_4) vector of current outputs We request \\bold w^* \\bold w^* parameters yielding \\bold o = \\bold t \\bold o = \\bold t Weight Updating Procedure E>0 \\Rightarrow o E>0 \\Rightarrow o shall be increased \\Rightarrow \\bold x \\cdot \\bold w \\Rightarrow \\bold x \\cdot \\bold w up \\Rightarrow \\bold w:= \\bold w+ \\alpha E x \\Rightarrow \\bold w:= \\bold w+ \\alpha E x E<0 \\Rightarrow o E<0 \\Rightarrow o shall be decreased \\Rightarrow \\bold x \\cdot \\bold w \\Rightarrow \\bold x \\cdot \\bold w down \\Rightarrow \\bold w:= \\bold w+ \\alpha E x \\Rightarrow \\bold w:= \\bold w+ \\alpha E x \\alpha \\alpha is called the learning rate With \\mathcal D \\mathcal D linearly separable and \\alpha \\alpha not too large, this procedure will converge to a correct set of parameters See example in slides 24 ( appendix of this page)","title":"Learning"},{"location":"5-semester/MI/10-28-neural-networks/#sum-of-squared-errors","text":"Given data (training examples): (x_i,y_i)\\quad (i=1,\\dots,N) (x_i,y_i)\\quad (i=1,\\dots,N) with x_i x_i : value of input features \\bold X \\bold X y_i y_i : value of output feature Y Y a neural network that computes outputs o_i=out(x_i) o_i=out(x_i) define sum of squared error SSE=\\sum^N_{i=1}(y_i-o_i)^2 SSE=\\sum^N_{i=1}(y_i-o_i)^2 For a given dataset the SSE is a smooth, convex function of the weights Example for n=2 n=2 and a linear activation function: Weights \\bold w \\bold w that minimize E(\\bold w) E(\\bold w) can be found by gradient descent","title":"Sum of Squared Errors"},{"location":"5-semester/MI/10-28-neural-networks/#gradient-descent-learning","text":"The gradient is the vector of partial derivatives: \\nabla E[\\bold w]=\\left(\\frac{\\partial E}{\\partial w_o},\\frac{\\partial E}{\\partial w_1},\\dots,\\frac{\\partial E}{\\partial w_n} \\right) \\nabla E[\\bold w]=\\left(\\frac{\\partial E}{\\partial w_o},\\frac{\\partial E}{\\partial w_1},\\dots,\\frac{\\partial E}{\\partial w_n} \\right) The partial derivatives are (with linear activation function): $$ \\frac{\\partial E}{\\partial w_k}=\\sum_{i=1}^N(t_i-\\bold w\\cdot x_i)(-x_{i,k})\\label{gradiant_descent} $$ Gradient descent rule: Initialize \\bold w \\bold w with random values repeat \\bold w := \\bold w - \\eta \\nabla E(\\bold w) \\bold w := \\bold w - \\eta \\nabla E(\\bold w) until \\nabla E(\\bold w) \\approx 0 \\nabla E(\\bold w) \\approx 0 \\eta \\eta is a small constant, the learning rate Properties The procedure converges to the weights \\bold w \\bold w that minimize the SSE (if \\eta \\eta is small enough)","title":"Gradient Descent Learning"},{"location":"5-semester/MI/10-28-neural-networks/#stochastic-gradient-descent","text":"Variation of gradient descent: Instead of following the gradient computed from the whole dataset ( \\ref{gradiant_descent} \\ref{gradiant_descent} ) iterate through the data instances one by one, and in one iteration follow the gradient defined by a single data instance (x_k,t_k) (x_k,t_k) \\frac{\\partial E}{\\partial w_i}=(t_k-\\bold w\\cdot x_k)(-x_k,i) \\frac{\\partial E}{\\partial w_i}=(t_k-\\bold w\\cdot x_k)(-x_k,i)","title":"Stochastic Gradient Descent"},{"location":"5-semester/MI/10-28-neural-networks/#the-task-of-learning","text":"Given : structure and activation functions. To be learned: weights. Goal: given the training examples Find the weights that minimize the sum of squared errors (SSE) $$ \\sum_{i=1}^N\\sum_{j=1}^m(t_{j,i}-o_{j,i})^2, $$ where o_{j,i} o_{j,i} is the value of the j j th output neuron for the i i th data instance.","title":"The Task of Learning"},{"location":"5-semester/MI/10-28-neural-networks/#back-propagation","text":"Back-propagation implements stochastic gradient descent for all weights. There are two properties of differentiation used in back-propagation: Linear rule: the derivative of a linear function is given by; \\derr{w}(aw+b)=a \\derr{w}(aw+b)=a Chain rule: if g g is a function of w w and function f f , which does not depend on w w , is applied to g(w) g(w) , then \\derr w f(g(w))=f'(g(w))*\\derr w g(w) \\derr w f(g(w))=f'(g(w))*\\derr w g(w) Learning consists of two passes through the network for each example: Prediction: given the values on the inputs for each layer, compute a value for the outputs of the layer Back-propagation: go backwards through the layers to update all of the weights of the network. Calculate an error term \\delta_h \\delta_h for a hidden unit by taking the weighted sum of the error terms, \\delta_k \\delta_k for each output units it influences.","title":"Back-Propagation"},{"location":"5-semester/MI/10-28-neural-networks/#updating-rules","text":"When using a sigmoid activation function we can derive the following update rule: where $$ \\delta_j=\\begin{array}{}\\left{ \\begin{align } &o_j(1-o_j)(t-o_j) &&\\quad \\text{for output nodes}\\ &o_j(1-o_j) \\sum_{k=1}^m w_{jk}\\delta_k &&\\quad \\text{for hidden nodes} \\end{align }\\right. \\end{array} $$ See example in slides 34 ( appendix of this page)","title":"Updating Rules"},{"location":"5-semester/MI/10-28appendix/","text":"Learning: Neural Network - Appendix \u00b6 Learning Weights and Threshold Example \u00b6 Back Propagation Example \u00b6","title":"Learning: Neural Network - Appendix"},{"location":"5-semester/MI/10-28appendix/#learning-neural-network-appendix","text":"","title":"Learning: Neural Network - Appendix"},{"location":"5-semester/MI/10-28appendix/#learning-weights-and-threshold-example","text":"","title":"Learning Weights and Threshold Example"},{"location":"5-semester/MI/10-28appendix/#back-propagation-example","text":"","title":"Back Propagation Example"},{"location":"5-semester/MI/11-04-methods-and-issues/","text":"Learning: Methods and Issues \u00b6 Probabilistic Models \u00b6 Word occurrence in emails (thousands of input features!): Naive Bayes Classifier \u00b6 Assumption The input features are conditionally independent of each other given the classification Classify email as spam if $$ P(Spam=yes \\mid \\bold X = \\bold x) > threshold $$ where \\bold X=(abacus, \\dots ,zytogenic) \\bold X=(abacus, \\dots ,zytogenic) and \\bold x \\bold x is a corresponding set of y/n values Structural Assumption Learning Need to learn entries in conditional probability tables Simplest approach: use empirical frequencies , e.g.: Given an example with inputs X_1=x_1,\\dots, X_k=x_k X_1=x_1,\\dots, X_k=x_k , Bayes Rule is used to compute the posterior probability distribution of the example's classification, Y Y $$ P(Y \\mid X_1=x_1,\\dots, X_k=x_k)= {P(Y)\\cdot P(x_1 \\mid Y)\\cdots P(x_k \\mid Y) \\over \\sum_Y P(Y)\\cdot P(x_1 \\mid Y)\\cdots P(x_k \\mid Y)} $$ Example \u00b6 Pseudo Counts \u00b6 When leaning the naive Bayes model, we estimated P(long \\mid reads) P(long \\mid reads) as $$ P(long \\mid reads) = {0\\over 9} $$ based on 9 cases only, \\leadsto \\leadsto unreliable parameter estimates and risk of zero probabilities. The solution is pseudo counts where p_{ac} p_{ac} is our prior estimate of the probability (often chosen as a uniform distribution), and m m is a virtual sample size (determining the weight of p_ac p_ac relative to the observed data) Example \u00b6 P(known \\mid reads) = {2+0.5\\cdot m \\over 3+m} P(known \\mid reads) = {2+0.5\\cdot m \\over 3+m} Assumptions \u00b6 Attributes not independent given Symbol=1! Attributes not independent given Spam=yes! \\leadsto \\leadsto Naive Bayes assumption often not realistic. Nevertheless, Naive Bayes often successful. When Naive Bayes Must Fail \u00b6 No Naive Bayes Classifier can produce the following classification because assume it did, then: Multiplying the four left sides and the four right sides of these inequalities: \\prod^4_{i=1}(left\\ side\\ of\\ i) > \\prod^4_{i=1}(right\\ side\\ of\\ i) \\prod^4_{i=1}(left\\ side\\ of\\ i) > \\prod^4_{i=1}(right\\ side\\ of\\ i) But this is false, because both products are actually equal. Case-based Reasoning \u00b6 Idea: To predict the output feature of a new example e e find among the training examples the one (several) most similar to e e predict the output for e e from the known output values of the similar cases Several names for this approach Instance Based Learning Lazy Learning Case-based Reasoning Required: Distance measure on values of input features Distance Measures for Numeric Features \u00b6 If all features \\bold X \\bold X are numeric: Euclidian distance d(x,x')=\\sqrt{\\Sigma_i(x_i-x'_i)^2} d(x,x')=\\sqrt{\\Sigma_i(x_i-x'_i)^2} Manhattan distance d(x,x')=\\Sigma_i|x_i-x_i'| d(x,x')=\\Sigma_i|x_i-x_i'| Distance Measure for Discrete Features \u00b6 For a single feature X X with domain \\{x_i,\\dots x_k\\}: \\{x_i,\\dots x_k\\}: Zero-One distance d(x_i,x_j) = \\begin{array}{}\\left\\{\\begin{align*} &0 &&\\quad\\text{if } j=i\\\\ &1 &&\\quad\\text{if } j\\neq i \\end{align*}\\right.\\end{array} d(x_i,x_j) = \\begin{array}{}\\left\\{\\begin{align*} &0 &&\\quad\\text{if } j=i\\\\ &1 &&\\quad\\text{if } j\\neq i \\end{align*}\\right.\\end{array} Distance Matrix For each pair x_i,x_j x_i,x_j specify the distance d(x_i,x_j) d(x_i,x_j) in a k\\times k k\\times k -matrix. Example For a set of discrete features \\bold X \\bold X : Define distance d_i d_i and weight w_i w_i for each X_i\\in \\bold X X_i\\in \\bold X Define d(x,x')=\\Sigma_i w_i d_i (x_i,x_i') d(x,x')=\\Sigma_i w_i d_i (x_i,x_i') K-Nearest-Neighbor Classifier \u00b6 Given training examples (x_i,y_i)\\quad(i=1,\\dots N) (x_i,y_i)\\quad(i=1,\\dots N) , a new case \\bold x \\bold x to be classified, and a distance measure d(x,x') d(x,x') classify \\bold x \\bold x as follows: Find the K K training examples x_{i_1},\\dots x_{i_K} x_{i_1},\\dots x_{i_K} with minimal distance to \\bold x \\bold x Predict for \\bold x \\bold x the most frequent value in y_{i_1},\\dots y_{i_K} y_{i_1},\\dots y_{i_K} Example \u00b6 Output feature: red, blue, green red, blue, green Two numeric input features Euclidian distance Colored dots: training examples Colored regions: regions of input values that will be classified as that color Overfitting \u00b6 Occurs when the learner makes predictions based on regularities that appears in the training examples, but not in the test examples or the real world. Example 7.14 Example 7.15 Overfitting caused by model complexity A model overfits the training data if a smaller error on future data would be obtained by a less complex model. Avoid Overfitting \u00b6 Do not allow overly complex models: Naive Bayes model K-NN for \"large\" K Do not allow to learn models that are too complex (relative to the available data): Decision Trees: use an early stopping criterion, e.g. stop construction when (sub-)set of trainnig examples contains fewer than k k elements (for not too small k k ) Add to evaluation: measure a penalty term for complexity. This penalty term can have an interpretation as a prior model probability , or model description length These techniques will usually lead to more complex models only when the data strongly supports it (especially: large number of examples) Validation Data \u00b6 Idea Reserve part of the training examples as a validation set Not used during model search Used as proxy for future data in model evaluation Train/Validation Split Simplest approach: split the available data randomly into a training and a validation set. Typically: 50/50, or 66/33 Example: Decision Trees \u00b6 Post Pruning Use of validation set in decision tree learning: Build decision tree using training set For each internal node: Test whether accuracy on validation set is improved by replacing sub-tree rooted at this node by single leaf (labeled with most frequent target feature value of training examples in this sub-tree) If yes: replace sub-tree with leaf ( prune sub-tree) Example K-NN and Neural Networks \u00b6 Selection of K K For K=1,2,3,\\dots K=1,2,3,\\dots Measure accuracy of K K -NN based on training examples for validation examples Use K K with best performance on validation examples Validation examples can now be merged with training examples for prediction future cases Selection of Neural Network Structure For \\#hl=1,2,\\dots ,max \\#hl=1,2,\\dots ,max and \\#nd=1,2,\\dots,max \\#nd=1,2,\\dots,max : Learn neural network with #hl hidden layers and #nd nodes per hidden layer using training examples Evaluate SSE of learned network on validation examples Select a network structure with minimal SSE Re-learn weights using merged training and validation examples Cross Validation \u00b6 The idea of using part of the training data as a surrogate for test data. Simplest: We split the data into two: A set of examples A validation set The idea is to choose a parameter setting or a representation in which the error of the validation set is a minimum (see figure 7.14) The method of k-fold cross validation allows us to reuse examples for both training and validation. It has the following steps: Partition training set into k k sets, of approximately equal size called folds To evaluate a parameter setting, train k k times for that parameter setting, each time using one of the folds as the validation set, and the remaining for training. The accuracy is evaluated using the validation set. For example, if k=10 k=10 then 90\\% 90\\% of the training examples are used for training and 10\\% 10\\% for validation. It does this 10 times, so each example is used once in a validation set. Optimize parameter settings based on the error on each example when it is used in the validation set. Return the model with the selected parameter settings, trained on all of the data. Example 7.19 At one extreme when k is the number of training examples, it becomes leave-one-out cross validation .","title":"Learning: Methods and Issues"},{"location":"5-semester/MI/11-04-methods-and-issues/#learning-methods-and-issues","text":"","title":"Learning: Methods and Issues"},{"location":"5-semester/MI/11-04-methods-and-issues/#probabilistic-models","text":"Word occurrence in emails (thousands of input features!):","title":"Probabilistic Models"},{"location":"5-semester/MI/11-04-methods-and-issues/#naive-bayes-classifier","text":"Assumption The input features are conditionally independent of each other given the classification Classify email as spam if $$ P(Spam=yes \\mid \\bold X = \\bold x) > threshold $$ where \\bold X=(abacus, \\dots ,zytogenic) \\bold X=(abacus, \\dots ,zytogenic) and \\bold x \\bold x is a corresponding set of y/n values Structural Assumption Learning Need to learn entries in conditional probability tables Simplest approach: use empirical frequencies , e.g.: Given an example with inputs X_1=x_1,\\dots, X_k=x_k X_1=x_1,\\dots, X_k=x_k , Bayes Rule is used to compute the posterior probability distribution of the example's classification, Y Y $$ P(Y \\mid X_1=x_1,\\dots, X_k=x_k)= {P(Y)\\cdot P(x_1 \\mid Y)\\cdots P(x_k \\mid Y) \\over \\sum_Y P(Y)\\cdot P(x_1 \\mid Y)\\cdots P(x_k \\mid Y)} $$","title":"Naive Bayes Classifier"},{"location":"5-semester/MI/11-04-methods-and-issues/#example","text":"","title":"Example"},{"location":"5-semester/MI/11-04-methods-and-issues/#pseudo-counts","text":"When leaning the naive Bayes model, we estimated P(long \\mid reads) P(long \\mid reads) as $$ P(long \\mid reads) = {0\\over 9} $$ based on 9 cases only, \\leadsto \\leadsto unreliable parameter estimates and risk of zero probabilities. The solution is pseudo counts where p_{ac} p_{ac} is our prior estimate of the probability (often chosen as a uniform distribution), and m m is a virtual sample size (determining the weight of p_ac p_ac relative to the observed data)","title":"Pseudo Counts"},{"location":"5-semester/MI/11-04-methods-and-issues/#example_1","text":"P(known \\mid reads) = {2+0.5\\cdot m \\over 3+m} P(known \\mid reads) = {2+0.5\\cdot m \\over 3+m}","title":"Example"},{"location":"5-semester/MI/11-04-methods-and-issues/#assumptions","text":"Attributes not independent given Symbol=1! Attributes not independent given Spam=yes! \\leadsto \\leadsto Naive Bayes assumption often not realistic. Nevertheless, Naive Bayes often successful.","title":"Assumptions"},{"location":"5-semester/MI/11-04-methods-and-issues/#when-naive-bayes-must-fail","text":"No Naive Bayes Classifier can produce the following classification because assume it did, then: Multiplying the four left sides and the four right sides of these inequalities: \\prod^4_{i=1}(left\\ side\\ of\\ i) > \\prod^4_{i=1}(right\\ side\\ of\\ i) \\prod^4_{i=1}(left\\ side\\ of\\ i) > \\prod^4_{i=1}(right\\ side\\ of\\ i) But this is false, because both products are actually equal.","title":"When Naive Bayes Must Fail"},{"location":"5-semester/MI/11-04-methods-and-issues/#case-based-reasoning","text":"Idea: To predict the output feature of a new example e e find among the training examples the one (several) most similar to e e predict the output for e e from the known output values of the similar cases Several names for this approach Instance Based Learning Lazy Learning Case-based Reasoning Required: Distance measure on values of input features","title":"Case-based Reasoning"},{"location":"5-semester/MI/11-04-methods-and-issues/#distance-measures-for-numeric-features","text":"If all features \\bold X \\bold X are numeric: Euclidian distance d(x,x')=\\sqrt{\\Sigma_i(x_i-x'_i)^2} d(x,x')=\\sqrt{\\Sigma_i(x_i-x'_i)^2} Manhattan distance d(x,x')=\\Sigma_i|x_i-x_i'| d(x,x')=\\Sigma_i|x_i-x_i'|","title":"Distance Measures for Numeric Features"},{"location":"5-semester/MI/11-04-methods-and-issues/#distance-measure-for-discrete-features","text":"For a single feature X X with domain \\{x_i,\\dots x_k\\}: \\{x_i,\\dots x_k\\}: Zero-One distance d(x_i,x_j) = \\begin{array}{}\\left\\{\\begin{align*} &0 &&\\quad\\text{if } j=i\\\\ &1 &&\\quad\\text{if } j\\neq i \\end{align*}\\right.\\end{array} d(x_i,x_j) = \\begin{array}{}\\left\\{\\begin{align*} &0 &&\\quad\\text{if } j=i\\\\ &1 &&\\quad\\text{if } j\\neq i \\end{align*}\\right.\\end{array} Distance Matrix For each pair x_i,x_j x_i,x_j specify the distance d(x_i,x_j) d(x_i,x_j) in a k\\times k k\\times k -matrix. Example For a set of discrete features \\bold X \\bold X : Define distance d_i d_i and weight w_i w_i for each X_i\\in \\bold X X_i\\in \\bold X Define d(x,x')=\\Sigma_i w_i d_i (x_i,x_i') d(x,x')=\\Sigma_i w_i d_i (x_i,x_i')","title":"Distance Measure for Discrete Features"},{"location":"5-semester/MI/11-04-methods-and-issues/#k-nearest-neighbor-classifier","text":"Given training examples (x_i,y_i)\\quad(i=1,\\dots N) (x_i,y_i)\\quad(i=1,\\dots N) , a new case \\bold x \\bold x to be classified, and a distance measure d(x,x') d(x,x') classify \\bold x \\bold x as follows: Find the K K training examples x_{i_1},\\dots x_{i_K} x_{i_1},\\dots x_{i_K} with minimal distance to \\bold x \\bold x Predict for \\bold x \\bold x the most frequent value in y_{i_1},\\dots y_{i_K} y_{i_1},\\dots y_{i_K}","title":"K-Nearest-Neighbor Classifier"},{"location":"5-semester/MI/11-04-methods-and-issues/#example_2","text":"Output feature: red, blue, green red, blue, green Two numeric input features Euclidian distance Colored dots: training examples Colored regions: regions of input values that will be classified as that color","title":"Example"},{"location":"5-semester/MI/11-04-methods-and-issues/#overfitting","text":"Occurs when the learner makes predictions based on regularities that appears in the training examples, but not in the test examples or the real world. Example 7.14 Example 7.15 Overfitting caused by model complexity A model overfits the training data if a smaller error on future data would be obtained by a less complex model.","title":"Overfitting"},{"location":"5-semester/MI/11-04-methods-and-issues/#avoid-overfitting","text":"Do not allow overly complex models: Naive Bayes model K-NN for \"large\" K Do not allow to learn models that are too complex (relative to the available data): Decision Trees: use an early stopping criterion, e.g. stop construction when (sub-)set of trainnig examples contains fewer than k k elements (for not too small k k ) Add to evaluation: measure a penalty term for complexity. This penalty term can have an interpretation as a prior model probability , or model description length These techniques will usually lead to more complex models only when the data strongly supports it (especially: large number of examples)","title":"Avoid Overfitting"},{"location":"5-semester/MI/11-04-methods-and-issues/#validation-data","text":"Idea Reserve part of the training examples as a validation set Not used during model search Used as proxy for future data in model evaluation Train/Validation Split Simplest approach: split the available data randomly into a training and a validation set. Typically: 50/50, or 66/33","title":"Validation Data"},{"location":"5-semester/MI/11-04-methods-and-issues/#example-decision-trees","text":"Post Pruning Use of validation set in decision tree learning: Build decision tree using training set For each internal node: Test whether accuracy on validation set is improved by replacing sub-tree rooted at this node by single leaf (labeled with most frequent target feature value of training examples in this sub-tree) If yes: replace sub-tree with leaf ( prune sub-tree)","title":"Example: Decision Trees"},{"location":"5-semester/MI/11-04-methods-and-issues/#example-k-nn-and-neural-networks","text":"Selection of K K For K=1,2,3,\\dots K=1,2,3,\\dots Measure accuracy of K K -NN based on training examples for validation examples Use K K with best performance on validation examples Validation examples can now be merged with training examples for prediction future cases Selection of Neural Network Structure For \\#hl=1,2,\\dots ,max \\#hl=1,2,\\dots ,max and \\#nd=1,2,\\dots,max \\#nd=1,2,\\dots,max : Learn neural network with #hl hidden layers and #nd nodes per hidden layer using training examples Evaluate SSE of learned network on validation examples Select a network structure with minimal SSE Re-learn weights using merged training and validation examples","title":"Example K-NN and Neural Networks"},{"location":"5-semester/MI/11-04-methods-and-issues/#cross-validation","text":"The idea of using part of the training data as a surrogate for test data. Simplest: We split the data into two: A set of examples A validation set The idea is to choose a parameter setting or a representation in which the error of the validation set is a minimum (see figure 7.14) The method of k-fold cross validation allows us to reuse examples for both training and validation. It has the following steps: Partition training set into k k sets, of approximately equal size called folds To evaluate a parameter setting, train k k times for that parameter setting, each time using one of the folds as the validation set, and the remaining for training. The accuracy is evaluated using the validation set. For example, if k=10 k=10 then 90\\% 90\\% of the training examples are used for training and 10\\% 10\\% for validation. It does this 10 times, so each example is used once in a validation set. Optimize parameter settings based on the error on each example when it is used in the validation set. Return the model with the selected parameter settings, trained on all of the data. Example 7.19 At one extreme when k is the number of training examples, it becomes leave-one-out cross validation .","title":"Cross Validation"},{"location":"5-semester/MI/11-11-clustering/","text":"Learning: Clustering \u00b6 Examples: Based on customer data, find group of customers with similar profiles Based on image data, find groups of images with similar motif. Based on article data, find groups of articles with the same topics ... K-Means Algorithm \u00b6 General goal: find clustering with: Large between-cluster variation small within-cluster variation We consider the scenario, where the number k k of clusters is known we have a distance measure d(x_i,x_j) d(x_i,x_j) between pairs of data points (feature vectors) we can calculate a centroid for a collection of data points S=\\{x_1,\\dots,x_n\\} S=\\{x_1,\\dots,x_n\\} Example: Session 11.11 Slide 9 Result can depend on choice of initial cluster centers! K-means as an Optimization Problem \u00b6 Assume we use Euclidian distance d as proximity measure and that the quality of the clustering is measured by the sum of squared errors: where c_i c_i is the i'th centroid C_i\\in S C_i\\in S is the points closets to c_i c_i according to d d In principle We can minimize the SSE by looking at all possible partitionings \\leadsto \\leadsto not feasible though! Instead, k-means : The centroid that minimizes the SSE is the mean of the data-points in that cluster: c_i = \\frac{1}{|C_i|}*\\sum_{x\\in C_i}{x} c_i = \\frac{1}{|C_i|}*\\sum_{x\\in C_i}{x} Local optimum found by alternating between cluster assignments and centroid estimation. Convergence \u00b6 The k -means algorithm is guaranteed to converge Each step reduces the sum of squared errors There is only a finite number of cluster assignments There is no guarantee of reaching the global optimum: Improve by running with multiple random restarts Outliers \u00b6 The result of partitional clustering can be skewed by outliers Different Measuring Scales \u00b6 Instances defined by attributes All distance functions for continuous attributes dominated by income values \\leadsto \\leadsto may need to rescale or normalize continuous attributes Min-Max Normalization \u00b6 Replace A_i A_i with $$ A_i-\\min(A_i)\\over \\max(A_i)-\\min(A_i) $$ where \\min(A_i),\\max(A_i) \\min(A_i),\\max(A_i) are min/max values of A_i A_i appearing in the data Will always be between 0 and 1 Be careful for extreme values (See the lowest green value) Z-Score Standardization \u00b6 Replace A_i A_i with A_i-mean(A_i) \\over standard\\ deviation(A_i) A_i-mean(A_i) \\over standard\\ deviation(A_i) where mean(A_i) mean(A_i) standard\\ deviation standard\\ deviation Is not between 0 and 1, we have no min and max value Slightly less sensitive to outliers Soft Clustering \u00b6 The k-means algorithm generates a hard clustering: each example is assigned to a single cluster. Alternatively: In soft clustering, each example is assigned to a cluster with a certain probability. EM-Algorithm \u00b6 When learning the probability distributions of the model, the variable C C is hidden \\leadsto \\leadsto we cannot directly estimate the probabilities using frequency counts Instead we employ the EM Algorithm The Expectation-maximization algorithm Combined with a naive Bayes classifier, it does soft clustering. The main idea: Use hypothetical completions of the data using the current probability estimates Infer the maximum likelihood probabilities for the model based on completed data set. Each original example gets mapped into k k augmented examples, one for each class. The count for these are aassigned to sum to 1. Example for 4 features and 3 classes: The EM algorithm repeats the two steps: E step : Update the augmented counts based on the probability distribution. For each example \\langle X_1 = x_1, \\dots,X_n=x_n \\rangle \\langle X_1 = x_1, \\dots,X_n=x_n \\rangle in the original data, the count associated with \\langle X_1 = x_1, \\dots,X_n=x_n, C=c \\rangle \\langle X_1 = x_1, \\dots,X_n=x_n, C=c \\rangle in the augmented data is updated to P(C=c\\mid X_1=x_1,\\dots,X_n=x_n) P(C=c\\mid X_1=x_1,\\dots,X_n=x_n) Note that this step involves probabilistic inference. This is an expectation step because it computes the expected values. M step: Infer the probabilities for the model from the augmented data. Because the augmented data has values associated with all the variables, this is the same problem as learning probabilities from data in a naive Bayes classifier. This is a maximization step because it computes the maximum likelihood estimate or the maximum a posteriori probability (MAP) estimate of the probability. Example: Session 11.11 Slide 20","title":"Learning: Clustering"},{"location":"5-semester/MI/11-11-clustering/#learning-clustering","text":"Examples: Based on customer data, find group of customers with similar profiles Based on image data, find groups of images with similar motif. Based on article data, find groups of articles with the same topics ...","title":"Learning: Clustering"},{"location":"5-semester/MI/11-11-clustering/#k-means-algorithm","text":"General goal: find clustering with: Large between-cluster variation small within-cluster variation We consider the scenario, where the number k k of clusters is known we have a distance measure d(x_i,x_j) d(x_i,x_j) between pairs of data points (feature vectors) we can calculate a centroid for a collection of data points S=\\{x_1,\\dots,x_n\\} S=\\{x_1,\\dots,x_n\\} Example: Session 11.11 Slide 9 Result can depend on choice of initial cluster centers!","title":"K-Means Algorithm"},{"location":"5-semester/MI/11-11-clustering/#k-means-as-an-optimization-problem","text":"Assume we use Euclidian distance d as proximity measure and that the quality of the clustering is measured by the sum of squared errors: where c_i c_i is the i'th centroid C_i\\in S C_i\\in S is the points closets to c_i c_i according to d d In principle We can minimize the SSE by looking at all possible partitionings \\leadsto \\leadsto not feasible though! Instead, k-means : The centroid that minimizes the SSE is the mean of the data-points in that cluster: c_i = \\frac{1}{|C_i|}*\\sum_{x\\in C_i}{x} c_i = \\frac{1}{|C_i|}*\\sum_{x\\in C_i}{x} Local optimum found by alternating between cluster assignments and centroid estimation.","title":"K-means as an Optimization Problem"},{"location":"5-semester/MI/11-11-clustering/#convergence","text":"The k -means algorithm is guaranteed to converge Each step reduces the sum of squared errors There is only a finite number of cluster assignments There is no guarantee of reaching the global optimum: Improve by running with multiple random restarts","title":"Convergence"},{"location":"5-semester/MI/11-11-clustering/#outliers","text":"The result of partitional clustering can be skewed by outliers","title":"Outliers"},{"location":"5-semester/MI/11-11-clustering/#different-measuring-scales","text":"Instances defined by attributes All distance functions for continuous attributes dominated by income values \\leadsto \\leadsto may need to rescale or normalize continuous attributes","title":"Different Measuring Scales"},{"location":"5-semester/MI/11-11-clustering/#min-max-normalization","text":"Replace A_i A_i with $$ A_i-\\min(A_i)\\over \\max(A_i)-\\min(A_i) $$ where \\min(A_i),\\max(A_i) \\min(A_i),\\max(A_i) are min/max values of A_i A_i appearing in the data Will always be between 0 and 1 Be careful for extreme values (See the lowest green value)","title":"Min-Max Normalization"},{"location":"5-semester/MI/11-11-clustering/#z-score-standardization","text":"Replace A_i A_i with A_i-mean(A_i) \\over standard\\ deviation(A_i) A_i-mean(A_i) \\over standard\\ deviation(A_i) where mean(A_i) mean(A_i) standard\\ deviation standard\\ deviation Is not between 0 and 1, we have no min and max value Slightly less sensitive to outliers","title":"Z-Score Standardization"},{"location":"5-semester/MI/11-11-clustering/#soft-clustering","text":"The k-means algorithm generates a hard clustering: each example is assigned to a single cluster. Alternatively: In soft clustering, each example is assigned to a cluster with a certain probability.","title":"Soft Clustering"},{"location":"5-semester/MI/11-11-clustering/#em-algorithm","text":"When learning the probability distributions of the model, the variable C C is hidden \\leadsto \\leadsto we cannot directly estimate the probabilities using frequency counts Instead we employ the EM Algorithm The Expectation-maximization algorithm Combined with a naive Bayes classifier, it does soft clustering. The main idea: Use hypothetical completions of the data using the current probability estimates Infer the maximum likelihood probabilities for the model based on completed data set. Each original example gets mapped into k k augmented examples, one for each class. The count for these are aassigned to sum to 1. Example for 4 features and 3 classes: The EM algorithm repeats the two steps: E step : Update the augmented counts based on the probability distribution. For each example \\langle X_1 = x_1, \\dots,X_n=x_n \\rangle \\langle X_1 = x_1, \\dots,X_n=x_n \\rangle in the original data, the count associated with \\langle X_1 = x_1, \\dots,X_n=x_n, C=c \\rangle \\langle X_1 = x_1, \\dots,X_n=x_n, C=c \\rangle in the augmented data is updated to P(C=c\\mid X_1=x_1,\\dots,X_n=x_n) P(C=c\\mid X_1=x_1,\\dots,X_n=x_n) Note that this step involves probabilistic inference. This is an expectation step because it computes the expected values. M step: Infer the probabilities for the model from the augmented data. Because the augmented data has values associated with all the variables, this is the same problem as learning probabilities from data in a naive Bayes classifier. This is a maximization step because it computes the maximum likelihood estimate or the maximum a posteriori probability (MAP) estimate of the probability. Example: Session 11.11 Slide 20","title":"EM-Algorithm"},{"location":"5-semester/MI/11-18-planning-uncertainty/","text":"Planning Under Uncertainty \u00b6 Preferences and Utility \u00b6 Agent chooses actions based on outcome . Whatever the agent has preferences over. If it does not prefer any outcome, it doesn't matter what it does. Assume finite number of outcomes. Weakly Preferred Suppose o_1 o_1 and o_2 o_2 are outcomes. We say that o_1 o_1 is weakly preferred to o_2 o_2 , written $o_1 \\succeq o_2 $, if o_1 o_1 is at least as desirable as o_2 o_2 Equally Preferred Define o_1 \\sim o_2 o_1 \\sim o_2 if o_1 \\succeq o_2 o_1 \\succeq o_2 and o_2 \\succeq o_1 o_2 \\succeq o_1 . They are equally preferred The agent is indifferent between o_1 o_1 and o_2 o_2 Strictly Preferred Define o_1 \\succ o_2 o_1 \\succ o_2 to mean o_1 \\succeq o_2 o_1 \\succeq o_2 and o_2 \\nsucceq o_1 o_2 \\nsucceq o_1 We say o_1 o_1 is strictly preferred to outcome o_2 o_2 A lottery is defined to be a finite distribution over outcomes, written: [p_1:o_1,p_2:o_2,\\dots , p_k:o_k] [p_1:o_1,p_2:o_2,\\dots , p_k:o_k] where each o_i o_i is an outcome and p_i p_i is a non-negative real number such that \\sum_ip_i=1 \\sum_ip_i=1 The lottery specifies that outcome o_i o_i occurs with probability p_i p_i . Axiom 9.1 - Completeness An agent has preference between all pairs of outcomes: o_1 \\succeq o_2\\ or\\ o_2 \\succeq o_1 o_1 \\succeq o_2\\ or\\ o_2 \\succeq o_1 Axiom 9.2 - Transitivity Preferences must be transitive: if\\ o_1 \\succeq o_2\\ and\\ o_2 \\succeq o_3\\ then\\ o_1 \\succeq o_3 if\\ o_1 \\succeq o_2\\ and\\ o_2 \\succeq o_3\\ then\\ o_1 \\succeq o_3 Axiom 9.3 - Monotonicity An agent prefers a larger chance of getting a better outcome than a smaller chance of getting the better outcome. That is, if o_1\\succ o_2 o_1\\succ o_2 and p>q p>q then [p:o_1, (1-p): o_2] \\succ [q:o_1, (1-q):o_2] [p:o_1, (1-p): o_2] \\succ [q:o_1, (1-q):o_2] Lotteries \u00b6 A lottery is a probability distribution over outcomes , e.g.: [0.4:\\$100,0.6:-\\$20] [0.4:\\$100,0.6:-\\$20] means you win $100 with probability 0.4, and loose $20 with probability 0.6 [0.3:00,0.5:7,0.2:10] [0.3:00,0.5:7,0.2:10] Preference between lotteries with \"money outcomes\" are not always determined by expected monetary value . Preference from Utilities \u00b6 A classical result: If preferences between lotteries obey a certain set of plausible rules, then there exists an assignment of real numbers ( utilities ) to all outcomes, such that one lottery is preferredover another if and only if it has a higher expected utility ** Example Utility function of a risk-averse agent: Ensurance Example \u00b6 Assume Utility(\\$999k)=0.9997 Utility(\\$999k)=0.9997 Then agent is indifferent between lotteries and prefers Interpretation right lottery: 0.03 risk of loosing a $ 1000k property left lottery: buying insurance against that risk for $ 600 The insurance company prefers \\leadsto \\leadsto the insurance company has a different utility function (near linear). Factored Utility \u00b6 Two compont utility function: Utility of full outcome (state) is sum of utility factors: Assumption: The utility contribution from one factor is independent of the values of other factors. E.g.: (rhc,swc) should perhaps be worth less than 5 when at the same time (mr, mw), because mail needs to be delivered first (the two utility factors are substitutes ) Single Stage Decision Networks \u00b6 Simple decisions can be seen as choices over lotteries. Decisions, outcomes, and utilities can all be composed of features or factors: Two components of decision: prepare some / all , start preparations sooner / later Two utility factors: utility of grade, and utility (cost) of preparation time Outcome composed of Grade, Attempt Graph represents: One utility factor depends on Attempt and Grade , another only on Start Both the Prepare and Start decision influence the probabilities for Grade In General \u00b6 A Single-Stage Decision Network (SSDN) is a directed acyclic graph with three types of nodes: Decision nodes \\bold D \\bold D Chance Nodes \\bold C \\bold C Utility Nodes \\bold U \\bold U The graph must have the following structure: All decision nodes are connected in one linear sequence (representing the order in which the different sub-decisions are taken) The only parent of a decision node is its predecessor in the order Chance Nodes can have Decision Node and Chance Node parent Utility nodes can have decision node and chance node parents Tables No table is associated with decision nodes (only the list of available decisions) A chance node is labeled with a conditional probability table that specifies for each value assignment to its parents (decision and chance nodes) a probability distribution over thedomain of the chance node. A utility node is labeled with a utility table that specifies for each value assignment to its parents (decision and chance nodes) a utility value Semantics \u00b6 A possible world \\omega \\omega is an assignment of values to all decision and chance variables. An SSDN defines: For each assignment \\bold D = \\bold d \\bold D = \\bold d of values to the decision nodes a probability distribution P(\\omega \\mid \\bold D = \\bold d) P(\\omega \\mid \\bold D = \\bold d) over possible worlds For each possible world \\omega \\omega a utility value U(\\omega) U(\\omega) Solving an SSDN \u00b6 To solve an SSDN means to find the decisions \\bold d \\bold d that maximize the expected utility $$ \\varepsilon(U\\mid \\bold D = \\bold d)=\\sum_\\omega U(\\omega)P(\\omega \\mid \\bold D = \\bold d) $$ Robot Example \u00b6 Solving By (Chance) Variable Elimination \u00b6 Sequential Decisions \u00b6 SSDNs are generalized to sequential decision problems by. Several decisions are taken (in fixed order) Some chance variables may be observed before the next decision is taken Examples Doctor first decides which test to perform, then observes test outcome, then decides which treatment to prescribe Before we decide to take the umbrella with us, we observe the weather forecast A company first decides whether to develop a certain product, then observes the customer reaction in a test market, then decides whether to go into full production. Example: Fire Scenario \u00b6 Decision Function \u00b6 A Decision Function for a decision node D D is an assignment of a decision d d to each possible configuration of D's D's parents. A Policy \\pi \\pi consists of one decision function for each decision node. General strategy for actions (decisions), taking into account the possible (uncertain) effects of previous actions Semantics \u00b6 As before: possible worlds \\omega \\omega are assignments for all decision and chance variables. A policy \\pi \\pi defines a probability distribution $$ P(\\omega \\mid \\pi) $$ over possible worlds: If \\omega \\omega contains assignments to a decision node D D and its parents which is not consistent with the decision function for D D \\to \\to P(\\omega \\mid \\pi) = 0 P(\\omega \\mid \\pi) = 0 otherwise P(\\omega\\mid\\pi) P(\\omega\\mid\\pi) is the product of all conditional probability values for the assignments to chance nodes C C , givenb the assignment to the parents of C C Each possible world has a utility U(\\omega) U(\\omega) Obtain expected utility of a policy \\varepsilon(U\\mid \\pi)=\\sum_\\omega U(\\omega)P(\\omega\\mid\\pi) \\varepsilon(U\\mid \\pi)=\\sum_\\omega U(\\omega)P(\\omega\\mid\\pi) An optimal policy is a policy with maximal expected utility (among all possible policies) Solving Sequential Decision Problems \u00b6 We now have a new decision problem with one decision less. This decision problem can be solved using the same procedure until no decisions are left. Intuition \u00b6 Given values assigned to its parents, the last decision node can be seen as a single-stage decision When all decisions following a given decision D D are taken according to fixed decision rules, then D D also behaves like a single-stage decision Backwards stragegy find the decision rule for the last decision D D that is not yet eliminated eliminate D D by replacing it with the resulting utlity factor Form way of \"What would I do if ...\" reasoning Variable Elimination \u00b6 Value of Information \u00b6 Question: what is it worth to know the exact state of Forecast F when making decision Umbrella ? Answer: Compare maximal expected utilities of Question: what is it worth to know the exact state of a random variable C C when making decision D D ? Answer: Compute the expected value val_0 val_0 of optimal policy in given decision network the expected value val_1 val_1 of optimal policy in midified decision network add an edge from C C to D D and all subsequent decisions val_1-val_0 val_1-val_0 is the value of knowing C C Properties Value of information is always non-negative Value of knowing C C for decision D D is zero, if no observed value of C C can change the decision rule, i.e. for all values \\bold p \\bold p of existing parents of D D , and all values c c of C C , the optimal decision given (\\bold p,c) (\\bold p,c) is the same as the optimal decision given (\\bold p) (\\bold p)","title":"Planning under Uncertainty"},{"location":"5-semester/MI/11-18-planning-uncertainty/#planning-under-uncertainty","text":"","title":"Planning Under Uncertainty"},{"location":"5-semester/MI/11-18-planning-uncertainty/#preferences-and-utility","text":"Agent chooses actions based on outcome . Whatever the agent has preferences over. If it does not prefer any outcome, it doesn't matter what it does. Assume finite number of outcomes. Weakly Preferred Suppose o_1 o_1 and o_2 o_2 are outcomes. We say that o_1 o_1 is weakly preferred to o_2 o_2 , written $o_1 \\succeq o_2 $, if o_1 o_1 is at least as desirable as o_2 o_2 Equally Preferred Define o_1 \\sim o_2 o_1 \\sim o_2 if o_1 \\succeq o_2 o_1 \\succeq o_2 and o_2 \\succeq o_1 o_2 \\succeq o_1 . They are equally preferred The agent is indifferent between o_1 o_1 and o_2 o_2 Strictly Preferred Define o_1 \\succ o_2 o_1 \\succ o_2 to mean o_1 \\succeq o_2 o_1 \\succeq o_2 and o_2 \\nsucceq o_1 o_2 \\nsucceq o_1 We say o_1 o_1 is strictly preferred to outcome o_2 o_2 A lottery is defined to be a finite distribution over outcomes, written: [p_1:o_1,p_2:o_2,\\dots , p_k:o_k] [p_1:o_1,p_2:o_2,\\dots , p_k:o_k] where each o_i o_i is an outcome and p_i p_i is a non-negative real number such that \\sum_ip_i=1 \\sum_ip_i=1 The lottery specifies that outcome o_i o_i occurs with probability p_i p_i . Axiom 9.1 - Completeness An agent has preference between all pairs of outcomes: o_1 \\succeq o_2\\ or\\ o_2 \\succeq o_1 o_1 \\succeq o_2\\ or\\ o_2 \\succeq o_1 Axiom 9.2 - Transitivity Preferences must be transitive: if\\ o_1 \\succeq o_2\\ and\\ o_2 \\succeq o_3\\ then\\ o_1 \\succeq o_3 if\\ o_1 \\succeq o_2\\ and\\ o_2 \\succeq o_3\\ then\\ o_1 \\succeq o_3 Axiom 9.3 - Monotonicity An agent prefers a larger chance of getting a better outcome than a smaller chance of getting the better outcome. That is, if o_1\\succ o_2 o_1\\succ o_2 and p>q p>q then [p:o_1, (1-p): o_2] \\succ [q:o_1, (1-q):o_2] [p:o_1, (1-p): o_2] \\succ [q:o_1, (1-q):o_2]","title":"Preferences and Utility"},{"location":"5-semester/MI/11-18-planning-uncertainty/#lotteries","text":"A lottery is a probability distribution over outcomes , e.g.: [0.4:\\$100,0.6:-\\$20] [0.4:\\$100,0.6:-\\$20] means you win $100 with probability 0.4, and loose $20 with probability 0.6 [0.3:00,0.5:7,0.2:10] [0.3:00,0.5:7,0.2:10] Preference between lotteries with \"money outcomes\" are not always determined by expected monetary value .","title":"Lotteries"},{"location":"5-semester/MI/11-18-planning-uncertainty/#preference-from-utilities","text":"A classical result: If preferences between lotteries obey a certain set of plausible rules, then there exists an assignment of real numbers ( utilities ) to all outcomes, such that one lottery is preferredover another if and only if it has a higher expected utility ** Example Utility function of a risk-averse agent:","title":"Preference from Utilities"},{"location":"5-semester/MI/11-18-planning-uncertainty/#ensurance-example","text":"Assume Utility(\\$999k)=0.9997 Utility(\\$999k)=0.9997 Then agent is indifferent between lotteries and prefers Interpretation right lottery: 0.03 risk of loosing a $ 1000k property left lottery: buying insurance against that risk for $ 600 The insurance company prefers \\leadsto \\leadsto the insurance company has a different utility function (near linear).","title":"Ensurance Example"},{"location":"5-semester/MI/11-18-planning-uncertainty/#factored-utility","text":"Two compont utility function: Utility of full outcome (state) is sum of utility factors: Assumption: The utility contribution from one factor is independent of the values of other factors. E.g.: (rhc,swc) should perhaps be worth less than 5 when at the same time (mr, mw), because mail needs to be delivered first (the two utility factors are substitutes )","title":"Factored Utility"},{"location":"5-semester/MI/11-18-planning-uncertainty/#single-stage-decision-networks","text":"Simple decisions can be seen as choices over lotteries. Decisions, outcomes, and utilities can all be composed of features or factors: Two components of decision: prepare some / all , start preparations sooner / later Two utility factors: utility of grade, and utility (cost) of preparation time Outcome composed of Grade, Attempt Graph represents: One utility factor depends on Attempt and Grade , another only on Start Both the Prepare and Start decision influence the probabilities for Grade","title":"Single Stage Decision Networks"},{"location":"5-semester/MI/11-18-planning-uncertainty/#in-general","text":"A Single-Stage Decision Network (SSDN) is a directed acyclic graph with three types of nodes: Decision nodes \\bold D \\bold D Chance Nodes \\bold C \\bold C Utility Nodes \\bold U \\bold U The graph must have the following structure: All decision nodes are connected in one linear sequence (representing the order in which the different sub-decisions are taken) The only parent of a decision node is its predecessor in the order Chance Nodes can have Decision Node and Chance Node parent Utility nodes can have decision node and chance node parents Tables No table is associated with decision nodes (only the list of available decisions) A chance node is labeled with a conditional probability table that specifies for each value assignment to its parents (decision and chance nodes) a probability distribution over thedomain of the chance node. A utility node is labeled with a utility table that specifies for each value assignment to its parents (decision and chance nodes) a utility value","title":"In General"},{"location":"5-semester/MI/11-18-planning-uncertainty/#semantics","text":"A possible world \\omega \\omega is an assignment of values to all decision and chance variables. An SSDN defines: For each assignment \\bold D = \\bold d \\bold D = \\bold d of values to the decision nodes a probability distribution P(\\omega \\mid \\bold D = \\bold d) P(\\omega \\mid \\bold D = \\bold d) over possible worlds For each possible world \\omega \\omega a utility value U(\\omega) U(\\omega)","title":"Semantics"},{"location":"5-semester/MI/11-18-planning-uncertainty/#solving-an-ssdn","text":"To solve an SSDN means to find the decisions \\bold d \\bold d that maximize the expected utility $$ \\varepsilon(U\\mid \\bold D = \\bold d)=\\sum_\\omega U(\\omega)P(\\omega \\mid \\bold D = \\bold d) $$","title":"Solving an SSDN"},{"location":"5-semester/MI/11-18-planning-uncertainty/#robot-example","text":"","title":"Robot Example"},{"location":"5-semester/MI/11-18-planning-uncertainty/#solving-by-chance-variable-elimination","text":"","title":"Solving By (Chance) Variable Elimination"},{"location":"5-semester/MI/11-18-planning-uncertainty/#sequential-decisions","text":"SSDNs are generalized to sequential decision problems by. Several decisions are taken (in fixed order) Some chance variables may be observed before the next decision is taken Examples Doctor first decides which test to perform, then observes test outcome, then decides which treatment to prescribe Before we decide to take the umbrella with us, we observe the weather forecast A company first decides whether to develop a certain product, then observes the customer reaction in a test market, then decides whether to go into full production.","title":"Sequential Decisions"},{"location":"5-semester/MI/11-18-planning-uncertainty/#example-fire-scenario","text":"","title":"Example: Fire Scenario"},{"location":"5-semester/MI/11-18-planning-uncertainty/#decision-function","text":"A Decision Function for a decision node D D is an assignment of a decision d d to each possible configuration of D's D's parents. A Policy \\pi \\pi consists of one decision function for each decision node. General strategy for actions (decisions), taking into account the possible (uncertain) effects of previous actions","title":"Decision Function"},{"location":"5-semester/MI/11-18-planning-uncertainty/#semantics_1","text":"As before: possible worlds \\omega \\omega are assignments for all decision and chance variables. A policy \\pi \\pi defines a probability distribution $$ P(\\omega \\mid \\pi) $$ over possible worlds: If \\omega \\omega contains assignments to a decision node D D and its parents which is not consistent with the decision function for D D \\to \\to P(\\omega \\mid \\pi) = 0 P(\\omega \\mid \\pi) = 0 otherwise P(\\omega\\mid\\pi) P(\\omega\\mid\\pi) is the product of all conditional probability values for the assignments to chance nodes C C , givenb the assignment to the parents of C C Each possible world has a utility U(\\omega) U(\\omega) Obtain expected utility of a policy \\varepsilon(U\\mid \\pi)=\\sum_\\omega U(\\omega)P(\\omega\\mid\\pi) \\varepsilon(U\\mid \\pi)=\\sum_\\omega U(\\omega)P(\\omega\\mid\\pi) An optimal policy is a policy with maximal expected utility (among all possible policies)","title":"Semantics"},{"location":"5-semester/MI/11-18-planning-uncertainty/#solving-sequential-decision-problems","text":"We now have a new decision problem with one decision less. This decision problem can be solved using the same procedure until no decisions are left.","title":"Solving Sequential Decision Problems"},{"location":"5-semester/MI/11-18-planning-uncertainty/#intuition","text":"Given values assigned to its parents, the last decision node can be seen as a single-stage decision When all decisions following a given decision D D are taken according to fixed decision rules, then D D also behaves like a single-stage decision Backwards stragegy find the decision rule for the last decision D D that is not yet eliminated eliminate D D by replacing it with the resulting utlity factor Form way of \"What would I do if ...\" reasoning","title":"Intuition"},{"location":"5-semester/MI/11-18-planning-uncertainty/#variable-elimination","text":"","title":"Variable Elimination"},{"location":"5-semester/MI/11-18-planning-uncertainty/#value-of-information","text":"Question: what is it worth to know the exact state of Forecast F when making decision Umbrella ? Answer: Compare maximal expected utilities of Question: what is it worth to know the exact state of a random variable C C when making decision D D ? Answer: Compute the expected value val_0 val_0 of optimal policy in given decision network the expected value val_1 val_1 of optimal policy in midified decision network add an edge from C C to D D and all subsequent decisions val_1-val_0 val_1-val_0 is the value of knowing C C Properties Value of information is always non-negative Value of knowing C C for decision D D is zero, if no observed value of C C can change the decision rule, i.e. for all values \\bold p \\bold p of existing parents of D D , and all values c c of C C , the optimal decision given (\\bold p,c) (\\bold p,c) is the same as the optimal decision given (\\bold p) (\\bold p)","title":"Value of Information"},{"location":"5-semester/MI/11-25-multi-agent/","text":"Multi-Agent Systems \u00b6 Game Trees \u00b6 Sharing game. Andy and Barb share two pieces of pie: Representation by game tree: tree whose nodes are labeled with agents outgoing arcs labeled by actions of agent leaves labeled with one utility value for each agent (can also have nature nodes that represent uncertainty from random effects, e.g. dealing of cards, rolling of dice) Imperfect Information Games \u00b6 Representation of game with simultaneous moves: Collect in an information set the nodes that the agent (Bob) can not distinguish (at all nodes in an information set the same actions must be possible) Other sources for imperfect information: Unobserved, random moves by nature (dealing of cards) Hidden moves by other agent Strategies \u00b6 A (pure) strategy for one agent is a mapping from information sets to (possible) actions. (Essentially a policy) A strategy profile consists of a strategy for each agent. Utility \u00b6 Utility for each agent given a strategy profile: each node has the utilities that will be reached at a leaf by following the strategy profile the utilities at the node represent the outcome of the game (given the strategy profile) (utilities at a nature node are computed by taking the expectation over the utilities of its successors) Solving Perfect Information Gain \u00b6 If game is perfect information (no information sets with more than 1 node) both agents play rationally (optimize their own utility) then the optimal strategies for both players are determined by bottom-up propagation of utilities under optimal strategies, where each player selects the action that leads to the child with the highest utility (for that player) Often these game trees can be extremely large. Example: Chess Pruning \u00b6 Zero Sum Game For two players: utility_1=-utility_2 utility_1=-utility_2 In this case: need only one utility value at leaves one player (Max) wants to reach leaf with max value, other (Min) wants to reach leaf with min value. In bottom-up utility computation, some sub trees can be pruned ( \\alpha\\text{-}\\beta \\text{ pruning} \\alpha\\text{-}\\beta \\text{ pruning} ) Imperfect Information \u00b6 Game Trees can be represented as tables Share Game Rock Paper Scissors Difference between perfect and imperfect information not directly visible in normal form representation. Nash Equilibrium \u00b6 Consider optimal strategy profile for share game: The two strategies are in Nash equilibrium no agent can improve utility by switching strategy while other agent keeps its strategy this also means: agent will stick to strategy when it knows the strategy of the other player Example Prisoner's Dilemma \u00b6 Alice and Bob are arrested for burglary. They are separately questioned by police. Alice and Bob are both given the offer to testify, in which case: The only Nash Equilibrium is Alice: testify , Bob: testify Nash equilibria do not represent cooperative behavior! Mixed Strategies \u00b6 No pure strategy Nash Equilibrium in Rock Paper Scissors A mixed strategy is a probability distribution over actions: Expected utility for Alice = expected utility for Bob = 1/9*(0+1-1-1+0+1+1-1+0)=0 1/9*(0+1-1-1+0+1+1-1+0)=0 Suppose Alice plays some other strategy: r:p_r\\ p:p_p\\ s:p_s r:p_r\\ p:p_p\\ s:p_s Expected utility for Alice then: If Bob plays r:1/3\\ p: 1/3\\ s: 1/3 r:1/3\\ p: 1/3\\ s: 1/3 , Alice cannot do better than playing r:1/3\\ p: 1/3\\ s: 1/3 r:1/3\\ p: 1/3\\ s: 1/3 also. Same for Bob Both playing r:1/3\\ p: 1/3\\ s: 1/3 r:1/3\\ p: 1/3\\ s: 1/3 is a (the only) Nash equilibrium. Key Results \u00b6 Every (finite) game has a Nash equilibrium (using mixed strategies) There can be multiple Playing a Nash equilibrium strategy profile does not necessarily lead to optimal utilities for the agents (prisoners dilemma )","title":"Multi-Agent Systems"},{"location":"5-semester/MI/11-25-multi-agent/#multi-agent-systems","text":"","title":"Multi-Agent Systems"},{"location":"5-semester/MI/11-25-multi-agent/#game-trees","text":"Sharing game. Andy and Barb share two pieces of pie: Representation by game tree: tree whose nodes are labeled with agents outgoing arcs labeled by actions of agent leaves labeled with one utility value for each agent (can also have nature nodes that represent uncertainty from random effects, e.g. dealing of cards, rolling of dice)","title":"Game Trees"},{"location":"5-semester/MI/11-25-multi-agent/#imperfect-information-games","text":"Representation of game with simultaneous moves: Collect in an information set the nodes that the agent (Bob) can not distinguish (at all nodes in an information set the same actions must be possible) Other sources for imperfect information: Unobserved, random moves by nature (dealing of cards) Hidden moves by other agent","title":"Imperfect Information Games"},{"location":"5-semester/MI/11-25-multi-agent/#strategies","text":"A (pure) strategy for one agent is a mapping from information sets to (possible) actions. (Essentially a policy) A strategy profile consists of a strategy for each agent.","title":"Strategies"},{"location":"5-semester/MI/11-25-multi-agent/#utility","text":"Utility for each agent given a strategy profile: each node has the utilities that will be reached at a leaf by following the strategy profile the utilities at the node represent the outcome of the game (given the strategy profile) (utilities at a nature node are computed by taking the expectation over the utilities of its successors)","title":"Utility"},{"location":"5-semester/MI/11-25-multi-agent/#solving-perfect-information-gain","text":"If game is perfect information (no information sets with more than 1 node) both agents play rationally (optimize their own utility) then the optimal strategies for both players are determined by bottom-up propagation of utilities under optimal strategies, where each player selects the action that leads to the child with the highest utility (for that player) Often these game trees can be extremely large. Example: Chess","title":"Solving Perfect Information Gain"},{"location":"5-semester/MI/11-25-multi-agent/#pruning","text":"Zero Sum Game For two players: utility_1=-utility_2 utility_1=-utility_2 In this case: need only one utility value at leaves one player (Max) wants to reach leaf with max value, other (Min) wants to reach leaf with min value. In bottom-up utility computation, some sub trees can be pruned ( \\alpha\\text{-}\\beta \\text{ pruning} \\alpha\\text{-}\\beta \\text{ pruning} )","title":"Pruning"},{"location":"5-semester/MI/11-25-multi-agent/#imperfect-information","text":"Game Trees can be represented as tables Share Game Rock Paper Scissors Difference between perfect and imperfect information not directly visible in normal form representation.","title":"Imperfect Information"},{"location":"5-semester/MI/11-25-multi-agent/#nash-equilibrium","text":"Consider optimal strategy profile for share game: The two strategies are in Nash equilibrium no agent can improve utility by switching strategy while other agent keeps its strategy this also means: agent will stick to strategy when it knows the strategy of the other player","title":"Nash Equilibrium"},{"location":"5-semester/MI/11-25-multi-agent/#example-prisoners-dilemma","text":"Alice and Bob are arrested for burglary. They are separately questioned by police. Alice and Bob are both given the offer to testify, in which case: The only Nash Equilibrium is Alice: testify , Bob: testify Nash equilibria do not represent cooperative behavior!","title":"Example Prisoner's Dilemma"},{"location":"5-semester/MI/11-25-multi-agent/#mixed-strategies","text":"No pure strategy Nash Equilibrium in Rock Paper Scissors A mixed strategy is a probability distribution over actions: Expected utility for Alice = expected utility for Bob = 1/9*(0+1-1-1+0+1+1-1+0)=0 1/9*(0+1-1-1+0+1+1-1+0)=0 Suppose Alice plays some other strategy: r:p_r\\ p:p_p\\ s:p_s r:p_r\\ p:p_p\\ s:p_s Expected utility for Alice then: If Bob plays r:1/3\\ p: 1/3\\ s: 1/3 r:1/3\\ p: 1/3\\ s: 1/3 , Alice cannot do better than playing r:1/3\\ p: 1/3\\ s: 1/3 r:1/3\\ p: 1/3\\ s: 1/3 also. Same for Bob Both playing r:1/3\\ p: 1/3\\ s: 1/3 r:1/3\\ p: 1/3\\ s: 1/3 is a (the only) Nash equilibrium.","title":"Mixed Strategies"},{"location":"5-semester/MI/11-25-multi-agent/#key-results","text":"Every (finite) game has a Nash equilibrium (using mixed strategies) There can be multiple Playing a Nash equilibrium strategy profile does not necessarily lead to optimal utilities for the agents (prisoners dilemma )","title":"Key Results"},{"location":"5-semester/MI/99_exam/","text":"Exam \u00b6 You can bring books, notes, pocket calculator. No computer.","title":"Exam"},{"location":"5-semester/MI/99_exam/#exam","text":"You can bring books, notes, pocket calculator. No computer.","title":"Exam"},{"location":"5-semester/MI/probability-calculus-cheatsheet/","text":"Cheat Sheet for Probability Calculus \u00b6 We consider probability distributions over variables. A variable can be considered an experiment, and for each outcome of the experiement, the variable has a corresponding state. We use upper case letters to denote vriables, eg. A A , and lower case to denote states. The state space of a variable A A is denoted sp(A)=(a_1,a_2,...,a_n) sp(A)=(a_1,a_2,...,a_n) Notation \u00b6 The probability of A A being in state a_i a_i is denoted P(a_i) P(a_i) or P(A=a_i) P(A=a_i) If we omit the state and write P(A) P(A) we have a table with probabilities, on one for each state of A A Example A A is tenary with states sp(A)=(a_1,a_2,a_3) sp(A)=(a_1,a_2,a_3) and P(A)=(0.1,0.3,0.6) P(A)=(0.1,0.3,0.6) . Thus P(A=a_1)=0.1 P(A=a_1)=0.1 Marginelization \u00b6 Given P(A,B) P(A,B) , we want P(A) P(A) P(A)=\\sum_BP(A,B)= \\sum_{B=b}P(A,B=b) P(A)=\\sum_BP(A,B)= \\sum_{B=b}P(A,B=b) P(A,B)= P(A,B)= A \\ B b_1 b_1 b_2 b_2 a_1 a_1 0.2 0.1 a_2 a_2 0.3 0.4 P(A)=(\\overset{a_1}{0.2+0.1},\\overset{a_2}{0.3+0.4})=(0.3,0.7) P(A)=(\\overset{a_1}{0.2+0.1},\\overset{a_2}{0.3+0.4})=(0.3,0.7) Conditional Probability \u00b6 (Can be seen either as a definititon or a theorem) P(B|A)=\\frac{P(A,B)}{P(A)} P(B|A)=\\frac{P(A,B)}{P(A)} Using the tables above we get Bayes Rule \u00b6 Given P(A|B) P(A|B) and P(B) P(B) , we want P(B|A) P(B|A) \\begin{align*} P(B|A)&=\\frac{P(A|B)\\cdot P(B)}{P(A)}\\\\\\\\ &=\\frac{P(A,B)}{P(A)}\\quad \\text{(due to the chain rule below)} \\end{align*} \\begin{align*} P(B|A)&=\\frac{P(A|B)\\cdot P(B)}{P(A)}\\\\\\\\ &=\\frac{P(A,B)}{P(A)}\\quad \\text{(due to the chain rule below)} \\end{align*} Note that by marginelization we have P(A)=\\sum_B P(A,B) P(A)=\\sum_B P(A,B) The Chain Rule \u00b6 AKA Fundamental Rule Given P(A|B) P(A|B) and P(B) P(B) , we want P(A,B) P(A,B) $$ P(A,B)=P(A|B)\\cdot P(B) $$","title":"Cheat Sheet for Probability Calculus"},{"location":"5-semester/MI/probability-calculus-cheatsheet/#cheat-sheet-for-probability-calculus","text":"We consider probability distributions over variables. A variable can be considered an experiment, and for each outcome of the experiement, the variable has a corresponding state. We use upper case letters to denote vriables, eg. A A , and lower case to denote states. The state space of a variable A A is denoted sp(A)=(a_1,a_2,...,a_n) sp(A)=(a_1,a_2,...,a_n)","title":"Cheat Sheet for Probability Calculus"},{"location":"5-semester/MI/probability-calculus-cheatsheet/#notation","text":"The probability of A A being in state a_i a_i is denoted P(a_i) P(a_i) or P(A=a_i) P(A=a_i) If we omit the state and write P(A) P(A) we have a table with probabilities, on one for each state of A A Example A A is tenary with states sp(A)=(a_1,a_2,a_3) sp(A)=(a_1,a_2,a_3) and P(A)=(0.1,0.3,0.6) P(A)=(0.1,0.3,0.6) . Thus P(A=a_1)=0.1 P(A=a_1)=0.1","title":"Notation"},{"location":"5-semester/MI/probability-calculus-cheatsheet/#marginelization","text":"Given P(A,B) P(A,B) , we want P(A) P(A) P(A)=\\sum_BP(A,B)= \\sum_{B=b}P(A,B=b) P(A)=\\sum_BP(A,B)= \\sum_{B=b}P(A,B=b) P(A,B)= P(A,B)= A \\ B b_1 b_1 b_2 b_2 a_1 a_1 0.2 0.1 a_2 a_2 0.3 0.4 P(A)=(\\overset{a_1}{0.2+0.1},\\overset{a_2}{0.3+0.4})=(0.3,0.7) P(A)=(\\overset{a_1}{0.2+0.1},\\overset{a_2}{0.3+0.4})=(0.3,0.7)","title":"Marginelization"},{"location":"5-semester/MI/probability-calculus-cheatsheet/#conditional-probability","text":"(Can be seen either as a definititon or a theorem) P(B|A)=\\frac{P(A,B)}{P(A)} P(B|A)=\\frac{P(A,B)}{P(A)} Using the tables above we get","title":"Conditional Probability"},{"location":"5-semester/MI/probability-calculus-cheatsheet/#bayes-rule","text":"Given P(A|B) P(A|B) and P(B) P(B) , we want P(B|A) P(B|A) \\begin{align*} P(B|A)&=\\frac{P(A|B)\\cdot P(B)}{P(A)}\\\\\\\\ &=\\frac{P(A,B)}{P(A)}\\quad \\text{(due to the chain rule below)} \\end{align*} \\begin{align*} P(B|A)&=\\frac{P(A|B)\\cdot P(B)}{P(A)}\\\\\\\\ &=\\frac{P(A,B)}{P(A)}\\quad \\text{(due to the chain rule below)} \\end{align*} Note that by marginelization we have P(A)=\\sum_B P(A,B) P(A)=\\sum_B P(A,B)","title":"Bayes Rule"},{"location":"5-semester/MI/probability-calculus-cheatsheet/#the-chain-rule","text":"AKA Fundamental Rule Given P(A|B) P(A|B) and P(B) P(B) , we want P(A,B) P(A,B) $$ P(A,B)=P(A|B)\\cdot P(B) $$","title":"The Chain Rule"},{"location":"5-semester/SOE/","text":"SOE - Software Engineering \u00b6 Moodle side: https://www.moodle.aau.dk/course/view.php?id=31432","title":"Course"},{"location":"5-semester/SOE/#soe-software-engineering","text":"Moodle side: https://www.moodle.aau.dk/course/view.php?id=31432","title":"SOE - Software Engineering"},{"location":"5-semester/SOE/01-introduction/","text":"Software Engineering \u00b6 SE Process Activities \u00b6 Four fundamental software engineering activities Software specification The functionality of the software and constraints on its operation must be defined Software development The software to meet the specification must be produced Software Validation The software must be validated to ensure that it does what the customer wants Validation: Are we building the right systems? Conforms to customers' expectations and experience Verification Are we building the system right? Conforms to specification Techniques: Testing of programs and prototypes Reviewing of specifications, documentation and programs Stages of testing Component testing \\to \\to System testing \\to \\to Customer testing Software evolution The software must evolve to meet changing customer needs Software Process Models \u00b6 Waterfall Model (Plan-driven) Seperate and distinct phases of specification, design, implementation, test, and operations Incremental Model (Plan-driven, agile or mix) Specification, development and validation are interleaved. System is developed as a series of versions (increments), with each version adding functionality to the previous version. Integration & configurations model (Reuse) System is assembled from existing configurable components. Plan-driven or agile Waterfall \u00b6 It is a plan-driven process, as the process activities are planned and scheduled before starting development. Requirements analysis and definition The system\u2019s services, constraints and goals are established by consultation with system users. They are then defined in detail and serve as a system specification. System and software design The systems design process allocates the requirements to either hardware or software systems. It establishes an overall system architecture. Software design involves identifying and describing the fundamental software system abstractions and their relationships. Implementation and unit testing During this stage, the software design is realized as a set of programs or program units. Unit testing involves verifying that each unit meets its specification. Integration and system testing The individual program units or programs are integrated and tested as a complete system to ensure that the software requirements have been met. After testing, the software system is delivered to the customer. Operation and maintenance Normally, this is the longest life-cycle phase. The system is installed and put into practical use. Maintenance involves correcting errors that were not discovered in earlier stages of the life cycle, improving the implementation of system units, and enhancing the system\u2019s services as new requirements are discovered. When should you consider waterfall? \u00b6 Embedded systems Because of the inflexibility of hardware it is usually not possibly to delay decisions on the software\u2019s functionality until it is being implemented. Life critical systems Because the specification and design documents must be complete so that it is possible to create an extensive security analysis of the software specification and design. Safety-related problems in the specs and design are usually very expensive to correct at the implementation stage. Large software systems that are a part of broader engineering systems Because the hardware in the system may be developed using a similar model and companies find it easier to use a common model for hardware and software. Furthermore, where several companies are involved, complete specs may be needed to allow for the independent development of different subsystems. How can I decide if agile or waterfall is best fit for my situation? \u00b6 Boehm: Analyse af home ground The waterfall model should not be used in developing systems where informal team communication is possible and software requirements change quickly. In this case, iterative and agile methods are better.","title":"Software Engineering"},{"location":"5-semester/SOE/01-introduction/#software-engineering","text":"","title":"Software Engineering"},{"location":"5-semester/SOE/01-introduction/#se-process-activities","text":"Four fundamental software engineering activities Software specification The functionality of the software and constraints on its operation must be defined Software development The software to meet the specification must be produced Software Validation The software must be validated to ensure that it does what the customer wants Validation: Are we building the right systems? Conforms to customers' expectations and experience Verification Are we building the system right? Conforms to specification Techniques: Testing of programs and prototypes Reviewing of specifications, documentation and programs Stages of testing Component testing \\to \\to System testing \\to \\to Customer testing Software evolution The software must evolve to meet changing customer needs","title":"SE Process Activities"},{"location":"5-semester/SOE/01-introduction/#software-process-models","text":"Waterfall Model (Plan-driven) Seperate and distinct phases of specification, design, implementation, test, and operations Incremental Model (Plan-driven, agile or mix) Specification, development and validation are interleaved. System is developed as a series of versions (increments), with each version adding functionality to the previous version. Integration & configurations model (Reuse) System is assembled from existing configurable components. Plan-driven or agile","title":"Software Process Models"},{"location":"5-semester/SOE/01-introduction/#waterfall","text":"It is a plan-driven process, as the process activities are planned and scheduled before starting development. Requirements analysis and definition The system\u2019s services, constraints and goals are established by consultation with system users. They are then defined in detail and serve as a system specification. System and software design The systems design process allocates the requirements to either hardware or software systems. It establishes an overall system architecture. Software design involves identifying and describing the fundamental software system abstractions and their relationships. Implementation and unit testing During this stage, the software design is realized as a set of programs or program units. Unit testing involves verifying that each unit meets its specification. Integration and system testing The individual program units or programs are integrated and tested as a complete system to ensure that the software requirements have been met. After testing, the software system is delivered to the customer. Operation and maintenance Normally, this is the longest life-cycle phase. The system is installed and put into practical use. Maintenance involves correcting errors that were not discovered in earlier stages of the life cycle, improving the implementation of system units, and enhancing the system\u2019s services as new requirements are discovered.","title":"Waterfall"},{"location":"5-semester/SOE/01-introduction/#when-should-you-consider-waterfall","text":"Embedded systems Because of the inflexibility of hardware it is usually not possibly to delay decisions on the software\u2019s functionality until it is being implemented. Life critical systems Because the specification and design documents must be complete so that it is possible to create an extensive security analysis of the software specification and design. Safety-related problems in the specs and design are usually very expensive to correct at the implementation stage. Large software systems that are a part of broader engineering systems Because the hardware in the system may be developed using a similar model and companies find it easier to use a common model for hardware and software. Furthermore, where several companies are involved, complete specs may be needed to allow for the independent development of different subsystems.","title":"When should you consider waterfall?"},{"location":"5-semester/SOE/01-introduction/#how-can-i-decide-if-agile-or-waterfall-is-best-fit-for-my-situation","text":"Boehm: Analyse af home ground The waterfall model should not be used in developing systems where informal team communication is possible and software requirements change quickly. In this case, iterative and agile methods are better.","title":"How can I decide if agile or waterfall is best fit for my situation?"},{"location":"5-semester/SOE/02-agile-xp-and-scrum/","text":"Agile, XP, and Scrum \u00b6 Extreme Programming (XP) \u00b6 Emphasizes collaboration, quick and early software creation, and skillful development practices. Values communication, simplicity, feedback, and courage It recommends 12 core practices: Planning Game Small, frequent releases System metaphors Simple designs Testing Frequent refactoring Pair programming (PP) Team code ownership Continuous integration Sustainable pace Whole team together Coding standards Scrum \u00b6 Pillars \u00b6 Transparency Significant aspects of the process must be visible to those responsible for the outcome. Transparency requires those aspects be defined by a common standard so observers share a common understanding of what is being seen. Inspection Scrum users must frequently inspect Scrum artifacts and progress toward a Sprint Goal to detect undesirable variances. Their inspection should not be so frequent that inspection gets in the way of the work. Inspections are most beneficial when diligently performed by skilled inspectors at the point of work Adaption If an inspector determines that one or more aspects of a process deviate outside acceptable limits, and that the resulting product will be unacceptable, the process or the material being processed must be adjusted. An adjustment must be made as soon as possible to minimize further deviation. Scrum prescribes four formal events for inspection and adaptation Sprint Planning Daily Scrum Sprint Review Sprint Retrospective Values \u00b6 Commitment People personally commit to achieving the goals of the Scrum Team Courage The Scrum Team members have courage to do the right thing and work on tough problems Focus Everyone focuses on the work of the Sprint and the goals of the Scrum Team Openness The Scrum Team and its stakeholders agree to be open about all the work and the challenges with performing the work Respect Scrum Team members respect each other to be capable, independent people Scrum Roles \u00b6 There are 3 roles: Product Owner The Product Owner is one person, not a committee. The Product Owner may represent the desires of a committee in the Product Backlog, but those wanting to change a Product Backlog item\u2019s priority must address the Product Owner. The Product Owner is responsible for maximizing the value of the product resulting from work of the Development Team. The Product Owner is the sole person responsible for managing the Product Backlog, this being a list of all things that needs to be done within the project. Scrum Master The Scrum Master is responsible for promoting and supporting Scrum as defined in the Scrum Guide. Scrum Masters do this by helping everyone understand Scrum theory, practices, rules, and values. The Scrum Master helps those outside the Scrum Team understand which of their interactions with the Scrum Team are helpful and which aren\u2019t. The Scrum Master helps everyone change these interactions to maximize the value created by the Scrum Team. Development Team The Development Team consists of professionals who do the work of delivering a potentially releasable Increment of \u201cDone\u201d product at the end of each Sprint. Development Teams are structured and empowered by the organization to organize and manage their own work. The resulting synergy optimizes the Development Team\u2019s overall efficiency and effectiveness. They are self-organizing. No one (not even the Scrum Master) tells the Development Team how to turn Product Backlog into Increments of potentially releasable functionality and they are crossfunctional, with all the skills as a team necessary to create a product Increment.","title":"Agile, XP, and Scrum"},{"location":"5-semester/SOE/02-agile-xp-and-scrum/#agile-xp-and-scrum","text":"","title":"Agile, XP, and Scrum"},{"location":"5-semester/SOE/02-agile-xp-and-scrum/#extreme-programming-xp","text":"Emphasizes collaboration, quick and early software creation, and skillful development practices. Values communication, simplicity, feedback, and courage It recommends 12 core practices: Planning Game Small, frequent releases System metaphors Simple designs Testing Frequent refactoring Pair programming (PP) Team code ownership Continuous integration Sustainable pace Whole team together Coding standards","title":"Extreme Programming (XP)"},{"location":"5-semester/SOE/02-agile-xp-and-scrum/#scrum","text":"","title":"Scrum"},{"location":"5-semester/SOE/02-agile-xp-and-scrum/#pillars","text":"Transparency Significant aspects of the process must be visible to those responsible for the outcome. Transparency requires those aspects be defined by a common standard so observers share a common understanding of what is being seen. Inspection Scrum users must frequently inspect Scrum artifacts and progress toward a Sprint Goal to detect undesirable variances. Their inspection should not be so frequent that inspection gets in the way of the work. Inspections are most beneficial when diligently performed by skilled inspectors at the point of work Adaption If an inspector determines that one or more aspects of a process deviate outside acceptable limits, and that the resulting product will be unacceptable, the process or the material being processed must be adjusted. An adjustment must be made as soon as possible to minimize further deviation. Scrum prescribes four formal events for inspection and adaptation Sprint Planning Daily Scrum Sprint Review Sprint Retrospective","title":"Pillars"},{"location":"5-semester/SOE/02-agile-xp-and-scrum/#values","text":"Commitment People personally commit to achieving the goals of the Scrum Team Courage The Scrum Team members have courage to do the right thing and work on tough problems Focus Everyone focuses on the work of the Sprint and the goals of the Scrum Team Openness The Scrum Team and its stakeholders agree to be open about all the work and the challenges with performing the work Respect Scrum Team members respect each other to be capable, independent people","title":"Values"},{"location":"5-semester/SOE/02-agile-xp-and-scrum/#scrum-roles","text":"There are 3 roles: Product Owner The Product Owner is one person, not a committee. The Product Owner may represent the desires of a committee in the Product Backlog, but those wanting to change a Product Backlog item\u2019s priority must address the Product Owner. The Product Owner is responsible for maximizing the value of the product resulting from work of the Development Team. The Product Owner is the sole person responsible for managing the Product Backlog, this being a list of all things that needs to be done within the project. Scrum Master The Scrum Master is responsible for promoting and supporting Scrum as defined in the Scrum Guide. Scrum Masters do this by helping everyone understand Scrum theory, practices, rules, and values. The Scrum Master helps those outside the Scrum Team understand which of their interactions with the Scrum Team are helpful and which aren\u2019t. The Scrum Master helps everyone change these interactions to maximize the value created by the Scrum Team. Development Team The Development Team consists of professionals who do the work of delivering a potentially releasable Increment of \u201cDone\u201d product at the end of each Sprint. Development Teams are structured and empowered by the organization to organize and manage their own work. The resulting synergy optimizes the Development Team\u2019s overall efficiency and effectiveness. They are self-organizing. No one (not even the Scrum Master) tells the Development Team how to turn Product Backlog into Increments of potentially releasable functionality and they are crossfunctional, with all the skills as a team necessary to create a product Increment.","title":"Scrum Roles"},{"location":"5-semester/SOE/04a-rup/","text":"UP \u00b6 Unified Process (UP) Refined in Rational Unified Process (RUP) An iterative process framework Key practices and guidelines Develop in short timeboxed iterations Develop the high-risk and high-value elements (fx the core architecture) in ealy iterations, preferring re-use of existing components Ensure that you deliver value to your customer Accomedate change early in the project Work together as one team UP Phases \u00b6 Organizes iteratiosn within four phases. Inception The purpose with this phase is to define the scope and priorities and identify the key risks. This is the shortest phase, from a few days to weeks. Elaboration The purpose with this phase is that the vision, requirements and architecture are stabilised. The major risks will be mitigated and architecturally significant elements will be programmed. Here the risky stuff will be build and tested. Contains several iterations. Construction Here the purpose is to have a system that is ready to deploy by building and testing the rest \u2013 this is the largest phase. Contains the most iterations. Transition The goal with this phase is to deploy the system. Time \u00b6 Milestones \u00b6","title":"(R)UP"},{"location":"5-semester/SOE/04a-rup/#up","text":"Unified Process (UP) Refined in Rational Unified Process (RUP) An iterative process framework Key practices and guidelines Develop in short timeboxed iterations Develop the high-risk and high-value elements (fx the core architecture) in ealy iterations, preferring re-use of existing components Ensure that you deliver value to your customer Accomedate change early in the project Work together as one team","title":"UP"},{"location":"5-semester/SOE/04a-rup/#up-phases","text":"Organizes iteratiosn within four phases. Inception The purpose with this phase is to define the scope and priorities and identify the key risks. This is the shortest phase, from a few days to weeks. Elaboration The purpose with this phase is that the vision, requirements and architecture are stabilised. The major risks will be mitigated and architecturally significant elements will be programmed. Here the risky stuff will be build and tested. Contains several iterations. Construction Here the purpose is to have a system that is ready to deploy by building and testing the rest \u2013 this is the largest phase. Contains the most iterations. Transition The goal with this phase is to deploy the system.","title":"UP Phases"},{"location":"5-semester/SOE/04a-rup/#time","text":"","title":"Time"},{"location":"5-semester/SOE/04a-rup/#milestones","text":"","title":"Milestones"},{"location":"5-semester/SOE/05-planning/","text":"Project Planning and Management \u00b6 Software Pricing \u00b6 Price is affected by (not complete) Estimate of cost to build Overhead Management Building, offices, training Expenses to write bid Discount - search for price that will beat competitive offers What is the value for the customer (as oppsed to production cost) Plan-driven Development \u00b6 The development process is planned in detail A project plan is created that records the work to be done, who will do it, the development schedule, and the work products Managers use the plan to support project decision making and as a way of measuring progress Project Planning \u00b6 Involves breaking down the work into parts assign these to project team members anticipate problems that might arise prepare tentative solutions to those problems The project plan is used to: communicate how the work will be done to the project team and customers assess progress on the project Startup Planning \u00b6 At this stage, you know more about the system requirements but do not have design or implementation information Create a plan with enough detail to make decisions about the project budget and staffing. This plan is the basis for project resource allocation The startup plan should also define project monitoring mechanisms A startup plan is still needed for agile development to allow resources to be allocated to the project Pros and Cons \u00b6 The arguments in favor of a plan-driven approach are that early planning allows organizational issues (availability of staff, other projects, etc.) to be closely taken into account, and that potential problems and dependencies are discovered before the project starts, rather than once the project is underway. The principal argument against plan-driven development is that many early decisions have to be revised because of changes to the environment in which the software is to be developed and used. Process \u00b6 Work Breakdown \u00b6 Break the work into activities and sub-activities Gantt charts PERT diagrams Text Progress measured against milestones (project monitoring) Milestone specification Assessing whether the milestone has been reached Gantt Chart \u00b6 PERT Diagram \u00b6 Milestones \u00b6 Specify products and results Which product? What state? When? Specify the assessment criteria and process Agile Planning \u00b6 Agile methods of software development are iterative approaches where the software is developed and delivered to customers in increments/iterations. The functionality of increments is not planned in advance but is decided during the development. The decision on what to include in an increment depends on progress and on the customer\u2019s priorities. The customer\u2019s priorities and requirements change so it makes sense to have a flexible plan that can accommodate these changes XP: Story-based Estimation and Planning \u00b6 The planning game is based on user stories that reflect the features that should be included in the system. The project team read and discuss the stories and rank them in order of the amount of time they think it will take to implement the story. Stories are assigned \u2018effort points\u2019 (also called story points) reflecting their size and difficulty of implementation The number of effort/story points implemented per day/sprint is measured giving an estimate of the team\u2019s \u2018velocity' This allows the total effort required to implement the system to be estimated Agile Planning Applicability \u00b6 Agile planning works well with small, stable development teams that can get together and discuss the stories to be implemented. However, where teams are large and/or geographically distributed, or when team membership changes frequently, it is practically impossible for everyone to be involved in the collaborative planning that is essential for agile project management Agile Estimation \u00b6 Based on requirements items XP: story points Scrum: Planning poker Experience-based Scrum \u00b6 Sprint Planning \u00b6 Monitor Progress \u00b6 Daily Scrum: Stand-up meeting every morning Organised by Scrum Master Update Scrum board Move items Update estimates on items Remove obstacles Update burn-down chart Scrum Boards \u00b6 Estimation \u00b6 Organizations need to make software effort and cost estimates Experience-based techniques Algorithmic cost modeling Estimate Uncertainty \u00b6 Experience-based Estimation \u00b6 Based on a triangular distribution E=(a+m+b)\\ /\\ 3 E=(a+m+b)\\ /\\ 3 Based on a double triangular distribution E=(a+4m+b)\\ /\\ 6\\\\ SD= (b-a)\\ /\\ 6 E=(a+4m+b)\\ /\\ 6\\\\ SD= (b-a)\\ /\\ 6 where a a is best-case estimate m m is most likely estimate b b is worst-case estimate Algorithmic Cost Modeling \u00b6 Cost is estimated as a mathematical function of product, project and process attributes whose values are estimated by project managers: Effort = A \u0301Size^B^ \u0301M A is an organisation-dependent constant, B reflects the disproportionate effort for laarge projects and M is a multiplier reflecting product, process, and people attributes The most commonly used product attribute for cost estimation is code size, (lines of code: LOC) Most models are similar but they use different values for A, B and M. COCOMO \u00b6 An empirical model based on project experience. Well-documented, \u2018independent\u2019 model which is not tied to a specific software vendor. Long history from initial version published in 1981 (COCOMO-81) through various instantiations to COCOMO 2. COCOMO 2 takes into account different approaches to software development, reuse, etc. An application-composition model This models the effort required to develop systems that are created from reusable components, scripting, or database programming. Software size estimates are based on application points, and a simple size/productivity formula is used to estimate the effort required. The number of application points in a program is a weighted estimate of the number of separate screens that are displayed, the number of reports that are produced, the number of modules in imperative programming languages (such as Java), and the number of lines of scripting language or database programming code. An early design model This model is used during early stages of the system design after the requirements have been established. The estimate is based on the standard estimation formula that I discussed in the introduction, with a simplified set of seven multipliers. Estimates are based on function points, which are then converted to number of lines of source code. Function points are a language-independent way of quantifying program functionality. You compute the total number of function points in a program by measuring or estimating the number of external inputs and outputs, user interactions, external interfaces, and files or database tables used by the system. A reuse model This model is used to compute the effort required to integrate reusable components and/or automatically generated program code. It is normally used in conjunction with the post-architecture model. A post-architecture model Once the system architecture has been designed, amore accurate estimate of the software size can be made. Again, this model uses the standard formula for cost estimation discussed above. However, it includes a more extensive set of 17 multipliers reflecting personnel capability, product, and project characteristics.","title":"Project Planning and Management"},{"location":"5-semester/SOE/05-planning/#project-planning-and-management","text":"","title":"Project Planning and Management"},{"location":"5-semester/SOE/05-planning/#software-pricing","text":"Price is affected by (not complete) Estimate of cost to build Overhead Management Building, offices, training Expenses to write bid Discount - search for price that will beat competitive offers What is the value for the customer (as oppsed to production cost)","title":"Software Pricing"},{"location":"5-semester/SOE/05-planning/#plan-driven-development","text":"The development process is planned in detail A project plan is created that records the work to be done, who will do it, the development schedule, and the work products Managers use the plan to support project decision making and as a way of measuring progress","title":"Plan-driven Development"},{"location":"5-semester/SOE/05-planning/#project-planning","text":"Involves breaking down the work into parts assign these to project team members anticipate problems that might arise prepare tentative solutions to those problems The project plan is used to: communicate how the work will be done to the project team and customers assess progress on the project","title":"Project Planning"},{"location":"5-semester/SOE/05-planning/#startup-planning","text":"At this stage, you know more about the system requirements but do not have design or implementation information Create a plan with enough detail to make decisions about the project budget and staffing. This plan is the basis for project resource allocation The startup plan should also define project monitoring mechanisms A startup plan is still needed for agile development to allow resources to be allocated to the project","title":"Startup Planning"},{"location":"5-semester/SOE/05-planning/#pros-and-cons","text":"The arguments in favor of a plan-driven approach are that early planning allows organizational issues (availability of staff, other projects, etc.) to be closely taken into account, and that potential problems and dependencies are discovered before the project starts, rather than once the project is underway. The principal argument against plan-driven development is that many early decisions have to be revised because of changes to the environment in which the software is to be developed and used.","title":"Pros and Cons"},{"location":"5-semester/SOE/05-planning/#process","text":"","title":"Process"},{"location":"5-semester/SOE/05-planning/#work-breakdown","text":"Break the work into activities and sub-activities Gantt charts PERT diagrams Text Progress measured against milestones (project monitoring) Milestone specification Assessing whether the milestone has been reached","title":"Work Breakdown"},{"location":"5-semester/SOE/05-planning/#gantt-chart","text":"","title":"Gantt Chart"},{"location":"5-semester/SOE/05-planning/#pert-diagram","text":"","title":"PERT Diagram"},{"location":"5-semester/SOE/05-planning/#milestones","text":"Specify products and results Which product? What state? When? Specify the assessment criteria and process","title":"Milestones"},{"location":"5-semester/SOE/05-planning/#agile-planning","text":"Agile methods of software development are iterative approaches where the software is developed and delivered to customers in increments/iterations. The functionality of increments is not planned in advance but is decided during the development. The decision on what to include in an increment depends on progress and on the customer\u2019s priorities. The customer\u2019s priorities and requirements change so it makes sense to have a flexible plan that can accommodate these changes","title":"Agile Planning"},{"location":"5-semester/SOE/05-planning/#xp-story-based-estimation-and-planning","text":"The planning game is based on user stories that reflect the features that should be included in the system. The project team read and discuss the stories and rank them in order of the amount of time they think it will take to implement the story. Stories are assigned \u2018effort points\u2019 (also called story points) reflecting their size and difficulty of implementation The number of effort/story points implemented per day/sprint is measured giving an estimate of the team\u2019s \u2018velocity' This allows the total effort required to implement the system to be estimated","title":"XP: Story-based Estimation and Planning"},{"location":"5-semester/SOE/05-planning/#agile-planning-applicability","text":"Agile planning works well with small, stable development teams that can get together and discuss the stories to be implemented. However, where teams are large and/or geographically distributed, or when team membership changes frequently, it is practically impossible for everyone to be involved in the collaborative planning that is essential for agile project management","title":"Agile Planning Applicability"},{"location":"5-semester/SOE/05-planning/#agile-estimation","text":"Based on requirements items XP: story points Scrum: Planning poker Experience-based","title":"Agile Estimation"},{"location":"5-semester/SOE/05-planning/#scrum","text":"","title":"Scrum"},{"location":"5-semester/SOE/05-planning/#sprint-planning","text":"","title":"Sprint Planning"},{"location":"5-semester/SOE/05-planning/#monitor-progress","text":"Daily Scrum: Stand-up meeting every morning Organised by Scrum Master Update Scrum board Move items Update estimates on items Remove obstacles Update burn-down chart","title":"Monitor Progress"},{"location":"5-semester/SOE/05-planning/#scrum-boards","text":"","title":"Scrum Boards"},{"location":"5-semester/SOE/05-planning/#estimation","text":"Organizations need to make software effort and cost estimates Experience-based techniques Algorithmic cost modeling","title":"Estimation"},{"location":"5-semester/SOE/05-planning/#estimate-uncertainty","text":"","title":"Estimate Uncertainty"},{"location":"5-semester/SOE/05-planning/#experience-based-estimation","text":"Based on a triangular distribution E=(a+m+b)\\ /\\ 3 E=(a+m+b)\\ /\\ 3 Based on a double triangular distribution E=(a+4m+b)\\ /\\ 6\\\\ SD= (b-a)\\ /\\ 6 E=(a+4m+b)\\ /\\ 6\\\\ SD= (b-a)\\ /\\ 6 where a a is best-case estimate m m is most likely estimate b b is worst-case estimate","title":"Experience-based Estimation"},{"location":"5-semester/SOE/05-planning/#algorithmic-cost-modeling","text":"Cost is estimated as a mathematical function of product, project and process attributes whose values are estimated by project managers: Effort = A \u0301Size^B^ \u0301M A is an organisation-dependent constant, B reflects the disproportionate effort for laarge projects and M is a multiplier reflecting product, process, and people attributes The most commonly used product attribute for cost estimation is code size, (lines of code: LOC) Most models are similar but they use different values for A, B and M.","title":"Algorithmic Cost Modeling"},{"location":"5-semester/SOE/05-planning/#cocomo","text":"An empirical model based on project experience. Well-documented, \u2018independent\u2019 model which is not tied to a specific software vendor. Long history from initial version published in 1981 (COCOMO-81) through various instantiations to COCOMO 2. COCOMO 2 takes into account different approaches to software development, reuse, etc. An application-composition model This models the effort required to develop systems that are created from reusable components, scripting, or database programming. Software size estimates are based on application points, and a simple size/productivity formula is used to estimate the effort required. The number of application points in a program is a weighted estimate of the number of separate screens that are displayed, the number of reports that are produced, the number of modules in imperative programming languages (such as Java), and the number of lines of scripting language or database programming code. An early design model This model is used during early stages of the system design after the requirements have been established. The estimate is based on the standard estimation formula that I discussed in the introduction, with a simplified set of seven multipliers. Estimates are based on function points, which are then converted to number of lines of source code. Function points are a language-independent way of quantifying program functionality. You compute the total number of function points in a program by measuring or estimating the number of external inputs and outputs, user interactions, external interfaces, and files or database tables used by the system. A reuse model This model is used to compute the effort required to integrate reusable components and/or automatically generated program code. It is normally used in conjunction with the post-architecture model. A post-architecture model Once the system architecture has been designed, amore accurate estimate of the software size can be made. Again, this model uses the standard formula for cost estimation discussed above. However, it includes a more extensive set of 17 multipliers reflecting personnel capability, product, and project characteristics.","title":"COCOMO"},{"location":"5-semester/SOE/06-risk-management/","text":"Risk Management \u00b6 First, risk concerns future happenings. Today and yesterday are beyond active concern, as we are already reaping what was previously sowed by our past actions. The question is, can we, therefore, by changing our actions today, create an opportunity for a different and hopefully better situation for ourselves tomorrow. This means, second, that risk involves change, such as in changes of mind, opinion, actions, or places. . . . [Third,] risk involves choice, and the uncertainty that choice itself entails. Thus paradoxically, risk, like death and taxes, is one of the few certainties of life. - Robert Charette What is a Risk? \u00b6 It's not a risk if its certain to happen. A possible consequence of a choice Risiko - Negativt Oportunity - Positivt Reactive vs Proactive Risk Strategies \u00b6 Reactive Risk Strategy Laughingly called \"Indiana Jones school of risk management\" At best, monitors the project for likely risks. Resources are set aside to deal with them, should they become problems. More commonly, team does nothing until something goes wrong. Often called fire fighting mode Proactive Risk Strategy A more intelligent strategy. Begins long before technical work is initiated. Potential Risks are identified. Their probability and impact are assessed. They are ranked by importance Software Risks \u00b6 Risk always involves two factors: Uncertainty Loss Different categories are created: Project Risks Threatens project plan. Increases cost Project risks identify potential budgetary, schedule, personnel (staffing and organization), resource, stakeholder, and requirements problems and their impact on a software project. Technical Risks Threatens the quality and timeliness of the software. Technical risks identify potential design, implementation, interface, verification, and maintenance problems. In addition, specification ambiguity, technical uncertainty, technical obsolescence, and \u201cleading- edge\u201d technology are also risk factors. \"the problem is harder to solve than you thought it would be\" Business Risks Candidates for the top five business risks are Market Risk : Building an excellent product or system that no one really wants Strategic Risk: Building a product that no longer fits into the overall business strategy for the company Sales Risk: Building a product that the sales force doesn\u2019t understand how to sell Management Risk : Losing the support of senior management due to a change in focus or a change in people Budget Risks: Losing budgetary or personnel commitment. Another general categorization has been proposed: Known Risks Can be uncovered by careful evaluation of the project plan. Predictable Risks Extrapolated from past project plans e.g. staff turnover, poor communication with the customer, dilution of staff effort as ongoing maintenance requests are serviced Unpredictable Risks They can and do occur, but they are extremely difficult to identify in advance. Risk Identification \u00b6 Two distinct types of risks in each category: Generic risks Potential threat for every software project Product-specific risks Identified only by those with a clear understanding of the technology, the people, and the environment that is specific to the software that is to be built Risk Item Checklist \u00b6 Used for identification and focuses of some subset of known and predictable risks in the following generic subcategories: Product size \u2014risks associated with the overall size of the software to be built or modified. Business impact \u2014risks associated with constraints imposed by management or the marketplace. Stakeholder characteristics \u2014risks associated with the sophistication of the stakeholders and the developer\u2019s ability to communicate with stakeholders in a timely manner. Process definition \u2014risks associated with the degree to which the software process has been defined and is followed by the development organization. Development environment \u2014risks associated with the availability and quality of the tools to be used to build the product. Technology to be built \u2014risks associated with the complexity of the system to be built and the \u201cnewness\u201d of the technology that is packaged by the system. Staff size and experience \u2014risks associated with the overall technical and project experience of the software engineers who will do the work. A number of comprehensive checklists for software project risk are available on the Web. Assessing Overall Project Risk \u00b6 The following questions have been derived from risk data obtained by surveying: Have top software and customer managers formally committed to support the project? Are end users enthusiastically committed to the project and the system/ product to be built? Are requirements fully understood by the software engineering team and its customers? Have customers been involved fully in the definition of requirements? Do end users have realistic expectations? Is the project scope stable? Does the software engineering team have the right mix of skills? Are project requirements stable? Does the project team have experience with the technology to be implemented? Is the number of people on the project team adequate to do the job? Do all customer/user constituencies agree on the importance of the project and on the requirements for the system/product to be built? If any of these are answered negatively, then mitigation, monitoring and management should be instituted without fail. The degree to which the project is at risk is directly proportional to the number of negative responses to these questions. Risk Components and Drivers \u00b6 The US Air Force have published guidelines for software risk identification and abatement. Risk components are defined: Performance risk \u2014the degree of uncertainty that the product will meet its requirements and be fit for its intended use. Cost risk \u2014the degree of uncertainty that the project budget will be maintained. Support risk \u2014the degree of uncertainty that the resultant software will be easy to correct, adapt, and enhance. Schedule risk \u2014the degree of uncertainty that the project schedule will be maintained and that the product will be delivered on time. The impact of each risk driver is divided into one of: Negligible Marginal Critical Catastrophic Risk Projection (Risk Estimation) \u00b6 Attempts to rate each risk in: The probability that risk is real The consequences There are four risk projection steps: Establish a scale that reflects the perceived likelihood of a risk. Delineate the consequences of the risk Estimate the impact of the risk on the project and the product Assess the overall accuracy of the risk projection so that there will be no misunderstandings Risk Table \u00b6 Sorted by probability and impact to rank risks You can define a cutoff line . Horizontal line somewhere in the table Only risks above the line are given further attention. RMMM contains a pointer into a risk mitigation, monitoring, and management plan Risk Impact \u00b6 Three factors affect the consequences. Its nature Its scope Its timing Overall risk exposure RE RE is calculated: $$ RE=P\\times C $$ where P P is the probability of the risk, and C C is the cost to the project if the risk occurs. Example: \u00b6 Risk Refinement \u00b6 Refine risks into a set of more detailed risks, each somewhat easier to mitigate, monitor and manage. Condition-Transition-Consequence (CTC) Format \u00b6 Using the CTC format for the example from before: Given that all reusable software components must conform to specific design standards and that some do not conform, then there is concern that (possibly) only 70 percent of the planned reusable modules may actually be integrated into the as-built system, resulting in the need to custom engineer the remaining 30 percent of components. This can be refined: Subcondition 1 Certain reusable components were developed by a third party with no knowledge of internal design standards. Subcondition 2 The design standard for component interfaces has not been solidified and may not conform to certain existing reusable components. Subcondition 3 Certain reusable components have been implemented in a language that is not supported on the target environment. Risk Mitigation, Monitoring, and Management \u00b6 An effective strategy must consider three issues: Risk avoidance Risk monitoring Risk Management Avoidance is always the best strategy. Achieved by making a risk mitigation plan. RMMM ( Risk - Mititgate - Monitor - Manage)","title":"Risk Management"},{"location":"5-semester/SOE/06-risk-management/#risk-management","text":"First, risk concerns future happenings. Today and yesterday are beyond active concern, as we are already reaping what was previously sowed by our past actions. The question is, can we, therefore, by changing our actions today, create an opportunity for a different and hopefully better situation for ourselves tomorrow. This means, second, that risk involves change, such as in changes of mind, opinion, actions, or places. . . . [Third,] risk involves choice, and the uncertainty that choice itself entails. Thus paradoxically, risk, like death and taxes, is one of the few certainties of life. - Robert Charette","title":"Risk Management"},{"location":"5-semester/SOE/06-risk-management/#what-is-a-risk","text":"It's not a risk if its certain to happen. A possible consequence of a choice Risiko - Negativt Oportunity - Positivt","title":"What is a Risk?"},{"location":"5-semester/SOE/06-risk-management/#reactive-vs-proactive-risk-strategies","text":"Reactive Risk Strategy Laughingly called \"Indiana Jones school of risk management\" At best, monitors the project for likely risks. Resources are set aside to deal with them, should they become problems. More commonly, team does nothing until something goes wrong. Often called fire fighting mode Proactive Risk Strategy A more intelligent strategy. Begins long before technical work is initiated. Potential Risks are identified. Their probability and impact are assessed. They are ranked by importance","title":"Reactive vs Proactive Risk Strategies"},{"location":"5-semester/SOE/06-risk-management/#software-risks","text":"Risk always involves two factors: Uncertainty Loss Different categories are created: Project Risks Threatens project plan. Increases cost Project risks identify potential budgetary, schedule, personnel (staffing and organization), resource, stakeholder, and requirements problems and their impact on a software project. Technical Risks Threatens the quality and timeliness of the software. Technical risks identify potential design, implementation, interface, verification, and maintenance problems. In addition, specification ambiguity, technical uncertainty, technical obsolescence, and \u201cleading- edge\u201d technology are also risk factors. \"the problem is harder to solve than you thought it would be\" Business Risks Candidates for the top five business risks are Market Risk : Building an excellent product or system that no one really wants Strategic Risk: Building a product that no longer fits into the overall business strategy for the company Sales Risk: Building a product that the sales force doesn\u2019t understand how to sell Management Risk : Losing the support of senior management due to a change in focus or a change in people Budget Risks: Losing budgetary or personnel commitment. Another general categorization has been proposed: Known Risks Can be uncovered by careful evaluation of the project plan. Predictable Risks Extrapolated from past project plans e.g. staff turnover, poor communication with the customer, dilution of staff effort as ongoing maintenance requests are serviced Unpredictable Risks They can and do occur, but they are extremely difficult to identify in advance.","title":"Software Risks"},{"location":"5-semester/SOE/06-risk-management/#risk-identification","text":"Two distinct types of risks in each category: Generic risks Potential threat for every software project Product-specific risks Identified only by those with a clear understanding of the technology, the people, and the environment that is specific to the software that is to be built","title":"Risk Identification"},{"location":"5-semester/SOE/06-risk-management/#risk-item-checklist","text":"Used for identification and focuses of some subset of known and predictable risks in the following generic subcategories: Product size \u2014risks associated with the overall size of the software to be built or modified. Business impact \u2014risks associated with constraints imposed by management or the marketplace. Stakeholder characteristics \u2014risks associated with the sophistication of the stakeholders and the developer\u2019s ability to communicate with stakeholders in a timely manner. Process definition \u2014risks associated with the degree to which the software process has been defined and is followed by the development organization. Development environment \u2014risks associated with the availability and quality of the tools to be used to build the product. Technology to be built \u2014risks associated with the complexity of the system to be built and the \u201cnewness\u201d of the technology that is packaged by the system. Staff size and experience \u2014risks associated with the overall technical and project experience of the software engineers who will do the work. A number of comprehensive checklists for software project risk are available on the Web.","title":"Risk Item Checklist"},{"location":"5-semester/SOE/06-risk-management/#assessing-overall-project-risk","text":"The following questions have been derived from risk data obtained by surveying: Have top software and customer managers formally committed to support the project? Are end users enthusiastically committed to the project and the system/ product to be built? Are requirements fully understood by the software engineering team and its customers? Have customers been involved fully in the definition of requirements? Do end users have realistic expectations? Is the project scope stable? Does the software engineering team have the right mix of skills? Are project requirements stable? Does the project team have experience with the technology to be implemented? Is the number of people on the project team adequate to do the job? Do all customer/user constituencies agree on the importance of the project and on the requirements for the system/product to be built? If any of these are answered negatively, then mitigation, monitoring and management should be instituted without fail. The degree to which the project is at risk is directly proportional to the number of negative responses to these questions.","title":"Assessing Overall Project Risk"},{"location":"5-semester/SOE/06-risk-management/#risk-components-and-drivers","text":"The US Air Force have published guidelines for software risk identification and abatement. Risk components are defined: Performance risk \u2014the degree of uncertainty that the product will meet its requirements and be fit for its intended use. Cost risk \u2014the degree of uncertainty that the project budget will be maintained. Support risk \u2014the degree of uncertainty that the resultant software will be easy to correct, adapt, and enhance. Schedule risk \u2014the degree of uncertainty that the project schedule will be maintained and that the product will be delivered on time. The impact of each risk driver is divided into one of: Negligible Marginal Critical Catastrophic","title":"Risk Components and Drivers"},{"location":"5-semester/SOE/06-risk-management/#risk-projection-risk-estimation","text":"Attempts to rate each risk in: The probability that risk is real The consequences There are four risk projection steps: Establish a scale that reflects the perceived likelihood of a risk. Delineate the consequences of the risk Estimate the impact of the risk on the project and the product Assess the overall accuracy of the risk projection so that there will be no misunderstandings","title":"Risk Projection (Risk Estimation)"},{"location":"5-semester/SOE/06-risk-management/#risk-table","text":"Sorted by probability and impact to rank risks You can define a cutoff line . Horizontal line somewhere in the table Only risks above the line are given further attention. RMMM contains a pointer into a risk mitigation, monitoring, and management plan","title":"Risk Table"},{"location":"5-semester/SOE/06-risk-management/#risk-impact","text":"Three factors affect the consequences. Its nature Its scope Its timing Overall risk exposure RE RE is calculated: $$ RE=P\\times C $$ where P P is the probability of the risk, and C C is the cost to the project if the risk occurs.","title":"Risk Impact"},{"location":"5-semester/SOE/06-risk-management/#example","text":"","title":"Example:"},{"location":"5-semester/SOE/06-risk-management/#risk-refinement","text":"Refine risks into a set of more detailed risks, each somewhat easier to mitigate, monitor and manage.","title":"Risk Refinement"},{"location":"5-semester/SOE/06-risk-management/#condition-transition-consequence-ctc-format","text":"Using the CTC format for the example from before: Given that all reusable software components must conform to specific design standards and that some do not conform, then there is concern that (possibly) only 70 percent of the planned reusable modules may actually be integrated into the as-built system, resulting in the need to custom engineer the remaining 30 percent of components. This can be refined: Subcondition 1 Certain reusable components were developed by a third party with no knowledge of internal design standards. Subcondition 2 The design standard for component interfaces has not been solidified and may not conform to certain existing reusable components. Subcondition 3 Certain reusable components have been implemented in a language that is not supported on the target environment.","title":"Condition-Transition-Consequence (CTC) Format"},{"location":"5-semester/SOE/06-risk-management/#risk-mitigation-monitoring-and-management","text":"An effective strategy must consider three issues: Risk avoidance Risk monitoring Risk Management Avoidance is always the best strategy. Achieved by making a risk mitigation plan. RMMM ( Risk - Mititgate - Monitor - Manage)","title":"Risk Mitigation, Monitoring, and Management"},{"location":"5-semester/SOE/07-quality/","text":"Quality \u00b6 Definition Quality is a reflection of one or more peoples' assessment of correspondence between their expectations and experience of a product or a service Can be divided into three categories Product Quality Process Quality Quality of expectations Cost of bad quality Direct cost: Loss Wasted work Maintenance usually more expensive than development Indirect cost Follows from poor quality Has potentially severe consequences Quality Management \u00b6 Quality Management consists of Quality Assurance Plan or design processes to prevent bad quality Quality Control Monitor that work products meet quality standards Theory of Cost and Quality \u00b6 Low quality(low quality management) is initially cheap, but becomes gradually very expensive High quality management, has an initial cost when quality processes are defined, but is cheaper later because users are reporting much less errors, and code is more stable The amount of quality management should be balanced to the cost \u2013 a process that is 100% defect free is often too expensive, so an appropriate compromise is normally made. As a result the initial cost is a little less than for very high quality management, at the cost that slightly more defects are reported over time. We plan and design, how and when to do verification and validation in our process Quality Assurance \u00b6 Validation (fit for use) Are we building the right systems? Conforms to customers' expectations and experience Verification (are all requirements implemented) Are we building the system right? Conforms to specification Relatively objective process Techniques Testing of programs and prototypes Reviewing of specifications, documentation and programs In the Different Models \u00b6 Waterfall \u00b6 Incremental \u00b6 Integration and Configuration \u00b6 Quality Management and Agile Development \u00b6 Quality management in agile is informal rather than document-based It relies on establishing a quality culture , where all team members feel responsible for software quality and take actions to ensure that quality is maintained. This quality culture is established through agile quality practices Agile Quality Practices \u00b6 Definition of Done Team agree on what criteria must be met before a task is complete Sprint Review Product Owner and other stakeholders validate the sprint delivery meets expectations Check before check-in Developers are responsible for organizing their own code reviews with other team members before the code is checked in to the build system Never break the build Team members should not check in code that causes the system to fail Developers have to test their code changes against the whole system and be confident that these work as expected Fix problems when you see them If a programmer discovers problems or obscurities in code developed by someone else, they can fix these directly rather than referring them back to the original developer Fundamental Process Theory (Process Quality) \u00b6 A software product can only be as good as the process through which it is produced You can only improve the quality of the product if you improve the process Repeating the same process, will create same level of quality Sources of bad quality can be used as input to improve the process","title":"Quality Management"},{"location":"5-semester/SOE/07-quality/#quality","text":"Definition Quality is a reflection of one or more peoples' assessment of correspondence between their expectations and experience of a product or a service Can be divided into three categories Product Quality Process Quality Quality of expectations Cost of bad quality Direct cost: Loss Wasted work Maintenance usually more expensive than development Indirect cost Follows from poor quality Has potentially severe consequences","title":"Quality"},{"location":"5-semester/SOE/07-quality/#quality-management","text":"Quality Management consists of Quality Assurance Plan or design processes to prevent bad quality Quality Control Monitor that work products meet quality standards","title":"Quality Management"},{"location":"5-semester/SOE/07-quality/#theory-of-cost-and-quality","text":"Low quality(low quality management) is initially cheap, but becomes gradually very expensive High quality management, has an initial cost when quality processes are defined, but is cheaper later because users are reporting much less errors, and code is more stable The amount of quality management should be balanced to the cost \u2013 a process that is 100% defect free is often too expensive, so an appropriate compromise is normally made. As a result the initial cost is a little less than for very high quality management, at the cost that slightly more defects are reported over time. We plan and design, how and when to do verification and validation in our process","title":"Theory of Cost and Quality"},{"location":"5-semester/SOE/07-quality/#quality-assurance","text":"Validation (fit for use) Are we building the right systems? Conforms to customers' expectations and experience Verification (are all requirements implemented) Are we building the system right? Conforms to specification Relatively objective process Techniques Testing of programs and prototypes Reviewing of specifications, documentation and programs","title":"Quality Assurance"},{"location":"5-semester/SOE/07-quality/#in-the-different-models","text":"","title":"In the Different Models"},{"location":"5-semester/SOE/07-quality/#waterfall","text":"","title":"Waterfall"},{"location":"5-semester/SOE/07-quality/#incremental","text":"","title":"Incremental"},{"location":"5-semester/SOE/07-quality/#integration-and-configuration","text":"","title":"Integration and Configuration"},{"location":"5-semester/SOE/07-quality/#quality-management-and-agile-development","text":"Quality management in agile is informal rather than document-based It relies on establishing a quality culture , where all team members feel responsible for software quality and take actions to ensure that quality is maintained. This quality culture is established through agile quality practices","title":"Quality Management and Agile Development"},{"location":"5-semester/SOE/07-quality/#agile-quality-practices","text":"Definition of Done Team agree on what criteria must be met before a task is complete Sprint Review Product Owner and other stakeholders validate the sprint delivery meets expectations Check before check-in Developers are responsible for organizing their own code reviews with other team members before the code is checked in to the build system Never break the build Team members should not check in code that causes the system to fail Developers have to test their code changes against the whole system and be confident that these work as expected Fix problems when you see them If a programmer discovers problems or obscurities in code developed by someone else, they can fix these directly rather than referring them back to the original developer","title":"Agile Quality Practices"},{"location":"5-semester/SOE/07-quality/#fundamental-process-theory-process-quality","text":"A software product can only be as good as the process through which it is produced You can only improve the quality of the product if you improve the process Repeating the same process, will create same level of quality Sources of bad quality can be used as input to improve the process","title":"Fundamental Process Theory (Process Quality)"},{"location":"5-semester/SOE/9ba-test/","text":"Test \u00b6 Test-driven Development (TDD) \u00b6 Tests are written before code and 'passing' the test is the critical driver of development You develop code incrementally, along with a test for that increment. You dont move on to the next increment before code has passed its test TDD was introduced as part of agile methods such as XP. However, it can also be used in plan-driven development processes. Process Activities \u00b6 Start by identifying the increment of functionality that is required. This should normally be small and implementable in a few lines of code Write a test for this functionality and implement this as an automated test Run the test, along with all other tests that have been implemented. Initially, you have not implemented the functionality so the new test will fail Implement the functionality and re-run the test Once all tests run successfully, you move on to implementing the next chunk of functionality Benefits \u00b6 Code Coverage Every code segment that you write has at least one associated test so all code written has at least one test Regression Testing A regression test suite is developed incrementally as a program is developed. Simplified Debugging When a test fails, it should be obvious where the problem lies. The newly written code needs to be checked and modified System Documentation The tests themselves are a form of documentation that describe what the code should be doing. Plan-Driven Testing \u00b6 V Model \u00b6","title":"Test"},{"location":"5-semester/SOE/9ba-test/#test","text":"","title":"Test"},{"location":"5-semester/SOE/9ba-test/#test-driven-development-tdd","text":"Tests are written before code and 'passing' the test is the critical driver of development You develop code incrementally, along with a test for that increment. You dont move on to the next increment before code has passed its test TDD was introduced as part of agile methods such as XP. However, it can also be used in plan-driven development processes.","title":"Test-driven Development (TDD)"},{"location":"5-semester/SOE/9ba-test/#process-activities","text":"Start by identifying the increment of functionality that is required. This should normally be small and implementable in a few lines of code Write a test for this functionality and implement this as an automated test Run the test, along with all other tests that have been implemented. Initially, you have not implemented the functionality so the new test will fail Implement the functionality and re-run the test Once all tests run successfully, you move on to implementing the next chunk of functionality","title":"Process Activities"},{"location":"5-semester/SOE/9ba-test/#benefits","text":"Code Coverage Every code segment that you write has at least one associated test so all code written has at least one test Regression Testing A regression test suite is developed incrementally as a program is developed. Simplified Debugging When a test fails, it should be obvious where the problem lies. The newly written code needs to be checked and modified System Documentation The tests themselves are a form of documentation that describe what the code should be doing.","title":"Benefits"},{"location":"5-semester/SOE/9ba-test/#plan-driven-testing","text":"","title":"Plan-Driven Testing"},{"location":"5-semester/SOE/9ba-test/#v-model","text":"","title":"V Model"},{"location":"5-semester/SOE/exam/","text":"Exam Questions \u00b6 Software process models: waterfall Software process model: incremental and iterative Software process model: integration & configuration Comparison of plan-driven and agile software engineering processes, including analysis of home grounds Key features of Scrum Key features of RUP Requirements Elicitation and Managing change to requirements Quality Control: Verification and Validation Risk Management Project Planning and Management Quality Management: How is quality defined - agile versus plan driven approaches Configuration Management","title":"Exam Questions"},{"location":"5-semester/SOE/exam/#exam-questions","text":"Software process models: waterfall Software process model: incremental and iterative Software process model: integration & configuration Comparison of plan-driven and agile software engineering processes, including analysis of home grounds Key features of Scrum Key features of RUP Requirements Elicitation and Managing change to requirements Quality Control: Verification and Validation Risk Management Project Planning and Management Quality Management: How is quality defined - agile versus plan driven approaches Configuration Management","title":"Exam Questions"},{"location":"5-semester/SOE/exam/01-waterfall/","text":"Software process models: Waterfall \u00b6 Software Engineering \u00b6 Pga. ingen fysiske constraints , kan software systemer hurtigt blive ekstrem komplekse , sv\u00e6re at forst\u00e5, og dyre at \u00e6ndre i. Mange fejl i software projekter skyldes 2 faktorer Increasing System Complexity Demands change, systemr skal leveres hurtigere ; st\u00f8rre , endnu mere komplekse . Failure to use software engineering methods Det er nemt at skrive computer programmer uden at bruge SE metoder og teknikker. Mange selskaber er blevet software udviklere med tiden De bruger ikke SE metoder i deres hverdagsarbejde Derfor bliver deres software ofte dyrere og mindre reliable end det burde. SE Process Activities \u00b6 Software specification The functionality of the software and constraints on its operation must be defined. Software development The software to meet the specification must be produced. Software validation The software must be validated to ensure that it does what the customer wants. Software evolution The software must evolve to meet changing customer needs. Waterfall \u00b6 Aktiviteter foreg\u00e5r i sekvens , overlevering af work products mellem faser, milestones og relateret work products bruges til at overv\u00e5ge progress Plan Drevet \u00b6 Plan drevet, da process aktiviteterne er planlagt f\u00f8r start. Requirements analysis and definition Services , constraints , og m\u00e5l etableres ved konsultation med system brugere. Beskrives i detajler som system specifikation System and software design Allokerer requirements til hardware eller software systems. Etablerer en overall system arkitektur Identificering og beskrivesle af fundamental software system abstraktion og relationships Implementation and unit testing Software design udarbejdes og unit testing verificerer at hver unit overholder dets specifikation Integration and system testing Program units eller programmer integrerets og testes som helhed . Udleveres til kunden efter testing Operation and maintenance System installeres og tages i brug . Vedligeholdelse Opdatering Hvorn\u00e5r waterfall? \u00b6 Embedded systems Infleksibilt hardware Life critical systems Specs og design documents skal v\u00e6re komplet . Ekstensiv sikkerhedsanalyse Store software systemer Del af bredere ingeni\u00f8r systemer Hardwaren kan v\u00e6re udviklet med lignende model. F\u00e6lles model for hardware og software","title":"1. Software process models: waterfall"},{"location":"5-semester/SOE/exam/01-waterfall/#software-process-models-waterfall","text":"","title":"Software process models: Waterfall"},{"location":"5-semester/SOE/exam/01-waterfall/#software-engineering","text":"Pga. ingen fysiske constraints , kan software systemer hurtigt blive ekstrem komplekse , sv\u00e6re at forst\u00e5, og dyre at \u00e6ndre i. Mange fejl i software projekter skyldes 2 faktorer Increasing System Complexity Demands change, systemr skal leveres hurtigere ; st\u00f8rre , endnu mere komplekse . Failure to use software engineering methods Det er nemt at skrive computer programmer uden at bruge SE metoder og teknikker. Mange selskaber er blevet software udviklere med tiden De bruger ikke SE metoder i deres hverdagsarbejde Derfor bliver deres software ofte dyrere og mindre reliable end det burde.","title":"Software Engineering"},{"location":"5-semester/SOE/exam/01-waterfall/#se-process-activities","text":"Software specification The functionality of the software and constraints on its operation must be defined. Software development The software to meet the specification must be produced. Software validation The software must be validated to ensure that it does what the customer wants. Software evolution The software must evolve to meet changing customer needs.","title":"SE Process Activities"},{"location":"5-semester/SOE/exam/01-waterfall/#waterfall","text":"Aktiviteter foreg\u00e5r i sekvens , overlevering af work products mellem faser, milestones og relateret work products bruges til at overv\u00e5ge progress","title":"Waterfall"},{"location":"5-semester/SOE/exam/01-waterfall/#plan-drevet","text":"Plan drevet, da process aktiviteterne er planlagt f\u00f8r start. Requirements analysis and definition Services , constraints , og m\u00e5l etableres ved konsultation med system brugere. Beskrives i detajler som system specifikation System and software design Allokerer requirements til hardware eller software systems. Etablerer en overall system arkitektur Identificering og beskrivesle af fundamental software system abstraktion og relationships Implementation and unit testing Software design udarbejdes og unit testing verificerer at hver unit overholder dets specifikation Integration and system testing Program units eller programmer integrerets og testes som helhed . Udleveres til kunden efter testing Operation and maintenance System installeres og tages i brug . Vedligeholdelse Opdatering","title":"Plan Drevet"},{"location":"5-semester/SOE/exam/01-waterfall/#hvornar-waterfall","text":"Embedded systems Infleksibilt hardware Life critical systems Specs og design documents skal v\u00e6re komplet . Ekstensiv sikkerhedsanalyse Store software systemer Del af bredere ingeni\u00f8r systemer Hardwaren kan v\u00e6re udviklet med lignende model. F\u00e6lles model for hardware og software","title":"Hvorn\u00e5r waterfall?"},{"location":"5-semester/SOE/exam/02-incremental-iterative/","text":"Software process models: Incremental and Iterative \u00b6 Software Engineering \u00b6 Pga. ingen fysiske constraints , kan software systemer hurtigt blive ekstrem komplekse , sv\u00e6re at forst\u00e5, og dyre at \u00e6ndre i. Mange fejl i software projekter skyldes 2 faktorer Increasing System Complexity Demands change, systemr skal leveres hurtigere ; st\u00f8rre , endnu mere komplekse . Failure to use software engineering methods Det er nemt at skrive computer programmer uden at bruge SE metoder og teknikker. Mange selskaber er blevet software udviklere med tiden De bruger ikke SE metoder i deres hverdagsarbejde Derfor bliver deres software ofte dyrere og mindre reliable end det burde. SE Process Activities \u00b6 Software specification The functionality of the software and constraints on its operation must be defined. Software development The software to meet the specification must be produced. Software validation The software must be validated to ensure that it does what the customer wants. Software evolution The software must evolve to meet changing customer needs. Incremental and Iterative \u00b6 Her udvikles systemet i iterationer . Som en serie af \" versioner \" Hver version tilf\u00f8jer funktionalitet til tidligere version Specifikation , Udvikling/development , og Validation sker i hver iteration . Planen deles op i flere bidder Der laves en initial implementering, hvorfra man f\u00e5r feedback fra brugerer, herefter udvikles softwaren i flere iterationer indtil f\u00e6rdigt . Det er godt hvis der er h\u00f8j sandsynlighed for at requirements \u00e6ndres undervejs Det er billigere og nemmere at lave \u00e6ndringer undervejs end waterfall Mest software bliver udviklet med incremental and iterative model. Waterfall fungerer bedst til safety-critical software. Massere af analyse og dokumentation f\u00f8r implementering Plandrevet eller Agilt? \u00b6 Kan v\u00e6re b\u00e5de plandrevet og agilt. Eksempelvis er det plandrevet hvis en waterfall model deles op i planlagte iterationer fra start. Ved agilt ville man kun planl\u00e6gge \u00e9t inkrement af gangen Fordele og Ulemper \u00b6 Fordele Billigere at implementere \u00e6ndringer undervejs Nemmere at f\u00e5 feedback p\u00e5 det der er lavet Kan leveres hurtigere til kunden selvom det ikke er f\u00e6rdigt endnu Ulemper Usynlig process. Managers skal have regul\u00e6r delivery af produktet for at m\u00e5le progress Struktur foringes med antallet af inkrements Rodet kode","title":"2. Software process model: incremental and iterative"},{"location":"5-semester/SOE/exam/02-incremental-iterative/#software-process-models-incremental-and-iterative","text":"","title":"Software process models: Incremental and Iterative"},{"location":"5-semester/SOE/exam/02-incremental-iterative/#software-engineering","text":"Pga. ingen fysiske constraints , kan software systemer hurtigt blive ekstrem komplekse , sv\u00e6re at forst\u00e5, og dyre at \u00e6ndre i. Mange fejl i software projekter skyldes 2 faktorer Increasing System Complexity Demands change, systemr skal leveres hurtigere ; st\u00f8rre , endnu mere komplekse . Failure to use software engineering methods Det er nemt at skrive computer programmer uden at bruge SE metoder og teknikker. Mange selskaber er blevet software udviklere med tiden De bruger ikke SE metoder i deres hverdagsarbejde Derfor bliver deres software ofte dyrere og mindre reliable end det burde.","title":"Software Engineering"},{"location":"5-semester/SOE/exam/02-incremental-iterative/#se-process-activities","text":"Software specification The functionality of the software and constraints on its operation must be defined. Software development The software to meet the specification must be produced. Software validation The software must be validated to ensure that it does what the customer wants. Software evolution The software must evolve to meet changing customer needs.","title":"SE Process Activities"},{"location":"5-semester/SOE/exam/02-incremental-iterative/#incremental-and-iterative","text":"Her udvikles systemet i iterationer . Som en serie af \" versioner \" Hver version tilf\u00f8jer funktionalitet til tidligere version Specifikation , Udvikling/development , og Validation sker i hver iteration . Planen deles op i flere bidder Der laves en initial implementering, hvorfra man f\u00e5r feedback fra brugerer, herefter udvikles softwaren i flere iterationer indtil f\u00e6rdigt . Det er godt hvis der er h\u00f8j sandsynlighed for at requirements \u00e6ndres undervejs Det er billigere og nemmere at lave \u00e6ndringer undervejs end waterfall Mest software bliver udviklet med incremental and iterative model. Waterfall fungerer bedst til safety-critical software. Massere af analyse og dokumentation f\u00f8r implementering","title":"Incremental and Iterative"},{"location":"5-semester/SOE/exam/02-incremental-iterative/#plandrevet-eller-agilt","text":"Kan v\u00e6re b\u00e5de plandrevet og agilt. Eksempelvis er det plandrevet hvis en waterfall model deles op i planlagte iterationer fra start. Ved agilt ville man kun planl\u00e6gge \u00e9t inkrement af gangen","title":"Plandrevet eller Agilt?"},{"location":"5-semester/SOE/exam/02-incremental-iterative/#fordele-og-ulemper","text":"Fordele Billigere at implementere \u00e6ndringer undervejs Nemmere at f\u00e5 feedback p\u00e5 det der er lavet Kan leveres hurtigere til kunden selvom det ikke er f\u00e6rdigt endnu Ulemper Usynlig process. Managers skal have regul\u00e6r delivery af produktet for at m\u00e5le progress Struktur foringes med antallet af inkrements Rodet kode","title":"Fordele og Ulemper"},{"location":"5-semester/SOE/exam/03-integration-configuration/","text":"Software process model: integration & configuration \u00b6 Software Engineering \u00b6 Pga. ingen fysiske constraints , kan software systemer hurtigt blive ekstrem komplekse , sv\u00e6re at forst\u00e5, og dyre at \u00e6ndre i. Mange fejl i software projekter skyldes 2 faktorer Increasing System Complexity Demands change, systemr skal leveres hurtigere ; st\u00f8rre , endnu mere komplekse . Failure to use software engineering methods Det er nemt at skrive computer programmer uden at bruge SE metoder og teknikker. Mange selskaber er blevet software udviklere med tiden De bruger ikke SE metoder i deres hverdagsarbejde Derfor bliver deres software ofte dyrere og mindre reliable end det burde. SE Process Activities \u00b6 Software specification The functionality of the software and constraints on its operation must be defined. Software development The software to meet the specification must be produced. Software validation The software must be validated to ensure that it does what the customer wants. Software evolution The software must evolve to meet changing customer needs. Integration and Configuration \u00b6 Afh\u00e6nger af tilg\u00e6ngeligheden af genbruglige components eller systemer . Fokuserer p\u00e5 at konfigurere disse komponenter til brug i nye omgivelser og integrere dem ind i et system Et system samles fra eksisterende konfigurerbare moduler. Det er meget almindeligt at genbruge kode. Disse 3 typer software genbruges ofte: Standalone systemer som konfigureres til et bestemt milj\u00f8 Collection of objects as component or package Web services , udvikles if\u00f8lge standarder Kan kaldes remotely, (eks. REST API'er) Plandrevet eller Agilt? \u00b6 Kan b\u00e5de v\u00e6re plandrevet eller agilt Fordele og Ulemper \u00b6 Fordele Reducerer m\u00e6ngden af software der skal udvikles Cost og risk reduceres Ulemper Uundg\u00e5ligt at kompromittere requirements Kan f\u00f8re til at system ikke opfylder reel behov fra bruger Ingen kontrol over genbrugt software Hvordan og hvorn\u00e5r nye versioner kommer \u00c6ndringer i funktionalitet Model \u00b6 Requirements specification Initial requirements foresl\u00e5s, ikke detaljeret - essential requirements og system features Software discovery and evaluation Der s\u00f8ges efter komponenter og systemer der kan give den kr\u00e6vede funktionalitet. Kandidater evalueres efter om de m\u00f8der essential requirements, og hvor suitable de er Requirements refinement Requirements refineres efter info om brugbare komponenter/systemer Application system configuration Hvis off-the-shelf system findes, konfigureres det til brug i det nye system Component adaption and integration Hvis ikke, bliver eksisterende komponenter evt. modificeret, og nye udvilkes til at integrere i systemet","title":"3. Software process model: integration & configuration"},{"location":"5-semester/SOE/exam/03-integration-configuration/#software-process-model-integration-configuration","text":"","title":"Software process model: integration &amp; configuration"},{"location":"5-semester/SOE/exam/03-integration-configuration/#software-engineering","text":"Pga. ingen fysiske constraints , kan software systemer hurtigt blive ekstrem komplekse , sv\u00e6re at forst\u00e5, og dyre at \u00e6ndre i. Mange fejl i software projekter skyldes 2 faktorer Increasing System Complexity Demands change, systemr skal leveres hurtigere ; st\u00f8rre , endnu mere komplekse . Failure to use software engineering methods Det er nemt at skrive computer programmer uden at bruge SE metoder og teknikker. Mange selskaber er blevet software udviklere med tiden De bruger ikke SE metoder i deres hverdagsarbejde Derfor bliver deres software ofte dyrere og mindre reliable end det burde.","title":"Software Engineering"},{"location":"5-semester/SOE/exam/03-integration-configuration/#se-process-activities","text":"Software specification The functionality of the software and constraints on its operation must be defined. Software development The software to meet the specification must be produced. Software validation The software must be validated to ensure that it does what the customer wants. Software evolution The software must evolve to meet changing customer needs.","title":"SE Process Activities"},{"location":"5-semester/SOE/exam/03-integration-configuration/#integration-and-configuration","text":"Afh\u00e6nger af tilg\u00e6ngeligheden af genbruglige components eller systemer . Fokuserer p\u00e5 at konfigurere disse komponenter til brug i nye omgivelser og integrere dem ind i et system Et system samles fra eksisterende konfigurerbare moduler. Det er meget almindeligt at genbruge kode. Disse 3 typer software genbruges ofte: Standalone systemer som konfigureres til et bestemt milj\u00f8 Collection of objects as component or package Web services , udvikles if\u00f8lge standarder Kan kaldes remotely, (eks. REST API'er)","title":"Integration and Configuration"},{"location":"5-semester/SOE/exam/03-integration-configuration/#plandrevet-eller-agilt","text":"Kan b\u00e5de v\u00e6re plandrevet eller agilt","title":"Plandrevet eller Agilt?"},{"location":"5-semester/SOE/exam/03-integration-configuration/#fordele-og-ulemper","text":"Fordele Reducerer m\u00e6ngden af software der skal udvikles Cost og risk reduceres Ulemper Uundg\u00e5ligt at kompromittere requirements Kan f\u00f8re til at system ikke opfylder reel behov fra bruger Ingen kontrol over genbrugt software Hvordan og hvorn\u00e5r nye versioner kommer \u00c6ndringer i funktionalitet","title":"Fordele og Ulemper"},{"location":"5-semester/SOE/exam/03-integration-configuration/#model","text":"Requirements specification Initial requirements foresl\u00e5s, ikke detaljeret - essential requirements og system features Software discovery and evaluation Der s\u00f8ges efter komponenter og systemer der kan give den kr\u00e6vede funktionalitet. Kandidater evalueres efter om de m\u00f8der essential requirements, og hvor suitable de er Requirements refinement Requirements refineres efter info om brugbare komponenter/systemer Application system configuration Hvis off-the-shelf system findes, konfigureres det til brug i det nye system Component adaption and integration Hvis ikke, bliver eksisterende komponenter evt. modificeret, og nye udvilkes til at integrere i systemet","title":"Model"},{"location":"5-semester/SOE/exam/04-plandriven-vs-agile/","text":"Comparison of plan-driven and agile SOE processes \u00b6 Plandrevet vs Agilt \u00b6 Plandrevet Alle \u00f8nskede egenskaber ved slutproduktet kan v\u00e6re kendt og pr\u00e6cist specificeres f\u00f8r start Projekter er predictable og skal derfor planl\u00e6gges i detaljer f\u00f8r start Afvigelse fra planen er tegn p\u00e5 sloppy work i tidlige stadier Agilt \u00d8nskede egenskaber kendes ikke f\u00f8r mindst en del af l\u00f8sningen er bygget Projekter er af princip uforudsigelige B\u00f6hm/Turner Primary Factors \u00b6 Application Agilt rapid value, reagere p\u00e5 \u00e6ndringer, mindre team og projekt turbulent, high chance, projekfokuseret Plandrevet forudsigligt, h\u00f8j forsikring, stabilitet, st\u00f8rre team og projekt stabil milj\u00f8, projek og organization fokus Management Agilt dedikerede on-site kunder, fokus p\u00e5 priorietet inkrements planer og viden er mere implicit og intern Plandrevet kun kundeinteraction n\u00e5r brug for det, kontrakt planer og viden er dokumenteret Technical Agilt Korte inkrements, simple design, billig refactoring, informelle stories og test cases prioriteret, uforudsiglige \u00e6ndringer Plandrevet struktureret og formel projekt, udviklingen er forudsiglig, dokumenterede test og procedurer, l\u00e6ngere increments, dyr refactoring Personell Agilt konstant tilg\u00e6ngelig kunde, stort behov for level 2 udviklere, ogs\u00e5 level 3, komfort og empowerment fra frihed Plandrevet ikke-konstant tilg\u00e6ngelig kunde, stort behov for level 3 udviklere, og level 1B, komfort og empowerment fra orden Cynefin \u00b6 Framework til beslutningstagning . Kan v\u00e6re sv\u00e6rt at vide hvor man ligger. Simple : oplagt at bruge plan-driven Complex, Complicated : m\u00e5ske bedre at bruge agilt . \u00c6ndringer i Requirements \u00b6 Requirements kan \u00e6ndre sig. Eksempelvis hvis teknologien bliver outdated eller hvis markedet \u00e6ndrer sig. Eksempelvis hvis der udkommer et system der kan det samme Eller hvis product owner l\u00e6rer noget nyt i l\u00f8bet af udviklingsfasen","title":"4. Comparison of plan-driven and agile SOE processes"},{"location":"5-semester/SOE/exam/04-plandriven-vs-agile/#comparison-of-plan-driven-and-agile-soe-processes","text":"","title":"Comparison of plan-driven and agile SOE processes"},{"location":"5-semester/SOE/exam/04-plandriven-vs-agile/#plandrevet-vs-agilt","text":"Plandrevet Alle \u00f8nskede egenskaber ved slutproduktet kan v\u00e6re kendt og pr\u00e6cist specificeres f\u00f8r start Projekter er predictable og skal derfor planl\u00e6gges i detaljer f\u00f8r start Afvigelse fra planen er tegn p\u00e5 sloppy work i tidlige stadier Agilt \u00d8nskede egenskaber kendes ikke f\u00f8r mindst en del af l\u00f8sningen er bygget Projekter er af princip uforudsigelige","title":"Plandrevet vs Agilt"},{"location":"5-semester/SOE/exam/04-plandriven-vs-agile/#bohmturner-primary-factors","text":"Application Agilt rapid value, reagere p\u00e5 \u00e6ndringer, mindre team og projekt turbulent, high chance, projekfokuseret Plandrevet forudsigligt, h\u00f8j forsikring, stabilitet, st\u00f8rre team og projekt stabil milj\u00f8, projek og organization fokus Management Agilt dedikerede on-site kunder, fokus p\u00e5 priorietet inkrements planer og viden er mere implicit og intern Plandrevet kun kundeinteraction n\u00e5r brug for det, kontrakt planer og viden er dokumenteret Technical Agilt Korte inkrements, simple design, billig refactoring, informelle stories og test cases prioriteret, uforudsiglige \u00e6ndringer Plandrevet struktureret og formel projekt, udviklingen er forudsiglig, dokumenterede test og procedurer, l\u00e6ngere increments, dyr refactoring Personell Agilt konstant tilg\u00e6ngelig kunde, stort behov for level 2 udviklere, ogs\u00e5 level 3, komfort og empowerment fra frihed Plandrevet ikke-konstant tilg\u00e6ngelig kunde, stort behov for level 3 udviklere, og level 1B, komfort og empowerment fra orden","title":"B\u00f6hm/Turner Primary Factors"},{"location":"5-semester/SOE/exam/04-plandriven-vs-agile/#cynefin","text":"Framework til beslutningstagning . Kan v\u00e6re sv\u00e6rt at vide hvor man ligger. Simple : oplagt at bruge plan-driven Complex, Complicated : m\u00e5ske bedre at bruge agilt .","title":"Cynefin"},{"location":"5-semester/SOE/exam/04-plandriven-vs-agile/#ndringer-i-requirements","text":"Requirements kan \u00e6ndre sig. Eksempelvis hvis teknologien bliver outdated eller hvis markedet \u00e6ndrer sig. Eksempelvis hvis der udkommer et system der kan det samme Eller hvis product owner l\u00e6rer noget nyt i l\u00f8bet af udviklingsfasen","title":"\u00c6ndringer i Requirements"},{"location":"5-semester/SOE/exam/05-scrum/","text":"Key features of Scrum \u00b6 Scrum er en iterativ agil metode 3-5-3 struktur 3 roller : Product Owner, Scrum Master, Team 5 events : sprint, sprint plan, daily scrum, sprint review, sprint retrospective 3 artifacts Product backlog, sprint backlog, product increment Scrum Roller \u00b6 Product Owner 1 person. \u00c6ndringer i backlog sker gennem Product Owner. Ansvarlig for maximering af produktets v\u00e6rdi fra arbejde fra Dev team Scrum Master Ansvarlig for at bruge Scrum p\u00e5 den rigtige m\u00e5de (iflg. Scrum Guide) Hj\u00e6lper med at forst\u00e5 Scrum teori, practices, regler og v\u00e6rdier Hj\u00e6lper dem udefra med at forst\u00e5 hvad der er nyttigt interaktion med Scrum team. Development Team Best\u00e5r af profesionelle, som leverer potentiel releasable inkrement af \"done\" produkt i slutningen af hver sprint Organizerer deres eget arbejde Scrum Aktiviteter \u00b6 Sprint Max en m\u00e5ned hvori en brugbar og potentiel udgivelig produkt inkrement udvikles. Starter med det samme efter konklusionen p\u00e5 en tidligere sprint Hver sprint har et m\u00e5l med hvad der skal udvikles Ingen \u00e6ndringer der kan endanger m\u00e5let m\u00e5 laves Sprint Planning Maximalt 8 timer for en 1 month sprint. En plan udarbejdes fra hele Scrum Team Hvad kan leveres i den kommende sprint? Hvordan kan vi levere det? Daily Scrum Stand-up m\u00f8de (15 min) Samme tid, samme sted hver dag Arbejde til de n\u00e6ste 24 timer bestemmes Bruges til at m\u00e5le progress imod sprint goal Sprint Review Holdes i slutningen af en sprint (4 timer) What was done? What to do? Resulterer i et genovervejet product backlog Sprint Retrospective Efter sprint review, f\u00f8r n\u00e6ste sprint planning (3 timer) Scrum Master holder det positivt og produktivt Kigger p\u00e5 sidste sprint ifht. mennesker, forhold, process, og v\u00e6rkt\u00f8jer identificerer potentielle forbedringer Scrum Assets \u00b6 Sprint Burndown Chart Bruges af development team til at overv\u00e5ge progress under en sprint Product Burndown Chart Bruges af management til at overv\u00e5ge produktets progress Product mangement purposes Scrum Board G\u00f8r product backlog visuel Opdateres af development team Viser alle items, der skal g\u00f8res under sprint Scrum S\u00f8jler \u00b6 Transparency Vigtige aspekter af processen skal v\u00e6re synlig for dem der er ansvarlige for outcome f\u00e6lles standard Eks. f\u00e6lles forst\u00e5else af \"done\" Inspection Scrum bruger skal ofte inspecte Scrum artifacts og progress for at opdate u\u00f8nsket varians Skal ikke komme i vejen for arbejde (ikke for tit) Adaption Hvis en inspector finder et aspekt uacceptabelt, skal processen \u00e6ndres Hurtigst muligt Core Values \u00b6 Commitment People personally commit to achieving the goals of the Scrum Team Courage The Scrum Team members have courage to do the right thing and work on tough problems Focus Everyone focuses on the work of the Sprint and the goals of the Scrum Team Openness The Scrum Team and its stakeholders agree to be open about all the work and the challenges with performing the work Respect Scrum Team members respect each other to be capable, independent people Scrum Fejl \u00b6 Scrum Master fungerer som manager Agile teams er self-organizing , Scrum Master skal ikke styre hvordan development team arbejder Scrum Master skal fungere som coach , ikke leder Kunder er ikke involveret i hver iteration Product Owner skal v\u00e6re involveret i hver iteration, ellers kan der opst\u00e5 forskellige visioner Nye requirements eller opgaver tilf\u00f8jes under en iteration Kan komme i vejen for at den nuv\u00e6rende inkrement bliver s\u00e5 v\u00e6rdifuld som muligt.","title":"5. Key features of Scrum"},{"location":"5-semester/SOE/exam/05-scrum/#key-features-of-scrum","text":"Scrum er en iterativ agil metode 3-5-3 struktur 3 roller : Product Owner, Scrum Master, Team 5 events : sprint, sprint plan, daily scrum, sprint review, sprint retrospective 3 artifacts Product backlog, sprint backlog, product increment","title":"Key features of Scrum"},{"location":"5-semester/SOE/exam/05-scrum/#scrum-roller","text":"Product Owner 1 person. \u00c6ndringer i backlog sker gennem Product Owner. Ansvarlig for maximering af produktets v\u00e6rdi fra arbejde fra Dev team Scrum Master Ansvarlig for at bruge Scrum p\u00e5 den rigtige m\u00e5de (iflg. Scrum Guide) Hj\u00e6lper med at forst\u00e5 Scrum teori, practices, regler og v\u00e6rdier Hj\u00e6lper dem udefra med at forst\u00e5 hvad der er nyttigt interaktion med Scrum team. Development Team Best\u00e5r af profesionelle, som leverer potentiel releasable inkrement af \"done\" produkt i slutningen af hver sprint Organizerer deres eget arbejde","title":"Scrum Roller"},{"location":"5-semester/SOE/exam/05-scrum/#scrum-aktiviteter","text":"Sprint Max en m\u00e5ned hvori en brugbar og potentiel udgivelig produkt inkrement udvikles. Starter med det samme efter konklusionen p\u00e5 en tidligere sprint Hver sprint har et m\u00e5l med hvad der skal udvikles Ingen \u00e6ndringer der kan endanger m\u00e5let m\u00e5 laves Sprint Planning Maximalt 8 timer for en 1 month sprint. En plan udarbejdes fra hele Scrum Team Hvad kan leveres i den kommende sprint? Hvordan kan vi levere det? Daily Scrum Stand-up m\u00f8de (15 min) Samme tid, samme sted hver dag Arbejde til de n\u00e6ste 24 timer bestemmes Bruges til at m\u00e5le progress imod sprint goal Sprint Review Holdes i slutningen af en sprint (4 timer) What was done? What to do? Resulterer i et genovervejet product backlog Sprint Retrospective Efter sprint review, f\u00f8r n\u00e6ste sprint planning (3 timer) Scrum Master holder det positivt og produktivt Kigger p\u00e5 sidste sprint ifht. mennesker, forhold, process, og v\u00e6rkt\u00f8jer identificerer potentielle forbedringer","title":"Scrum Aktiviteter"},{"location":"5-semester/SOE/exam/05-scrum/#scrum-assets","text":"Sprint Burndown Chart Bruges af development team til at overv\u00e5ge progress under en sprint Product Burndown Chart Bruges af management til at overv\u00e5ge produktets progress Product mangement purposes Scrum Board G\u00f8r product backlog visuel Opdateres af development team Viser alle items, der skal g\u00f8res under sprint","title":"Scrum Assets"},{"location":"5-semester/SOE/exam/05-scrum/#scrum-sjler","text":"Transparency Vigtige aspekter af processen skal v\u00e6re synlig for dem der er ansvarlige for outcome f\u00e6lles standard Eks. f\u00e6lles forst\u00e5else af \"done\" Inspection Scrum bruger skal ofte inspecte Scrum artifacts og progress for at opdate u\u00f8nsket varians Skal ikke komme i vejen for arbejde (ikke for tit) Adaption Hvis en inspector finder et aspekt uacceptabelt, skal processen \u00e6ndres Hurtigst muligt","title":"Scrum S\u00f8jler"},{"location":"5-semester/SOE/exam/05-scrum/#core-values","text":"Commitment People personally commit to achieving the goals of the Scrum Team Courage The Scrum Team members have courage to do the right thing and work on tough problems Focus Everyone focuses on the work of the Sprint and the goals of the Scrum Team Openness The Scrum Team and its stakeholders agree to be open about all the work and the challenges with performing the work Respect Scrum Team members respect each other to be capable, independent people","title":"Core Values"},{"location":"5-semester/SOE/exam/05-scrum/#scrum-fejl","text":"Scrum Master fungerer som manager Agile teams er self-organizing , Scrum Master skal ikke styre hvordan development team arbejder Scrum Master skal fungere som coach , ikke leder Kunder er ikke involveret i hver iteration Product Owner skal v\u00e6re involveret i hver iteration, ellers kan der opst\u00e5 forskellige visioner Nye requirements eller opgaver tilf\u00f8jes under en iteration Kan komme i vejen for at den nuv\u00e6rende inkrement bliver s\u00e5 v\u00e6rdifuld som muligt.","title":"Scrum Fejl"},{"location":"5-semester/SOE/exam/06-rup/","text":"Key features of RUP \u00b6 RUP er inkrementiel iterativt metode framework . Rational Unified Process Udvikles i korte timeboxed iterationer. High risk , high-value udvikles i de tidlige iterationer re-use af eksisterende komponenter Accomedate \u00e6ndringer tidligt i projektet Work together as one team UP definerer omkring 50 optional artifacts eller work products (ex Vision, Risk List) Less is better : Brug en minimal m\u00e6ngde af work products RUP er baseret p\u00e5 et s\u00e6t \"byggeklodser\" Roles (Who) Defineret s\u00e6t af relaterede skills og ansvarligheder Work Product (What) Resultat fra en tasks, inkl. dokumenter og modeller Tasks (How) En unit arbejde assigned til en rolle Tasks \u00b6 Tasks deles op i discipliner Faser \u00b6 4 faser Inception Scope og prioriteter definieres Key risks identificeres. Korteste fase (f\u00e5 date til uger) Elaboration Vision , requirements og arkitektur stabiliseres. Major risks mitigeres og arkitektuelt signifikante elementer programmeres Risky stuff bygges og testes Construction Resten bygges og testes Systemet klarg\u00f8res og er ready-to-deply i slutningen St\u00f8rste fase Transition Systemet deployes Hvad Skr\u00e6dersyes til hvert projekt \u00b6 Practices og work products udv\u00e6lges fra en stor bunke af valgbare elementer. Less is better UP Best Practices \u00b6 Udvikl ved brug af timeboxed iterations Anbefalet mellem 2 og 6 uger Start udvikling tidligt Understreg programmering af high-risk, high-value elementer og sammenh\u00e6ngende arkitektur i tidlige iterationer Re-use eksisterende komponenter Kontinuerligt verificer kvalitet . Test tidligt, ofte og realistisk TDD og Continous Integration Lav en lille smule visuel modelling f\u00f8r start programmering af iteration. Manage requirements Find og refiner requirements iterativt og inkrementielt Manage Change gennem disciplinerede configuration management og versions-kontrol Typiske Pitfalls ved RUP \u00b6 RUP er ikke waterfall. De forskellige faser er et mix af disciplinerne RUP iterationer b\u00f8r ikke v\u00e6re l\u00e6ngere end 6 uger. Hvis m\u00e5l ikke kan opn\u00e5s , flyttes eller simplificeres m\u00e5l. Scrum og XP med RUP \u00b6 Scrum \u00b6 Scrum practices er consistent med UP practices. Projekt manager fungere som Scrum Master Sprint Backlog svarer til UP artifact; Iteration Plan UP er mere defineret end Scrum i dets processer XP \u00b6 Fleste XP practices er consistent med UP practices TDD er specialisering af UP quality; continously verifying quality Alle UP workprodukts er optional Passer med XPs \" minimal modeling and documentation\"","title":"6. Key features of RUP"},{"location":"5-semester/SOE/exam/06-rup/#key-features-of-rup","text":"RUP er inkrementiel iterativt metode framework . Rational Unified Process Udvikles i korte timeboxed iterationer. High risk , high-value udvikles i de tidlige iterationer re-use af eksisterende komponenter Accomedate \u00e6ndringer tidligt i projektet Work together as one team UP definerer omkring 50 optional artifacts eller work products (ex Vision, Risk List) Less is better : Brug en minimal m\u00e6ngde af work products RUP er baseret p\u00e5 et s\u00e6t \"byggeklodser\" Roles (Who) Defineret s\u00e6t af relaterede skills og ansvarligheder Work Product (What) Resultat fra en tasks, inkl. dokumenter og modeller Tasks (How) En unit arbejde assigned til en rolle","title":"Key features of RUP"},{"location":"5-semester/SOE/exam/06-rup/#tasks","text":"Tasks deles op i discipliner","title":"Tasks"},{"location":"5-semester/SOE/exam/06-rup/#faser","text":"4 faser Inception Scope og prioriteter definieres Key risks identificeres. Korteste fase (f\u00e5 date til uger) Elaboration Vision , requirements og arkitektur stabiliseres. Major risks mitigeres og arkitektuelt signifikante elementer programmeres Risky stuff bygges og testes Construction Resten bygges og testes Systemet klarg\u00f8res og er ready-to-deply i slutningen St\u00f8rste fase Transition Systemet deployes","title":"Faser"},{"location":"5-semester/SOE/exam/06-rup/#hvad-skrdersyes-til-hvert-projekt","text":"Practices og work products udv\u00e6lges fra en stor bunke af valgbare elementer. Less is better","title":"Hvad Skr\u00e6dersyes til hvert projekt"},{"location":"5-semester/SOE/exam/06-rup/#up-best-practices","text":"Udvikl ved brug af timeboxed iterations Anbefalet mellem 2 og 6 uger Start udvikling tidligt Understreg programmering af high-risk, high-value elementer og sammenh\u00e6ngende arkitektur i tidlige iterationer Re-use eksisterende komponenter Kontinuerligt verificer kvalitet . Test tidligt, ofte og realistisk TDD og Continous Integration Lav en lille smule visuel modelling f\u00f8r start programmering af iteration. Manage requirements Find og refiner requirements iterativt og inkrementielt Manage Change gennem disciplinerede configuration management og versions-kontrol","title":"UP Best Practices"},{"location":"5-semester/SOE/exam/06-rup/#typiske-pitfalls-ved-rup","text":"RUP er ikke waterfall. De forskellige faser er et mix af disciplinerne RUP iterationer b\u00f8r ikke v\u00e6re l\u00e6ngere end 6 uger. Hvis m\u00e5l ikke kan opn\u00e5s , flyttes eller simplificeres m\u00e5l.","title":"Typiske Pitfalls ved RUP"},{"location":"5-semester/SOE/exam/06-rup/#scrum-og-xp-med-rup","text":"","title":"Scrum og XP med RUP"},{"location":"5-semester/SOE/exam/06-rup/#scrum","text":"Scrum practices er consistent med UP practices. Projekt manager fungere som Scrum Master Sprint Backlog svarer til UP artifact; Iteration Plan UP er mere defineret end Scrum i dets processer","title":"Scrum"},{"location":"5-semester/SOE/exam/06-rup/#xp","text":"Fleste XP practices er consistent med UP practices TDD er specialisering af UP quality; continously verifying quality Alle UP workprodukts er optional Passer med XPs \" minimal modeling and documentation\"","title":"XP"},{"location":"5-semester/SOE/exam/07-requirements/","text":"Requirements Elicitation and Managing change to requirements \u00b6 Requirement Elicitation \u00b6 Er sv\u00e6rt da: Stakeholders ved ikke hvad de vil have Stakeholders bruger deres eget sprog og implicit viden Forskellige stakeholders med diverse eller modstridende requirements Politik \u00d8konomisk og business milj\u00f8 er dynamisk Teknikker \u00b6 Interview Lukkede interviews Predefinerede sp\u00f8rgsm\u00e5l \u00c5bne intervies Ingen predefieret agenda Observation / Ethnography Se folk g\u00f8re deres arbejde Se hvordan arbejdet virkeligt udf\u00f8res Requirements gennem samarbejde og awareness Forst\u00e5 eksisterende systemer Stories og Scenarier Nemmere at relatere real-life eksempler end abstrakte beskrivelser Requirements \u00c6ndringer \u00b6 I Waterfall bliver \u00e6ndringer approved af en project manager eller en styringsgruppe Dog ofte ikke gjort i waterfall I agilt kan kun Product owner approve changes ved at opdatere product backlog Reducer Cost ved \u00c6ndringer \u00b6 Change Anticipation Eksempelvis ved brug af prototyper som kan vise key features til kunden Change Tolerance Eksempelvis via inkrementiel udvilking Prototyper \u00b6 Kan bruges til at validate fra brugere at del af l\u00f8sningen er \"fit for use\" Bruges i tidlig fase til at pr\u00f8ve design muligheder eller l\u00e6re mere om problemet Kan give brugere nye ideer til requirements Configuration Management \u00b6 Faktorer ved \u00e6ndringer i systemet Konsekvens ved ikke at lave \u00e6ndring Hvis \u00e6ndring er associeret med system fejl, skal seri\u00f8siteten af fejlen vurderes. Fordele ved \u00e6ndringen Vil det v\u00e6re til fordel fore mange brugere eller kun enkelte? Antal brugerer p\u00e5virket af \u00e6ndringen Hvis kun f\u00e5 brugere bliver affected er det m\u00e5ske lav prioritet Cost ved at lave \u00e6ndringen Hvis \u00e6ndringen p\u00e5virker mange system komponenter, og derfor st\u00f8rre chance for nye bugs Eller \u00e6ndringen tager lang lang tid Product release cycle Er der lige udgivet nyt release er det m\u00e5ske bedre at vente til n\u00e6\u00e6ste release Version Management \u00b6 Eksempelvis vha. Git Version and release identification Managed versioner assignes identifiers n\u00e5r de submittes til systemet Storage Management VM holder delta'er (list of diferences) for at reduce storage Change history Alle \u00e6ndringer i systemet er recorded og listed Independant developement VM lader udviklere arbejde p\u00e5 samme komponent p\u00e5 samme tid.","title":"7. Requirements Elicitation and Managing change to requirements"},{"location":"5-semester/SOE/exam/07-requirements/#requirements-elicitation-and-managing-change-to-requirements","text":"","title":"Requirements Elicitation and Managing change to requirements"},{"location":"5-semester/SOE/exam/07-requirements/#requirement-elicitation","text":"Er sv\u00e6rt da: Stakeholders ved ikke hvad de vil have Stakeholders bruger deres eget sprog og implicit viden Forskellige stakeholders med diverse eller modstridende requirements Politik \u00d8konomisk og business milj\u00f8 er dynamisk","title":"Requirement Elicitation"},{"location":"5-semester/SOE/exam/07-requirements/#teknikker","text":"Interview Lukkede interviews Predefinerede sp\u00f8rgsm\u00e5l \u00c5bne intervies Ingen predefieret agenda Observation / Ethnography Se folk g\u00f8re deres arbejde Se hvordan arbejdet virkeligt udf\u00f8res Requirements gennem samarbejde og awareness Forst\u00e5 eksisterende systemer Stories og Scenarier Nemmere at relatere real-life eksempler end abstrakte beskrivelser","title":"Teknikker"},{"location":"5-semester/SOE/exam/07-requirements/#requirements-ndringer","text":"I Waterfall bliver \u00e6ndringer approved af en project manager eller en styringsgruppe Dog ofte ikke gjort i waterfall I agilt kan kun Product owner approve changes ved at opdatere product backlog","title":"Requirements \u00c6ndringer"},{"location":"5-semester/SOE/exam/07-requirements/#reducer-cost-ved-ndringer","text":"Change Anticipation Eksempelvis ved brug af prototyper som kan vise key features til kunden Change Tolerance Eksempelvis via inkrementiel udvilking","title":"Reducer Cost ved \u00c6ndringer"},{"location":"5-semester/SOE/exam/07-requirements/#prototyper","text":"Kan bruges til at validate fra brugere at del af l\u00f8sningen er \"fit for use\" Bruges i tidlig fase til at pr\u00f8ve design muligheder eller l\u00e6re mere om problemet Kan give brugere nye ideer til requirements","title":"Prototyper"},{"location":"5-semester/SOE/exam/07-requirements/#configuration-management","text":"Faktorer ved \u00e6ndringer i systemet Konsekvens ved ikke at lave \u00e6ndring Hvis \u00e6ndring er associeret med system fejl, skal seri\u00f8siteten af fejlen vurderes. Fordele ved \u00e6ndringen Vil det v\u00e6re til fordel fore mange brugere eller kun enkelte? Antal brugerer p\u00e5virket af \u00e6ndringen Hvis kun f\u00e5 brugere bliver affected er det m\u00e5ske lav prioritet Cost ved at lave \u00e6ndringen Hvis \u00e6ndringen p\u00e5virker mange system komponenter, og derfor st\u00f8rre chance for nye bugs Eller \u00e6ndringen tager lang lang tid Product release cycle Er der lige udgivet nyt release er det m\u00e5ske bedre at vente til n\u00e6\u00e6ste release","title":"Configuration Management"},{"location":"5-semester/SOE/exam/07-requirements/#version-management","text":"Eksempelvis vha. Git Version and release identification Managed versioner assignes identifiers n\u00e5r de submittes til systemet Storage Management VM holder delta'er (list of diferences) for at reduce storage Change history Alle \u00e6ndringer i systemet er recorded og listed Independant developement VM lader udviklere arbejde p\u00e5 samme komponent p\u00e5 samme tid.","title":"Version Management"},{"location":"5-semester/SOE/exam/08-quality-control/","text":"Quality Control: Verification and Validation \u00b6 Kvalitet kan deles ind i 3 kategorier Produkt kvalitet Process kvalitet Forventingskvalitet D\u00e5rlig kvalitet kan koste Tab Spildt arbejde Vedligeholdelse er ofte dyrere end udvikling Quality Management \u00b6 Quality Assurance Planl\u00e6gge eller designe processer for at undg\u00e5 d\u00e5rlig kvalitet Quality Control Det at overv\u00e5ge at work products overholder kvalitetsstandarder Lav quality management er billigt i starten , men bliver dyrerer og dyrerer H\u00f8j quality management har initial cost, men bliver billigere senere fordi der sker f\u00e6rre fejl, og koden er mere stabil Det handler om at finde en balance Validation and Verification \u00b6 Validation Bygger vi de rigtige systemer? Overholder kundens forventninger og erfaringer Verifikation Bygger vi systemet ordenligt ? Overholder specifikationer Objektiv process Tjekker om softwaren er af h\u00f8j kvalitet , men ikke om det er brugbart Teknikker \u00b6 Testing af programmer og prototyper Reviewing af specifikationer, dokumentation og programmer Inspection \u00b6 Verifikation , tjekker om specifikationerne overholdes, men ikke med kunden. Tjekker ikke ikke-funktionelle karaktaristika s\u00e5som reliability og maintainability . Menneske-baseret tjek af dokumenter og filer s\u00e5som kode . Ikke eksekvering af kode Testing \u00b6 Validation , da vi tester produktet eller prototyper. Eksekvering af kode. Peer Review \u00b6 Software review , work product unders\u00f8ges af skaber , samt en eller flere kollegaer. Evaluerer teknisk indhold og kvalitet. V-Modellen \u00b6","title":"8. Quality Control: Verification and Validation"},{"location":"5-semester/SOE/exam/08-quality-control/#quality-control-verification-and-validation","text":"Kvalitet kan deles ind i 3 kategorier Produkt kvalitet Process kvalitet Forventingskvalitet D\u00e5rlig kvalitet kan koste Tab Spildt arbejde Vedligeholdelse er ofte dyrere end udvikling","title":"Quality Control: Verification and Validation"},{"location":"5-semester/SOE/exam/08-quality-control/#quality-management","text":"Quality Assurance Planl\u00e6gge eller designe processer for at undg\u00e5 d\u00e5rlig kvalitet Quality Control Det at overv\u00e5ge at work products overholder kvalitetsstandarder Lav quality management er billigt i starten , men bliver dyrerer og dyrerer H\u00f8j quality management har initial cost, men bliver billigere senere fordi der sker f\u00e6rre fejl, og koden er mere stabil Det handler om at finde en balance","title":"Quality Management"},{"location":"5-semester/SOE/exam/08-quality-control/#validation-and-verification","text":"Validation Bygger vi de rigtige systemer? Overholder kundens forventninger og erfaringer Verifikation Bygger vi systemet ordenligt ? Overholder specifikationer Objektiv process Tjekker om softwaren er af h\u00f8j kvalitet , men ikke om det er brugbart","title":"Validation and Verification"},{"location":"5-semester/SOE/exam/08-quality-control/#teknikker","text":"Testing af programmer og prototyper Reviewing af specifikationer, dokumentation og programmer","title":"Teknikker"},{"location":"5-semester/SOE/exam/08-quality-control/#inspection","text":"Verifikation , tjekker om specifikationerne overholdes, men ikke med kunden. Tjekker ikke ikke-funktionelle karaktaristika s\u00e5som reliability og maintainability . Menneske-baseret tjek af dokumenter og filer s\u00e5som kode . Ikke eksekvering af kode","title":"Inspection"},{"location":"5-semester/SOE/exam/08-quality-control/#testing","text":"Validation , da vi tester produktet eller prototyper. Eksekvering af kode.","title":"Testing"},{"location":"5-semester/SOE/exam/08-quality-control/#peer-review","text":"Software review , work product unders\u00f8ges af skaber , samt en eller flere kollegaer. Evaluerer teknisk indhold og kvalitet.","title":"Peer Review"},{"location":"5-semester/SOE/exam/08-quality-control/#v-modellen","text":"","title":"V-Modellen"},{"location":"5-semester/SOE/exam/09-risk-management/","text":"Risk Management \u00b6 Risk - Risiko En mulig konsekvens af en beslutning Noget der er sikker p\u00e5 at ske er ikke en risiko Risk Strategies \u00b6 Reaktive Risk Strategi Overv\u00e5ger i bedste fald projektet for mulige risici. Resourcer s\u00e6ttes til side for at h\u00e5ndtere dem hvis de bliver problemer Oftes sker der ikke noget f\u00f8r noget g\u00e5r galt . fire fighting mode Proaktiv Risk Strategi Mere inteligent end reaktiv Her starter man f\u00f8r det tekniske arbejde g\u00e5r igang. Potentielle risici identificeres Deres sandsynlighed og impact vurdereres De rangeres efter vigtighed Risici i Software \u00b6 Risk involverere to faktorer Uncertainty Loss Deles op i kategorier Project Risks Truer projekt planen stigning i cost budget, tidsplan, personale, ressource, stakeholder, og requirement problemer Technical Risks Truer kvalitet og aktualitet af softwaren design, implementering, interface, verifikation, og vedligholdelses problemer Business Risks Market Bygge et produkt som ingen vil have Strategic Bygge et produkt der ikke passer in i business strategi Sales Bygge et produkt som ikke kan s\u00e6lges af s\u00e6lgerne Management Miste support fra senior management pga. fokusskifte eller menneske forandringer Budget Miste budget eller personale engagement Risk Analysis \u00b6 Risk exposure = probability * loss Risici prioritetes efter exposure, herefter etableres en cut-line. Alle risks under cut-lines reevalueres For hver risk over cut-line: RMMM: Risk Mitigation Monitoring Management Mitigation Hvordan kan vi undg\u00e5 at en risk sker? Managemen Hvad g\u00f8r vi hvis det sker alligevel? Monitoring Hvordan overv\u00e5ger vi udviklingen af risicien?","title":"9. Risk Management"},{"location":"5-semester/SOE/exam/09-risk-management/#risk-management","text":"Risk - Risiko En mulig konsekvens af en beslutning Noget der er sikker p\u00e5 at ske er ikke en risiko","title":"Risk Management"},{"location":"5-semester/SOE/exam/09-risk-management/#risk-strategies","text":"Reaktive Risk Strategi Overv\u00e5ger i bedste fald projektet for mulige risici. Resourcer s\u00e6ttes til side for at h\u00e5ndtere dem hvis de bliver problemer Oftes sker der ikke noget f\u00f8r noget g\u00e5r galt . fire fighting mode Proaktiv Risk Strategi Mere inteligent end reaktiv Her starter man f\u00f8r det tekniske arbejde g\u00e5r igang. Potentielle risici identificeres Deres sandsynlighed og impact vurdereres De rangeres efter vigtighed","title":"Risk Strategies"},{"location":"5-semester/SOE/exam/09-risk-management/#risici-i-software","text":"Risk involverere to faktorer Uncertainty Loss Deles op i kategorier Project Risks Truer projekt planen stigning i cost budget, tidsplan, personale, ressource, stakeholder, og requirement problemer Technical Risks Truer kvalitet og aktualitet af softwaren design, implementering, interface, verifikation, og vedligholdelses problemer Business Risks Market Bygge et produkt som ingen vil have Strategic Bygge et produkt der ikke passer in i business strategi Sales Bygge et produkt som ikke kan s\u00e6lges af s\u00e6lgerne Management Miste support fra senior management pga. fokusskifte eller menneske forandringer Budget Miste budget eller personale engagement","title":"Risici i Software"},{"location":"5-semester/SOE/exam/09-risk-management/#risk-analysis","text":"Risk exposure = probability * loss Risici prioritetes efter exposure, herefter etableres en cut-line. Alle risks under cut-lines reevalueres For hver risk over cut-line: RMMM: Risk Mitigation Monitoring Management Mitigation Hvordan kan vi undg\u00e5 at en risk sker? Managemen Hvad g\u00f8r vi hvis det sker alligevel? Monitoring Hvordan overv\u00e5ger vi udviklingen af risicien?","title":"Risk Analysis"},{"location":"5-semester/SOE/exam/10-project-planning-management/","text":"Project Planning and Management \u00b6 Plan-driven Development \u00b6 Udviklingsprocessen er planlagt i detaljer inden teknisk arbejde startes. Managers kan bruge planen til at underst\u00f8tte projekt decision making , og til at m\u00e5le progress Project Planning \u00b6 Opdele arbjedet i dele Tildele dem til project team members Forudse problemer der m\u00e5ske opst\u00e5r Forberede forel\u00f8bige l\u00f8sninger til disse problemer En plan indeholder normalt : Introduktion Organization Bekriver organizationen af devteam Risk analysis Hardware / Software resource requirements Work Breakdown Opdeling af arbejde into aktiviteter og identificering af milestones Project Schedule Dependency between activities , estimated time for hver milestone Monitoring and reporting mechanisms Fordele og Ulemper \u00b6 Fordele Kan tage h\u00e5nd om organizational issues . Potentielle problemer og dependancies opdages f\u00f8r start Ulemper Mange beslutninger skal laves om pga. \u00e6ndringer i milj\u00f8 hvori softwaren udvikles og skal bruges M\u00e5ling af Progress \u00b6 M\u00e5les vha. Milestones og relateret dokumentation Process \u00b6 Agile Planning \u00b6 XP: Story-based Estimation and Planning \u00b6 Planning game , baseret p\u00e5 user stories der reflekterer features der burde v\u00e6re inkluderet i systemet. Projekt team l\u00e6ser og diskuterer stories og rangerer dem ifht. hvor lang tid de synes at tage at implementere. Stories tildeles effort points / story points antallet af story points implemteret pr. dag/sprint m\u00e5les ud fra devteams \" velocity \" Scrum Planning \u00b6 Estimation \u00b6 Based on a triangular distribution E=(a+m+b)\\ /\\ 3 E=(a+m+b)\\ /\\ 3 Based on a double triangular distribution E=(a+4m+b)\\ /\\ 6\\\\ SD= (b-a)\\ /\\ 6 E=(a+4m+b)\\ /\\ 6\\\\ SD= (b-a)\\ /\\ 6 where a a is best-case estimate m m is most likely estimate b b is worst-case estimate COCOMO \u00b6 An empirical model based on project experience. Well-documented, \u2018independent\u2019 model which is not tied to a specific software vendor. Long history from initial version published in 1981 (COCOMO-81) through various instantiations to COCOMO 2. COCOMO 2 takes into account different approaches to software development, reuse, etc.","title":"10. Project Planning and Management"},{"location":"5-semester/SOE/exam/10-project-planning-management/#project-planning-and-management","text":"","title":"Project Planning and Management"},{"location":"5-semester/SOE/exam/10-project-planning-management/#plan-driven-development","text":"Udviklingsprocessen er planlagt i detaljer inden teknisk arbejde startes. Managers kan bruge planen til at underst\u00f8tte projekt decision making , og til at m\u00e5le progress","title":"Plan-driven Development"},{"location":"5-semester/SOE/exam/10-project-planning-management/#project-planning","text":"Opdele arbjedet i dele Tildele dem til project team members Forudse problemer der m\u00e5ske opst\u00e5r Forberede forel\u00f8bige l\u00f8sninger til disse problemer En plan indeholder normalt : Introduktion Organization Bekriver organizationen af devteam Risk analysis Hardware / Software resource requirements Work Breakdown Opdeling af arbejde into aktiviteter og identificering af milestones Project Schedule Dependency between activities , estimated time for hver milestone Monitoring and reporting mechanisms","title":"Project Planning"},{"location":"5-semester/SOE/exam/10-project-planning-management/#fordele-og-ulemper","text":"Fordele Kan tage h\u00e5nd om organizational issues . Potentielle problemer og dependancies opdages f\u00f8r start Ulemper Mange beslutninger skal laves om pga. \u00e6ndringer i milj\u00f8 hvori softwaren udvikles og skal bruges","title":"Fordele og Ulemper"},{"location":"5-semester/SOE/exam/10-project-planning-management/#maling-af-progress","text":"M\u00e5les vha. Milestones og relateret dokumentation","title":"M\u00e5ling af Progress"},{"location":"5-semester/SOE/exam/10-project-planning-management/#process","text":"","title":"Process"},{"location":"5-semester/SOE/exam/10-project-planning-management/#agile-planning","text":"","title":"Agile Planning"},{"location":"5-semester/SOE/exam/10-project-planning-management/#xp-story-based-estimation-and-planning","text":"Planning game , baseret p\u00e5 user stories der reflekterer features der burde v\u00e6re inkluderet i systemet. Projekt team l\u00e6ser og diskuterer stories og rangerer dem ifht. hvor lang tid de synes at tage at implementere. Stories tildeles effort points / story points antallet af story points implemteret pr. dag/sprint m\u00e5les ud fra devteams \" velocity \"","title":"XP: Story-based Estimation and Planning"},{"location":"5-semester/SOE/exam/10-project-planning-management/#scrum-planning","text":"","title":"Scrum Planning"},{"location":"5-semester/SOE/exam/10-project-planning-management/#estimation","text":"Based on a triangular distribution E=(a+m+b)\\ /\\ 3 E=(a+m+b)\\ /\\ 3 Based on a double triangular distribution E=(a+4m+b)\\ /\\ 6\\\\ SD= (b-a)\\ /\\ 6 E=(a+4m+b)\\ /\\ 6\\\\ SD= (b-a)\\ /\\ 6 where a a is best-case estimate m m is most likely estimate b b is worst-case estimate","title":"Estimation"},{"location":"5-semester/SOE/exam/10-project-planning-management/#cocomo","text":"An empirical model based on project experience. Well-documented, \u2018independent\u2019 model which is not tied to a specific software vendor. Long history from initial version published in 1981 (COCOMO-81) through various instantiations to COCOMO 2. COCOMO 2 takes into account different approaches to software development, reuse, etc.","title":"COCOMO"},{"location":"5-semester/SOE/exam/11-quality-management/","text":"Quality Management \u00b6 \"Quality Management: How is quality defined - agile versus plan driven approaches\" Kvalitet kan deles ind i 3 kategorier Produkt kvalitet Process kvalitet Forventingskvalitet D\u00e5rlig kvalitet kan koste Tab Spildt arbejde Vedligeholdelse er ofte dyrere end udvikling \u200b Software Qualities \u00b6 Trade-off \u00b6 Quality Management \u00b6 Quality Assurance Planl\u00e6gge eller designe processer for at undg\u00e5 d\u00e5rlig kvalitet Quality Control Det at overv\u00e5ge at work products overholder kvalitetsstandarder Lav quality management er billigt i starten, men bliver dyrerer og dyrerer H\u00f8j quality management har initial cost, men bliver billigere senere fordi der sker f\u00e6rre fejl, og koden er mere stabil Det handler om at finde en balance \u200b Validation and Verification \u00b6 Validation Bygger vi de rigtige systemer? Overholder kundens forventninger og erfaringer Verifikation Bygger vi systemet ordenligt ? Overholder specifikationer Objektiv process Tjekker om softwaren er af h\u00f8j kvalitet , men ikke om det er brugbart Teknikker \u00b6 Testing af programmer og prototyper Reviewing af specifikationer, dokumentation og programmer Inspection \u00b6 Verifikation , tjekker om specifikationerne overholdes, men ikke med kunden . Tjekker ikke ikke-funktionelle karaktaristika s\u00e5som reliability og maintainability . Menneske-baseret tjek af dokumenter og filer s\u00e5som kode . Ikke eksekvering af kode Testing \u00b6 Validation , da vi tester produktet eller prototyper . Eksekvering af kode . Peer Review \u00b6 Software review , work product unders\u00f8ges af skaber, samt en eller flere kollegaer. Evaluerer teknisk indhold og kvalitet. V-Modellen \u00b6 Test and Review in Models \u00b6 Waterfall \u00b6 Incremental and Iterative \u00b6 Integration and Configuration \u00b6 Quality in Agile \u00b6 Definition of Done Team agree on what criteria must be met before a task is complete Sprint Review Product Owner and other stakeholders validate the sprint delivery meets expectations Check before check-in Developers are responsible for organizing their own code reviews with other team members before the code is checked in to the build system Never break the build Team members should not check in code that causes the system to fail Developers have to test their code changes against the whole system and be confident that these work as expected Fix problems when you see them If a programmer discovers problems or obscurities in code developed by someone else, they can fix these directly rather than referring them back to the original developer Pair Programming in XP Flere programmeringstimer i stedet for procrastinating Stamina og insight i koden hvis en i parret er syg Real-time code review","title":"11. Quality Management"},{"location":"5-semester/SOE/exam/11-quality-management/#quality-management","text":"\"Quality Management: How is quality defined - agile versus plan driven approaches\" Kvalitet kan deles ind i 3 kategorier Produkt kvalitet Process kvalitet Forventingskvalitet D\u00e5rlig kvalitet kan koste Tab Spildt arbejde Vedligeholdelse er ofte dyrere end udvikling \u200b","title":"Quality Management"},{"location":"5-semester/SOE/exam/11-quality-management/#software-qualities","text":"","title":"Software Qualities"},{"location":"5-semester/SOE/exam/11-quality-management/#trade-off","text":"","title":"Trade-off"},{"location":"5-semester/SOE/exam/11-quality-management/#quality-management_1","text":"Quality Assurance Planl\u00e6gge eller designe processer for at undg\u00e5 d\u00e5rlig kvalitet Quality Control Det at overv\u00e5ge at work products overholder kvalitetsstandarder Lav quality management er billigt i starten, men bliver dyrerer og dyrerer H\u00f8j quality management har initial cost, men bliver billigere senere fordi der sker f\u00e6rre fejl, og koden er mere stabil Det handler om at finde en balance \u200b","title":"Quality Management"},{"location":"5-semester/SOE/exam/11-quality-management/#validation-and-verification","text":"Validation Bygger vi de rigtige systemer? Overholder kundens forventninger og erfaringer Verifikation Bygger vi systemet ordenligt ? Overholder specifikationer Objektiv process Tjekker om softwaren er af h\u00f8j kvalitet , men ikke om det er brugbart","title":"Validation and Verification"},{"location":"5-semester/SOE/exam/11-quality-management/#teknikker","text":"Testing af programmer og prototyper Reviewing af specifikationer, dokumentation og programmer","title":"Teknikker"},{"location":"5-semester/SOE/exam/11-quality-management/#inspection","text":"Verifikation , tjekker om specifikationerne overholdes, men ikke med kunden . Tjekker ikke ikke-funktionelle karaktaristika s\u00e5som reliability og maintainability . Menneske-baseret tjek af dokumenter og filer s\u00e5som kode . Ikke eksekvering af kode","title":"Inspection"},{"location":"5-semester/SOE/exam/11-quality-management/#testing","text":"Validation , da vi tester produktet eller prototyper . Eksekvering af kode .","title":"Testing"},{"location":"5-semester/SOE/exam/11-quality-management/#peer-review","text":"Software review , work product unders\u00f8ges af skaber, samt en eller flere kollegaer. Evaluerer teknisk indhold og kvalitet.","title":"Peer Review"},{"location":"5-semester/SOE/exam/11-quality-management/#v-modellen","text":"","title":"V-Modellen"},{"location":"5-semester/SOE/exam/11-quality-management/#test-and-review-in-models","text":"","title":"Test and Review in Models"},{"location":"5-semester/SOE/exam/11-quality-management/#waterfall","text":"","title":"Waterfall"},{"location":"5-semester/SOE/exam/11-quality-management/#incremental-and-iterative","text":"","title":"Incremental and Iterative"},{"location":"5-semester/SOE/exam/11-quality-management/#integration-and-configuration","text":"","title":"Integration and Configuration"},{"location":"5-semester/SOE/exam/11-quality-management/#quality-in-agile","text":"Definition of Done Team agree on what criteria must be met before a task is complete Sprint Review Product Owner and other stakeholders validate the sprint delivery meets expectations Check before check-in Developers are responsible for organizing their own code reviews with other team members before the code is checked in to the build system Never break the build Team members should not check in code that causes the system to fail Developers have to test their code changes against the whole system and be confident that these work as expected Fix problems when you see them If a programmer discovers problems or obscurities in code developed by someone else, they can fix these directly rather than referring them back to the original developer Pair Programming in XP Flere programmeringstimer i stedet for procrastinating Stamina og insight i koden hvis en i parret er syg Real-time code review","title":"Quality in Agile"},{"location":"5-semester/SOE/exam/12-configuration-management/","text":"Configuration Management \u00b6 Configuration management har at g\u00f8re med policies , processer , og v\u00e6rkt\u00f8jer til at h\u00e5ndtere skiftende software versioner . Der kan v\u00e6re flere versioner under udvikling p\u00e5 samme tid Essentielt for teams der arbejder p\u00e5 samme tid p\u00e5 koden. Configuration Management policies og processes definerer hvordan forsl\u00e5ede system \u00e6ndringer skal processeres og recordes. Key Activities \u00b6 Change Management Hold styr p\u00e5 foresl\u00e5ede \u00e6ndringer i software fra udviklere og kunder. Udregn cost og impact ved \u00e6ndringer Version Management Holde styr p\u00e5 de forskellige versioner af system komponenter Sikre sig at \u00e6ndringer ikke interfere med hinanden System Building Samle program komponenter , data og libraries . Compile og linke disse for at lave et eksekverbart system Release Management G\u00f8re software klar til ekstern release Holde styr p\u00e5 de versioner der er released til kunder. Challenges \u00b6 To personer vil \u00e6ndre i den samme fil. Pessimistisk file locking Filen kan kun \u00e5bnes af en person af gangen Ikke optimalt Optimisisk version merging (Git) VM holder styr p\u00e5 de \u00e6ndringer der er lavet i filen fra hver bruger. VM kan herefter merge \u00e6ndringer sammen. Centraliseret vs Distribueret VC \u00b6 Centraliseret Et single master repository holder styr p\u00e5 alle versioner der er under udvilking Subversion (SVN) Decentraliseret Flere versioner af repository eksistere p\u00e5 samme tid. Git Release \u00b6 Best\u00e5r af: Eksekverbar kode Konfigurationsfiler Data filer Installer Elektronisk eller papir dokumentation Continous Integration (Agile) \u00b6 Check out mainline fra VM into udviklers workspace Byg systemet og k\u00f8r automatiske tests Hvis ikke test passer skal sidste \"committer\" advares Lav \u00e6ndringer Byg systemet i lokalt workspace N\u00e5r tests er passed, check into build sistem Byg system p\u00e5 build server og k\u00f8r tests Hvis tests passer, commit til ny mainline Daily Build \u00b6 Development organization s\u00e6tter en delivery time. En ny version bygges ud fra leverede komponenter Version udleveres til test team som k\u00f8rer predefinerede tests Fejl fundet under tests dokumenteres og returneres til udviklere, som reparerer disse fejl Plandriven vs Agile CM \u00b6","title":"12. Configuration Management"},{"location":"5-semester/SOE/exam/12-configuration-management/#configuration-management","text":"Configuration management har at g\u00f8re med policies , processer , og v\u00e6rkt\u00f8jer til at h\u00e5ndtere skiftende software versioner . Der kan v\u00e6re flere versioner under udvikling p\u00e5 samme tid Essentielt for teams der arbejder p\u00e5 samme tid p\u00e5 koden. Configuration Management policies og processes definerer hvordan forsl\u00e5ede system \u00e6ndringer skal processeres og recordes.","title":"Configuration Management"},{"location":"5-semester/SOE/exam/12-configuration-management/#key-activities","text":"Change Management Hold styr p\u00e5 foresl\u00e5ede \u00e6ndringer i software fra udviklere og kunder. Udregn cost og impact ved \u00e6ndringer Version Management Holde styr p\u00e5 de forskellige versioner af system komponenter Sikre sig at \u00e6ndringer ikke interfere med hinanden System Building Samle program komponenter , data og libraries . Compile og linke disse for at lave et eksekverbart system Release Management G\u00f8re software klar til ekstern release Holde styr p\u00e5 de versioner der er released til kunder.","title":"Key Activities"},{"location":"5-semester/SOE/exam/12-configuration-management/#challenges","text":"To personer vil \u00e6ndre i den samme fil. Pessimistisk file locking Filen kan kun \u00e5bnes af en person af gangen Ikke optimalt Optimisisk version merging (Git) VM holder styr p\u00e5 de \u00e6ndringer der er lavet i filen fra hver bruger. VM kan herefter merge \u00e6ndringer sammen.","title":"Challenges"},{"location":"5-semester/SOE/exam/12-configuration-management/#centraliseret-vs-distribueret-vc","text":"Centraliseret Et single master repository holder styr p\u00e5 alle versioner der er under udvilking Subversion (SVN) Decentraliseret Flere versioner af repository eksistere p\u00e5 samme tid. Git","title":"Centraliseret vs Distribueret VC"},{"location":"5-semester/SOE/exam/12-configuration-management/#release","text":"Best\u00e5r af: Eksekverbar kode Konfigurationsfiler Data filer Installer Elektronisk eller papir dokumentation","title":"Release"},{"location":"5-semester/SOE/exam/12-configuration-management/#continous-integration-agile","text":"Check out mainline fra VM into udviklers workspace Byg systemet og k\u00f8r automatiske tests Hvis ikke test passer skal sidste \"committer\" advares Lav \u00e6ndringer Byg systemet i lokalt workspace N\u00e5r tests er passed, check into build sistem Byg system p\u00e5 build server og k\u00f8r tests Hvis tests passer, commit til ny mainline","title":"Continous Integration (Agile)"},{"location":"5-semester/SOE/exam/12-configuration-management/#daily-build","text":"Development organization s\u00e6tter en delivery time. En ny version bygges ud fra leverede komponenter Version udleveres til test team som k\u00f8rer predefinerede tests Fejl fundet under tests dokumenteres og returneres til udviklere, som reparerer disse fejl","title":"Daily Build"},{"location":"5-semester/SOE/exam/12-configuration-management/#plandriven-vs-agile-cm","text":"","title":"Plandriven vs Agile CM"},{"location":"6-semester/","text":"Indhold \u00b6 Kurser: VIT - Theory of Science Moodle side: https://www.moodle.aau.dk/course/view.php?id=33030","title":"Index"},{"location":"6-semester/#indhold","text":"Kurser: VIT - Theory of Science Moodle side: https://www.moodle.aau.dk/course/view.php?id=33030","title":"Indhold"},{"location":"6-semester/AALG/03-flow-networks/","text":"Flow Networks and Maximum Flow \u00b6 Intended Learning Outcome to understand the formalization of flow networks and flows; and the definition of the maximum-flow problem. to understand the Ford-Fulkerson method for finding maximum flows. to understand the Edmonds-Karp algorithm and to be able to analyze its worst-case running time; to be able to apply the Ford-Fulkerson method to solve the maximum-bipartite-matching problem. Flow Network \u00b6 A flow network G=(V,E) G=(V,E) is a directed graph Each edge (u,v)\\in E (u,v)\\in E has a nonnegative capacity c(u,v)\\geq0 c(u,v)\\geq0 If (u,v)\\notin E (u,v)\\notin E then c(u,v)=0 c(u,v)=0 If E contains an edge (u, v), then there is no edge (v, u) in the reverse direction. Two special vertices: a source s s and a sink t t . For any other vertex v, there is a path s\\to v\\to t s\\to v\\to t A flow in G is a real-valued function f:V\\times V \\to R f:V\\times V \\to R Flow in equals flow out Examples \u00b6 Maximum-flow Problem \u00b6 Consider the source s The value of flow f f , denoted as |f| |f| , is defined as |f|=\\sum_{v\\in V}f(s,v)-\\sum_{v_\\in V} f(v,s) |f|=\\sum_{v\\in V}f(s,v)-\\sum_{v_\\in V} f(v,s) Total flow out of the source minus the flow into the source. Typically, a flow network will not have any edges into the source, and the flow into the source will be zero. Maximum-flow problem: Given a flow network G with source s and sink t, we wish to find a flow of maximum value. Anti-parallel edges \u00b6 To simplify the discussion, we do not allow both (u, v) and (v, u) together in the graph. If E contains an edge (u, v), then there is no edge (v, u) in the reverse direction. Easy to eliminate such antiparallel edges by introducing artificial vertices. Multiple sources and multiple sinks \u00b6 Example: multiple factories and warehouses Introducing a super-source s and super-sink t Connect s to each of the original source s_i s_i and set is capacity to \\infty \\infty Connect t to each of the original sink t_i t_i and set its capacity to \\infty \\infty The Ford-Fulkerson Method \u00b6 A method but not an algorithm It encompasses several implementations with different running times The Ford-Fulkerson method is based on Residual Networks Augmenting paths Residual Networks \u00b6 Given a flow network G G and a flow f f , the residual network G_f G_f consists of edges whose residual capacities are greater than 0 Formally : G_f=(V,E_f) G_f=(V,E_f) , where E_f=\\{(u,v)\\in V \\times V:c_f(u,v)>0\\} E_f=\\{(u,v)\\in V \\times V:c_f(u,v)>0\\} Residual capacities: The amount of additional flow that can be allowed on edge (u, v). The amount of flow that can be allowed on edge (v, u), i.e., the amount of flow that can be canceled on the opposite direction of edge (u, v). Example \u00b6 Augmenting Paths \u00b6 Given a flow network G and a flow f, an augmenting path p p is a simple path from s to t in the residual network G_f G_f p=<s, v_2, v_3, t> p=<s, v_2, v_3, t> Residual capacity of an augmenting path p p : How much additional flow can we send through an augmenting path? c_f(p)=\\min(c_f(u,v):(u,v) c_f(p)=\\min(c_f(u,v):(u,v) is on path p) p) c_f(p)=\\min(5,4,5)=4 c_f(p)=\\min(5,4,5)=4 The edge with the minimum capacity in p p is called critical edge (bottleneck) (v_2,v_3) (v_2,v_3) is the critical edge of p p Augmenting a flow \u00b6 Given an agumenting path p p we define a flow f_p f_p on the residual network G_f G_f the flow value of |f_p|=c_f(p)>0 |f_p|=c_f(p)>0 Examples \u00b6 The Method \u00b6 Find an augmenting path in the residual network Augment the existing flow by the flow of the augmenting path Keep doing this until no augmenting path exists in the residual network The algorithms based on this method differ in how they choose p in line 3 Correctness is proved by the Max-flow min-cut theorem Example \u00b6 Correctness of Ford-Fulkerson \u00b6 Why is this method correct? How do we know that when the method terminates, i.e., when there are no more augmenting paths, we have actually found a maximum flow? Max-flow min-cut theorem Cuts \u00b6 Minimum Cut \u00b6 Max-flow min-cut theorem \u00b6 If f f is a flow in G G the follwoing conditions are equivalent: f is a maximum flow The residual network G_f G_f contains no augmenting paths |f|=c(S,T) |f|=c(S,T) for some cut (S,T) (S,T) of G G The correctness of Ford-Fulkerson method 2\\to1 2\\to1 We prove 2\\to 3 2\\to 3 and then 3\\to1 3\\to1 2 to 3 \u00b6 3 to 1 \u00b6 Worst-case Running time \u00b6 Assume integer flows: capacities are integer values Appropriate scaling transformation can transfer rational numbers to integral numbers Each augmentation increases the value of the flow by some positive amount worst case. each time the flow value increases by 1 <s,u,v,t> <s,u,v,t> , <s,v,u,t> <s,v,u,t> , <s,u,v,t> <s,u,v,t> , ... ... Identifying the augmenting path and augmentation can be done in O(E) O(E) Total worst-case running time O(E\\cdot|f^*|) O(E\\cdot|f^*|) , where f^* f^* is the max-flow found by the algorithm Lesson learned: how an augmenting path is chosen is very important The Edmonds-Karp Algorithm \u00b6 In line 3 of Ford-Fulkerson method, the Edmonds-Karp regards the residual network as an unweighted graph and finds the shortest path as an augmenting path Finding the shortest path in an un-weighted graph is done by calling breath first search (BFS) from source vertex s. Example on L03 slides 38-42","title":"Flow Networks and Maximum Flow"},{"location":"6-semester/AALG/03-flow-networks/#flow-networks-and-maximum-flow","text":"Intended Learning Outcome to understand the formalization of flow networks and flows; and the definition of the maximum-flow problem. to understand the Ford-Fulkerson method for finding maximum flows. to understand the Edmonds-Karp algorithm and to be able to analyze its worst-case running time; to be able to apply the Ford-Fulkerson method to solve the maximum-bipartite-matching problem.","title":"Flow Networks and Maximum Flow"},{"location":"6-semester/AALG/03-flow-networks/#flow-network","text":"A flow network G=(V,E) G=(V,E) is a directed graph Each edge (u,v)\\in E (u,v)\\in E has a nonnegative capacity c(u,v)\\geq0 c(u,v)\\geq0 If (u,v)\\notin E (u,v)\\notin E then c(u,v)=0 c(u,v)=0 If E contains an edge (u, v), then there is no edge (v, u) in the reverse direction. Two special vertices: a source s s and a sink t t . For any other vertex v, there is a path s\\to v\\to t s\\to v\\to t A flow in G is a real-valued function f:V\\times V \\to R f:V\\times V \\to R Flow in equals flow out","title":"Flow Network"},{"location":"6-semester/AALG/03-flow-networks/#examples","text":"","title":"Examples"},{"location":"6-semester/AALG/03-flow-networks/#maximum-flow-problem","text":"Consider the source s The value of flow f f , denoted as |f| |f| , is defined as |f|=\\sum_{v\\in V}f(s,v)-\\sum_{v_\\in V} f(v,s) |f|=\\sum_{v\\in V}f(s,v)-\\sum_{v_\\in V} f(v,s) Total flow out of the source minus the flow into the source. Typically, a flow network will not have any edges into the source, and the flow into the source will be zero. Maximum-flow problem: Given a flow network G with source s and sink t, we wish to find a flow of maximum value.","title":"Maximum-flow Problem"},{"location":"6-semester/AALG/03-flow-networks/#anti-parallel-edges","text":"To simplify the discussion, we do not allow both (u, v) and (v, u) together in the graph. If E contains an edge (u, v), then there is no edge (v, u) in the reverse direction. Easy to eliminate such antiparallel edges by introducing artificial vertices.","title":"Anti-parallel edges"},{"location":"6-semester/AALG/03-flow-networks/#multiple-sources-and-multiple-sinks","text":"Example: multiple factories and warehouses Introducing a super-source s and super-sink t Connect s to each of the original source s_i s_i and set is capacity to \\infty \\infty Connect t to each of the original sink t_i t_i and set its capacity to \\infty \\infty","title":"Multiple sources and multiple sinks"},{"location":"6-semester/AALG/03-flow-networks/#the-ford-fulkerson-method","text":"A method but not an algorithm It encompasses several implementations with different running times The Ford-Fulkerson method is based on Residual Networks Augmenting paths","title":"The Ford-Fulkerson Method"},{"location":"6-semester/AALG/03-flow-networks/#residual-networks","text":"Given a flow network G G and a flow f f , the residual network G_f G_f consists of edges whose residual capacities are greater than 0 Formally : G_f=(V,E_f) G_f=(V,E_f) , where E_f=\\{(u,v)\\in V \\times V:c_f(u,v)>0\\} E_f=\\{(u,v)\\in V \\times V:c_f(u,v)>0\\} Residual capacities: The amount of additional flow that can be allowed on edge (u, v). The amount of flow that can be allowed on edge (v, u), i.e., the amount of flow that can be canceled on the opposite direction of edge (u, v).","title":"Residual Networks"},{"location":"6-semester/AALG/03-flow-networks/#example","text":"","title":"Example"},{"location":"6-semester/AALG/03-flow-networks/#augmenting-paths","text":"Given a flow network G and a flow f, an augmenting path p p is a simple path from s to t in the residual network G_f G_f p=<s, v_2, v_3, t> p=<s, v_2, v_3, t> Residual capacity of an augmenting path p p : How much additional flow can we send through an augmenting path? c_f(p)=\\min(c_f(u,v):(u,v) c_f(p)=\\min(c_f(u,v):(u,v) is on path p) p) c_f(p)=\\min(5,4,5)=4 c_f(p)=\\min(5,4,5)=4 The edge with the minimum capacity in p p is called critical edge (bottleneck) (v_2,v_3) (v_2,v_3) is the critical edge of p p","title":"Augmenting Paths"},{"location":"6-semester/AALG/03-flow-networks/#augmenting-a-flow","text":"Given an agumenting path p p we define a flow f_p f_p on the residual network G_f G_f the flow value of |f_p|=c_f(p)>0 |f_p|=c_f(p)>0","title":"Augmenting a flow"},{"location":"6-semester/AALG/03-flow-networks/#examples_1","text":"","title":"Examples"},{"location":"6-semester/AALG/03-flow-networks/#the-method","text":"Find an augmenting path in the residual network Augment the existing flow by the flow of the augmenting path Keep doing this until no augmenting path exists in the residual network The algorithms based on this method differ in how they choose p in line 3 Correctness is proved by the Max-flow min-cut theorem","title":"The Method"},{"location":"6-semester/AALG/03-flow-networks/#example_1","text":"","title":"Example"},{"location":"6-semester/AALG/03-flow-networks/#correctness-of-ford-fulkerson","text":"Why is this method correct? How do we know that when the method terminates, i.e., when there are no more augmenting paths, we have actually found a maximum flow? Max-flow min-cut theorem","title":"Correctness of Ford-Fulkerson"},{"location":"6-semester/AALG/03-flow-networks/#cuts","text":"","title":"Cuts"},{"location":"6-semester/AALG/03-flow-networks/#minimum-cut","text":"","title":"Minimum Cut"},{"location":"6-semester/AALG/03-flow-networks/#max-flow-min-cut-theorem","text":"If f f is a flow in G G the follwoing conditions are equivalent: f is a maximum flow The residual network G_f G_f contains no augmenting paths |f|=c(S,T) |f|=c(S,T) for some cut (S,T) (S,T) of G G The correctness of Ford-Fulkerson method 2\\to1 2\\to1 We prove 2\\to 3 2\\to 3 and then 3\\to1 3\\to1","title":"Max-flow min-cut theorem"},{"location":"6-semester/AALG/03-flow-networks/#2-to-3","text":"","title":"2 to 3"},{"location":"6-semester/AALG/03-flow-networks/#3-to-1","text":"","title":"3 to 1"},{"location":"6-semester/AALG/03-flow-networks/#worst-case-running-time","text":"Assume integer flows: capacities are integer values Appropriate scaling transformation can transfer rational numbers to integral numbers Each augmentation increases the value of the flow by some positive amount worst case. each time the flow value increases by 1 <s,u,v,t> <s,u,v,t> , <s,v,u,t> <s,v,u,t> , <s,u,v,t> <s,u,v,t> , ... ... Identifying the augmenting path and augmentation can be done in O(E) O(E) Total worst-case running time O(E\\cdot|f^*|) O(E\\cdot|f^*|) , where f^* f^* is the max-flow found by the algorithm Lesson learned: how an augmenting path is chosen is very important","title":"Worst-case Running time"},{"location":"6-semester/AALG/03-flow-networks/#the-edmonds-karp-algorithm","text":"In line 3 of Ford-Fulkerson method, the Edmonds-Karp regards the residual network as an unweighted graph and finds the shortest path as an augmenting path Finding the shortest path in an un-weighted graph is done by calling breath first search (BFS) from source vertex s. Example on L03 slides 38-42","title":"The Edmonds-Karp Algorithm"},{"location":"6-semester/AALG/06-computational-geometry-sweeping/","text":"Computational Geometry Algorithms: Sweeping Techniques \u00b6 Basic Geometric Operations \u00b6 Line-segment properties \u00b6 The line-segment \\overline{p_1p_2} \\overline{p_1p_2} between p_1=(x_1,y_1) p_1=(x_1,y_1) and p_2=(x_2,y_2) p_2=(x_2,y_2) Contains any point p_3 p_3 that is on the line passing through p_1 p_1 and p_2 p_2 and is on or between p_1 p_1 and p_2 p_2 on the line The set of convex combinations p_3=\\alpha \\cdot p_1 + (1-\\alpha)p_2 p_3=\\alpha \\cdot p_1 + (1-\\alpha)p_2 where 0\\leq \\alpha \\leq 0\\leq \\alpha \\leq We call p_1 p_1 and p_2 p_2 the endpoints of the line-segment \\overline{p_1p_2} \\overline{p_1p_2} Directed line-segment \\overrightarrow{p_1p_2} \\overrightarrow{p_1p_2} from p_1 p_1 to p_2 p_2 Cross Product \u00b6 Cross product (p_3-p_1)\\times(p_2-p_1)=(x_3-x_1)(y_2-y_1)-(x_2-x_1)(y_3-y_1) (p_3-p_1)\\times(p_2-p_1)=(x_3-x_1)(y_2-y_1)-(x_2-x_1)(y_3-y_1) Or determinant of the following matrix Positive \\to \\to p_1p_3 p_1p_3 is clockwise from p_1p_2 p_1p_2 Negative \\to \\to p_1p_3 p_1p_3 is counterclockwise from p_1p_2 p_1p_2 Zero \\to \\to collinear Summary \u00b6 Line Intersection \u00b6 A segment \\overline {p_1p_2} \\overline {p_1p_2} straddles a line if point p_1 p_1 lies on one side of the line but p_2 p_2 lies on the other side Along the line, one needs to turn different directions to go to p_1 p_1 and p_2 p_2 Two line segments intersect if and only if either of the following two conditions holds: Each segment straddles the line containing the other An endpoint of one segment lies on the other segment","title":"Computational Geometry Algorithms: Sweeping Techniques"},{"location":"6-semester/AALG/06-computational-geometry-sweeping/#computational-geometry-algorithms-sweeping-techniques","text":"","title":"Computational Geometry Algorithms: Sweeping Techniques"},{"location":"6-semester/AALG/06-computational-geometry-sweeping/#basic-geometric-operations","text":"","title":"Basic Geometric Operations"},{"location":"6-semester/AALG/06-computational-geometry-sweeping/#line-segment-properties","text":"The line-segment \\overline{p_1p_2} \\overline{p_1p_2} between p_1=(x_1,y_1) p_1=(x_1,y_1) and p_2=(x_2,y_2) p_2=(x_2,y_2) Contains any point p_3 p_3 that is on the line passing through p_1 p_1 and p_2 p_2 and is on or between p_1 p_1 and p_2 p_2 on the line The set of convex combinations p_3=\\alpha \\cdot p_1 + (1-\\alpha)p_2 p_3=\\alpha \\cdot p_1 + (1-\\alpha)p_2 where 0\\leq \\alpha \\leq 0\\leq \\alpha \\leq We call p_1 p_1 and p_2 p_2 the endpoints of the line-segment \\overline{p_1p_2} \\overline{p_1p_2} Directed line-segment \\overrightarrow{p_1p_2} \\overrightarrow{p_1p_2} from p_1 p_1 to p_2 p_2","title":"Line-segment properties"},{"location":"6-semester/AALG/06-computational-geometry-sweeping/#cross-product","text":"Cross product (p_3-p_1)\\times(p_2-p_1)=(x_3-x_1)(y_2-y_1)-(x_2-x_1)(y_3-y_1) (p_3-p_1)\\times(p_2-p_1)=(x_3-x_1)(y_2-y_1)-(x_2-x_1)(y_3-y_1) Or determinant of the following matrix Positive \\to \\to p_1p_3 p_1p_3 is clockwise from p_1p_2 p_1p_2 Negative \\to \\to p_1p_3 p_1p_3 is counterclockwise from p_1p_2 p_1p_2 Zero \\to \\to collinear","title":"Cross Product"},{"location":"6-semester/AALG/06-computational-geometry-sweeping/#summary","text":"","title":"Summary"},{"location":"6-semester/AALG/06-computational-geometry-sweeping/#line-intersection","text":"A segment \\overline {p_1p_2} \\overline {p_1p_2} straddles a line if point p_1 p_1 lies on one side of the line but p_2 p_2 lies on the other side Along the line, one needs to turn different directions to go to p_1 p_1 and p_2 p_2 Two line segments intersect if and only if either of the following two conditions holds: Each segment straddles the line containing the other An endpoint of one segment lies on the other segment","title":"Line Intersection"},{"location":"6-semester/AALG/08-computational-geometry-range-searching/","text":"Computational Geometry Data Structures: Range Searching \u00b6 Litterature: https://link.springer.com/chapter/10.1007/978-3-540-77974-2_5 Queries in a database can be interpreted geometrically Important Factors in Analysis \u00b6 Run time for building the data structure Run time for processing range searches Additional space that the data structure takes 1-Dimensional Range Searching \u00b6 Let P:=\\{p_1,p_2,\\dots,p_n\\} P:=\\{p_1,p_2,\\dots,p_n\\} be the given set of points on the real line. We can solve the range searching problem with a balanced binary search tree \\mathcal{T} \\mathcal{T} Kd-Trees \u00b6 Let P P be a set of n n points in the plane. We assume that no two points in P P have the same x-coordinate, and no two points have the same y-coordinate. A 2-dimensional rectangular range query on P P asks for the points from P P lying inside a query rectangle [x:x']\\times[y:y'] [x:x']\\times[y:y'] A point p:=(p_x,p_y) p:=(p_x,p_y) lies inside this rectangle if and only if: p_x\\in[x:x']\\quad \\text{and}\\quad p_y\\in[y:y'] \\nonumber p_x\\in[x:x']\\quad \\text{and}\\quad p_y\\in[y:y'] \\nonumber We could say that a 2-dimensional rectangular range query is composed of two 1-dimensional sub-queries. At the root we split the set P P with a vertical line \\ell \\ell into two subsets of roughly equal size. The splitting line is stored at the root. P_\\text{left} P_\\text{left} is stored in the left subtree, and P_\\text{right} P_\\text{right} in the right subtree. In general, we split with a vertical line at nodes whose depth is even, and we split with a horizontal line at nodes whose depth is odd. Building a Kd-Tree \u00b6 Divide-and-conquer Sort the points in P P w.r.t. their x-coordinates into array X X Sort the points in P P w.r.t. their y-coordinates into array Y Y Base case: If P P contains only one point, return a leaf with the point Otherwise: Divide into 2 sub-problems and conquer them recursively If the depth is even (split w.r.t. x-axis or a vertical line) Take the median v v of X X and create a root v_{root} v_{root} Split X X into sorted X_L X_L and X_R X_R and split Y Y into sorted Y_L Y_L and Y_R Y_R For any p\\in X_L p\\in X_L or p\\in Y_L p\\in Y_L , p.x \\leq v.x p.x \\leq v.x and for any p\\in X_R p\\in X_R or p\\in Y_R p\\in Y_R , p.x > v.x p.x > v.x Build recursively the left child of v_{root} v_{root} from X_L X_L and Y_L Y_L If the depth is odd (split w.r.t. y-axis or a horizontal line) Take the median v v of Y Y and create root v_{root} v_{root} Split X X into sorted X_L X_L and X_R X_R and split Y Y into sorted Y_L Y_L and Y_R Y_R For any p\\in X_L p\\in X_L or p\\in Y_L p\\in Y_L , p.y \\leq v.y p.y \\leq v.y and for any p\\in X_R p\\in X_R or p\\in Y_R p\\in Y_R , p.y > v.y p.y > v.y Build recursively the right child of v_{root} v_{root} from X_R X_R and Y_R Y_R Running time : \\Theta(n \\log n) \\Theta(n \\log n) Querying a Kd-Tree \u00b6 A query It uses a subroutine REPORTSUBTREE(\u03bd), which traverses the subtree rooted at a node \u03bd and reports all the points stored at its leaves. Running time : O(\\sqrt n + k) O(\\sqrt n + k) Range Trees \u00b6","title":"Computational Geometry Data Structures: Range Searching"},{"location":"6-semester/AALG/08-computational-geometry-range-searching/#computational-geometry-data-structures-range-searching","text":"Litterature: https://link.springer.com/chapter/10.1007/978-3-540-77974-2_5 Queries in a database can be interpreted geometrically","title":"Computational Geometry Data Structures: Range Searching"},{"location":"6-semester/AALG/08-computational-geometry-range-searching/#important-factors-in-analysis","text":"Run time for building the data structure Run time for processing range searches Additional space that the data structure takes","title":"Important Factors in Analysis"},{"location":"6-semester/AALG/08-computational-geometry-range-searching/#1-dimensional-range-searching","text":"Let P:=\\{p_1,p_2,\\dots,p_n\\} P:=\\{p_1,p_2,\\dots,p_n\\} be the given set of points on the real line. We can solve the range searching problem with a balanced binary search tree \\mathcal{T} \\mathcal{T}","title":"1-Dimensional Range Searching"},{"location":"6-semester/AALG/08-computational-geometry-range-searching/#kd-trees","text":"Let P P be a set of n n points in the plane. We assume that no two points in P P have the same x-coordinate, and no two points have the same y-coordinate. A 2-dimensional rectangular range query on P P asks for the points from P P lying inside a query rectangle [x:x']\\times[y:y'] [x:x']\\times[y:y'] A point p:=(p_x,p_y) p:=(p_x,p_y) lies inside this rectangle if and only if: p_x\\in[x:x']\\quad \\text{and}\\quad p_y\\in[y:y'] \\nonumber p_x\\in[x:x']\\quad \\text{and}\\quad p_y\\in[y:y'] \\nonumber We could say that a 2-dimensional rectangular range query is composed of two 1-dimensional sub-queries. At the root we split the set P P with a vertical line \\ell \\ell into two subsets of roughly equal size. The splitting line is stored at the root. P_\\text{left} P_\\text{left} is stored in the left subtree, and P_\\text{right} P_\\text{right} in the right subtree. In general, we split with a vertical line at nodes whose depth is even, and we split with a horizontal line at nodes whose depth is odd.","title":"Kd-Trees"},{"location":"6-semester/AALG/08-computational-geometry-range-searching/#building-a-kd-tree","text":"Divide-and-conquer Sort the points in P P w.r.t. their x-coordinates into array X X Sort the points in P P w.r.t. their y-coordinates into array Y Y Base case: If P P contains only one point, return a leaf with the point Otherwise: Divide into 2 sub-problems and conquer them recursively If the depth is even (split w.r.t. x-axis or a vertical line) Take the median v v of X X and create a root v_{root} v_{root} Split X X into sorted X_L X_L and X_R X_R and split Y Y into sorted Y_L Y_L and Y_R Y_R For any p\\in X_L p\\in X_L or p\\in Y_L p\\in Y_L , p.x \\leq v.x p.x \\leq v.x and for any p\\in X_R p\\in X_R or p\\in Y_R p\\in Y_R , p.x > v.x p.x > v.x Build recursively the left child of v_{root} v_{root} from X_L X_L and Y_L Y_L If the depth is odd (split w.r.t. y-axis or a horizontal line) Take the median v v of Y Y and create root v_{root} v_{root} Split X X into sorted X_L X_L and X_R X_R and split Y Y into sorted Y_L Y_L and Y_R Y_R For any p\\in X_L p\\in X_L or p\\in Y_L p\\in Y_L , p.y \\leq v.y p.y \\leq v.y and for any p\\in X_R p\\in X_R or p\\in Y_R p\\in Y_R , p.y > v.y p.y > v.y Build recursively the right child of v_{root} v_{root} from X_R X_R and Y_R Y_R Running time : \\Theta(n \\log n) \\Theta(n \\log n)","title":"Building a Kd-Tree"},{"location":"6-semester/AALG/08-computational-geometry-range-searching/#querying-a-kd-tree","text":"A query It uses a subroutine REPORTSUBTREE(\u03bd), which traverses the subtree rooted at a node \u03bd and reports all the points stored at its leaves. Running time : O(\\sqrt n + k) O(\\sqrt n + k)","title":"Querying a Kd-Tree"},{"location":"6-semester/AALG/08-computational-geometry-range-searching/#range-trees","text":"","title":"Range Trees"},{"location":"6-semester/AALG/11-multithreaded-algorithms/","text":"Multithreaded Algorithms \u00b6 Concurrency Keywords \u00b6 Spawn If it precedes a procedure call, it indicates a nested parallelism No need to wait for the procedure with keyword spawn Sync Must wait for all spawned procedures to complete before going to the statement after sync Parallel For parallel loops Just like a for loop, but loop executions run concurrently Work, Span, Parallelism \u00b6 Work The running time on a machine with one processor ( T_1 T_1 ) Fibonacci: \\Theta(\\phi^n) \\Theta(\\phi^n) Span The running time on a machine with infinite processors ( T_\\infty T_\\infty ) Fibonacci: \\Theta(n) \\Theta(n) Parallelism Work/Span Work/Span How many processors on average are used by the algorithm Fibonacci: \\Theta(\\phi^n/n) \\Theta(\\phi^n/n) Multithreaded computation on P P processors : T_P T_P Computation DAG \u00b6 Using an example of computing Fibonacci number of 4 Edge (u,v) (u,v) means that u u must execute before v v Work: The number of vertices 17 Span : The length of the longest path (critical path) 8 Work Law and Span Law \u00b6 Work Law T_P \\geq T_1/P T_P \\geq T_1/P An ideal parallel computer with P P processors can do at most P P units of work Span Law T_P \\geq T_\\infty T_P \\geq T_\\infty An ideal parallel computer with P P processors cannot run any faster than a machine with unlimited number of processors Speedup T_1/T_P T_1/T_P How many times faster the computation on P P processors than on a single processor Recall the work law says that T_P\\geq T_1/P T_P\\geq T_1/P , so the speedup must be smaller than or equal to P P The speedup on a P-processors machine can at most be P If the speedup is P, we have a perfect linear speedup Example \u00b6 Computing P-Fib(4) we know that the work T_1 = 17 T_1 = 17 and span T_\\infty=8 T_\\infty=8 P=2 Can have perfect linear speedup T_2= 17/2=8.5,\\quad \\geq T_\\infty T_2= 17/2=8.5,\\quad \\geq T_\\infty P=3 Cannot have perfect linear speedup T_3=17/3=5.7, \\quad \\leq T_\\infty T_3=17/3=5.7, \\quad \\leq T_\\infty which is impossible Slackness \u00b6 Slackness = Parallelism / P The larger the slackness, the more likely to achieve perfect speedup. When slackness is less than 1, it is impossible to achieve perfect linear speedup Summarize \u00b6","title":"Multithreaded Algorithms"},{"location":"6-semester/AALG/11-multithreaded-algorithms/#multithreaded-algorithms","text":"","title":"Multithreaded Algorithms"},{"location":"6-semester/AALG/11-multithreaded-algorithms/#concurrency-keywords","text":"Spawn If it precedes a procedure call, it indicates a nested parallelism No need to wait for the procedure with keyword spawn Sync Must wait for all spawned procedures to complete before going to the statement after sync Parallel For parallel loops Just like a for loop, but loop executions run concurrently","title":"Concurrency Keywords"},{"location":"6-semester/AALG/11-multithreaded-algorithms/#work-span-parallelism","text":"Work The running time on a machine with one processor ( T_1 T_1 ) Fibonacci: \\Theta(\\phi^n) \\Theta(\\phi^n) Span The running time on a machine with infinite processors ( T_\\infty T_\\infty ) Fibonacci: \\Theta(n) \\Theta(n) Parallelism Work/Span Work/Span How many processors on average are used by the algorithm Fibonacci: \\Theta(\\phi^n/n) \\Theta(\\phi^n/n) Multithreaded computation on P P processors : T_P T_P","title":"Work, Span, Parallelism"},{"location":"6-semester/AALG/11-multithreaded-algorithms/#computation-dag","text":"Using an example of computing Fibonacci number of 4 Edge (u,v) (u,v) means that u u must execute before v v Work: The number of vertices 17 Span : The length of the longest path (critical path) 8","title":"Computation DAG"},{"location":"6-semester/AALG/11-multithreaded-algorithms/#work-law-and-span-law","text":"Work Law T_P \\geq T_1/P T_P \\geq T_1/P An ideal parallel computer with P P processors can do at most P P units of work Span Law T_P \\geq T_\\infty T_P \\geq T_\\infty An ideal parallel computer with P P processors cannot run any faster than a machine with unlimited number of processors Speedup T_1/T_P T_1/T_P How many times faster the computation on P P processors than on a single processor Recall the work law says that T_P\\geq T_1/P T_P\\geq T_1/P , so the speedup must be smaller than or equal to P P The speedup on a P-processors machine can at most be P If the speedup is P, we have a perfect linear speedup","title":"Work Law and Span Law"},{"location":"6-semester/AALG/11-multithreaded-algorithms/#example","text":"Computing P-Fib(4) we know that the work T_1 = 17 T_1 = 17 and span T_\\infty=8 T_\\infty=8 P=2 Can have perfect linear speedup T_2= 17/2=8.5,\\quad \\geq T_\\infty T_2= 17/2=8.5,\\quad \\geq T_\\infty P=3 Cannot have perfect linear speedup T_3=17/3=5.7, \\quad \\leq T_\\infty T_3=17/3=5.7, \\quad \\leq T_\\infty which is impossible","title":"Example"},{"location":"6-semester/AALG/11-multithreaded-algorithms/#slackness","text":"Slackness = Parallelism / P The larger the slackness, the more likely to achieve perfect speedup. When slackness is less than 1, it is impossible to achieve perfect linear speedup","title":"Slackness"},{"location":"6-semester/AALG/11-multithreaded-algorithms/#summarize","text":"","title":"Summarize"},{"location":"6-semester/AALG/12-backtracking/","text":"Backtracking and Branch-and-Bound \u00b6 We can use heuristics to speed up exponential running time, to solve NP-complete problems. Backtracking for decision problems, and branch-and-bound for optimization problems Satisfiability Problem \u00b6 http://www.nytimes.com/library/national/science/071399sci-satisfiability-problems.html Four actors become Boolean variables: a, b, c, d true: play; false: not play Constraints a\\or c a\\or c a \\to \\neg c a \\to \\neg c d d b b b \\to (\\neg a \\and \\neg d) b \\to (\\neg a \\and \\neg d) It is impossible to satisfy all constraints! Definition \u00b6 Given a Boolean formula that is composed of n Boolean variables: x_1, \\dots, x_n x_1, \\dots, x_n m Boolean connectives: and ( \\and \\and ), or (\\or) (\\or) , not (\\neg) (\\neg) , implication (\\to) (\\to) , if and only if (\\leftrightarrow) (\\leftrightarrow) , and parentheses ( ) We want to see if there is a satisfying assignment for the Boolean formula A set of values for the variables such that makes the formula be true CNF-Sat \u00b6 Conjunctive Normal Form (CNF) for Boolean formulas A literal is a variables or its negation x, y,\\neg x, \\neg y x, y,\\neg x, \\neg y Each clause is a disjunction of literals OR of one or more literals: x\\or y x\\or y , \\neg x \\or y \\or x \\neg x \\or y \\or x CNF is a conjunctyion of clauses AND of clauses (x\\or y) \\and (x\\or \\neg y) (x\\or y) \\and (x\\or \\neg y) Any Boolean formula can be transformed to CNF! CNF-Sat is NP-complete Backtracking \u00b6 We use the structure of an NP-complete problem: If we have a certificate, we can check NP-complete problems can be verified in pol. time A certificate is constructed by making a number of choices What are these choices for CNF-Sat? Assign T or F to variables A backtracking algorithm searches through a large (possibly even exponential-size) set of possibilities in a systematic way. It traverses through possible search paths to locate solutions or dead ends . The configuration of a path consists of a pair (X, Y) X is the remaining sub-problem to be solved. Y is the set of choices that have been made to get to this sub-problem x from the original problem instance. Dead end : a configuration (X, Y) that cannot lead to a valid solution no matter how additional choices are made Cuts off all future searches from this configuration and backtracks to another configuration. Example \u00b6","title":"Backtracking and Branch-and-Bound"},{"location":"6-semester/AALG/12-backtracking/#backtracking-and-branch-and-bound","text":"We can use heuristics to speed up exponential running time, to solve NP-complete problems. Backtracking for decision problems, and branch-and-bound for optimization problems","title":"Backtracking and Branch-and-Bound"},{"location":"6-semester/AALG/12-backtracking/#satisfiability-problem","text":"http://www.nytimes.com/library/national/science/071399sci-satisfiability-problems.html Four actors become Boolean variables: a, b, c, d true: play; false: not play Constraints a\\or c a\\or c a \\to \\neg c a \\to \\neg c d d b b b \\to (\\neg a \\and \\neg d) b \\to (\\neg a \\and \\neg d) It is impossible to satisfy all constraints!","title":"Satisfiability Problem"},{"location":"6-semester/AALG/12-backtracking/#definition","text":"Given a Boolean formula that is composed of n Boolean variables: x_1, \\dots, x_n x_1, \\dots, x_n m Boolean connectives: and ( \\and \\and ), or (\\or) (\\or) , not (\\neg) (\\neg) , implication (\\to) (\\to) , if and only if (\\leftrightarrow) (\\leftrightarrow) , and parentheses ( ) We want to see if there is a satisfying assignment for the Boolean formula A set of values for the variables such that makes the formula be true","title":"Definition"},{"location":"6-semester/AALG/12-backtracking/#cnf-sat","text":"Conjunctive Normal Form (CNF) for Boolean formulas A literal is a variables or its negation x, y,\\neg x, \\neg y x, y,\\neg x, \\neg y Each clause is a disjunction of literals OR of one or more literals: x\\or y x\\or y , \\neg x \\or y \\or x \\neg x \\or y \\or x CNF is a conjunctyion of clauses AND of clauses (x\\or y) \\and (x\\or \\neg y) (x\\or y) \\and (x\\or \\neg y) Any Boolean formula can be transformed to CNF! CNF-Sat is NP-complete","title":"CNF-Sat"},{"location":"6-semester/AALG/12-backtracking/#backtracking","text":"We use the structure of an NP-complete problem: If we have a certificate, we can check NP-complete problems can be verified in pol. time A certificate is constructed by making a number of choices What are these choices for CNF-Sat? Assign T or F to variables A backtracking algorithm searches through a large (possibly even exponential-size) set of possibilities in a systematic way. It traverses through possible search paths to locate solutions or dead ends . The configuration of a path consists of a pair (X, Y) X is the remaining sub-problem to be solved. Y is the set of choices that have been made to get to this sub-problem x from the original problem instance. Dead end : a configuration (X, Y) that cannot lead to a valid solution no matter how additional choices are made Cuts off all future searches from this configuration and backtracks to another configuration.","title":"Backtracking"},{"location":"6-semester/AALG/12-backtracking/#example","text":"","title":"Example"},{"location":"6-semester/DBS/%23exam-pref/","text":"Exam Preparation \u00b6 ### Introduction ### Relational Model and Relational Algebra ### Entity Relationship Model 2 sessions ### Relational Database Design Theory ### SQL 2 sessions ### Transactions 2 sessions ### Physical Database Design ### Query Processing and Optimization 2 sessions","title":"Exam Preparation"},{"location":"6-semester/DBS/%23exam-pref/#exam-preparation","text":"### Introduction ### Relational Model and Relational Algebra ### Entity Relationship Model 2 sessions ### Relational Database Design Theory ### SQL 2 sessions ### Transactions 2 sessions ### Physical Database Design ### Query Processing and Optimization 2 sessions","title":"Exam Preparation"},{"location":"6-semester/DBS/01-introduction/","text":"Introduction \u00b6 Database System \u00b6 3-Layer Schema Architecture \u00b6 Physical Data Independence \u00b6 Changes regarding file structure and access paths (physical layer) have no influence on the conceptual schema (logical layer) Examples of changes on physical layer: New hard disk added New processor added Files are split into multiple files Logical Data Independence \u00b6 Changes on the logical layer have no influence on external schemas and applications Examples of changes on the logical layer: Adding another attribute to the conceptual schema Users still see the same set of attributes Changing the name of an attribute on the conceptual schema In the users view the attributes still has the same name Definitions \u00b6 Mini-world Some part of the real world about which information is stored Data/Information Known facts about the mini-world that can be recorded and have an implicit meaning Database (DB) A collection of related data Database Management Systems (DBMS) A software package to facilitate the creation and maintenance of a database Database Systems (DBS) A database and a DBMS Database Instance The content of a DB at a particular time Entity Relationship Modeling \u00b6 Entity \\to \\to Entity type Relationship \\to \\to Relationship type Attribute (characteristics) Primary Key (identification) Role (clarification) Relational Model \u00b6","title":"Introduction"},{"location":"6-semester/DBS/01-introduction/#introduction","text":"","title":"Introduction"},{"location":"6-semester/DBS/01-introduction/#database-system","text":"","title":"Database System"},{"location":"6-semester/DBS/01-introduction/#3-layer-schema-architecture","text":"","title":"3-Layer Schema Architecture"},{"location":"6-semester/DBS/01-introduction/#physical-data-independence","text":"Changes regarding file structure and access paths (physical layer) have no influence on the conceptual schema (logical layer) Examples of changes on physical layer: New hard disk added New processor added Files are split into multiple files","title":"Physical Data Independence"},{"location":"6-semester/DBS/01-introduction/#logical-data-independence","text":"Changes on the logical layer have no influence on external schemas and applications Examples of changes on the logical layer: Adding another attribute to the conceptual schema Users still see the same set of attributes Changing the name of an attribute on the conceptual schema In the users view the attributes still has the same name","title":"Logical Data Independence"},{"location":"6-semester/DBS/01-introduction/#definitions","text":"Mini-world Some part of the real world about which information is stored Data/Information Known facts about the mini-world that can be recorded and have an implicit meaning Database (DB) A collection of related data Database Management Systems (DBMS) A software package to facilitate the creation and maintenance of a database Database Systems (DBS) A database and a DBMS Database Instance The content of a DB at a particular time","title":"Definitions"},{"location":"6-semester/DBS/01-introduction/#entity-relationship-modeling","text":"Entity \\to \\to Entity type Relationship \\to \\to Relationship type Attribute (characteristics) Primary Key (identification) Role (clarification)","title":"Entity Relationship Modeling"},{"location":"6-semester/DBS/01-introduction/#relational-model","text":"","title":"Relational Model"},{"location":"6-semester/DBS/02-relational-model-and-relational-algebra/","text":"Relational Model and Relational Algebra \u00b6 Learning Goals Explain the relational model Create non-trivial relational algebra queries Use the various join types in relational algebra queries Explain the limitations of relational algebra The Relational Model \u00b6 Foundations \u00b6 Assume D_1, D_2,\\dots,D_n D_1, D_2,\\dots,D_n are domains Relation: \u00b6 R\\subseteq D_1\\times \\dots \\times D_n R\\subseteq D_1\\times \\dots \\times D_n Example: telephoneBook\\subseteq string \\times string \\times integer telephoneBook\\subseteq string \\times string \\times integer Domains can be identical D_i=D_j D_i=D_j for i \\neq j i \\neq j Based on mathematical sets Relational Schema \u00b6 Defines the structure of stored data Is denoted as sch(R) sch(R) or \\mathcal R \\mathcal R Notation: R(A_1: D_1, A_2:D_2, \\dots) R(A_1: D_1, A_2:D_2, \\dots) with A_i A_i denoting attributes Example: telephoneBook(name: string, street:string, \\underline{phoneNumber:integer}) telephoneBook(name: string, street:string, \\underline{phoneNumber:integer}) Illustration of Basic Concepts \u00b6 Header: relation schema Column header: attribute Entries in the table: relation Row in the table: tuble An entry of a cell: attribute value Underlined attributes: primary keys Foreign Key \u00b6 A relation may incluyde the primary key attributes of another table Valid values for foreign key attributes must appear in the primary key of the referenced table Example \u00b6 Characteristics \u00b6 Tuple Ordering \u00b6 Tuples in a relation are unordered These relations have the same information content: Attribute Ordering \u00b6 In accordance with the mathematical defintion of tuples, attributes in a tuple/relation are ordered These relations have different information content: However: The order of attributes are immaterial for most applications Using attribute names instead of ordering is more convenient The Cartisian product becomes commutative Atomic Values \u00b6 Values in a tuple are atomic (indivisible) A value cannot be a structure, a record, a collection type, or a relation Example: Null Values \u00b6 A special null value is used to represent values that are unknown or inapplicable to certain tuples Example: Duplicates \u00b6 A relation adheres to the mathematical definition of a set No two tuples in a relation may have identical values for all attributes Example: Relational Algebra \u00b6 \\newcommand{\\leftouterjoin}{\u27d5} \\newcommand{\\rightouterjoin}{\u27d6} \\newcommand{\\outerjoin}{\u27d7} \\newcommand{\\leftsemijoin}{\u22c9} \\newcommand{\\rightsemijoin}{\u22ca} \\nonumber \\newcommand{\\leftouterjoin}{\u27d5} \\newcommand{\\rightouterjoin}{\u27d6} \\newcommand{\\outerjoin}{\u27d7} \\newcommand{\\leftsemijoin}{\u22c9} \\newcommand{\\rightsemijoin}{\u22ca} \\nonumber Relational Algebra Operations \u00b6 Name Symbol Projection \\pi \\pi Selection \\sigma \\sigma Rename \\rho \\rho Cartesian product \\times \\times Union \\cup \\cup Difference - - Intersection \\cap \\cap Join \\Join \\Join Left Outer Join \\leftouterjoin \\leftouterjoin Right Outer Join \\rightouterjoin \\rightouterjoin Outer Join \\outerjoin \\outerjoin Left Semi Join \\leftsemijoin \\leftsemijoin Right Semi Join \\rightsemijoin \\rightsemijoin Grouping \\gamma \\gamma Division \\div \\div The operations marked with bold is the fundamental operations Any relational algebra query can be expressed with the set of fundamental operations only Removing any one of these operations reduces the expressive power Unary vs Binary operations Unary operations: \\sigma, \\pi, \\rho \\sigma, \\pi, \\rho Binary operations: \\times, \\cup, - \\times, \\cup, - Fundamental Operations \u00b6 Operations and their use: Input: one or multiple relations Output: a relation Operations can be combined (with some rules) Projection \u00b6 The result is a relation of n n columns obtained by removing the columns that are not specified. Extended Projection \u00b6 Selection \u00b6 Symbol: \\sigma_F \\sigma_F Selection predicate F F consist of: Logic operations: \\or \\or (or), \\and \\and (and), \\neg \\neg (not) Arithmetic comparison operators: <, \\leq, =, >, \\geq, \\neq <, \\leq, =, >, \\geq, \\neq Attribute names of the argument relations or constants as operands Selecting/filtering rows of a table according to the selection predicate $$ \\sigma_{salary>80000}(instructor) $$ Rename \u00b6 Relation Renaming a relation R R to S S : \\rho_S(R) \\rho_S(R) Attribute Renaming attribute B B to A A : \\rho_{A\\leftarrow B}(R) \\rho_{A\\leftarrow B}(R) Some literature uses \\beta \\beta for the rename operation Cartesian Product \u00b6 AKA Cross Product The Cartesian product ( R\\times S R\\times S ) between relations R R and S S consists of all possible combinations ( |R|*|S| |R|*|S| pairs) of tuples from both relations Result schema: $$ sch(R\\times S)=sch(R) \\cup sch(S) = \\mathcal R \\cup \\mathcal S $$ Contains many (useless) combinations! Attributes in the result are referenced as R.A R.A or S.A S.A to resolve ambiguity. Set Operations \u00b6 Set operations union, intersection , and difference can also be applied to relations Requirements Both involved relations must be union-compatible: they have the same number of attributes the domain of each attribute in column order is the same in both relations Union \u00b6 The result of a union (R\\cup S) (R\\cup S) between two relations R R and S S contains all tuples from both relations without duplicates Example: Difference \u00b6 The difference ( R-S R-S or R \\setminus S R \\setminus S ) of two relations R R and S S removes all tuples from the first relation that are also contained in the second relation Overview of Fundamental Operations \u00b6 Non-Fundamental Operations \u00b6 Intersection \u00b6 The intersection (R\\cap S) (R\\cap S) of two relations R R and S S consists of a set of tuples that occur in both relations. With Fundamental Operations \u00b6 Intersection can be expressed as difference (R\\cap S= R- (R-S)) (R\\cap S= R- (R-S)) Join \u00b6 The natural join combines two relations via common attributes (same name and domains) by combining only tuples with the same values for common attributes. Given two relations (and their schema) R(A_1,\\dots,A_m, B_1,\\dots,B_k) R(A_1,\\dots,A_m, B_1,\\dots,B_k) S(B_1,\\dots,B_k,C_1,\\dots,C_n) S(B_1,\\dots,B_k,C_1,\\dots,C_n) Example: Result: Tuples without matching partners ( dangling tuples ) are eliminated With Fundamental Operations \u00b6 Natural join can be expressed as a Cartesian product followed by selections and projections Example Is Join Commutative \u00b6 For now, we do not consider joins and Cartesian products to be commutative. For query optimization later, we usually consider joins as well as Cartesian product and other join variants to be commutative. If we want to hold on to the mathematical definition of tuples and still consider joins to be commutative, we need to apply a projection operation to reorder the attributes: $$ \\pi_L(R\\Join S) = \\pi_L(S\\Join R) $$ Outer Join \u00b6 \u27d5 - Left Outer Join Keep dangling tuples in the left operand relation \u27d6 - Right Outer Join Keep dangling tuples in the right operand relation \u27d7 - (Full) Outer Join Keep dangling tuples of both operand relations Examples Semi Join \u00b6 Find all tuples in a relation for which there are matching tuples in the other relation Left semi join: $$ L \\leftsemijoin R= \\pi_{\\mathcal L}(L\\Join R) $$ where \\mathcal L \\mathcal L represents the set of L L 's attributes Right semi join : $$ L\\rightsemijoin R = R \\leftsemijoin L = \\pi_{\\mathcal R}(L \\Join R) $$ Examples Grouping \u00b6 Tuples with the same attribute values (for a specified list of attributes) are grouped. An aggregate function is applied to each group (computing one value for each group) Typical aggregate functions: count - number of tuples in a group sum - sum of attribute values in a group min, max, avg Notation $$ \\gamma_{L;F}(R) $$ L L : list of attributes for grouping F F : aggregate function Alternative symbols \\mathcal G \\mathcal G or \\beta \\beta Example : Determine the number of students per semester: More examples in DBS2 slides p75 Division \u00b6 Example Find all studID studID s of students that took all 4 ECTS courses takes(studID, courseID) takes(studID, courseID) course(courseID, title, ects, teacher) course(courseID, title, ects, teacher) takes \\div \\pi_{courseID}(\\sigma_{ects=4}(course)) takes \\div \\pi_{courseID}(\\sigma_{ects=4}(course)) Formal definition: See examples in DBS2 slides p81","title":"Relational Model and Relational Algebra"},{"location":"6-semester/DBS/02-relational-model-and-relational-algebra/#relational-model-and-relational-algebra","text":"Learning Goals Explain the relational model Create non-trivial relational algebra queries Use the various join types in relational algebra queries Explain the limitations of relational algebra","title":"Relational Model and Relational Algebra"},{"location":"6-semester/DBS/02-relational-model-and-relational-algebra/#the-relational-model","text":"","title":"The Relational Model"},{"location":"6-semester/DBS/02-relational-model-and-relational-algebra/#foundations","text":"Assume D_1, D_2,\\dots,D_n D_1, D_2,\\dots,D_n are domains","title":"Foundations"},{"location":"6-semester/DBS/02-relational-model-and-relational-algebra/#relation","text":"R\\subseteq D_1\\times \\dots \\times D_n R\\subseteq D_1\\times \\dots \\times D_n Example: telephoneBook\\subseteq string \\times string \\times integer telephoneBook\\subseteq string \\times string \\times integer Domains can be identical D_i=D_j D_i=D_j for i \\neq j i \\neq j Based on mathematical sets","title":"Relation:"},{"location":"6-semester/DBS/02-relational-model-and-relational-algebra/#relational-schema","text":"Defines the structure of stored data Is denoted as sch(R) sch(R) or \\mathcal R \\mathcal R Notation: R(A_1: D_1, A_2:D_2, \\dots) R(A_1: D_1, A_2:D_2, \\dots) with A_i A_i denoting attributes Example: telephoneBook(name: string, street:string, \\underline{phoneNumber:integer}) telephoneBook(name: string, street:string, \\underline{phoneNumber:integer})","title":"Relational Schema"},{"location":"6-semester/DBS/02-relational-model-and-relational-algebra/#illustration-of-basic-concepts","text":"Header: relation schema Column header: attribute Entries in the table: relation Row in the table: tuble An entry of a cell: attribute value Underlined attributes: primary keys","title":"Illustration of Basic Concepts"},{"location":"6-semester/DBS/02-relational-model-and-relational-algebra/#foreign-key","text":"A relation may incluyde the primary key attributes of another table Valid values for foreign key attributes must appear in the primary key of the referenced table","title":"Foreign Key"},{"location":"6-semester/DBS/02-relational-model-and-relational-algebra/#example","text":"","title":"Example"},{"location":"6-semester/DBS/02-relational-model-and-relational-algebra/#characteristics","text":"","title":"Characteristics"},{"location":"6-semester/DBS/02-relational-model-and-relational-algebra/#tuple-ordering","text":"Tuples in a relation are unordered These relations have the same information content:","title":"Tuple Ordering"},{"location":"6-semester/DBS/02-relational-model-and-relational-algebra/#attribute-ordering","text":"In accordance with the mathematical defintion of tuples, attributes in a tuple/relation are ordered These relations have different information content: However: The order of attributes are immaterial for most applications Using attribute names instead of ordering is more convenient The Cartisian product becomes commutative","title":"Attribute Ordering"},{"location":"6-semester/DBS/02-relational-model-and-relational-algebra/#atomic-values","text":"Values in a tuple are atomic (indivisible) A value cannot be a structure, a record, a collection type, or a relation Example:","title":"Atomic Values"},{"location":"6-semester/DBS/02-relational-model-and-relational-algebra/#null-values","text":"A special null value is used to represent values that are unknown or inapplicable to certain tuples Example:","title":"Null Values"},{"location":"6-semester/DBS/02-relational-model-and-relational-algebra/#duplicates","text":"A relation adheres to the mathematical definition of a set No two tuples in a relation may have identical values for all attributes Example:","title":"Duplicates"},{"location":"6-semester/DBS/02-relational-model-and-relational-algebra/#relational-algebra","text":"\\newcommand{\\leftouterjoin}{\u27d5} \\newcommand{\\rightouterjoin}{\u27d6} \\newcommand{\\outerjoin}{\u27d7} \\newcommand{\\leftsemijoin}{\u22c9} \\newcommand{\\rightsemijoin}{\u22ca} \\nonumber \\newcommand{\\leftouterjoin}{\u27d5} \\newcommand{\\rightouterjoin}{\u27d6} \\newcommand{\\outerjoin}{\u27d7} \\newcommand{\\leftsemijoin}{\u22c9} \\newcommand{\\rightsemijoin}{\u22ca} \\nonumber","title":"Relational Algebra"},{"location":"6-semester/DBS/02-relational-model-and-relational-algebra/#relational-algebra-operations","text":"Name Symbol Projection \\pi \\pi Selection \\sigma \\sigma Rename \\rho \\rho Cartesian product \\times \\times Union \\cup \\cup Difference - - Intersection \\cap \\cap Join \\Join \\Join Left Outer Join \\leftouterjoin \\leftouterjoin Right Outer Join \\rightouterjoin \\rightouterjoin Outer Join \\outerjoin \\outerjoin Left Semi Join \\leftsemijoin \\leftsemijoin Right Semi Join \\rightsemijoin \\rightsemijoin Grouping \\gamma \\gamma Division \\div \\div The operations marked with bold is the fundamental operations Any relational algebra query can be expressed with the set of fundamental operations only Removing any one of these operations reduces the expressive power Unary vs Binary operations Unary operations: \\sigma, \\pi, \\rho \\sigma, \\pi, \\rho Binary operations: \\times, \\cup, - \\times, \\cup, -","title":"Relational Algebra Operations"},{"location":"6-semester/DBS/02-relational-model-and-relational-algebra/#fundamental-operations","text":"Operations and their use: Input: one or multiple relations Output: a relation Operations can be combined (with some rules)","title":"Fundamental Operations"},{"location":"6-semester/DBS/02-relational-model-and-relational-algebra/#projection","text":"The result is a relation of n n columns obtained by removing the columns that are not specified.","title":"Projection"},{"location":"6-semester/DBS/02-relational-model-and-relational-algebra/#extended-projection","text":"","title":"Extended Projection"},{"location":"6-semester/DBS/02-relational-model-and-relational-algebra/#selection","text":"Symbol: \\sigma_F \\sigma_F Selection predicate F F consist of: Logic operations: \\or \\or (or), \\and \\and (and), \\neg \\neg (not) Arithmetic comparison operators: <, \\leq, =, >, \\geq, \\neq <, \\leq, =, >, \\geq, \\neq Attribute names of the argument relations or constants as operands Selecting/filtering rows of a table according to the selection predicate $$ \\sigma_{salary>80000}(instructor) $$","title":"Selection"},{"location":"6-semester/DBS/02-relational-model-and-relational-algebra/#rename","text":"Relation Renaming a relation R R to S S : \\rho_S(R) \\rho_S(R) Attribute Renaming attribute B B to A A : \\rho_{A\\leftarrow B}(R) \\rho_{A\\leftarrow B}(R) Some literature uses \\beta \\beta for the rename operation","title":"Rename"},{"location":"6-semester/DBS/02-relational-model-and-relational-algebra/#cartesian-product","text":"AKA Cross Product The Cartesian product ( R\\times S R\\times S ) between relations R R and S S consists of all possible combinations ( |R|*|S| |R|*|S| pairs) of tuples from both relations Result schema: $$ sch(R\\times S)=sch(R) \\cup sch(S) = \\mathcal R \\cup \\mathcal S $$ Contains many (useless) combinations! Attributes in the result are referenced as R.A R.A or S.A S.A to resolve ambiguity.","title":"Cartesian Product"},{"location":"6-semester/DBS/02-relational-model-and-relational-algebra/#set-operations","text":"Set operations union, intersection , and difference can also be applied to relations Requirements Both involved relations must be union-compatible: they have the same number of attributes the domain of each attribute in column order is the same in both relations","title":"Set Operations"},{"location":"6-semester/DBS/02-relational-model-and-relational-algebra/#union","text":"The result of a union (R\\cup S) (R\\cup S) between two relations R R and S S contains all tuples from both relations without duplicates Example:","title":"Union"},{"location":"6-semester/DBS/02-relational-model-and-relational-algebra/#difference","text":"The difference ( R-S R-S or R \\setminus S R \\setminus S ) of two relations R R and S S removes all tuples from the first relation that are also contained in the second relation","title":"Difference"},{"location":"6-semester/DBS/02-relational-model-and-relational-algebra/#overview-of-fundamental-operations","text":"","title":"Overview of Fundamental Operations"},{"location":"6-semester/DBS/02-relational-model-and-relational-algebra/#non-fundamental-operations","text":"","title":"Non-Fundamental Operations"},{"location":"6-semester/DBS/02-relational-model-and-relational-algebra/#intersection","text":"The intersection (R\\cap S) (R\\cap S) of two relations R R and S S consists of a set of tuples that occur in both relations.","title":"Intersection"},{"location":"6-semester/DBS/02-relational-model-and-relational-algebra/#with-fundamental-operations","text":"Intersection can be expressed as difference (R\\cap S= R- (R-S)) (R\\cap S= R- (R-S))","title":"With Fundamental Operations"},{"location":"6-semester/DBS/02-relational-model-and-relational-algebra/#join","text":"The natural join combines two relations via common attributes (same name and domains) by combining only tuples with the same values for common attributes. Given two relations (and their schema) R(A_1,\\dots,A_m, B_1,\\dots,B_k) R(A_1,\\dots,A_m, B_1,\\dots,B_k) S(B_1,\\dots,B_k,C_1,\\dots,C_n) S(B_1,\\dots,B_k,C_1,\\dots,C_n) Example: Result: Tuples without matching partners ( dangling tuples ) are eliminated","title":"Join"},{"location":"6-semester/DBS/02-relational-model-and-relational-algebra/#with-fundamental-operations_1","text":"Natural join can be expressed as a Cartesian product followed by selections and projections Example","title":"With Fundamental Operations"},{"location":"6-semester/DBS/02-relational-model-and-relational-algebra/#is-join-commutative","text":"For now, we do not consider joins and Cartesian products to be commutative. For query optimization later, we usually consider joins as well as Cartesian product and other join variants to be commutative. If we want to hold on to the mathematical definition of tuples and still consider joins to be commutative, we need to apply a projection operation to reorder the attributes: $$ \\pi_L(R\\Join S) = \\pi_L(S\\Join R) $$","title":"Is Join Commutative"},{"location":"6-semester/DBS/02-relational-model-and-relational-algebra/#outer-join","text":"\u27d5 - Left Outer Join Keep dangling tuples in the left operand relation \u27d6 - Right Outer Join Keep dangling tuples in the right operand relation \u27d7 - (Full) Outer Join Keep dangling tuples of both operand relations Examples","title":"Outer Join"},{"location":"6-semester/DBS/02-relational-model-and-relational-algebra/#semi-join","text":"Find all tuples in a relation for which there are matching tuples in the other relation Left semi join: $$ L \\leftsemijoin R= \\pi_{\\mathcal L}(L\\Join R) $$ where \\mathcal L \\mathcal L represents the set of L L 's attributes Right semi join : $$ L\\rightsemijoin R = R \\leftsemijoin L = \\pi_{\\mathcal R}(L \\Join R) $$ Examples","title":"Semi Join"},{"location":"6-semester/DBS/02-relational-model-and-relational-algebra/#grouping","text":"Tuples with the same attribute values (for a specified list of attributes) are grouped. An aggregate function is applied to each group (computing one value for each group) Typical aggregate functions: count - number of tuples in a group sum - sum of attribute values in a group min, max, avg Notation $$ \\gamma_{L;F}(R) $$ L L : list of attributes for grouping F F : aggregate function Alternative symbols \\mathcal G \\mathcal G or \\beta \\beta Example : Determine the number of students per semester: More examples in DBS2 slides p75","title":"Grouping"},{"location":"6-semester/DBS/02-relational-model-and-relational-algebra/#division","text":"Example Find all studID studID s of students that took all 4 ECTS courses takes(studID, courseID) takes(studID, courseID) course(courseID, title, ects, teacher) course(courseID, title, ects, teacher) takes \\div \\pi_{courseID}(\\sigma_{ects=4}(course)) takes \\div \\pi_{courseID}(\\sigma_{ects=4}(course)) Formal definition: See examples in DBS2 slides p81","title":"Division"},{"location":"6-semester/DBS/03-entity-relationship-model/","text":"The Entity Relationship Model \u00b6 \\newcommand{\\relationRaw}[3]{ \\newcommand{\\pk}{\\underline} #3\\mathbf{#1}#3#3:\\{[\\mathrm{#2}]\\} } \\newcommand{\\relation}[2]{\\relationRaw{#1}{#2}{}} \\newcommand{\\relational}[2]{\\relationRaw{#1}{#2}{&}} \\nonumber \\newcommand{\\relationRaw}[3]{ \\newcommand{\\pk}{\\underline} #3\\mathbf{#1}#3#3:\\{[\\mathrm{#2}]\\} } \\newcommand{\\relation}[2]{\\relationRaw{#1}{#2}{}} \\newcommand{\\relational}[2]{\\relationRaw{#1}{#2}{&}} \\nonumber Learning Goals Create non-trivial ER diagrams Assess the quality of an ER diagram Perform and explain the mapping of ER diagrams to relations Use a particular ER notation properly Steps of Database Design \u00b6 Requirements analysis What are we dealing with? Mapping onto a conceptual model (conceptual design) What data and relationships have to be captured? Mapping onto a data model (logical design) How to structure data in a specific model (here: the relational model)? Realization and implementation (physical design) Which adaptations and optimizations does a specific DBMS require? Basics of Entity Relationship Model \u00b6 Entity and Entity Types \u00b6 Entities are objects of the real world about which we want to store information Entities are grouped into entity types The extension of an entity type ( entity set ) is a particular collection of entities. Attributes \u00b6 Attributes model characteristics of entities or relationships All entities of an entity type have the same characteristics Attributes are declared for entity types Attributes have a domain or values set Single-Valued vs Multi-Valued \u00b6 A person might have multiple phone numbers (or a single one) Simple vs Composite \u00b6 An address can be modeled as a string or composed of street and city Stored vs Derived \u00b6 Eg. birthday and age Keys \u00b6 A (super) key consists of a subset of an entity type's attributes E(A_1,\\dots,A_m) E(A_1,\\dots,A_m) $$ {S_1,\\dots,S_k} \\subseteq {A_1, \\dots, A_m} $$ The attributes S_1, \\dots S_k S_1, \\dots S_k of the key are called key attributes The key attribute's values uniquely identify an individual entity Candidate Key \u00b6 A candidate key corresponds to a minimal subset of attributes that fulfills the above condition Primary Key \u00b6 If there are multiple candidate keys, one is chosen as primary key Primary key attributes are marked by underlining Relationships \u00b6 Relationships describe connections between entities Relationships between entities are grouped into relationship types An association between two or more entities is called relationship (instance) . A relationship set is a collection of relationship instances. Mathematical \u00b6 A relationship type R R between entity types E_1, \\dots, E_n E_1, \\dots, E_n can be considered a mathematical relation Instance of a relationship type R R : $$ R \\subseteq E_1 \\times \\dots \\times E_n $$ A particular element (e_1,\\dots,e_n)\\in R (e_1,\\dots,e_n)\\in R is called an instance of the relationship type with e_i \\in E_i e_i \\in E_i for all 1 \\leq i \\leq n 1 \\leq i \\leq n Role Names \u00b6 Role names are optional and used to characterize a relationship type. Especially useful for recursive relationship types, i.e., an entity type is participating multiple times in a relationship type. Attributes of Relationship Types \u00b6 Relationship types can also have (descriptive) attributes. Summary of Basics \u00b6 student take courses See \"animation\" in DBS3 slides p 39 Characteristics of Relationship Types \u00b6 Degree \u00b6 Number of participating entity types Mostly: binary Rarely: ternary In general: n-ary or n-way (multiway relationship types) Cardinality Ratio / Cardinality Limits / Participation Constraint \u00b6 Number of times entities are involved in relationship instances Cardinality ratio (Chen notation): 1:1, 1:N, N:M Participation constraint: partial or total Cardinality limits ([min,max] notation): [min, max] Chen Notation \u00b6 (im using \\nrightarrow \\nrightarrow to express ) 1:1, 1:N and N:1 can be considered partial functions (often also a total function) 1:1 relationship: R:E_1\\nrightarrow E_2 R:E_1\\nrightarrow E_2 and R^{-1}: E_2 \\nrightarrow E_1 R^{-1}: E_2 \\nrightarrow E_1 1:N relationship: R^{-1}: E_2 \\nrightarrow E_1 R^{-1}: E_2 \\nrightarrow E_1 N:1 relationship: R:E_1\\nrightarrow E_2 R:E_1\\nrightarrow E_2 also referred to as functional relationship The \"direction\" is important! The function always leads from the \"N\" entity type to the \"1\" entity type. Graphical Notation \u00b6 Participation Constraint \u00b6 Total Each entity of an entity type must participate in a relationship, i.e. it cannot exist without any participation ( E_2 E_2 in the left example) Partial Each entity of an entity type can participate in a relationship, i.e., it can exist without any participation. Graphical Notation \u00b6 Example Relationship \u00b6 Min-Max Notation \u00b6 Special values: for min: 0 for max: * [0,*] represents no restrictions \\to \\to default The book uses a slightly different notation: 1..* instead of [1, \u2217] Additional Concepts \u00b6 Weak Entity Types \u00b6 The existence of a weak entity depends on the existence of a strong entity (aka. the identifying or owning entity) associated by an identifying relationship Total participation on the weak entity type Only in combination with 1:N (N:1) (or rarely also 1:1) relationship types The strong entity type is always on the \"1\"-side Weak entities are uniquely identifiable in combination with the corresponding strong entity's key The weak entity type's key attributes are marked by underlining with a dashed line ( partial key, discriminator ) ISA Relationship Type \u00b6 Specialization and generalization is expressed by the ISA relationship type (inheritance) Each sparkling wine entity is associated with exactly one wine entity \\leadsto \\leadsto sparkling wine entities are identifiable by the functional ISA relationship Not every wine is also a sparkling wine Attributes of entity type wine are inherited by entity type sparkling wine The cardinalities are always ISA(E_1[1,1],E_2[0,1]) ISA(E_1[1,1],E_2[0,1]) Each entity of entity type E_1 E_1 (sparkling wine ) participates exactly once, entities of entity type E_2 E_2 (wine) participates at most once Special Characteristics \u00b6 Overlapping specialization An entity may belong to multiple specialized entity sets separate ISA symbols are used Disjoint specialization An entity may belong to at most one specialized entity set arrows to a shared ISA symbol in the diagram Attributes \u00b6 Lower-level entity types inherit : attributes of the higher-level entity type participation in relationship types of the higher-level entity type Lower-level entity types can : have attributes participate in relationship types that the higher-level entity does not participate in Participation Constraints \u00b6 Total generalization/specialization Each higher-level entity must belong to a lower-level entity type Notation: double line Partial generalization/specialization (default) Each higher-level can (may or may not) belong to a lower-level entity type Alternative Notations \u00b6 For alternative notations see DBS3 slides p 108 Mapping Basic Concepts to Relations \u00b6 Entities correspond to nouns, relationships to verbs Each statement in the requirement specification should be reflected somewhere in the ER schema Each ER diagram (ERD) should be located somewhere in the requirement specification Conceptual design often reveals inconsistencies and ambiguities in the requirement specification, which must first be resolved. Basic Approach For each entity type \\to \\to relation Name of the entity type \\to \\to name of the relation Attributes of the entity type \\to \\to Attributes of the relation Primary key of the entity type \\to \\to Primary key of the relation We do not care about the order of attributes in this context! Mapping of N:M Relationship Types \u00b6 Basic Approach New relation with all attributes of the relationship type Add the primary key attributes of all involved entity types Primary keys of involved entity types together become the key of the new relation \\bold{takes}:\\{[\\underline{\\text{studID} \\to \\text{student}}, \\underline{\\text{courseID} \\to \\text{course}}]\\} \\bold{takes}:\\{[\\underline{\\text{studID} \\to \\text{student}}, \\underline{\\text{courseID} \\to \\text{course}}]\\} Key attributes \"imported\" from involved entity types (relations) are called foreign keys In General \u00b6 Mapping of 1:N Relationship Types \u00b6 Basic Approach New relation with all attributes of the relationship type Add the primary key attributes of all involved entity types Primary key of the \"N\"-side becomes the key in the new relation Initial \\begin{align*} &\\bold{course} &&:\\{[\\underline{\\text{courseID}}, \\text{title}, \\text{ects}]\\}\\\\ &\\bold{professor} &&:\\{[\\underline{\\text{empID}}, \\text{name}, \\text{rank},\\text{office}]\\}\\\\ &\\bold{teaches} &&: \\{[\\underline{\\text{courseID}\\to \\text{course}}, \\text{empID} \\to \\text{professor}]\\} \\end{align*} \\begin{align*} &\\bold{course} &&:\\{[\\underline{\\text{courseID}}, \\text{title}, \\text{ects}]\\}\\\\ &\\bold{professor} &&:\\{[\\underline{\\text{empID}}, \\text{name}, \\text{rank},\\text{office}]\\}\\\\ &\\bold{teaches} &&: \\{[\\underline{\\text{courseID}\\to \\text{course}}, \\text{empID} \\to \\text{professor}]\\} \\end{align*} Improved by Merging \\begin{align*} &\\bold{course} &&:\\{[\\underline{\\text{courseID}}, \\text{title}, \\text{ects}, \\color{darkred}{\\text{taughtBy} \\to \\text{professor}}]\\}\\\\ &\\bold{professor} &&:\\{[\\underline{\\text{empID}}, \\text{name}, \\text{rank},\\text{office}]\\}\\\\ \\end{align*} \\begin{align*} &\\bold{course} &&:\\{[\\underline{\\text{courseID}}, \\text{title}, \\text{ects}, \\color{darkred}{\\text{taughtBy} \\to \\text{professor}}]\\}\\\\ &\\bold{professor} &&:\\{[\\underline{\\text{empID}}, \\text{name}, \\text{rank},\\text{office}]\\}\\\\ \\end{align*} Relations with the same key can be combined, but only these and no others! If the participation is not total , merging requires null values for the foreign key. In such cases, it might be preferable for some applications to have a separate relation. Mapping of 1:1 Relationship Types \u00b6 New relation with all attributes of the relationship type Add primary key attributes of all involved entity types Primary key of any of the involved entity types can become the key in the new relation Initial \\begin{align*} \\relational{license}{\\underline{licenseID}, amount}\\\\ \\relational{producer}{\\underline{vineyard}, address}\\\\ \\relational{owns}{\\underline{licenseID \\to license}, vineyard \\to producer}\\text{ or}\\\\ \\relational{owns}{licenseID \\to license, \\underline{vineyard \\to producer}} \\end{align*} \\begin{align*} \\relational{license}{\\underline{licenseID}, amount}\\\\ \\relational{producer}{\\underline{vineyard}, address}\\\\ \\relational{owns}{\\underline{licenseID \\to license}, vineyard \\to producer}\\text{ or}\\\\ \\relational{owns}{licenseID \\to license, \\underline{vineyard \\to producer}} \\end{align*} Improvement \\begin{align*} \\relational{license}{\\underline{licenseID}, amount, ownedBy \\to producer}\\\\ \\relational{producer}{\\pk{vineyard}, address} \\end{align*} \\begin{align*} \\relational{license}{\\underline{licenseID}, amount, ownedBy \\to producer}\\\\ \\relational{producer}{\\pk{vineyard}, address} \\end{align*} Or \\begin{align*} \\relational{license}{\\underline{licenseID}, amount}\\\\ \\relational{producer}{\\pk{vineyard}, address, ownsLicense \\to license} \\end{align*} \\begin{align*} \\relational{license}{\\underline{licenseID}, amount}\\\\ \\relational{producer}{\\pk{vineyard}, address, ownsLicense \\to license} \\end{align*} It is best to extend a relation of an entity type with total participation Summary: Mapping Relationship Types to Relations \u00b6 M:N New relation with all attributes of the relationship type Add attributes referencing the primary keys of the involved entity type relations Primary key: set of foreign keys 1:N Add information to the entity type relation of the \u201cN\u201d-side: Add foreign key referencing the primary key of the \u201c1\u201d-side entity type relation Add attributes of the relationship type 1:1 Add information to one of the involved entity type relations: Add foreign key referencing the primary key of the other entity type relation Add attributes of the relationship type Foreign Keys \u00b6 A foreign key is an attribute (or a combination of attributes) of a relation that references the primary key (or candidate key) of another relation Example \\relation{course}{\\pk{courseID}, title, ects, \\color{darkred}{taughtBy \\to professor}} \\relation{course}{\\pk{courseID}, title, ects, \\color{darkred}{taughtBy \\to professor}} \\relation{professor}{\\pk{empID}, name, rank, office} \\relation{professor}{\\pk{empID}, name, rank, office} Here \\mathrm{taughtBy} \\mathrm{taughtBy} is a foreign key referencing relation professor Alternative Notation \\relation{course}{\\pk{courseID}, title, ects, \\color{darkred}{taughtBy}} \\relation{course}{\\pk{courseID}, title, ects, \\color{darkred}{taughtBy}} \\relation{professor}{\\pk{empID}, name, rank, office} \\relation{professor}{\\pk{empID}, name, rank, office} Foreign key: \\mathrm{course.taughtBy \\to professor.empID} \\mathrm{course.taughtBy \\to professor.empID} Notation for composite keys: \\{R.A_1, R.A_2\\} \\to \\{S.B_1, S.B_2\\} \\{R.A_1, R.A_2\\} \\to \\{S.B_1, S.B_2\\} Mapping Additional Concepts to Relations \u00b6 Weak Entity Types \u00b6 Entities of a weak entity type are existentially dependent on a strong entity type uniquely identifiable in combination with the strong entity type's key Mapping: New relation with all attributes of the relationship type Add primary key attributes of all involved entity types Foreign key of the \"N\"-side becomes the key in the new relation Initially \\relation{wine}{color,\\pk{name}} \\relation{wine}{color,\\pk{name}} \\relation{vintage}{\\pk{name\\to wine, year}, residualSweetness} \\relation{vintage}{\\pk{name\\to wine, year}, residualSweetness} \\relation{belongsTo}{\\pk{name \\to wine, year \\to vintage}} \\relation{belongsTo}{\\pk{name \\to wine, year \\to vintage}} Merged Weak entity types and their identifying relationship types can always be merged \\relation{wine}{color,\\pk{name}} \\relation{wine}{color,\\pk{name}} \\relation{vintage}{\\pk{name\\to wine, year}, residualSweetness} \\relation{vintage}{\\pk{name\\to wine, year}, residualSweetness} More complex example in DBS3 slides p 157 Recursive Relationship Types \u00b6 Mapping just like standard N:M relationship types and renaming of foreign keys \\relation{area}{\\pk{name}, region} \\relation{area}{\\pk{name}, region} \\relation{border}{\\pk{from\\to area, to \\to area}} \\relation{border}{\\pk{from\\to area, to \\to area}} Recursive Functional Relationship Types \u00b6 Mapping just like standard 1:N relationship types and merging \\relation{critic}{\\pk{name}, organization, mentor \\to critic} \\relation{critic}{\\pk{name}, organization, mentor \\to critic} N-ary Relationship Types \u00b6 Entity Types All participating entity types are mapped according to the standard rules \\relation{critic}{\\pk{name}, organization} \\relation{critic}{\\pk{name}, organization} \\relation{dish}{\\pk{description}, sideOrder} \\relation{dish}{\\pk{description}, sideOrder} \\relation{wine}{color, \\pk{WName}, year, residualSweetness} \\relation{wine}{color, \\pk{WName}, year, residualSweetness} N-ary Relationship Types (N:M:P) \\relation{recommends}{\\pk{WName \\to wine, description \\to dish, name \\to critic}} \\relation{recommends}{\\pk{WName \\to wine, description \\to dish, name \\to critic}} N:M:1 Relationship Type \u00b6 Relations \\relation{student}{\\pk{studID}, name, semester} \\relation{student}{\\pk{studID}, name, semester} \\relation{course}{\\pk{courseID}, title, ects} \\relation{course}{\\pk{courseID}, title, ects} \\relation{professor}{\\pk{empID}, name, rank, office} \\relation{professor}{\\pk{empID}, name, rank, office} \\relation{grades}{\\pk{studID\\to student, courseID\\to course},empID \\to professor, grade} \\relation{grades}{\\pk{studID\\to student, courseID\\to course},empID \\to professor, grade} A student + course only exists once in this relation, since the professor is the (1). Therefore, the professor is not part of the primary key. Multi-Valued Attributes \u00b6 Relations \\relation{person}{\\pk{PID}, name} \\relation{person}{\\pk{PID}, name} \\relation{phoneNumber}{\\pk{PID \\to person, number}} \\relation{phoneNumber}{\\pk{PID \\to person, number}} Composite Attributes \u00b6 Include the component attributes in the relation \\relation{person}{\\pk{PID}, name, street, city} \\relation{person}{\\pk{PID}, name, street, city} Derived Attributes \u00b6 Ignored during mapping to relations, can be added later by using views Overview of the steps \u00b6 Regular entity type Create a relation, consider special attribute types Weak entity type Create a relation 1:1 binary relationship type Extend a relation with foreign key 1:N binary relationship type Extend a relation with foreign key N:M relationship type Create a relation N-ary relationship type Create a relation Relational Modeling of Generalization \u00b6 Alternative 1 - Main Classes \u00b6 A particular entity is mapped to a single tuple in a single relation (to its main class) \\relation{eployee}{\\pk{empID}, name} \\relation{eployee}{\\pk{empID}, name} \\relation{professor}{\\pk{empID}, name, rank, office} \\relation{professor}{\\pk{empID}, name, rank, office} \\relation{assistant}{\\pk{empID}, name, department} \\relation{assistant}{\\pk{empID}, name, department} Alternative 2 - Partitioning \u00b6 Parts of a particular entity are mapped to multiple relations, the key is duplicated \\relation{eployee}{\\pk{empID}, name} \\relation{eployee}{\\pk{empID}, name} \\relation{professor}{\\pk{empID \\to employee}, rank, office} \\relation{professor}{\\pk{empID \\to employee}, rank, office} \\relation{assistant}{\\pk{empID\\to employee}, department} \\relation{assistant}{\\pk{empID\\to employee}, department} Alternative 3 - Full Redundancy \u00b6 A particular entity is stored redundantly in the relations with all its inherited attributes \\relation{eployee}{\\pk{empID}, name} \\relation{eployee}{\\pk{empID}, name} \\relation{professor}{\\pk{empID}, name, rank, office} \\relation{professor}{\\pk{empID}, name, rank, office} \\relation{assistant}{\\pk{empID}, name, department} \\relation{assistant}{\\pk{empID}, name, department} Alternative 4 - Single Relation \u00b6 All entities are stored in a single relation . An additional attribute encodes the membership in a particular entity type. \\relation{employee}{\\pk{empID}, name, {\\color{darkred}{type}}, rank, office, department} \\relation{employee}{\\pk{empID}, name, {\\color{darkred}{type}}, rank, office, department} Appendix \u00b6 Appendix can be seen in DBS3 slides 185","title":"The Entity Relationship Model"},{"location":"6-semester/DBS/03-entity-relationship-model/#the-entity-relationship-model","text":"\\newcommand{\\relationRaw}[3]{ \\newcommand{\\pk}{\\underline} #3\\mathbf{#1}#3#3:\\{[\\mathrm{#2}]\\} } \\newcommand{\\relation}[2]{\\relationRaw{#1}{#2}{}} \\newcommand{\\relational}[2]{\\relationRaw{#1}{#2}{&}} \\nonumber \\newcommand{\\relationRaw}[3]{ \\newcommand{\\pk}{\\underline} #3\\mathbf{#1}#3#3:\\{[\\mathrm{#2}]\\} } \\newcommand{\\relation}[2]{\\relationRaw{#1}{#2}{}} \\newcommand{\\relational}[2]{\\relationRaw{#1}{#2}{&}} \\nonumber Learning Goals Create non-trivial ER diagrams Assess the quality of an ER diagram Perform and explain the mapping of ER diagrams to relations Use a particular ER notation properly","title":"The Entity Relationship Model"},{"location":"6-semester/DBS/03-entity-relationship-model/#steps-of-database-design","text":"Requirements analysis What are we dealing with? Mapping onto a conceptual model (conceptual design) What data and relationships have to be captured? Mapping onto a data model (logical design) How to structure data in a specific model (here: the relational model)? Realization and implementation (physical design) Which adaptations and optimizations does a specific DBMS require?","title":"Steps of Database Design"},{"location":"6-semester/DBS/03-entity-relationship-model/#basics-of-entity-relationship-model","text":"","title":"Basics of Entity Relationship Model"},{"location":"6-semester/DBS/03-entity-relationship-model/#entity-and-entity-types","text":"Entities are objects of the real world about which we want to store information Entities are grouped into entity types The extension of an entity type ( entity set ) is a particular collection of entities.","title":"Entity and Entity Types"},{"location":"6-semester/DBS/03-entity-relationship-model/#attributes","text":"Attributes model characteristics of entities or relationships All entities of an entity type have the same characteristics Attributes are declared for entity types Attributes have a domain or values set","title":"Attributes"},{"location":"6-semester/DBS/03-entity-relationship-model/#single-valued-vs-multi-valued","text":"A person might have multiple phone numbers (or a single one)","title":"Single-Valued vs Multi-Valued"},{"location":"6-semester/DBS/03-entity-relationship-model/#simple-vs-composite","text":"An address can be modeled as a string or composed of street and city","title":"Simple vs Composite"},{"location":"6-semester/DBS/03-entity-relationship-model/#stored-vs-derived","text":"Eg. birthday and age","title":"Stored vs Derived"},{"location":"6-semester/DBS/03-entity-relationship-model/#keys","text":"A (super) key consists of a subset of an entity type's attributes E(A_1,\\dots,A_m) E(A_1,\\dots,A_m) $$ {S_1,\\dots,S_k} \\subseteq {A_1, \\dots, A_m} $$ The attributes S_1, \\dots S_k S_1, \\dots S_k of the key are called key attributes The key attribute's values uniquely identify an individual entity","title":"Keys"},{"location":"6-semester/DBS/03-entity-relationship-model/#candidate-key","text":"A candidate key corresponds to a minimal subset of attributes that fulfills the above condition","title":"Candidate Key"},{"location":"6-semester/DBS/03-entity-relationship-model/#primary-key","text":"If there are multiple candidate keys, one is chosen as primary key Primary key attributes are marked by underlining","title":"Primary Key"},{"location":"6-semester/DBS/03-entity-relationship-model/#relationships","text":"Relationships describe connections between entities Relationships between entities are grouped into relationship types An association between two or more entities is called relationship (instance) . A relationship set is a collection of relationship instances.","title":"Relationships"},{"location":"6-semester/DBS/03-entity-relationship-model/#mathematical","text":"A relationship type R R between entity types E_1, \\dots, E_n E_1, \\dots, E_n can be considered a mathematical relation Instance of a relationship type R R : $$ R \\subseteq E_1 \\times \\dots \\times E_n $$ A particular element (e_1,\\dots,e_n)\\in R (e_1,\\dots,e_n)\\in R is called an instance of the relationship type with e_i \\in E_i e_i \\in E_i for all 1 \\leq i \\leq n 1 \\leq i \\leq n","title":"Mathematical"},{"location":"6-semester/DBS/03-entity-relationship-model/#role-names","text":"Role names are optional and used to characterize a relationship type. Especially useful for recursive relationship types, i.e., an entity type is participating multiple times in a relationship type.","title":"Role Names"},{"location":"6-semester/DBS/03-entity-relationship-model/#attributes-of-relationship-types","text":"Relationship types can also have (descriptive) attributes.","title":"Attributes of Relationship Types"},{"location":"6-semester/DBS/03-entity-relationship-model/#summary-of-basics","text":"student take courses See \"animation\" in DBS3 slides p 39","title":"Summary of Basics"},{"location":"6-semester/DBS/03-entity-relationship-model/#characteristics-of-relationship-types","text":"","title":"Characteristics of Relationship Types"},{"location":"6-semester/DBS/03-entity-relationship-model/#degree","text":"Number of participating entity types Mostly: binary Rarely: ternary In general: n-ary or n-way (multiway relationship types)","title":"Degree"},{"location":"6-semester/DBS/03-entity-relationship-model/#cardinality-ratio-cardinality-limits-participation-constraint","text":"Number of times entities are involved in relationship instances Cardinality ratio (Chen notation): 1:1, 1:N, N:M Participation constraint: partial or total Cardinality limits ([min,max] notation): [min, max]","title":"Cardinality Ratio / Cardinality Limits / Participation Constraint"},{"location":"6-semester/DBS/03-entity-relationship-model/#chen-notation","text":"(im using \\nrightarrow \\nrightarrow to express ) 1:1, 1:N and N:1 can be considered partial functions (often also a total function) 1:1 relationship: R:E_1\\nrightarrow E_2 R:E_1\\nrightarrow E_2 and R^{-1}: E_2 \\nrightarrow E_1 R^{-1}: E_2 \\nrightarrow E_1 1:N relationship: R^{-1}: E_2 \\nrightarrow E_1 R^{-1}: E_2 \\nrightarrow E_1 N:1 relationship: R:E_1\\nrightarrow E_2 R:E_1\\nrightarrow E_2 also referred to as functional relationship The \"direction\" is important! The function always leads from the \"N\" entity type to the \"1\" entity type.","title":"Chen Notation"},{"location":"6-semester/DBS/03-entity-relationship-model/#graphical-notation","text":"","title":"Graphical Notation"},{"location":"6-semester/DBS/03-entity-relationship-model/#participation-constraint","text":"Total Each entity of an entity type must participate in a relationship, i.e. it cannot exist without any participation ( E_2 E_2 in the left example) Partial Each entity of an entity type can participate in a relationship, i.e., it can exist without any participation.","title":"Participation Constraint"},{"location":"6-semester/DBS/03-entity-relationship-model/#graphical-notation_1","text":"","title":"Graphical Notation"},{"location":"6-semester/DBS/03-entity-relationship-model/#example-relationship","text":"","title":"Example Relationship"},{"location":"6-semester/DBS/03-entity-relationship-model/#min-max-notation","text":"Special values: for min: 0 for max: * [0,*] represents no restrictions \\to \\to default The book uses a slightly different notation: 1..* instead of [1, \u2217]","title":"Min-Max Notation"},{"location":"6-semester/DBS/03-entity-relationship-model/#additional-concepts","text":"","title":"Additional Concepts"},{"location":"6-semester/DBS/03-entity-relationship-model/#weak-entity-types","text":"The existence of a weak entity depends on the existence of a strong entity (aka. the identifying or owning entity) associated by an identifying relationship Total participation on the weak entity type Only in combination with 1:N (N:1) (or rarely also 1:1) relationship types The strong entity type is always on the \"1\"-side Weak entities are uniquely identifiable in combination with the corresponding strong entity's key The weak entity type's key attributes are marked by underlining with a dashed line ( partial key, discriminator )","title":"Weak Entity Types"},{"location":"6-semester/DBS/03-entity-relationship-model/#isa-relationship-type","text":"Specialization and generalization is expressed by the ISA relationship type (inheritance) Each sparkling wine entity is associated with exactly one wine entity \\leadsto \\leadsto sparkling wine entities are identifiable by the functional ISA relationship Not every wine is also a sparkling wine Attributes of entity type wine are inherited by entity type sparkling wine The cardinalities are always ISA(E_1[1,1],E_2[0,1]) ISA(E_1[1,1],E_2[0,1]) Each entity of entity type E_1 E_1 (sparkling wine ) participates exactly once, entities of entity type E_2 E_2 (wine) participates at most once","title":"ISA Relationship Type"},{"location":"6-semester/DBS/03-entity-relationship-model/#special-characteristics","text":"Overlapping specialization An entity may belong to multiple specialized entity sets separate ISA symbols are used Disjoint specialization An entity may belong to at most one specialized entity set arrows to a shared ISA symbol in the diagram","title":"Special Characteristics"},{"location":"6-semester/DBS/03-entity-relationship-model/#attributes_1","text":"Lower-level entity types inherit : attributes of the higher-level entity type participation in relationship types of the higher-level entity type Lower-level entity types can : have attributes participate in relationship types that the higher-level entity does not participate in","title":"Attributes"},{"location":"6-semester/DBS/03-entity-relationship-model/#participation-constraints","text":"Total generalization/specialization Each higher-level entity must belong to a lower-level entity type Notation: double line Partial generalization/specialization (default) Each higher-level can (may or may not) belong to a lower-level entity type","title":"Participation Constraints"},{"location":"6-semester/DBS/03-entity-relationship-model/#alternative-notations","text":"For alternative notations see DBS3 slides p 108","title":"Alternative Notations"},{"location":"6-semester/DBS/03-entity-relationship-model/#mapping-basic-concepts-to-relations","text":"Entities correspond to nouns, relationships to verbs Each statement in the requirement specification should be reflected somewhere in the ER schema Each ER diagram (ERD) should be located somewhere in the requirement specification Conceptual design often reveals inconsistencies and ambiguities in the requirement specification, which must first be resolved. Basic Approach For each entity type \\to \\to relation Name of the entity type \\to \\to name of the relation Attributes of the entity type \\to \\to Attributes of the relation Primary key of the entity type \\to \\to Primary key of the relation We do not care about the order of attributes in this context!","title":"Mapping Basic Concepts to Relations"},{"location":"6-semester/DBS/03-entity-relationship-model/#mapping-of-nm-relationship-types","text":"Basic Approach New relation with all attributes of the relationship type Add the primary key attributes of all involved entity types Primary keys of involved entity types together become the key of the new relation \\bold{takes}:\\{[\\underline{\\text{studID} \\to \\text{student}}, \\underline{\\text{courseID} \\to \\text{course}}]\\} \\bold{takes}:\\{[\\underline{\\text{studID} \\to \\text{student}}, \\underline{\\text{courseID} \\to \\text{course}}]\\} Key attributes \"imported\" from involved entity types (relations) are called foreign keys","title":"Mapping of N:M Relationship Types"},{"location":"6-semester/DBS/03-entity-relationship-model/#in-general","text":"","title":"In General"},{"location":"6-semester/DBS/03-entity-relationship-model/#mapping-of-1n-relationship-types","text":"Basic Approach New relation with all attributes of the relationship type Add the primary key attributes of all involved entity types Primary key of the \"N\"-side becomes the key in the new relation Initial \\begin{align*} &\\bold{course} &&:\\{[\\underline{\\text{courseID}}, \\text{title}, \\text{ects}]\\}\\\\ &\\bold{professor} &&:\\{[\\underline{\\text{empID}}, \\text{name}, \\text{rank},\\text{office}]\\}\\\\ &\\bold{teaches} &&: \\{[\\underline{\\text{courseID}\\to \\text{course}}, \\text{empID} \\to \\text{professor}]\\} \\end{align*} \\begin{align*} &\\bold{course} &&:\\{[\\underline{\\text{courseID}}, \\text{title}, \\text{ects}]\\}\\\\ &\\bold{professor} &&:\\{[\\underline{\\text{empID}}, \\text{name}, \\text{rank},\\text{office}]\\}\\\\ &\\bold{teaches} &&: \\{[\\underline{\\text{courseID}\\to \\text{course}}, \\text{empID} \\to \\text{professor}]\\} \\end{align*} Improved by Merging \\begin{align*} &\\bold{course} &&:\\{[\\underline{\\text{courseID}}, \\text{title}, \\text{ects}, \\color{darkred}{\\text{taughtBy} \\to \\text{professor}}]\\}\\\\ &\\bold{professor} &&:\\{[\\underline{\\text{empID}}, \\text{name}, \\text{rank},\\text{office}]\\}\\\\ \\end{align*} \\begin{align*} &\\bold{course} &&:\\{[\\underline{\\text{courseID}}, \\text{title}, \\text{ects}, \\color{darkred}{\\text{taughtBy} \\to \\text{professor}}]\\}\\\\ &\\bold{professor} &&:\\{[\\underline{\\text{empID}}, \\text{name}, \\text{rank},\\text{office}]\\}\\\\ \\end{align*} Relations with the same key can be combined, but only these and no others! If the participation is not total , merging requires null values for the foreign key. In such cases, it might be preferable for some applications to have a separate relation.","title":"Mapping of 1:N Relationship Types"},{"location":"6-semester/DBS/03-entity-relationship-model/#mapping-of-11-relationship-types","text":"New relation with all attributes of the relationship type Add primary key attributes of all involved entity types Primary key of any of the involved entity types can become the key in the new relation Initial \\begin{align*} \\relational{license}{\\underline{licenseID}, amount}\\\\ \\relational{producer}{\\underline{vineyard}, address}\\\\ \\relational{owns}{\\underline{licenseID \\to license}, vineyard \\to producer}\\text{ or}\\\\ \\relational{owns}{licenseID \\to license, \\underline{vineyard \\to producer}} \\end{align*} \\begin{align*} \\relational{license}{\\underline{licenseID}, amount}\\\\ \\relational{producer}{\\underline{vineyard}, address}\\\\ \\relational{owns}{\\underline{licenseID \\to license}, vineyard \\to producer}\\text{ or}\\\\ \\relational{owns}{licenseID \\to license, \\underline{vineyard \\to producer}} \\end{align*} Improvement \\begin{align*} \\relational{license}{\\underline{licenseID}, amount, ownedBy \\to producer}\\\\ \\relational{producer}{\\pk{vineyard}, address} \\end{align*} \\begin{align*} \\relational{license}{\\underline{licenseID}, amount, ownedBy \\to producer}\\\\ \\relational{producer}{\\pk{vineyard}, address} \\end{align*} Or \\begin{align*} \\relational{license}{\\underline{licenseID}, amount}\\\\ \\relational{producer}{\\pk{vineyard}, address, ownsLicense \\to license} \\end{align*} \\begin{align*} \\relational{license}{\\underline{licenseID}, amount}\\\\ \\relational{producer}{\\pk{vineyard}, address, ownsLicense \\to license} \\end{align*} It is best to extend a relation of an entity type with total participation","title":"Mapping of 1:1 Relationship Types"},{"location":"6-semester/DBS/03-entity-relationship-model/#summary-mapping-relationship-types-to-relations","text":"M:N New relation with all attributes of the relationship type Add attributes referencing the primary keys of the involved entity type relations Primary key: set of foreign keys 1:N Add information to the entity type relation of the \u201cN\u201d-side: Add foreign key referencing the primary key of the \u201c1\u201d-side entity type relation Add attributes of the relationship type 1:1 Add information to one of the involved entity type relations: Add foreign key referencing the primary key of the other entity type relation Add attributes of the relationship type","title":"Summary: Mapping Relationship Types to Relations"},{"location":"6-semester/DBS/03-entity-relationship-model/#foreign-keys","text":"A foreign key is an attribute (or a combination of attributes) of a relation that references the primary key (or candidate key) of another relation Example \\relation{course}{\\pk{courseID}, title, ects, \\color{darkred}{taughtBy \\to professor}} \\relation{course}{\\pk{courseID}, title, ects, \\color{darkred}{taughtBy \\to professor}} \\relation{professor}{\\pk{empID}, name, rank, office} \\relation{professor}{\\pk{empID}, name, rank, office} Here \\mathrm{taughtBy} \\mathrm{taughtBy} is a foreign key referencing relation professor Alternative Notation \\relation{course}{\\pk{courseID}, title, ects, \\color{darkred}{taughtBy}} \\relation{course}{\\pk{courseID}, title, ects, \\color{darkred}{taughtBy}} \\relation{professor}{\\pk{empID}, name, rank, office} \\relation{professor}{\\pk{empID}, name, rank, office} Foreign key: \\mathrm{course.taughtBy \\to professor.empID} \\mathrm{course.taughtBy \\to professor.empID} Notation for composite keys: \\{R.A_1, R.A_2\\} \\to \\{S.B_1, S.B_2\\} \\{R.A_1, R.A_2\\} \\to \\{S.B_1, S.B_2\\}","title":"Foreign Keys"},{"location":"6-semester/DBS/03-entity-relationship-model/#mapping-additional-concepts-to-relations","text":"","title":"Mapping Additional Concepts to Relations"},{"location":"6-semester/DBS/03-entity-relationship-model/#weak-entity-types_1","text":"Entities of a weak entity type are existentially dependent on a strong entity type uniquely identifiable in combination with the strong entity type's key Mapping: New relation with all attributes of the relationship type Add primary key attributes of all involved entity types Foreign key of the \"N\"-side becomes the key in the new relation Initially \\relation{wine}{color,\\pk{name}} \\relation{wine}{color,\\pk{name}} \\relation{vintage}{\\pk{name\\to wine, year}, residualSweetness} \\relation{vintage}{\\pk{name\\to wine, year}, residualSweetness} \\relation{belongsTo}{\\pk{name \\to wine, year \\to vintage}} \\relation{belongsTo}{\\pk{name \\to wine, year \\to vintage}} Merged Weak entity types and their identifying relationship types can always be merged \\relation{wine}{color,\\pk{name}} \\relation{wine}{color,\\pk{name}} \\relation{vintage}{\\pk{name\\to wine, year}, residualSweetness} \\relation{vintage}{\\pk{name\\to wine, year}, residualSweetness} More complex example in DBS3 slides p 157","title":"Weak Entity Types"},{"location":"6-semester/DBS/03-entity-relationship-model/#recursive-relationship-types","text":"Mapping just like standard N:M relationship types and renaming of foreign keys \\relation{area}{\\pk{name}, region} \\relation{area}{\\pk{name}, region} \\relation{border}{\\pk{from\\to area, to \\to area}} \\relation{border}{\\pk{from\\to area, to \\to area}}","title":"Recursive Relationship Types"},{"location":"6-semester/DBS/03-entity-relationship-model/#recursive-functional-relationship-types","text":"Mapping just like standard 1:N relationship types and merging \\relation{critic}{\\pk{name}, organization, mentor \\to critic} \\relation{critic}{\\pk{name}, organization, mentor \\to critic}","title":"Recursive Functional Relationship Types"},{"location":"6-semester/DBS/03-entity-relationship-model/#n-ary-relationship-types","text":"Entity Types All participating entity types are mapped according to the standard rules \\relation{critic}{\\pk{name}, organization} \\relation{critic}{\\pk{name}, organization} \\relation{dish}{\\pk{description}, sideOrder} \\relation{dish}{\\pk{description}, sideOrder} \\relation{wine}{color, \\pk{WName}, year, residualSweetness} \\relation{wine}{color, \\pk{WName}, year, residualSweetness} N-ary Relationship Types (N:M:P) \\relation{recommends}{\\pk{WName \\to wine, description \\to dish, name \\to critic}} \\relation{recommends}{\\pk{WName \\to wine, description \\to dish, name \\to critic}}","title":"N-ary Relationship Types"},{"location":"6-semester/DBS/03-entity-relationship-model/#nm1-relationship-type","text":"Relations \\relation{student}{\\pk{studID}, name, semester} \\relation{student}{\\pk{studID}, name, semester} \\relation{course}{\\pk{courseID}, title, ects} \\relation{course}{\\pk{courseID}, title, ects} \\relation{professor}{\\pk{empID}, name, rank, office} \\relation{professor}{\\pk{empID}, name, rank, office} \\relation{grades}{\\pk{studID\\to student, courseID\\to course},empID \\to professor, grade} \\relation{grades}{\\pk{studID\\to student, courseID\\to course},empID \\to professor, grade} A student + course only exists once in this relation, since the professor is the (1). Therefore, the professor is not part of the primary key.","title":"N:M:1 Relationship Type"},{"location":"6-semester/DBS/03-entity-relationship-model/#multi-valued-attributes","text":"Relations \\relation{person}{\\pk{PID}, name} \\relation{person}{\\pk{PID}, name} \\relation{phoneNumber}{\\pk{PID \\to person, number}} \\relation{phoneNumber}{\\pk{PID \\to person, number}}","title":"Multi-Valued Attributes"},{"location":"6-semester/DBS/03-entity-relationship-model/#composite-attributes","text":"Include the component attributes in the relation \\relation{person}{\\pk{PID}, name, street, city} \\relation{person}{\\pk{PID}, name, street, city}","title":"Composite Attributes"},{"location":"6-semester/DBS/03-entity-relationship-model/#derived-attributes","text":"Ignored during mapping to relations, can be added later by using views","title":"Derived Attributes"},{"location":"6-semester/DBS/03-entity-relationship-model/#overview-of-the-steps","text":"Regular entity type Create a relation, consider special attribute types Weak entity type Create a relation 1:1 binary relationship type Extend a relation with foreign key 1:N binary relationship type Extend a relation with foreign key N:M relationship type Create a relation N-ary relationship type Create a relation","title":"Overview of the steps"},{"location":"6-semester/DBS/03-entity-relationship-model/#relational-modeling-of-generalization","text":"","title":"Relational Modeling of Generalization"},{"location":"6-semester/DBS/03-entity-relationship-model/#alternative-1-main-classes","text":"A particular entity is mapped to a single tuple in a single relation (to its main class) \\relation{eployee}{\\pk{empID}, name} \\relation{eployee}{\\pk{empID}, name} \\relation{professor}{\\pk{empID}, name, rank, office} \\relation{professor}{\\pk{empID}, name, rank, office} \\relation{assistant}{\\pk{empID}, name, department} \\relation{assistant}{\\pk{empID}, name, department}","title":"Alternative 1 - Main Classes"},{"location":"6-semester/DBS/03-entity-relationship-model/#alternative-2-partitioning","text":"Parts of a particular entity are mapped to multiple relations, the key is duplicated \\relation{eployee}{\\pk{empID}, name} \\relation{eployee}{\\pk{empID}, name} \\relation{professor}{\\pk{empID \\to employee}, rank, office} \\relation{professor}{\\pk{empID \\to employee}, rank, office} \\relation{assistant}{\\pk{empID\\to employee}, department} \\relation{assistant}{\\pk{empID\\to employee}, department}","title":"Alternative 2 - Partitioning"},{"location":"6-semester/DBS/03-entity-relationship-model/#alternative-3-full-redundancy","text":"A particular entity is stored redundantly in the relations with all its inherited attributes \\relation{eployee}{\\pk{empID}, name} \\relation{eployee}{\\pk{empID}, name} \\relation{professor}{\\pk{empID}, name, rank, office} \\relation{professor}{\\pk{empID}, name, rank, office} \\relation{assistant}{\\pk{empID}, name, department} \\relation{assistant}{\\pk{empID}, name, department}","title":"Alternative 3 - Full Redundancy"},{"location":"6-semester/DBS/03-entity-relationship-model/#alternative-4-single-relation","text":"All entities are stored in a single relation . An additional attribute encodes the membership in a particular entity type. \\relation{employee}{\\pk{empID}, name, {\\color{darkred}{type}}, rank, office, department} \\relation{employee}{\\pk{empID}, name, {\\color{darkred}{type}}, rank, office, department}","title":"Alternative 4 - Single Relation"},{"location":"6-semester/DBS/03-entity-relationship-model/#appendix","text":"Appendix can be seen in DBS3 slides 185","title":"Appendix"},{"location":"6-semester/DBS/04-relational-database-theory/","text":"Relational Database Design Theory \u00b6 \\newcommand{\\dep}[2]{\\{#1\\} \\to \\{#2\\}} \\newcommand{\\schema}{\\mathcal{R}} \\newcommand{\\oneton}[1]{\\onetonop{#1}{,}} \\newcommand{\\onetonop}[2]{#1_{n} #2 \\dots #2 #1_{n}} \\nonumber \\newcommand{\\dep}[2]{\\{#1\\} \\to \\{#2\\}} \\newcommand{\\schema}{\\mathcal{R}} \\newcommand{\\oneton}[1]{\\onetonop{#1}{,}} \\newcommand{\\onetonop}[2]{#1_{n} #2 \\dots #2 #1_{n}} \\nonumber Learning Goals Understanding the concepts of functional dependencies and normal forms Describe the quality of a design by using normal forms Improving a database design by decomposition Functional Dependencies \u00b6 Symbols: Schema \\mathcal R = \\{A,B,C,D\\} \\mathcal R = \\{A,B,C,D\\} Instance R R Let \\alpha \\subseteq \\mathcal R \\alpha \\subseteq \\mathcal R and \\beta \\subseteq \\mathcal R \\beta \\subseteq \\mathcal R be sets of attributes A functional dependency \\alpha \\to \\beta \\alpha \\to \\beta holds on \\mathcal R \\mathcal R if for all legal instances R R of \\schema \\schema : \\forall r,s \\in R: r.\\alpha = s.\\alpha \\Rightarrow r.\\beta = s.\\beta \\forall r,s \\in R: r.\\alpha = s.\\alpha \\Rightarrow r.\\beta = s.\\beta The \\alpha \\alpha values uniquely identify the \\beta \\beta values \\alpha \\alpha functionally determines \\beta \\beta A functional dependency \\alpha \\to \\beta \\alpha \\to \\beta is called trivial if \\beta \\subseteq \\alpha \\beta \\subseteq \\alpha Examples \u00b6 If I have a zip code, I know the town Zip code 9220 gives one and only one town : Aalborg Zip code 9000 will not return { Aalborg, Viborg } Notation: { zipCode } \\to \\to { town } Functional dependencies are semantic constraints that need to be true for all possible instances, not just for the current one! Keys \u00b6 Super Keys \u00b6 \\alpha \\subseteq \\schema \\alpha \\subseteq \\schema is a super key if \\alpha \\to \\schema \\alpha \\to \\schema , i.e. \\alpha \\alpha determines all attribute values The set of all attributes is a super key: \\schema \\to \\schema \\schema \\to \\schema Super keys are not necessarily minimal Fully Functional Dependent \u00b6 \\beta \\beta is fully functionally dependent on \\alpha \\alpha if: \\alpha \\to \\beta \\alpha \\to \\beta and \\alpha \\alpha cannot be further reduced (= left reduced), i.e. \\forall A \\in \\alpha:(\\alpha - \\{A\\}) \\nrightarrow \\beta \\forall A \\in \\alpha:(\\alpha - \\{A\\}) \\nrightarrow \\beta Candidate Keys \u00b6 \\alpha\\in\\schema \\alpha\\in\\schema is a candidate key if \\schema \\schema is fully functionally dependent on \\alpha \\alpha . One of the candidate keys is chosen as primary key Deriving Functional Dependencies \u00b6 Given a set of FDs (Functional dependencies) F F we can derive additional FDs F^+ F^+ contains all FDs that can be derived from F F , i.e., all FDs logically implied by dependencies in F F F^+ F^+ is F F 's closure Inference rules ( Armstrong Axioms ) help computing F^+ F^+ Armstrong Axioms \u00b6 \\alpha, \\beta, \\gamma, \\delta \\alpha, \\beta, \\gamma, \\delta are subsets of attributes in \\schema \\schema Reflexivity If \\beta \\subseteq \\alpha \\beta \\subseteq \\alpha then \\alpha \\to \\beta \\alpha \\to \\beta in particular: \\alpha \\to \\alpha \\alpha \\to \\alpha Augmentation If \\alpha \\to \\beta \\alpha \\to \\beta then \\alpha \\gamma \\to \\alpha\\gamma \\alpha \\gamma \\to \\alpha\\gamma Transitivity If \\alpha \\to \\beta \\alpha \\to \\beta and \\beta \\to \\gamma \\beta \\to \\gamma then \\alpha \\to \\gamma \\alpha \\to \\gamma The Armstrong axioms are sound and complete . They are sound in the sense that they generate only correct functional dependencies They are complete in the sense that they generate all possible FDs ( F^+ F^+ ) from a given set F F Additional Rules \u00b6 Not essential, but sound and ease the derivation process Union If \\alpha\\to\\beta \\alpha\\to\\beta and \\alpha\\to\\gamma \\alpha\\to\\gamma then \\alpha\\to\\beta\\gamma \\alpha\\to\\beta\\gamma Decomposition If \\alpha\\to\\beta\\gamma \\alpha\\to\\beta\\gamma then \\alpha\\to\\beta \\alpha\\to\\beta and \\alpha\\to\\gamma \\alpha\\to\\gamma Pseudotransitivity If \\alpha\\to\\beta \\alpha\\to\\beta and \\gamma\\beta\\to\\delta \\gamma\\beta\\to\\delta then \\alpha\\gamma\\to\\delta \\alpha\\gamma\\to\\delta Examples \u00b6 Given the following functional dependencies F F , derive additional ones by applying the Armstrong axioms A \u2192 BC CD \u2192 E B \u2192 D E \u2192 A Derived FDs E \u2192 A and A \u2192 BC, then E \u2192 BC (transitivity) B \u2192 D, then CB \u2192 CD (augmentation) CB \u2192 CD and CD \u2192 E, then CB \u2192 E (transitivity) Closure of a Set of Attributes \u00b6 The closure of a set of attributes ( \\alpha^+ \\alpha^+ ) with respect to a set of FDs F F and a set of attributes \\alpha \\alpha is \\alpha^+ = \\{A\\mid \\alpha\\to A \\in F^+\\} \\alpha^+ = \\{A\\mid \\alpha\\to A \\in F^+\\} Observation: If \\alpha\\to \\beta \\alpha\\to \\beta is in F^+ F^+ then \\beta \\beta is in \\alpha^+ \\alpha^+ Attribute Closure Algorithm \u00b6 Input: a set F F of FDs a set of attributes \\alpha \\in \\schema \\alpha \\in \\schema Applications: Test if a functional dependency \\alpha \\to \\beta \\alpha \\to \\beta holds Test if a given set of attributes \\kappa \\subseteq \\schema \\kappa \\subseteq \\schema is a super key Test for super keys By calling attrClosure( F, \\kappa F, \\kappa ) we obtain \\kappa^+ \\kappa^+ if \\kappa^+ = \\schema \\kappa^+ = \\schema then \\kappa \\kappa is a super key of \\schema \\schema Example in DBS4 slides p 49 Canonical Cover (Minimal Cover) \u00b6 Two sets of FDs F F and G G are considered equivalent F \\equiv G F \\equiv G if their closures are the same i.e. F^+ = G^+ F^+ = G^+ Both sets allow for deriving the same set of FDs Observation F^+ F^+ can be huge Many redundant dependencies Difficult to overview Goal: Find the smallest possible set F_c F_c for F F so that F^+_c \\equiv F^+ F^+_c \\equiv F^+ There might be alternative minimal sets ! A minimal cover F_c F_c is a canonical representation of a set F F of functional dependencies Characteristics: F_c \\equiv F F_c \\equiv F therefore F_c^+ = F^+ F_c^+ = F^+ (equivalent if closures are the same) FDs \\alpha \\to \\beta \\alpha \\to \\beta in F_c F_c do not contain extraneous attributes , i.e.: \\forall A \\in \\alpha:(F_c - \\{\\alpha \\to \\beta\\})\\cup \\{(\\alpha - A) \\to \\beta\\} \\not\\equiv F_c \\forall A \\in \\alpha:(F_c - \\{\\alpha \\to \\beta\\})\\cup \\{(\\alpha - A) \\to \\beta\\} \\not\\equiv F_c \\forall B \\in \\beta:(F_c - \\{\\alpha \\to \\beta\\})\\cup \\{\\alpha - (\\beta - B)\\} \\not\\equiv F_c \\forall B \\in \\beta:(F_c - \\{\\alpha \\to \\beta\\})\\cup \\{\\alpha - (\\beta - B)\\} \\not\\equiv F_c The left side of an FD in F_c F_c is unique. Applying the union rule \\alpha \\to \\beta \\alpha \\to \\beta and \\alpha \\to \\gamma \\alpha \\to \\gamma can be combined to \\alpha\\to\\beta\\gamma \\alpha\\to\\beta\\gamma Check if Attribute is extraneous Check if A \u2208 \u03b1 is an extraneous attribute in \u03b1 \u2192 \u03b2 by computing the attribute closure: A \u2208 \u03b1 is extraneous if \u03b2 \u2286 attrClosure(F, \u03b1 \u2212 A) Check if B \u2208 \u03b2 is an extraneous attribute in \u03b1 \u2192 \u03b2 by computing the attribute closure: B \u2208 \u03b2 is extraneous if B \u2208 attrClosure((F \u2212 {\u03b1 \u2192 \u03b2}) \u222a {\u03b1 \u2192 (\u03b2 \u2212 B)}, \u03b1) Minimal Cover Algorithm \u00b6 Example in DBS4 slides p 69 Normalization by Decomposition of Relations \u00b6 Normalization by decomposition \u00b6 Decompose a relation schema \\schema \\schema into multiple relational schemas \\schema_1,\\dots\\schema_n \\schema_1,\\dots\\schema_n to eliminate problems in the original design Normal forms Normal forms describe the quality of a design 1NF, 2NF, 3NF, BCNF, 4NF, \\dots \\dots Prohibit particular functional dependencies in a relation to avoid redundancy, null values, and anomalies Good ER modeling typically directly leads to 3NF (or higher NF) relations Normalization eliminates problems caused by functional dependencies among attributes of any entity type Valid and Lossless Decompositions \u00b6 A decomposition is valid if \\schema = \\schema_1 \\cup \\schema_2 \\schema = \\schema_1 \\cup \\schema_2 , i.e. no attributes in \\schema \\schema get lost R_1 := \\pi_{\\schema_1}(R) R_1 := \\pi_{\\schema_1}(R) R_2 := \\pi_{\\schema_2}(R) R_2 := \\pi_{\\schema_2}(R) A decomposition of \\schema \\schema into \\schema_1 \\schema_1 and \\schema_2 \\schema_2 is lossless if the following holds for all possible instances R R of \\schema \\schema (also referred to as lossless-join decomposition): R=R_1 \\Join R_2 R=R_1 \\Join R_2 All data contained in the original instance R R of schema \\schema \\schema must be reconstructible with a natural join from the instances R_1,\\dots,R_n R_1,\\dots,R_n of the new schemas \\schema_1,\\dots,\\schema_n \\schema_1,\\dots,\\schema_n Formal Characterization of a Lossless Decomposition \u00b6 Given A decomposition of \\schema \\schema into \\schema_1 \\schema_1 and \\schema_2 \\schema_2 F_\\schema F_\\schema is the set of FDs in \\schema \\schema A decomposition is lossless if we can derive at least one of the following FDs: (\\schema_1 \\cap \\schema_2) \\to \\schema_1 \\in F_\\schema^+ (\\schema_1 \\cap \\schema_2) \\to \\schema_1 \\in F_\\schema^+ i.e., common attributes are super key in \\schema_1 \\schema_1 or (\\schema_1 \\cap \\schema_2) \\to \\schema_2 \\in F_\\schema^+ (\\schema_1 \\cap \\schema_2) \\to \\schema_2 \\in F_\\schema^+ i.e., common attributes are super key in \\schema_2 \\schema_2 If this is not the case, the decomposition is said to be lossy Example of a LOSSY Decomposition \u00b6 The relationship between guest, pub, and beer got lost. Lossy decomposition sometimes means that the reconstruction leads to additional tuples. Dependency Preservation \u00b6 Second characteristic of a good decomposition All functional dependencies that hold for \\schema \\schema must be verifiable in the new schemas \\schema_1,\\dots,\\schema_n \\schema_1,\\dots,\\schema_n We can check all dependencies locally on \\oneton{\\schema} \\oneton{\\schema} We avoid the alternative: computing the join \\onetonop{\\schema}{\\Join} \\onetonop{\\schema}{\\Join} to test if an FD is violated A decomposition is dependency preserving if $$ F_{\\schema} \\equiv (F_{\\schema_1} \\cup \\cdots \\cup F_{\\schema_n}) $$ i.e. $F^+ \\schema = (F {\\schema_1} \\cup \\cdots \\cup F_{\\schema_n})^+ $ with F_{\\schema_i} F_{\\schema_i} representing functional dependencies that can be checked efficiently on R_i R_i Example The following is dependency preserving, since we can check all FDs locally ( A\\to B A\\to B on R_1 R_1 and B\\to C B\\to C on R_2 R_2 ) The next one is not dependency preserving, since we cannot check B\\to C B\\to C on any of the new relations! Examples in DBS4 slides p 90 Summary Functional Dependencies \u00b6 Normal Forms \u00b6 Normal forms define characteristics of relational schemas forbid certain combinations of FDs in a relation avoid redundancies and anomalies guideline to obtain good decompositions First Normal Form 1NF \u00b6 A relation \\schema \\schema is in 1NF if the domains of all its attributes are atomic (no composite or set-valued domains) Third Normal Form 3NF \u00b6 A relation schema \\schema \\schema is in 3NF if at least one of the following conditions holds for each of its FDs \\alpha \\to B \\alpha \\to B with B \\in \\schema B \\in \\schema B \\in \\alpha B \\in \\alpha , i.e. the FD is trivial \\alpha \\alpha is a super key of R R B B is part of a candidate key for \\schema \\schema Main characteristics 3NF prevents (some) transitive dependencies Exception: Condition 3 Non-Example \u00b6 The relation is not in 3NF Eliminates Transitive Dependencies \u00b6 Boyce Codd Normal Form BCNF \u00b6 A relation schema \\schema \\schema is in BCNF if at least one of the following conditions holds for each of its FDs \\alpha \\to B \\alpha \\to B with B \\in \\schema B \\in \\schema B \\in \\alpha B \\in \\alpha , i.e. the FD is trivial \\alpha \\alpha is a super key of R R Main characteristics Difference to 3NF: no third option ( B B is part of a candidate key for \\schema \\schema ) BCNF is more strict than 3NF (\u201cincludes\u201d 3NF) BCNF prevents all transitive dependencies Example \u00b6 3NF vs. BCNF Is the relation in 3NF? YES Is the relation in BCNF? NO Decomposition \u00b6 It is always possible to decompose a relational schema \\schema \\schema with FDs F F into 3NF relational schemas \\oneton{\\schema} \\oneton{\\schema} so that the decomposition is lossless dependency preserving BCNF relational schema \\oneton{\\schema} \\oneton{\\schema} so that the decomposition is lossless It is not always possible to create a BCNF decomposition \\oneton{\\schema} \\oneton{\\schema} of \\schema \\schema that is dependency preserving Decomposition Algorithm for BCNF \u00b6 Instead of computing F^+ F^+ to check \\alpha \\alpha for its super key characteristics, we can compute \\alpha^+ \\alpha^+ for this purpose Example in DBS4 slides p 133- Summary \u00b6 Normal Form Main Characteristics 1NF Only atomic attributes 3NF Some transitive dependencies BCNF No transitive dependencies In practice, if a BCNF composition is impossible without loosing dependency preservation, we go for the 3NF decomposition (although it allows for some redundancy) Decomp. algorithms for all normal forms guarantee lossless decompositions. Dependency preservation can only be guaranteed until 3NF.","title":"Relational Database Design Theory"},{"location":"6-semester/DBS/04-relational-database-theory/#relational-database-design-theory","text":"\\newcommand{\\dep}[2]{\\{#1\\} \\to \\{#2\\}} \\newcommand{\\schema}{\\mathcal{R}} \\newcommand{\\oneton}[1]{\\onetonop{#1}{,}} \\newcommand{\\onetonop}[2]{#1_{n} #2 \\dots #2 #1_{n}} \\nonumber \\newcommand{\\dep}[2]{\\{#1\\} \\to \\{#2\\}} \\newcommand{\\schema}{\\mathcal{R}} \\newcommand{\\oneton}[1]{\\onetonop{#1}{,}} \\newcommand{\\onetonop}[2]{#1_{n} #2 \\dots #2 #1_{n}} \\nonumber Learning Goals Understanding the concepts of functional dependencies and normal forms Describe the quality of a design by using normal forms Improving a database design by decomposition","title":"Relational Database Design Theory"},{"location":"6-semester/DBS/04-relational-database-theory/#functional-dependencies","text":"Symbols: Schema \\mathcal R = \\{A,B,C,D\\} \\mathcal R = \\{A,B,C,D\\} Instance R R Let \\alpha \\subseteq \\mathcal R \\alpha \\subseteq \\mathcal R and \\beta \\subseteq \\mathcal R \\beta \\subseteq \\mathcal R be sets of attributes A functional dependency \\alpha \\to \\beta \\alpha \\to \\beta holds on \\mathcal R \\mathcal R if for all legal instances R R of \\schema \\schema : \\forall r,s \\in R: r.\\alpha = s.\\alpha \\Rightarrow r.\\beta = s.\\beta \\forall r,s \\in R: r.\\alpha = s.\\alpha \\Rightarrow r.\\beta = s.\\beta The \\alpha \\alpha values uniquely identify the \\beta \\beta values \\alpha \\alpha functionally determines \\beta \\beta A functional dependency \\alpha \\to \\beta \\alpha \\to \\beta is called trivial if \\beta \\subseteq \\alpha \\beta \\subseteq \\alpha","title":"Functional Dependencies"},{"location":"6-semester/DBS/04-relational-database-theory/#examples","text":"If I have a zip code, I know the town Zip code 9220 gives one and only one town : Aalborg Zip code 9000 will not return { Aalborg, Viborg } Notation: { zipCode } \\to \\to { town } Functional dependencies are semantic constraints that need to be true for all possible instances, not just for the current one!","title":"Examples"},{"location":"6-semester/DBS/04-relational-database-theory/#keys","text":"","title":"Keys"},{"location":"6-semester/DBS/04-relational-database-theory/#super-keys","text":"\\alpha \\subseteq \\schema \\alpha \\subseteq \\schema is a super key if \\alpha \\to \\schema \\alpha \\to \\schema , i.e. \\alpha \\alpha determines all attribute values The set of all attributes is a super key: \\schema \\to \\schema \\schema \\to \\schema Super keys are not necessarily minimal","title":"Super Keys"},{"location":"6-semester/DBS/04-relational-database-theory/#fully-functional-dependent","text":"\\beta \\beta is fully functionally dependent on \\alpha \\alpha if: \\alpha \\to \\beta \\alpha \\to \\beta and \\alpha \\alpha cannot be further reduced (= left reduced), i.e. \\forall A \\in \\alpha:(\\alpha - \\{A\\}) \\nrightarrow \\beta \\forall A \\in \\alpha:(\\alpha - \\{A\\}) \\nrightarrow \\beta","title":"Fully Functional Dependent"},{"location":"6-semester/DBS/04-relational-database-theory/#candidate-keys","text":"\\alpha\\in\\schema \\alpha\\in\\schema is a candidate key if \\schema \\schema is fully functionally dependent on \\alpha \\alpha . One of the candidate keys is chosen as primary key","title":"Candidate Keys"},{"location":"6-semester/DBS/04-relational-database-theory/#deriving-functional-dependencies","text":"Given a set of FDs (Functional dependencies) F F we can derive additional FDs F^+ F^+ contains all FDs that can be derived from F F , i.e., all FDs logically implied by dependencies in F F F^+ F^+ is F F 's closure Inference rules ( Armstrong Axioms ) help computing F^+ F^+","title":"Deriving Functional Dependencies"},{"location":"6-semester/DBS/04-relational-database-theory/#armstrong-axioms","text":"\\alpha, \\beta, \\gamma, \\delta \\alpha, \\beta, \\gamma, \\delta are subsets of attributes in \\schema \\schema Reflexivity If \\beta \\subseteq \\alpha \\beta \\subseteq \\alpha then \\alpha \\to \\beta \\alpha \\to \\beta in particular: \\alpha \\to \\alpha \\alpha \\to \\alpha Augmentation If \\alpha \\to \\beta \\alpha \\to \\beta then \\alpha \\gamma \\to \\alpha\\gamma \\alpha \\gamma \\to \\alpha\\gamma Transitivity If \\alpha \\to \\beta \\alpha \\to \\beta and \\beta \\to \\gamma \\beta \\to \\gamma then \\alpha \\to \\gamma \\alpha \\to \\gamma The Armstrong axioms are sound and complete . They are sound in the sense that they generate only correct functional dependencies They are complete in the sense that they generate all possible FDs ( F^+ F^+ ) from a given set F F","title":"Armstrong Axioms"},{"location":"6-semester/DBS/04-relational-database-theory/#additional-rules","text":"Not essential, but sound and ease the derivation process Union If \\alpha\\to\\beta \\alpha\\to\\beta and \\alpha\\to\\gamma \\alpha\\to\\gamma then \\alpha\\to\\beta\\gamma \\alpha\\to\\beta\\gamma Decomposition If \\alpha\\to\\beta\\gamma \\alpha\\to\\beta\\gamma then \\alpha\\to\\beta \\alpha\\to\\beta and \\alpha\\to\\gamma \\alpha\\to\\gamma Pseudotransitivity If \\alpha\\to\\beta \\alpha\\to\\beta and \\gamma\\beta\\to\\delta \\gamma\\beta\\to\\delta then \\alpha\\gamma\\to\\delta \\alpha\\gamma\\to\\delta","title":"Additional Rules"},{"location":"6-semester/DBS/04-relational-database-theory/#examples_1","text":"Given the following functional dependencies F F , derive additional ones by applying the Armstrong axioms A \u2192 BC CD \u2192 E B \u2192 D E \u2192 A Derived FDs E \u2192 A and A \u2192 BC, then E \u2192 BC (transitivity) B \u2192 D, then CB \u2192 CD (augmentation) CB \u2192 CD and CD \u2192 E, then CB \u2192 E (transitivity)","title":"Examples"},{"location":"6-semester/DBS/04-relational-database-theory/#closure-of-a-set-of-attributes","text":"The closure of a set of attributes ( \\alpha^+ \\alpha^+ ) with respect to a set of FDs F F and a set of attributes \\alpha \\alpha is \\alpha^+ = \\{A\\mid \\alpha\\to A \\in F^+\\} \\alpha^+ = \\{A\\mid \\alpha\\to A \\in F^+\\} Observation: If \\alpha\\to \\beta \\alpha\\to \\beta is in F^+ F^+ then \\beta \\beta is in \\alpha^+ \\alpha^+","title":"Closure of a Set of Attributes"},{"location":"6-semester/DBS/04-relational-database-theory/#attribute-closure-algorithm","text":"Input: a set F F of FDs a set of attributes \\alpha \\in \\schema \\alpha \\in \\schema Applications: Test if a functional dependency \\alpha \\to \\beta \\alpha \\to \\beta holds Test if a given set of attributes \\kappa \\subseteq \\schema \\kappa \\subseteq \\schema is a super key Test for super keys By calling attrClosure( F, \\kappa F, \\kappa ) we obtain \\kappa^+ \\kappa^+ if \\kappa^+ = \\schema \\kappa^+ = \\schema then \\kappa \\kappa is a super key of \\schema \\schema Example in DBS4 slides p 49","title":"Attribute Closure Algorithm"},{"location":"6-semester/DBS/04-relational-database-theory/#canonical-cover-minimal-cover","text":"Two sets of FDs F F and G G are considered equivalent F \\equiv G F \\equiv G if their closures are the same i.e. F^+ = G^+ F^+ = G^+ Both sets allow for deriving the same set of FDs Observation F^+ F^+ can be huge Many redundant dependencies Difficult to overview Goal: Find the smallest possible set F_c F_c for F F so that F^+_c \\equiv F^+ F^+_c \\equiv F^+ There might be alternative minimal sets ! A minimal cover F_c F_c is a canonical representation of a set F F of functional dependencies Characteristics: F_c \\equiv F F_c \\equiv F therefore F_c^+ = F^+ F_c^+ = F^+ (equivalent if closures are the same) FDs \\alpha \\to \\beta \\alpha \\to \\beta in F_c F_c do not contain extraneous attributes , i.e.: \\forall A \\in \\alpha:(F_c - \\{\\alpha \\to \\beta\\})\\cup \\{(\\alpha - A) \\to \\beta\\} \\not\\equiv F_c \\forall A \\in \\alpha:(F_c - \\{\\alpha \\to \\beta\\})\\cup \\{(\\alpha - A) \\to \\beta\\} \\not\\equiv F_c \\forall B \\in \\beta:(F_c - \\{\\alpha \\to \\beta\\})\\cup \\{\\alpha - (\\beta - B)\\} \\not\\equiv F_c \\forall B \\in \\beta:(F_c - \\{\\alpha \\to \\beta\\})\\cup \\{\\alpha - (\\beta - B)\\} \\not\\equiv F_c The left side of an FD in F_c F_c is unique. Applying the union rule \\alpha \\to \\beta \\alpha \\to \\beta and \\alpha \\to \\gamma \\alpha \\to \\gamma can be combined to \\alpha\\to\\beta\\gamma \\alpha\\to\\beta\\gamma Check if Attribute is extraneous Check if A \u2208 \u03b1 is an extraneous attribute in \u03b1 \u2192 \u03b2 by computing the attribute closure: A \u2208 \u03b1 is extraneous if \u03b2 \u2286 attrClosure(F, \u03b1 \u2212 A) Check if B \u2208 \u03b2 is an extraneous attribute in \u03b1 \u2192 \u03b2 by computing the attribute closure: B \u2208 \u03b2 is extraneous if B \u2208 attrClosure((F \u2212 {\u03b1 \u2192 \u03b2}) \u222a {\u03b1 \u2192 (\u03b2 \u2212 B)}, \u03b1)","title":"Canonical Cover (Minimal Cover)"},{"location":"6-semester/DBS/04-relational-database-theory/#minimal-cover-algorithm","text":"Example in DBS4 slides p 69","title":"Minimal Cover Algorithm"},{"location":"6-semester/DBS/04-relational-database-theory/#normalization-by-decomposition-of-relations","text":"","title":"Normalization by Decomposition of Relations"},{"location":"6-semester/DBS/04-relational-database-theory/#normalization-by-decomposition","text":"Decompose a relation schema \\schema \\schema into multiple relational schemas \\schema_1,\\dots\\schema_n \\schema_1,\\dots\\schema_n to eliminate problems in the original design Normal forms Normal forms describe the quality of a design 1NF, 2NF, 3NF, BCNF, 4NF, \\dots \\dots Prohibit particular functional dependencies in a relation to avoid redundancy, null values, and anomalies Good ER modeling typically directly leads to 3NF (or higher NF) relations Normalization eliminates problems caused by functional dependencies among attributes of any entity type","title":"Normalization by decomposition"},{"location":"6-semester/DBS/04-relational-database-theory/#valid-and-lossless-decompositions","text":"A decomposition is valid if \\schema = \\schema_1 \\cup \\schema_2 \\schema = \\schema_1 \\cup \\schema_2 , i.e. no attributes in \\schema \\schema get lost R_1 := \\pi_{\\schema_1}(R) R_1 := \\pi_{\\schema_1}(R) R_2 := \\pi_{\\schema_2}(R) R_2 := \\pi_{\\schema_2}(R) A decomposition of \\schema \\schema into \\schema_1 \\schema_1 and \\schema_2 \\schema_2 is lossless if the following holds for all possible instances R R of \\schema \\schema (also referred to as lossless-join decomposition): R=R_1 \\Join R_2 R=R_1 \\Join R_2 All data contained in the original instance R R of schema \\schema \\schema must be reconstructible with a natural join from the instances R_1,\\dots,R_n R_1,\\dots,R_n of the new schemas \\schema_1,\\dots,\\schema_n \\schema_1,\\dots,\\schema_n","title":"Valid and Lossless Decompositions"},{"location":"6-semester/DBS/04-relational-database-theory/#formal-characterization-of-a-lossless-decomposition","text":"Given A decomposition of \\schema \\schema into \\schema_1 \\schema_1 and \\schema_2 \\schema_2 F_\\schema F_\\schema is the set of FDs in \\schema \\schema A decomposition is lossless if we can derive at least one of the following FDs: (\\schema_1 \\cap \\schema_2) \\to \\schema_1 \\in F_\\schema^+ (\\schema_1 \\cap \\schema_2) \\to \\schema_1 \\in F_\\schema^+ i.e., common attributes are super key in \\schema_1 \\schema_1 or (\\schema_1 \\cap \\schema_2) \\to \\schema_2 \\in F_\\schema^+ (\\schema_1 \\cap \\schema_2) \\to \\schema_2 \\in F_\\schema^+ i.e., common attributes are super key in \\schema_2 \\schema_2 If this is not the case, the decomposition is said to be lossy","title":"Formal Characterization of a Lossless Decomposition"},{"location":"6-semester/DBS/04-relational-database-theory/#example-of-a-lossy-decomposition","text":"The relationship between guest, pub, and beer got lost. Lossy decomposition sometimes means that the reconstruction leads to additional tuples.","title":"Example of a LOSSY Decomposition"},{"location":"6-semester/DBS/04-relational-database-theory/#dependency-preservation","text":"Second characteristic of a good decomposition All functional dependencies that hold for \\schema \\schema must be verifiable in the new schemas \\schema_1,\\dots,\\schema_n \\schema_1,\\dots,\\schema_n We can check all dependencies locally on \\oneton{\\schema} \\oneton{\\schema} We avoid the alternative: computing the join \\onetonop{\\schema}{\\Join} \\onetonop{\\schema}{\\Join} to test if an FD is violated A decomposition is dependency preserving if $$ F_{\\schema} \\equiv (F_{\\schema_1} \\cup \\cdots \\cup F_{\\schema_n}) $$ i.e. $F^+ \\schema = (F {\\schema_1} \\cup \\cdots \\cup F_{\\schema_n})^+ $ with F_{\\schema_i} F_{\\schema_i} representing functional dependencies that can be checked efficiently on R_i R_i Example The following is dependency preserving, since we can check all FDs locally ( A\\to B A\\to B on R_1 R_1 and B\\to C B\\to C on R_2 R_2 ) The next one is not dependency preserving, since we cannot check B\\to C B\\to C on any of the new relations! Examples in DBS4 slides p 90","title":"Dependency Preservation"},{"location":"6-semester/DBS/04-relational-database-theory/#summary-functional-dependencies","text":"","title":"Summary Functional Dependencies"},{"location":"6-semester/DBS/04-relational-database-theory/#normal-forms","text":"Normal forms define characteristics of relational schemas forbid certain combinations of FDs in a relation avoid redundancies and anomalies guideline to obtain good decompositions","title":"Normal Forms"},{"location":"6-semester/DBS/04-relational-database-theory/#first-normal-form-1nf","text":"A relation \\schema \\schema is in 1NF if the domains of all its attributes are atomic (no composite or set-valued domains)","title":"First Normal Form 1NF"},{"location":"6-semester/DBS/04-relational-database-theory/#third-normal-form-3nf","text":"A relation schema \\schema \\schema is in 3NF if at least one of the following conditions holds for each of its FDs \\alpha \\to B \\alpha \\to B with B \\in \\schema B \\in \\schema B \\in \\alpha B \\in \\alpha , i.e. the FD is trivial \\alpha \\alpha is a super key of R R B B is part of a candidate key for \\schema \\schema Main characteristics 3NF prevents (some) transitive dependencies Exception: Condition 3","title":"Third Normal Form 3NF"},{"location":"6-semester/DBS/04-relational-database-theory/#non-example","text":"The relation is not in 3NF","title":"Non-Example"},{"location":"6-semester/DBS/04-relational-database-theory/#eliminates-transitive-dependencies","text":"","title":"Eliminates Transitive Dependencies"},{"location":"6-semester/DBS/04-relational-database-theory/#boyce-codd-normal-form-bcnf","text":"A relation schema \\schema \\schema is in BCNF if at least one of the following conditions holds for each of its FDs \\alpha \\to B \\alpha \\to B with B \\in \\schema B \\in \\schema B \\in \\alpha B \\in \\alpha , i.e. the FD is trivial \\alpha \\alpha is a super key of R R Main characteristics Difference to 3NF: no third option ( B B is part of a candidate key for \\schema \\schema ) BCNF is more strict than 3NF (\u201cincludes\u201d 3NF) BCNF prevents all transitive dependencies","title":"Boyce Codd Normal Form BCNF"},{"location":"6-semester/DBS/04-relational-database-theory/#example","text":"3NF vs. BCNF Is the relation in 3NF? YES Is the relation in BCNF? NO","title":"Example"},{"location":"6-semester/DBS/04-relational-database-theory/#decomposition","text":"It is always possible to decompose a relational schema \\schema \\schema with FDs F F into 3NF relational schemas \\oneton{\\schema} \\oneton{\\schema} so that the decomposition is lossless dependency preserving BCNF relational schema \\oneton{\\schema} \\oneton{\\schema} so that the decomposition is lossless It is not always possible to create a BCNF decomposition \\oneton{\\schema} \\oneton{\\schema} of \\schema \\schema that is dependency preserving","title":"Decomposition"},{"location":"6-semester/DBS/04-relational-database-theory/#decomposition-algorithm-for-bcnf","text":"Instead of computing F^+ F^+ to check \\alpha \\alpha for its super key characteristics, we can compute \\alpha^+ \\alpha^+ for this purpose Example in DBS4 slides p 133-","title":"Decomposition Algorithm for BCNF"},{"location":"6-semester/DBS/04-relational-database-theory/#summary","text":"Normal Form Main Characteristics 1NF Only atomic attributes 3NF Some transitive dependencies BCNF No transitive dependencies In practice, if a BCNF composition is impossible without loosing dependency preservation, we go for the 3NF decomposition (although it allows for some redundancy) Decomp. algorithms for all normal forms guarantee lossless decompositions. Dependency preservation can only be guaranteed until 3NF.","title":"Summary"},{"location":"6-semester/DBS/05-sql/","text":"SQL \u00b6 \\newcommand{\\relationRaw}[3]{ \\newcommand{\\pk}{\\underline} #3\\mathbf{#1}#3#3:\\{[\\mathrm{#2}]\\} } \\newcommand{\\relation}[2]{\\relationRaw{#1}{#2}{}} \\newcommand{\\relational}[2]{\\relationRaw{#1}{#2}{&}} \\nonumber \\newcommand{\\relationRaw}[3]{ \\newcommand{\\pk}{\\underline} #3\\mathbf{#1}#3#3:\\{[\\mathrm{#2}]\\} } \\newcommand{\\relation}[2]{\\relationRaw{#1}{#2}{}} \\newcommand{\\relational}[2]{\\relationRaw{#1}{#2}{&}} \\nonumber Learning Goals Explain and use the SQL data model Create non-trivial database tables Modify non-trivial database tables Create non-trivial SQL statements SQL \u00b6 SQL is a declarative query language (\" what \" not \"how\") It consists of multiple parts Data Definition Language (DDL) Create/change the schema create, alter, drop Data Manipulation Language (DML) Changes to an instance insert, update, delete Data Query Language (DQL) Evaluate queries on an instance select * from where ... Transaction Control Language (TCL) Controls transactions commit, rollback Data Control Language (DCL) Grant/revoke access rights grant, revoke Basic Data Definitions \u00b6 Data Types character (n), char (n) character varying (n), varchar (n) integer, smallint numeric (p,s), decimal (p,s), ... p - precision: max number of digits in total s - scale: number of digits after the comma real, double blob or raw for very large binary data clob for large string attributes data for dates xml for xml documents ... varchar vs char Both are limited to a length of n char always uses n bytes varchar uses only the required place, plus length information Create Tables \u00b6 1 2 3 4 CREATE TABLE professor ( empidinteger UNIQUE NOT NULL , namevarchar ( 10 ) NOT NULL , rankchar ( 2 )); Is equal to 1 2 3 4 CREATE TABLE professor ( empidinteger PRIMARY KEY , namevarchar ( 10 ) NOT NULL , rankchar ( 2 )); The PRIMARY KEY constraint includes the NOT NULL and UNIQUE constraints. Foreign Keys \u00b6 1 2 3 4 5 6 7 8 CREATE TABLE course ( courseID integer , title varchar ( 30 ) NOT NULL , ects integer , taughtby integer , PRIMARY KEY ( courseID ), FOREIGN KEY ( taughtBy ) REFERENCES professor ( empID ) ); Default Values \u00b6 When inserting data into a table, all values that are not explicitly stated are set to null (standard default value). When defining a table, we can specify another default value 1 2 3 4 5 6 7 CREATE TABLE wine ( wineID integer NOT NULL , name varchar ( 20 ) NOT NULL , color varchar ( 10 ) DEFAULT 'red' , year integer , vineyard varchar ( 20 ) ); Here the default value of color will be 'red' Sequence Number Generators \u00b6 Sequence number generators automatically create continuous identifiers. 1 2 3 4 5 6 7 8 9 CREATE SEQUENCE serial START 101 ; CREATE TABLE wine ( wineID integer PRIMARY KEY DEFAULT nextval ( \u2019 serial \u2019 ), name varchar ( 20 ) NOT NULL , color varchar ( 10 ), year integer , vineyard varchar ( 20 ) ); Alter Tables \u00b6 Initial version 1 2 3 4 5 CREATE TABLE professor ( empid integer NOT NULL , name varchar ( 10 ) NOT NULL , rank char ( 2 ) ); Change Table Definitions \u00b6 Add an attribute 1 2 ALTER TABLE professor ADD COLUMN ( office integer ); Delete an attribute 1 2 ALTER TABLE professor DROP COLUMN name ; Change an attribute type 1 2 ALTER TABLE professor ALTER COLUMN name type varchar ( 30 ); Deletion of a Table \u00b6 Delete a table 1 DROP TABLE professor ; Delete the content of a table 1 TRUNCATE TABLE professor ; Cannot be used if there is another table with a foreign key referencing the table that we want to delete. Data Insertion \u00b6 Standard insertion w3schools Insert Into : 1 2 INSERT INTO table_name ( column1 , column2 , column3 , ...) VALUES ( value1 , value2 , value3 , ...); 1 2 3 4 INSERT INTO takees SELECT studid , courseid FROM student , course WHERE title = 'Logics' ; Rights Management \u00b6 DCL Grant access rights 1 2 3 GRANT select , update ON professor TO some user , another user ; 1 2 3 GRANT select ( empid ), update ( office ) ON professor TO some user , another user ; Revoke access rights 1 2 3 REVOKE ALL PRIVILEGES ON professor FROM some user , another user ; Privileges (rights) on tables, columns,. . . : select , insert , update , delete , rule , references , trigger Query Language \u00b6 Basic building block of an SQL query (SFW block) SELECT <list of columns> projection list with arithmetic operators and aggregation functions FROM <list of tables> list of involved tables, with optional renaming WHERE <condition> selection and join conditions, nested queries Relational algebra \u2192 SQL projection \u03c0 \u2192 SELECT cross product \u00d7 \u2192 FROM selection \u03c3 \u2192 WHERE Cross Product \u00b6 If the from clause enumerates more than a single table, the cross product is computed. 1 2 SELECT * FROM wine , producer ; The result is the set of all combinations of tuples in the involved tables! Duplicate Elimination \u00b6 The DISTINCT keyword removes duplicated entries. 1 2 SELECT name FROM wine ; 1 2 SELECT DISTINCT name FROM wine ; Corresponds to the projection operation in relational algebra! Set Operations \u00b6 Set operations require union compatibility: same number of attributes with compatible domains. domains are identical domains are based on characters (length does not matter) domains are based on numerical values (exact type does not matter, e.g., integer and float) Result schema: column names of the first table For set operations, duplicate elimination (e.g., UNION DISTINCT ) is the default! 1 2 3 4 5 ( SELECT name FROM assistant ) UNION ( SELECT name FROM assistant ); 1 2 3 4 5 ( SELECT name FROM assistant ) UNION DISTINCT ( SELECT name FROM assistant ); 1 2 3 4 5 ( SELECT name FROM assistant ) UNION ALL ( SELECT name FROM assistant ); Intersection ( INTERSECT ) and set minus ( EXCEPT ) are also supported. Nested Queries \u00b6 Subqueries are necessary for comparisons to sets of values. Standard comparisons in combination with quantifiers: ALL or ANY Special keywords to access sets: IN and EXISTS Uncorrelated Subqueries \u00b6 Notation: attribute IN ( SFW block ) 1 2 3 4 SELECT name FROM professor WHERE empid IN ( SELECT taughtby FROM course ); Comparison of a value to a set of values Negation in combination with the IN keyword Simulation of the difference operator $$ \\mathrm{\\pi_{vineyard}(producer) - \\pi_{vineyard}(wine)} $$ corresponding SQL query 1 2 3 4 SELECT vineyard FROM producer WHERE vineyard NOT IN ( sqSELECT vineyard FROM wine ); Correlated Subqueries \u00b6 The subquery is correlated to the parent query. See the WHERE clause: 1 2 3 4 5 SELECT name FROM professor p WHERE EXISTS ( SELECT * FROM course v WHERE v . taughtby = p . empid ); Computes names of professors that teach any courses. Quantifiers IN vs EXISTS \u00b6 IN Is the \u201cleft tuple\u201d contained in the \u201cright set\u201d? Example: (studid, courseid) IN (SELECT * FROM takes); EXISTS Is the \u201cright set\u201d nonempty? Example: EXISTS (SELECT * FROM takes WHERE ...); There are more quantifiers: ANY and ALL Advanced SQL \u00b6 Extension of the SFW block FROM clause: additional join variants WHERE clause: additional types of constraints and quantifiers SELECT clause: application of scalar operations and aggregate functions Joins \u00b6 Join variants CROSS JOIN NATURAL JOIN JOIN or INNER JOIN LEFT , RIGHT , or FULL OUTER JOIN Standard formulation: 1 2 3 SELECT * FROM R1 , R2 WHERE R1 . A = R2 . B ; Alternative Formulation 1 2 3 SELECT * FROM R1 JOIN R2 ON R1 . A = R2 . B ; See joins visualized with this tool Aggregate Functions \u00b6 How can we formulate the following queries in SQL? Average price of all articles on sale Total sales volume of all sold products Aggregate functions compute new values for a column, e.g., the sum or the average of all values in a column. Aggregate functions: AVG , MAX , MIN , COUNT , SUM The argument columns (except in case of COUNT(\u2217) ) can optionally be accompanied by the keyword DISTINCT and ALL . DISTINCT before evaluating the aggregate function, duplicates are removed ALL duplicates are considered for evaluation ( default !) Null values are removed before evaluation (except in case of COUNT(\u2217) ). Examples \u00b6 Number of wines 1 2 SELECT COUNT ( * ) AS number FROM wine ; The number of different regions that produce wine 1 2 SELECT COUNT ( DISTINCT region ) FROM producer ; The names and years of wines that are older than the average 1 2 3 SELECT name , year FROM wine WHERE year < ( SELECT AVG ( year ) FROM wine ); Aggregate Functions in the WHERE Clause \u00b6 Aggregate functions produce a single value \\leadsto \\leadsto usable in comparison with constants in the WHERE clause. All vineyards producing a single wine 1 2 3 SELECT * FROM producer e WHERE 1 = ( SELECT COUNT ( * ) FROM wine w WHERE w . vineyard = e . vineyard ); Nesting of Aggregate Functions \u00b6 Nesting of aggregate functions is not allowed! WRONG 1 SELECT f1 ( f2 ( A )) AS result FROM R ...; Instead 1 2 SELECT f1 ( temp ) AS result FROM ( SELECT f2 ( A ) AS temp FROM R ...); Grouping \u00b6 Computation of the aggregate function per group Notation 1 2 3 4 5 SELECT ... FROM ... [ WHERE ... ] [ GROUP BY columnList ] [ HAVING condition ]; Computation of the aggregate function per group 1 2 3 SELECT taughtBy , SUM ( ects ) FROM course GROUP BY taughtBy ; All tuples with the same value for column taughtBy form a group The sum is computed for each group Mistakes \u00b6 SQL generates one result tuple per group All columns referenced in the SELECT clause must either be listed in the GROUP BY clause or involved only in aggregate functions The HAVING Clause \u00b6 Example in slides p 122 Null Values \u00b6 Null values may lead to unexpected query results. 1 2 SELECT COUNT ( semester ) FROM student WHERE semester < 13 OR semester >= 13 ; And 1 SELECT COUNT ( semester ) FROM student ; Produces the same result, because tuples with null values in column semester are not counted Arithmetic expressions \"Propagation\" of null values null + 1 \\leadsto \\leadsto null null * 0 \\leadsto \\leadsto null Comparison Operations SQL has a three-valued logic: true , false , and unknown If at least one argument is null, then the result is unknown studid = 5 \\leadsto \\leadsto unknown whenever studid is null Logical expressions are evaluated according to the following tables Null Values in Where Clause and Grouping \u00b6 WHERE clause The WHERE clause forwards only tuples evaluated to true Tuples evaluated to unknown will not be part of the result Grouping null is interpreted as an independent value Results in its own group Recursion \u00b6 Information in slides p. 146 Which courses need to be taken before taking course \u201cTheory of Science\u201d? \u200b \\relation{requires}{predecessor, successor} \\relation{requires}{predecessor, successor} \u200b \\relation{course}{courseid, title, ects, taughtBy} \\relation{course}{courseid, title, ects, taughtBy} Non-Recursive This query only finds direct predecessors: 1 2 3 4 SELECT predecessor FROM requires , course WHERE successor = courseid AND title = \u2019 Theory of Science \u2019 ; Recursive 1 2 3 4 5 6 7 8 9 10 11 WITH RECURSIVE transitiveCourse ( pred , succ ) AS ( SELECT predecessor , successor FROM requires UNION SELECT DISTINCT t . pred , r . succ FROM transitiveCourse t , requires r WHERE t . succ = r . succ ) SELECT * FROM transitiveCourse ORDER BY ( pred , succ ) ASC ; General Recursive SQL \u00b6 1 2 3 4 5 6 7 8 9 WITH RECURSIVE mytable ( number ) AS ( VALUES ( 1 ) | non - recursive part UNION | UNION SELECT number + 1 | recursive part FROM mytable | only this part may reference WHERE number < 100 | mytable ) SELECT sum ( number ) | main query FROM mytable ; Result: 5050 Avoid Infinite Recursion \u00b6 Most DBMS have a parameter that limits maximum recursion depth Encode it directly in the query 1 2 3 4 5 6 7 8 9 10 11 12 WITH RECURSIVE transitiveCourse ( pred , succ , depth ) AS ( SELECT predecessor , successor , 0 FROM requires UNION SELECT DISTINCT t . pred , r . successor , t . depth + 1 FROM transitiveCourse t , requires r WHERE t . succ = r . predecessor AND t . depth < 1 ) SELECT * FROM transitiveCourse ORDER BY ( pred , succ ) ASC ; Limiting Size of Results \u00b6 Using LIMIT is accepted in exam solutions 1 SELECT * FROM student LIMIT 5 ; Other possible solutions can be seen in DBS5 Slides p. 169 Views \u00b6 Examples of views 1 2 3 4 CREATE VIEW profsAndtheirCourses AS SELECT c . title , p . name FROM professor p , course c WHERE p . empid = c . taughtBy ; 1 SELECT * FROM profsAndtheirCourses ; 1 2 3 4 5 CREATE VIEW ectsPerStud AS SELECT s . name , t . studid , SUM ( c . ects ) AS sum FROM student s , takes t , course c WHERE t . courseid = c . courseid AND s . studid = t . studid GROUP BY s . name , t . studid ; 1 SELECT sum FROM ectsPerStud ; Views can be used to represent derived attributes (ER diagram). Altering Views \u00b6 REPLACE VIEW expects the same columns in the same order with the same types. 1 2 3 4 CREATE OR REPLACE VIEW profsAndtheirCourses AS SELECT c . title , p . name FROM professor p , course c WHERE p . empid = c . taughtBy ; Alternative: Delete the view and recreate it afterwards, or non-standard SQL extensions, e.g., PostgresSQL\u2019s ALTER VIEW 1 2 3 4 5 6 7 DROP VIEW ectsPerStud ; CREATE VIEW ectsPerStud AS SELECT s . name , t . studid , SUM ( c . ects ) AS sum FROM student s , takes t , course c WHERE t . courseid = c . courseid AND s . studid = t . studid GROUP BY s . name , t . studid ; Views vs Materialized Views \u00b6 (Dynamic) view Represents a macro of a query The query result is not pre-computed but computed when used Materialized View The query result is pre-computed Computation load before any queries are executed More on views in DBS5 slides p. 184 Integrity Constraints \u00b6 Additional instrument to avoid inconsistency. Try to avoid insertion of inconsistent data Static Integrity Constraints \u00b6 Each instance of a database must fulfill all static integrity constraints. 1 2 CREATE TABLE professor ... ... empid integer NOT NULL ... Restricting the domain of valid values 1 2 CREATE TABLE student ... ... CHECK semester BETWEEN 1 AND 20 ... Enumeration of valid values 1 2 CREATE TABLE professor ... ... CHECK rank IN ( 'C2' , 'C3' , 'C4' ) ... Definition of user-defined domains 1 2 3 CREATE DOMAIN wineColor varchar ( 5 ) DEFAULT \u2019 red \u2019 CHECK ( VALUE IN ( \u2019 red \u2019 , \u2019 white \u2019 , \u2019 rose \u2019 )); 1 2 3 4 5 CREATE TABLE wine ( wineID int PRIMARY KEY , name varchar ( 20 ) NOT NULL , color wineColor , ...); Dynamic Integrity Constraints \u00b6 Referential integrity requires that foreign keys must always reference existing tuples or be null. What happens if there is no professor with empid 007? 1 2 insert into course values ( 5100 , \u2019 Spying for Dummies \u2019 , 4 , 007 ) And how can we prevent the insertion? Definition of Keys \u00b6 Candidate keys UNIQUE A table can have multiple UNIQUE constraints Allows null values! Primary Keys PRIMARY KEY At most one per table Implies UNIQUE NOT NULL Foreign Keys FOREIGN KEY Allows null values Handling Updates \u00b6 Dynamic integrity constraints need to be fulfilled by each change of a database. In response to changes of referenced data: Rejection of updates (default behavior) Propagation of updates (CASCADE) Set references to \u201cunknown\u201d (SET null) In addition available in PostgreSQL Set to a default value (SET DEFAULT) Examples in DBS5 slides p. 204 Complex Constraints \u00b6 1 2 3 4 5 6 7 8 9 10 11 CREATE TABLE grades ( studid integer REFERENCES student ON DELETE CASCADE , courseid integer REFERENCES course , grade numeric ( 2 , 1 ) CHECK ( grade BETWEEN 0 . 7 AND 5 . 0 ), PRIMARY KEY ( studid , courseid ) CONSTRAINT hasTaken CHECK ( EXISTS ( SELECT * FROM takes h WHERE h . courseid = grades . courseid AND h . studid = grades . studid )) ); The CHECK clause is evaluated for each update or insert Operation is rejected if the check is evaluated to false! True and unknown do not violate the constraint! Not (yet) supported by PostgreSQL: ERROR: cannot use subquery in check constraint Workaround by using triggers Appendix \u00b6 See Appendix in DBS5 slides p. 222","title":"SQL"},{"location":"6-semester/DBS/05-sql/#sql","text":"\\newcommand{\\relationRaw}[3]{ \\newcommand{\\pk}{\\underline} #3\\mathbf{#1}#3#3:\\{[\\mathrm{#2}]\\} } \\newcommand{\\relation}[2]{\\relationRaw{#1}{#2}{}} \\newcommand{\\relational}[2]{\\relationRaw{#1}{#2}{&}} \\nonumber \\newcommand{\\relationRaw}[3]{ \\newcommand{\\pk}{\\underline} #3\\mathbf{#1}#3#3:\\{[\\mathrm{#2}]\\} } \\newcommand{\\relation}[2]{\\relationRaw{#1}{#2}{}} \\newcommand{\\relational}[2]{\\relationRaw{#1}{#2}{&}} \\nonumber Learning Goals Explain and use the SQL data model Create non-trivial database tables Modify non-trivial database tables Create non-trivial SQL statements","title":"SQL"},{"location":"6-semester/DBS/05-sql/#sql_1","text":"SQL is a declarative query language (\" what \" not \"how\") It consists of multiple parts Data Definition Language (DDL) Create/change the schema create, alter, drop Data Manipulation Language (DML) Changes to an instance insert, update, delete Data Query Language (DQL) Evaluate queries on an instance select * from where ... Transaction Control Language (TCL) Controls transactions commit, rollback Data Control Language (DCL) Grant/revoke access rights grant, revoke","title":"SQL"},{"location":"6-semester/DBS/05-sql/#basic-data-definitions","text":"Data Types character (n), char (n) character varying (n), varchar (n) integer, smallint numeric (p,s), decimal (p,s), ... p - precision: max number of digits in total s - scale: number of digits after the comma real, double blob or raw for very large binary data clob for large string attributes data for dates xml for xml documents ... varchar vs char Both are limited to a length of n char always uses n bytes varchar uses only the required place, plus length information","title":"Basic Data Definitions"},{"location":"6-semester/DBS/05-sql/#create-tables","text":"1 2 3 4 CREATE TABLE professor ( empidinteger UNIQUE NOT NULL , namevarchar ( 10 ) NOT NULL , rankchar ( 2 )); Is equal to 1 2 3 4 CREATE TABLE professor ( empidinteger PRIMARY KEY , namevarchar ( 10 ) NOT NULL , rankchar ( 2 )); The PRIMARY KEY constraint includes the NOT NULL and UNIQUE constraints.","title":"Create Tables"},{"location":"6-semester/DBS/05-sql/#foreign-keys","text":"1 2 3 4 5 6 7 8 CREATE TABLE course ( courseID integer , title varchar ( 30 ) NOT NULL , ects integer , taughtby integer , PRIMARY KEY ( courseID ), FOREIGN KEY ( taughtBy ) REFERENCES professor ( empID ) );","title":"Foreign Keys"},{"location":"6-semester/DBS/05-sql/#default-values","text":"When inserting data into a table, all values that are not explicitly stated are set to null (standard default value). When defining a table, we can specify another default value 1 2 3 4 5 6 7 CREATE TABLE wine ( wineID integer NOT NULL , name varchar ( 20 ) NOT NULL , color varchar ( 10 ) DEFAULT 'red' , year integer , vineyard varchar ( 20 ) ); Here the default value of color will be 'red'","title":"Default Values"},{"location":"6-semester/DBS/05-sql/#sequence-number-generators","text":"Sequence number generators automatically create continuous identifiers. 1 2 3 4 5 6 7 8 9 CREATE SEQUENCE serial START 101 ; CREATE TABLE wine ( wineID integer PRIMARY KEY DEFAULT nextval ( \u2019 serial \u2019 ), name varchar ( 20 ) NOT NULL , color varchar ( 10 ), year integer , vineyard varchar ( 20 ) );","title":"Sequence Number Generators"},{"location":"6-semester/DBS/05-sql/#alter-tables","text":"Initial version 1 2 3 4 5 CREATE TABLE professor ( empid integer NOT NULL , name varchar ( 10 ) NOT NULL , rank char ( 2 ) );","title":"Alter Tables"},{"location":"6-semester/DBS/05-sql/#change-table-definitions","text":"Add an attribute 1 2 ALTER TABLE professor ADD COLUMN ( office integer ); Delete an attribute 1 2 ALTER TABLE professor DROP COLUMN name ; Change an attribute type 1 2 ALTER TABLE professor ALTER COLUMN name type varchar ( 30 );","title":"Change Table Definitions"},{"location":"6-semester/DBS/05-sql/#deletion-of-a-table","text":"Delete a table 1 DROP TABLE professor ; Delete the content of a table 1 TRUNCATE TABLE professor ; Cannot be used if there is another table with a foreign key referencing the table that we want to delete.","title":"Deletion of a Table"},{"location":"6-semester/DBS/05-sql/#data-insertion","text":"Standard insertion w3schools Insert Into : 1 2 INSERT INTO table_name ( column1 , column2 , column3 , ...) VALUES ( value1 , value2 , value3 , ...); 1 2 3 4 INSERT INTO takees SELECT studid , courseid FROM student , course WHERE title = 'Logics' ;","title":"Data Insertion"},{"location":"6-semester/DBS/05-sql/#rights-management","text":"DCL Grant access rights 1 2 3 GRANT select , update ON professor TO some user , another user ; 1 2 3 GRANT select ( empid ), update ( office ) ON professor TO some user , another user ; Revoke access rights 1 2 3 REVOKE ALL PRIVILEGES ON professor FROM some user , another user ; Privileges (rights) on tables, columns,. . . : select , insert , update , delete , rule , references , trigger","title":"Rights Management"},{"location":"6-semester/DBS/05-sql/#query-language","text":"Basic building block of an SQL query (SFW block) SELECT <list of columns> projection list with arithmetic operators and aggregation functions FROM <list of tables> list of involved tables, with optional renaming WHERE <condition> selection and join conditions, nested queries Relational algebra \u2192 SQL projection \u03c0 \u2192 SELECT cross product \u00d7 \u2192 FROM selection \u03c3 \u2192 WHERE","title":"Query Language"},{"location":"6-semester/DBS/05-sql/#cross-product","text":"If the from clause enumerates more than a single table, the cross product is computed. 1 2 SELECT * FROM wine , producer ; The result is the set of all combinations of tuples in the involved tables!","title":"Cross Product"},{"location":"6-semester/DBS/05-sql/#duplicate-elimination","text":"The DISTINCT keyword removes duplicated entries. 1 2 SELECT name FROM wine ; 1 2 SELECT DISTINCT name FROM wine ; Corresponds to the projection operation in relational algebra!","title":"Duplicate Elimination"},{"location":"6-semester/DBS/05-sql/#set-operations","text":"Set operations require union compatibility: same number of attributes with compatible domains. domains are identical domains are based on characters (length does not matter) domains are based on numerical values (exact type does not matter, e.g., integer and float) Result schema: column names of the first table For set operations, duplicate elimination (e.g., UNION DISTINCT ) is the default! 1 2 3 4 5 ( SELECT name FROM assistant ) UNION ( SELECT name FROM assistant ); 1 2 3 4 5 ( SELECT name FROM assistant ) UNION DISTINCT ( SELECT name FROM assistant ); 1 2 3 4 5 ( SELECT name FROM assistant ) UNION ALL ( SELECT name FROM assistant ); Intersection ( INTERSECT ) and set minus ( EXCEPT ) are also supported.","title":"Set Operations"},{"location":"6-semester/DBS/05-sql/#nested-queries","text":"Subqueries are necessary for comparisons to sets of values. Standard comparisons in combination with quantifiers: ALL or ANY Special keywords to access sets: IN and EXISTS","title":"Nested Queries"},{"location":"6-semester/DBS/05-sql/#uncorrelated-subqueries","text":"Notation: attribute IN ( SFW block ) 1 2 3 4 SELECT name FROM professor WHERE empid IN ( SELECT taughtby FROM course ); Comparison of a value to a set of values Negation in combination with the IN keyword Simulation of the difference operator $$ \\mathrm{\\pi_{vineyard}(producer) - \\pi_{vineyard}(wine)} $$ corresponding SQL query 1 2 3 4 SELECT vineyard FROM producer WHERE vineyard NOT IN ( sqSELECT vineyard FROM wine );","title":"Uncorrelated Subqueries"},{"location":"6-semester/DBS/05-sql/#correlated-subqueries","text":"The subquery is correlated to the parent query. See the WHERE clause: 1 2 3 4 5 SELECT name FROM professor p WHERE EXISTS ( SELECT * FROM course v WHERE v . taughtby = p . empid ); Computes names of professors that teach any courses.","title":"Correlated Subqueries"},{"location":"6-semester/DBS/05-sql/#quantifiers-in-vs-exists","text":"IN Is the \u201cleft tuple\u201d contained in the \u201cright set\u201d? Example: (studid, courseid) IN (SELECT * FROM takes); EXISTS Is the \u201cright set\u201d nonempty? Example: EXISTS (SELECT * FROM takes WHERE ...); There are more quantifiers: ANY and ALL","title":"Quantifiers IN vs EXISTS"},{"location":"6-semester/DBS/05-sql/#advanced-sql","text":"Extension of the SFW block FROM clause: additional join variants WHERE clause: additional types of constraints and quantifiers SELECT clause: application of scalar operations and aggregate functions","title":"Advanced SQL"},{"location":"6-semester/DBS/05-sql/#joins","text":"Join variants CROSS JOIN NATURAL JOIN JOIN or INNER JOIN LEFT , RIGHT , or FULL OUTER JOIN Standard formulation: 1 2 3 SELECT * FROM R1 , R2 WHERE R1 . A = R2 . B ; Alternative Formulation 1 2 3 SELECT * FROM R1 JOIN R2 ON R1 . A = R2 . B ; See joins visualized with this tool","title":"Joins"},{"location":"6-semester/DBS/05-sql/#aggregate-functions","text":"How can we formulate the following queries in SQL? Average price of all articles on sale Total sales volume of all sold products Aggregate functions compute new values for a column, e.g., the sum or the average of all values in a column. Aggregate functions: AVG , MAX , MIN , COUNT , SUM The argument columns (except in case of COUNT(\u2217) ) can optionally be accompanied by the keyword DISTINCT and ALL . DISTINCT before evaluating the aggregate function, duplicates are removed ALL duplicates are considered for evaluation ( default !) Null values are removed before evaluation (except in case of COUNT(\u2217) ).","title":"Aggregate Functions"},{"location":"6-semester/DBS/05-sql/#examples","text":"Number of wines 1 2 SELECT COUNT ( * ) AS number FROM wine ; The number of different regions that produce wine 1 2 SELECT COUNT ( DISTINCT region ) FROM producer ; The names and years of wines that are older than the average 1 2 3 SELECT name , year FROM wine WHERE year < ( SELECT AVG ( year ) FROM wine );","title":"Examples"},{"location":"6-semester/DBS/05-sql/#aggregate-functions-in-the-where-clause","text":"Aggregate functions produce a single value \\leadsto \\leadsto usable in comparison with constants in the WHERE clause. All vineyards producing a single wine 1 2 3 SELECT * FROM producer e WHERE 1 = ( SELECT COUNT ( * ) FROM wine w WHERE w . vineyard = e . vineyard );","title":"Aggregate Functions in the WHERE Clause"},{"location":"6-semester/DBS/05-sql/#nesting-of-aggregate-functions","text":"Nesting of aggregate functions is not allowed! WRONG 1 SELECT f1 ( f2 ( A )) AS result FROM R ...; Instead 1 2 SELECT f1 ( temp ) AS result FROM ( SELECT f2 ( A ) AS temp FROM R ...);","title":"Nesting of Aggregate Functions"},{"location":"6-semester/DBS/05-sql/#grouping","text":"Computation of the aggregate function per group Notation 1 2 3 4 5 SELECT ... FROM ... [ WHERE ... ] [ GROUP BY columnList ] [ HAVING condition ]; Computation of the aggregate function per group 1 2 3 SELECT taughtBy , SUM ( ects ) FROM course GROUP BY taughtBy ; All tuples with the same value for column taughtBy form a group The sum is computed for each group","title":"Grouping"},{"location":"6-semester/DBS/05-sql/#mistakes","text":"SQL generates one result tuple per group All columns referenced in the SELECT clause must either be listed in the GROUP BY clause or involved only in aggregate functions","title":"Mistakes"},{"location":"6-semester/DBS/05-sql/#the-having-clause","text":"Example in slides p 122","title":"The HAVING Clause"},{"location":"6-semester/DBS/05-sql/#null-values","text":"Null values may lead to unexpected query results. 1 2 SELECT COUNT ( semester ) FROM student WHERE semester < 13 OR semester >= 13 ; And 1 SELECT COUNT ( semester ) FROM student ; Produces the same result, because tuples with null values in column semester are not counted Arithmetic expressions \"Propagation\" of null values null + 1 \\leadsto \\leadsto null null * 0 \\leadsto \\leadsto null Comparison Operations SQL has a three-valued logic: true , false , and unknown If at least one argument is null, then the result is unknown studid = 5 \\leadsto \\leadsto unknown whenever studid is null Logical expressions are evaluated according to the following tables","title":"Null Values"},{"location":"6-semester/DBS/05-sql/#null-values-in-where-clause-and-grouping","text":"WHERE clause The WHERE clause forwards only tuples evaluated to true Tuples evaluated to unknown will not be part of the result Grouping null is interpreted as an independent value Results in its own group","title":"Null Values in Where Clause and Grouping"},{"location":"6-semester/DBS/05-sql/#recursion","text":"Information in slides p. 146 Which courses need to be taken before taking course \u201cTheory of Science\u201d? \u200b \\relation{requires}{predecessor, successor} \\relation{requires}{predecessor, successor} \u200b \\relation{course}{courseid, title, ects, taughtBy} \\relation{course}{courseid, title, ects, taughtBy} Non-Recursive This query only finds direct predecessors: 1 2 3 4 SELECT predecessor FROM requires , course WHERE successor = courseid AND title = \u2019 Theory of Science \u2019 ; Recursive 1 2 3 4 5 6 7 8 9 10 11 WITH RECURSIVE transitiveCourse ( pred , succ ) AS ( SELECT predecessor , successor FROM requires UNION SELECT DISTINCT t . pred , r . succ FROM transitiveCourse t , requires r WHERE t . succ = r . succ ) SELECT * FROM transitiveCourse ORDER BY ( pred , succ ) ASC ;","title":"Recursion"},{"location":"6-semester/DBS/05-sql/#general-recursive-sql","text":"1 2 3 4 5 6 7 8 9 WITH RECURSIVE mytable ( number ) AS ( VALUES ( 1 ) | non - recursive part UNION | UNION SELECT number + 1 | recursive part FROM mytable | only this part may reference WHERE number < 100 | mytable ) SELECT sum ( number ) | main query FROM mytable ; Result: 5050","title":"General Recursive SQL"},{"location":"6-semester/DBS/05-sql/#avoid-infinite-recursion","text":"Most DBMS have a parameter that limits maximum recursion depth Encode it directly in the query 1 2 3 4 5 6 7 8 9 10 11 12 WITH RECURSIVE transitiveCourse ( pred , succ , depth ) AS ( SELECT predecessor , successor , 0 FROM requires UNION SELECT DISTINCT t . pred , r . successor , t . depth + 1 FROM transitiveCourse t , requires r WHERE t . succ = r . predecessor AND t . depth < 1 ) SELECT * FROM transitiveCourse ORDER BY ( pred , succ ) ASC ;","title":"Avoid Infinite Recursion"},{"location":"6-semester/DBS/05-sql/#limiting-size-of-results","text":"Using LIMIT is accepted in exam solutions 1 SELECT * FROM student LIMIT 5 ; Other possible solutions can be seen in DBS5 Slides p. 169","title":"Limiting Size of Results"},{"location":"6-semester/DBS/05-sql/#views","text":"Examples of views 1 2 3 4 CREATE VIEW profsAndtheirCourses AS SELECT c . title , p . name FROM professor p , course c WHERE p . empid = c . taughtBy ; 1 SELECT * FROM profsAndtheirCourses ; 1 2 3 4 5 CREATE VIEW ectsPerStud AS SELECT s . name , t . studid , SUM ( c . ects ) AS sum FROM student s , takes t , course c WHERE t . courseid = c . courseid AND s . studid = t . studid GROUP BY s . name , t . studid ; 1 SELECT sum FROM ectsPerStud ; Views can be used to represent derived attributes (ER diagram).","title":"Views"},{"location":"6-semester/DBS/05-sql/#altering-views","text":"REPLACE VIEW expects the same columns in the same order with the same types. 1 2 3 4 CREATE OR REPLACE VIEW profsAndtheirCourses AS SELECT c . title , p . name FROM professor p , course c WHERE p . empid = c . taughtBy ; Alternative: Delete the view and recreate it afterwards, or non-standard SQL extensions, e.g., PostgresSQL\u2019s ALTER VIEW 1 2 3 4 5 6 7 DROP VIEW ectsPerStud ; CREATE VIEW ectsPerStud AS SELECT s . name , t . studid , SUM ( c . ects ) AS sum FROM student s , takes t , course c WHERE t . courseid = c . courseid AND s . studid = t . studid GROUP BY s . name , t . studid ;","title":"Altering Views"},{"location":"6-semester/DBS/05-sql/#views-vs-materialized-views","text":"(Dynamic) view Represents a macro of a query The query result is not pre-computed but computed when used Materialized View The query result is pre-computed Computation load before any queries are executed More on views in DBS5 slides p. 184","title":"Views vs Materialized Views"},{"location":"6-semester/DBS/05-sql/#integrity-constraints","text":"Additional instrument to avoid inconsistency. Try to avoid insertion of inconsistent data","title":"Integrity Constraints"},{"location":"6-semester/DBS/05-sql/#static-integrity-constraints","text":"Each instance of a database must fulfill all static integrity constraints. 1 2 CREATE TABLE professor ... ... empid integer NOT NULL ... Restricting the domain of valid values 1 2 CREATE TABLE student ... ... CHECK semester BETWEEN 1 AND 20 ... Enumeration of valid values 1 2 CREATE TABLE professor ... ... CHECK rank IN ( 'C2' , 'C3' , 'C4' ) ... Definition of user-defined domains 1 2 3 CREATE DOMAIN wineColor varchar ( 5 ) DEFAULT \u2019 red \u2019 CHECK ( VALUE IN ( \u2019 red \u2019 , \u2019 white \u2019 , \u2019 rose \u2019 )); 1 2 3 4 5 CREATE TABLE wine ( wineID int PRIMARY KEY , name varchar ( 20 ) NOT NULL , color wineColor , ...);","title":"Static Integrity Constraints"},{"location":"6-semester/DBS/05-sql/#dynamic-integrity-constraints","text":"Referential integrity requires that foreign keys must always reference existing tuples or be null. What happens if there is no professor with empid 007? 1 2 insert into course values ( 5100 , \u2019 Spying for Dummies \u2019 , 4 , 007 ) And how can we prevent the insertion?","title":"Dynamic Integrity Constraints"},{"location":"6-semester/DBS/05-sql/#definition-of-keys","text":"Candidate keys UNIQUE A table can have multiple UNIQUE constraints Allows null values! Primary Keys PRIMARY KEY At most one per table Implies UNIQUE NOT NULL Foreign Keys FOREIGN KEY Allows null values","title":"Definition of Keys"},{"location":"6-semester/DBS/05-sql/#handling-updates","text":"Dynamic integrity constraints need to be fulfilled by each change of a database. In response to changes of referenced data: Rejection of updates (default behavior) Propagation of updates (CASCADE) Set references to \u201cunknown\u201d (SET null) In addition available in PostgreSQL Set to a default value (SET DEFAULT) Examples in DBS5 slides p. 204","title":"Handling Updates"},{"location":"6-semester/DBS/05-sql/#complex-constraints","text":"1 2 3 4 5 6 7 8 9 10 11 CREATE TABLE grades ( studid integer REFERENCES student ON DELETE CASCADE , courseid integer REFERENCES course , grade numeric ( 2 , 1 ) CHECK ( grade BETWEEN 0 . 7 AND 5 . 0 ), PRIMARY KEY ( studid , courseid ) CONSTRAINT hasTaken CHECK ( EXISTS ( SELECT * FROM takes h WHERE h . courseid = grades . courseid AND h . studid = grades . studid )) ); The CHECK clause is evaluated for each update or insert Operation is rejected if the check is evaluated to false! True and unknown do not violate the constraint! Not (yet) supported by PostgreSQL: ERROR: cannot use subquery in check constraint Workaround by using triggers","title":"Complex Constraints"},{"location":"6-semester/DBS/05-sql/#appendix","text":"See Appendix in DBS5 slides p. 222","title":"Appendix"},{"location":"6-semester/DBS/06-transactions/","text":"Transactions \u00b6 Learning Goals Understanding the transaction concept Understanding the ACID properties Understanding the schedule concept Understanding serializability Understanding recoverable and cascadeless schedules Concurrency Control Understand and use lock-based concurrency control Understand and use two-phase locking Recovery Understanding basic logging algorithms Understanding the importance of atomicity and durability Transactions \u00b6 A transaction is a collection of operations that forms a logical unit of work, during which various data items are accessed and possibly updated. Transaction boundaries are user-defined! ACID Properties \u00b6 Atomicity Either all operations of the transaction are properly reflected in the database or none are. Often implemented via logs Consistency Execution of a transaction in isolation preserves the consistency of the database. According to constraints, checks, assertions In addition, consistency is defined by the application, e.g., fund transfers should not generate or destroy money \u2013 the overall sum is the same before and afterwards Isolation Each transaction appears to have the DB exclusively on its own. Intermediate results must be hidden for other transactions. Often implemented via locks Durability Updates of successfully completed transactions must not get lost despite system failures Often implemented via logs Operations on Transactions \u00b6 BEGIN Starts a transaction COMMIT Ends a transaction ROLLBACK All changes are undone/discarded SAVEPOINT SAVEPOINT <savepoint_name>; Defines a point/state within a transaction A transaction can be rolled back partially back up to the savepoint ROLLBACK TO <savepoint_name> Rolls the active transaction back to the savepoint <savepoint_name> Transaction States \u00b6 How do DBMSs Support Transactions? \u00b6 The two most important components of transaction management are: Multi-user Synchronization (isolation) Semantic correctness despite concurrency Concurrency allows for high throughput Serializability Weaker isolation levels Recovery (atomicity and durability) Roll back partially executed transactions Re-executing transactions after failures Guaranteeing persistence of transactional updates Schedules and Serializability \u00b6 Concurrency \u00b6 Affects the I in ACID Problems Lost Updates Overwriting updates Dirty Read Dependency on non-committed updates Non-repeatable Read Dependency on other updates T2 loses the illusion that it is alone in the database Phantom Problem Dependency on new/deleted tuples Schedules \u00b6 A schedule is a sequence of operations from one ore more transactions. For concurrent transactions, the operations are interleaved. Operations read(Q,q) Read the value of database item Q and store it in the local variable q. write(Q,q) Store the value of the local variable q in the database item Q Arithmetic operations commit abort Serial Schedule The operations of the transactions are executed sequentially with no overlap in time Concurrent Schedule The operations of the transactions are executed with overlap in time Valid Schedule A schedule is valid if the result of its executions is \"correct\" Example Schedules \u00b6 Correctness \u00b6 Definition 1 (D1) A concurrent execution of transactions must leave the database in a consistent state Definition 2 (D2) Concurrent execution of transactions must be (result) equivalent to some serial execution of the transactions We use Definition 2 Simplifying assumptions Only reads and writes are used to determine correctness This assumption is stronger than definition D2, as even fewer schedules are considered correct. Conflicts \u00b6 Definition 4 (D4) [^1] A schedule is conflict serializable if it is conflict equivalent to a serial schedule It ensures that after execution the database is in a consistent state. [^1]: Third definition D3 is view serializability, and is not covered in the course Alternative Definition from Web A schedule is called conflict serializable if it can be transformed into a serial schedule by swapping non-conflicting operations. There is a conflict if there is a read and a write on the same data unit. Also if there is a write on the same data unit. Let I and J be consecutive instructions of a schedule S of multiple transactions If I and J do not conflict, we can swap their order to produce a new schedule S' The instructions appear in the same order in S and S', except for I and J, whose order does not matter S and S' are termed conflict equivalent schedules Examples \u00b6 Conflict Graph \u00b6 AKA Precedence graph Directed graph Assumption : A transaction will always read an item before it writes that item Given a schedule for a set of transactions T_1,T_2,\\dots,T_n T_1,T_2,\\dots,T_n The vertices of the conflict graph are the transactions identifiers An edge from T_i T_i to T_j T_j denotes that the two transactions are conflicting, with T_i T_i making the relevant access earlier Sometimes the edge is labeled with the item involved in the conflict Determining Serializability Given a schedule S and a conflict graph A schedule is conflict serializable if its conflict graph is acyclic Intuitively, a conflict between two transactions forces an execution order between them (topological sorting) Example \u00b6 Which of the following are conflict serial schedules? Relationship Among Schedules \u00b6 Recoverable and Cascadeless Schedules \u00b6 Transactions can fail! If T_i T_i fails, it must be rolled back to retain the atomicity property of transactions If another transaction T_j T_j has read a data item written by T_i T_i , then T_j T_j must also be rolled back \\Rightarrow \\Rightarrow database systems must ensure that schedules are recoverable This schedule is not recoverable: Recoverable \u00b6 A schedule is recoverable if for each pair of transactions T_i T_i and T_j T_j where T_j T_j reads data items written by T_i T_i , then T_i T_i must commit before T_j T_j commits. Cascading Rollbacks \u00b6 A schedule is cascadeless if for each pair of transactions T_i T_i and T_j T_j , where T_j T_j reads data items written by T_i T_i , the commit operation of T_i T_i must appear before the read by T_j T_j In other words, if you only read committed data Every cascadeless schedule is also recoverable. Cascading rollbacks can easily become expensive. It is desirable to restrict the schedules to those that are cascadeless. Concurrency Control \u00b6 Scheduler \u00b6 Task of the scheduler: produce serializable schedules of instructions (transactions T_1, \\dots, T_n T_1, \\dots, T_n ) that avoid cascading rollbacks Realized by synchronization strategies pessimistic lock-based synchronization timestamp-based synchronization optimistic Lock-based Synchronization \u00b6 Ensuring (conflict) serializable schedules by delaying transactions that could violate serializability. Two types of locks can be held on a data item Q S (shared, read lock) X (exclusive, write lock) Operations on locks: lock_S(Q) set shared lock on data item Q lock_X(Q) set exclusive lock on data item Q unlock(Q) release lock on data item Q Lock Privileges \u00b6 A transaction holding an exclusive lock may issue a write or read access request on the item a shared lock may issue a read access request on the item Compatibility Matrix NL - no lock Concurrent transactions can only be granted compatible locks A transaction might have to wait until a requested lock can be granted! Problems with Early Unlocking \u00b6 Early unlocking can cause incorrect results (non-serializable schedules) but allows for a higher degree of concurrency. Initially A = 100 and B = 200 serial schedule T_{15};T_{16} T_{15};T_{16} prints 300 serial schedule T_{16};T_{15} T_{16};T_{15} prints 300 S_7 S_7 prints 250 Problems with Late Unlocking \u00b6 Late unlocking avoids non-serializable schedules. But it increases the chances of deadlocks . Learn to live with it! Two-Phase Locking (2PL) \u00b6 Is a protocol First phase (growing phase): Transactions may request locks Transactions may not release locks Second phase (shrinking phase): Transactions may not request locks Transactions may release locks When the first lock is release, the we move from first to second phase. Examples \u00b6 Remember that we look at transactions indi Characteristics of 2PL \u00b6 Produces only serializable schedules Insures conflict serializability Produces a subset of all possible serializable schedules Does not prevent deadlocks Does not prevent cascading rollbacks \"Dirty\" reads are possible (reading from non-committed transactions) Variations of 2PL \u00b6 Strict 2PL Exclusive locks are not released before the transaction commits Prevents \"dirty reads\" Rigorous 2PL All locks are released after commit time Transactions can be serialized in the order they commit Advantage No cascading rollbacks Disadvantage Loss of potential concurrency Lock Conversion \u00b6 Goal: apply 2PL but allow for a higher degree of concurrency First phase Acquire an S-lock on a data item Acquire an X-lock on a data item Convert (upgrade) an S-lock to an X-lock Second phase Release S-, and X-locks Convert (downgrade) an X-lock to an S-lock This protocol still ensures serializability. It relies on the application programmer to insert the appropriate locks. More Examples \u00b6 Overview \u00b6 Deadlocks \u00b6 Solutions detection and recovery prevention timeout (not covered in presentation) Deadlock Detection \u00b6 Create a \" Wait-for graph \" and check for cycles One node for each active transaction T_i T_i Edge T_i\\to T_j T_i\\to T_j if T_i T_i waits for the release of locks by T_j T_j A deadlock exists if the wait-for graph has a cycle If a deadlock is detected: Select an appropriate victim Abort the victim and release its locks Example \u00b6 Rollback Candidates \u00b6 Choosing a good victim transaction Rollback of one or more transactions that are involved in the cycle The latest (minimization of rollback effort) The one holding the most locks (maximization of released resources) Prevent that always the same victim is chosen (starvation) \"rollback counter\" if above a certain threshold: no more rollbacks to break deadlocks Deadlock Prevention \u00b6 Conservative 2PL 2PL as well as strict and rigorous 2PL do not prevent deadlocks Additional requirement: All locks (shared and exclusive) are obtained right in the beginning of a transaction Summary: Concurrency Control \u00b6 Many concurrency control protocols have been developed Main goal: allowing only serializable, recoverable and cascadeless schedules Two-phase locking (2PL) Most relational DBMS's use rigorous two-phase locking Deadlock detection (wait-for graph) and prevention (conservative 2PL) Serializability vs. concurrency Recovery \u00b6 \"Problems\" with transactions Atomicity Transactions may abort (rollback) Durability What if a DBMS crashes? The role of the recovery component is to ensure atomicity and durability of transactions in the presence of system failures. Durability \u00b6 How can durability be guaranteed (DBS6 Slides p. 159) Durability is relative and depends on the number of copies and the geographical location Guarantees only possible if we first update the copies and notify the user afterwards that a transaction's commit was successful We hence assume that the WAL (Write Ahead Logging) rule is satisfied Variations of applying the WAL rule: Log-based recovery Full redundancy: mirroring/shadowing all data on multiple computers (disks, computing centers) that redundantly do the same Not covered in course Failure Classification \u00b6 Transaction failure (failure of a not yet committed transaction) Undo the changes of the transaction System crash (failure with main memory loss) Changes of committed transactions must be preserved Changes of all non-committed transactions need to be undone Disk failure (failure with hard disk loss) Recovery based on archives/dumps Data Storage \u00b6 Two-level storage hierarchy Data is organized in pages and blocks Volatile storage (main memory buffer) Non-volatile storage (hard disk) Stable storage (RAIDS, remote backups, ... ) not covered in course Movement of Values \u00b6 Storage Operations \u00b6 Transactions access and update the database Operations for moving blocks with data items between disk and main memory (system buffer) Input(Q) Transfer block containing data item Q to main memory Output(Q) Transfer block containing Q to disk and replace Operations for moving values between data items and application variables read(Q,q) Assign the value of data item Q to variable q write(Q,q) Assign the value of variable q to data item Q Logging \u00b6 WAL (Write Ahead Logging) Before a transaction enters the commit state, \u201call its\u201d log entries have to be written to stable storage, including the commit log entry Before a modified page (or block) in main memory can be written to the database (non-volatile storage), \u201call its\u201d log entries have to be written to stable storage During normal operation When starting, a transaction T T registers itself in the log : [T start] When modifying data item X by write(X,x) Add log entry with [ T T , X, V-old, V-new] T T : transaction's ID X: data item name old value of item new value of item Write the new value of X The buffer manager asynchronously outputs the value to disk later When finishing, a transaction T T appends [T commit] to the log, T T then commits The transaction commits precisely when the commit entry (after all previous entries for this transaction) is output to the log! Log Entries \u00b6 Structure of a log entry (log record) [TID. DID. old, new] TID identifier of the transaction that caused the update DID data item identifier location on disk (page, block, offset) old value of the data item before the update new value of the data item after the update Additional entries start [TID start] Transaction TID has started commit [TID commit] Transaction TID has committed abort [TID abort] Transaction TID has aborted Example \u00b6 Log-based Recovery \u00b6 Operations to recover from failures Redo : perform the changes to the database again Undo : restore database to state prior to execution Recovery Algorithm \u00b6 To recover from a failure Reproduce ( redo ) results for committed transactions Undo changes of transactions that did not commit Remarks In a multitasking system, more than one transaction may need to be undone. If a system crashes during the recovery stage, the new recovery must still give correct results ( idempotence ). Phases of Recovery \u00b6 Redo (repeat history) Forward scan through the log Repeat all updates in the same order as in the log file Determine \"undo\" transactions [ T_i T_i start] add T_i T_i to the \"undo list\" [ T_i T_i abort] or [ T_i T_i commit] remove T_i T_i from the \"undo list\" Undo (rollback) all transactions in the \"undo list\" Backwards scan through the log Undo all updates of transactions in the \"undo list\" create a compensating log record For a [ T_i T_i start] record of a transaction T_i T_i in the \"undo list\", add a [ T_i T_i abort] record to the log file, remove T_i T_i from the \"undo list\" Stop when \"undo list\" is empty Compensating Log Records \u00b6 [TID, DID, value] Created to undo (compensate) the changes of [TID, DID, value, newValue] Redo-only log record Can also be used to rollback a transaction during normal o Example \u00b6 Example can be seen in slides: DBS6 - Transactions - Slide 85 - p. 202","title":"Transactions"},{"location":"6-semester/DBS/06-transactions/#transactions","text":"Learning Goals Understanding the transaction concept Understanding the ACID properties Understanding the schedule concept Understanding serializability Understanding recoverable and cascadeless schedules Concurrency Control Understand and use lock-based concurrency control Understand and use two-phase locking Recovery Understanding basic logging algorithms Understanding the importance of atomicity and durability","title":"Transactions"},{"location":"6-semester/DBS/06-transactions/#transactions_1","text":"A transaction is a collection of operations that forms a logical unit of work, during which various data items are accessed and possibly updated. Transaction boundaries are user-defined!","title":"Transactions"},{"location":"6-semester/DBS/06-transactions/#acid-properties","text":"Atomicity Either all operations of the transaction are properly reflected in the database or none are. Often implemented via logs Consistency Execution of a transaction in isolation preserves the consistency of the database. According to constraints, checks, assertions In addition, consistency is defined by the application, e.g., fund transfers should not generate or destroy money \u2013 the overall sum is the same before and afterwards Isolation Each transaction appears to have the DB exclusively on its own. Intermediate results must be hidden for other transactions. Often implemented via locks Durability Updates of successfully completed transactions must not get lost despite system failures Often implemented via logs","title":"ACID Properties"},{"location":"6-semester/DBS/06-transactions/#operations-on-transactions","text":"BEGIN Starts a transaction COMMIT Ends a transaction ROLLBACK All changes are undone/discarded SAVEPOINT SAVEPOINT <savepoint_name>; Defines a point/state within a transaction A transaction can be rolled back partially back up to the savepoint ROLLBACK TO <savepoint_name> Rolls the active transaction back to the savepoint <savepoint_name>","title":"Operations on Transactions"},{"location":"6-semester/DBS/06-transactions/#transaction-states","text":"","title":"Transaction States"},{"location":"6-semester/DBS/06-transactions/#how-do-dbmss-support-transactions","text":"The two most important components of transaction management are: Multi-user Synchronization (isolation) Semantic correctness despite concurrency Concurrency allows for high throughput Serializability Weaker isolation levels Recovery (atomicity and durability) Roll back partially executed transactions Re-executing transactions after failures Guaranteeing persistence of transactional updates","title":"How do DBMSs Support Transactions?"},{"location":"6-semester/DBS/06-transactions/#schedules-and-serializability","text":"","title":"Schedules and Serializability"},{"location":"6-semester/DBS/06-transactions/#concurrency","text":"Affects the I in ACID Problems Lost Updates Overwriting updates Dirty Read Dependency on non-committed updates Non-repeatable Read Dependency on other updates T2 loses the illusion that it is alone in the database Phantom Problem Dependency on new/deleted tuples","title":"Concurrency"},{"location":"6-semester/DBS/06-transactions/#schedules","text":"A schedule is a sequence of operations from one ore more transactions. For concurrent transactions, the operations are interleaved. Operations read(Q,q) Read the value of database item Q and store it in the local variable q. write(Q,q) Store the value of the local variable q in the database item Q Arithmetic operations commit abort Serial Schedule The operations of the transactions are executed sequentially with no overlap in time Concurrent Schedule The operations of the transactions are executed with overlap in time Valid Schedule A schedule is valid if the result of its executions is \"correct\"","title":"Schedules"},{"location":"6-semester/DBS/06-transactions/#example-schedules","text":"","title":"Example Schedules"},{"location":"6-semester/DBS/06-transactions/#correctness","text":"Definition 1 (D1) A concurrent execution of transactions must leave the database in a consistent state Definition 2 (D2) Concurrent execution of transactions must be (result) equivalent to some serial execution of the transactions We use Definition 2 Simplifying assumptions Only reads and writes are used to determine correctness This assumption is stronger than definition D2, as even fewer schedules are considered correct.","title":"Correctness"},{"location":"6-semester/DBS/06-transactions/#conflicts","text":"Definition 4 (D4) [^1] A schedule is conflict serializable if it is conflict equivalent to a serial schedule It ensures that after execution the database is in a consistent state. [^1]: Third definition D3 is view serializability, and is not covered in the course Alternative Definition from Web A schedule is called conflict serializable if it can be transformed into a serial schedule by swapping non-conflicting operations. There is a conflict if there is a read and a write on the same data unit. Also if there is a write on the same data unit. Let I and J be consecutive instructions of a schedule S of multiple transactions If I and J do not conflict, we can swap their order to produce a new schedule S' The instructions appear in the same order in S and S', except for I and J, whose order does not matter S and S' are termed conflict equivalent schedules","title":"Conflicts"},{"location":"6-semester/DBS/06-transactions/#examples","text":"","title":"Examples"},{"location":"6-semester/DBS/06-transactions/#conflict-graph","text":"AKA Precedence graph Directed graph Assumption : A transaction will always read an item before it writes that item Given a schedule for a set of transactions T_1,T_2,\\dots,T_n T_1,T_2,\\dots,T_n The vertices of the conflict graph are the transactions identifiers An edge from T_i T_i to T_j T_j denotes that the two transactions are conflicting, with T_i T_i making the relevant access earlier Sometimes the edge is labeled with the item involved in the conflict Determining Serializability Given a schedule S and a conflict graph A schedule is conflict serializable if its conflict graph is acyclic Intuitively, a conflict between two transactions forces an execution order between them (topological sorting)","title":"Conflict Graph"},{"location":"6-semester/DBS/06-transactions/#example","text":"Which of the following are conflict serial schedules?","title":"Example"},{"location":"6-semester/DBS/06-transactions/#relationship-among-schedules","text":"","title":"Relationship Among Schedules"},{"location":"6-semester/DBS/06-transactions/#recoverable-and-cascadeless-schedules","text":"Transactions can fail! If T_i T_i fails, it must be rolled back to retain the atomicity property of transactions If another transaction T_j T_j has read a data item written by T_i T_i , then T_j T_j must also be rolled back \\Rightarrow \\Rightarrow database systems must ensure that schedules are recoverable This schedule is not recoverable:","title":"Recoverable and Cascadeless Schedules"},{"location":"6-semester/DBS/06-transactions/#recoverable","text":"A schedule is recoverable if for each pair of transactions T_i T_i and T_j T_j where T_j T_j reads data items written by T_i T_i , then T_i T_i must commit before T_j T_j commits.","title":"Recoverable"},{"location":"6-semester/DBS/06-transactions/#cascading-rollbacks","text":"A schedule is cascadeless if for each pair of transactions T_i T_i and T_j T_j , where T_j T_j reads data items written by T_i T_i , the commit operation of T_i T_i must appear before the read by T_j T_j In other words, if you only read committed data Every cascadeless schedule is also recoverable. Cascading rollbacks can easily become expensive. It is desirable to restrict the schedules to those that are cascadeless.","title":"Cascading Rollbacks"},{"location":"6-semester/DBS/06-transactions/#concurrency-control","text":"","title":"Concurrency Control"},{"location":"6-semester/DBS/06-transactions/#scheduler","text":"Task of the scheduler: produce serializable schedules of instructions (transactions T_1, \\dots, T_n T_1, \\dots, T_n ) that avoid cascading rollbacks Realized by synchronization strategies pessimistic lock-based synchronization timestamp-based synchronization optimistic","title":"Scheduler"},{"location":"6-semester/DBS/06-transactions/#lock-based-synchronization","text":"Ensuring (conflict) serializable schedules by delaying transactions that could violate serializability. Two types of locks can be held on a data item Q S (shared, read lock) X (exclusive, write lock) Operations on locks: lock_S(Q) set shared lock on data item Q lock_X(Q) set exclusive lock on data item Q unlock(Q) release lock on data item Q","title":"Lock-based Synchronization"},{"location":"6-semester/DBS/06-transactions/#lock-privileges","text":"A transaction holding an exclusive lock may issue a write or read access request on the item a shared lock may issue a read access request on the item Compatibility Matrix NL - no lock Concurrent transactions can only be granted compatible locks A transaction might have to wait until a requested lock can be granted!","title":"Lock Privileges"},{"location":"6-semester/DBS/06-transactions/#problems-with-early-unlocking","text":"Early unlocking can cause incorrect results (non-serializable schedules) but allows for a higher degree of concurrency. Initially A = 100 and B = 200 serial schedule T_{15};T_{16} T_{15};T_{16} prints 300 serial schedule T_{16};T_{15} T_{16};T_{15} prints 300 S_7 S_7 prints 250","title":"Problems with Early Unlocking"},{"location":"6-semester/DBS/06-transactions/#problems-with-late-unlocking","text":"Late unlocking avoids non-serializable schedules. But it increases the chances of deadlocks . Learn to live with it!","title":"Problems with Late Unlocking"},{"location":"6-semester/DBS/06-transactions/#two-phase-locking-2pl","text":"Is a protocol First phase (growing phase): Transactions may request locks Transactions may not release locks Second phase (shrinking phase): Transactions may not request locks Transactions may release locks When the first lock is release, the we move from first to second phase.","title":"Two-Phase Locking (2PL)"},{"location":"6-semester/DBS/06-transactions/#examples_1","text":"Remember that we look at transactions indi","title":"Examples"},{"location":"6-semester/DBS/06-transactions/#characteristics-of-2pl","text":"Produces only serializable schedules Insures conflict serializability Produces a subset of all possible serializable schedules Does not prevent deadlocks Does not prevent cascading rollbacks \"Dirty\" reads are possible (reading from non-committed transactions)","title":"Characteristics of 2PL"},{"location":"6-semester/DBS/06-transactions/#variations-of-2pl","text":"Strict 2PL Exclusive locks are not released before the transaction commits Prevents \"dirty reads\" Rigorous 2PL All locks are released after commit time Transactions can be serialized in the order they commit Advantage No cascading rollbacks Disadvantage Loss of potential concurrency","title":"Variations of 2PL"},{"location":"6-semester/DBS/06-transactions/#lock-conversion","text":"Goal: apply 2PL but allow for a higher degree of concurrency First phase Acquire an S-lock on a data item Acquire an X-lock on a data item Convert (upgrade) an S-lock to an X-lock Second phase Release S-, and X-locks Convert (downgrade) an X-lock to an S-lock This protocol still ensures serializability. It relies on the application programmer to insert the appropriate locks.","title":"Lock Conversion"},{"location":"6-semester/DBS/06-transactions/#more-examples","text":"","title":"More Examples"},{"location":"6-semester/DBS/06-transactions/#overview","text":"","title":"Overview"},{"location":"6-semester/DBS/06-transactions/#deadlocks","text":"Solutions detection and recovery prevention timeout (not covered in presentation)","title":"Deadlocks"},{"location":"6-semester/DBS/06-transactions/#deadlock-detection","text":"Create a \" Wait-for graph \" and check for cycles One node for each active transaction T_i T_i Edge T_i\\to T_j T_i\\to T_j if T_i T_i waits for the release of locks by T_j T_j A deadlock exists if the wait-for graph has a cycle If a deadlock is detected: Select an appropriate victim Abort the victim and release its locks","title":"Deadlock Detection"},{"location":"6-semester/DBS/06-transactions/#example_1","text":"","title":"Example"},{"location":"6-semester/DBS/06-transactions/#rollback-candidates","text":"Choosing a good victim transaction Rollback of one or more transactions that are involved in the cycle The latest (minimization of rollback effort) The one holding the most locks (maximization of released resources) Prevent that always the same victim is chosen (starvation) \"rollback counter\" if above a certain threshold: no more rollbacks to break deadlocks","title":"Rollback Candidates"},{"location":"6-semester/DBS/06-transactions/#deadlock-prevention","text":"Conservative 2PL 2PL as well as strict and rigorous 2PL do not prevent deadlocks Additional requirement: All locks (shared and exclusive) are obtained right in the beginning of a transaction","title":"Deadlock Prevention"},{"location":"6-semester/DBS/06-transactions/#summary-concurrency-control","text":"Many concurrency control protocols have been developed Main goal: allowing only serializable, recoverable and cascadeless schedules Two-phase locking (2PL) Most relational DBMS's use rigorous two-phase locking Deadlock detection (wait-for graph) and prevention (conservative 2PL) Serializability vs. concurrency","title":"Summary: Concurrency Control"},{"location":"6-semester/DBS/06-transactions/#recovery","text":"\"Problems\" with transactions Atomicity Transactions may abort (rollback) Durability What if a DBMS crashes? The role of the recovery component is to ensure atomicity and durability of transactions in the presence of system failures.","title":"Recovery"},{"location":"6-semester/DBS/06-transactions/#durability","text":"How can durability be guaranteed (DBS6 Slides p. 159) Durability is relative and depends on the number of copies and the geographical location Guarantees only possible if we first update the copies and notify the user afterwards that a transaction's commit was successful We hence assume that the WAL (Write Ahead Logging) rule is satisfied Variations of applying the WAL rule: Log-based recovery Full redundancy: mirroring/shadowing all data on multiple computers (disks, computing centers) that redundantly do the same Not covered in course","title":"Durability"},{"location":"6-semester/DBS/06-transactions/#failure-classification","text":"Transaction failure (failure of a not yet committed transaction) Undo the changes of the transaction System crash (failure with main memory loss) Changes of committed transactions must be preserved Changes of all non-committed transactions need to be undone Disk failure (failure with hard disk loss) Recovery based on archives/dumps","title":"Failure Classification"},{"location":"6-semester/DBS/06-transactions/#data-storage","text":"Two-level storage hierarchy Data is organized in pages and blocks Volatile storage (main memory buffer) Non-volatile storage (hard disk) Stable storage (RAIDS, remote backups, ... ) not covered in course","title":"Data Storage"},{"location":"6-semester/DBS/06-transactions/#movement-of-values","text":"","title":"Movement of Values"},{"location":"6-semester/DBS/06-transactions/#storage-operations","text":"Transactions access and update the database Operations for moving blocks with data items between disk and main memory (system buffer) Input(Q) Transfer block containing data item Q to main memory Output(Q) Transfer block containing Q to disk and replace Operations for moving values between data items and application variables read(Q,q) Assign the value of data item Q to variable q write(Q,q) Assign the value of variable q to data item Q","title":"Storage Operations"},{"location":"6-semester/DBS/06-transactions/#logging","text":"WAL (Write Ahead Logging) Before a transaction enters the commit state, \u201call its\u201d log entries have to be written to stable storage, including the commit log entry Before a modified page (or block) in main memory can be written to the database (non-volatile storage), \u201call its\u201d log entries have to be written to stable storage During normal operation When starting, a transaction T T registers itself in the log : [T start] When modifying data item X by write(X,x) Add log entry with [ T T , X, V-old, V-new] T T : transaction's ID X: data item name old value of item new value of item Write the new value of X The buffer manager asynchronously outputs the value to disk later When finishing, a transaction T T appends [T commit] to the log, T T then commits The transaction commits precisely when the commit entry (after all previous entries for this transaction) is output to the log!","title":"Logging"},{"location":"6-semester/DBS/06-transactions/#log-entries","text":"Structure of a log entry (log record) [TID. DID. old, new] TID identifier of the transaction that caused the update DID data item identifier location on disk (page, block, offset) old value of the data item before the update new value of the data item after the update Additional entries start [TID start] Transaction TID has started commit [TID commit] Transaction TID has committed abort [TID abort] Transaction TID has aborted","title":"Log Entries"},{"location":"6-semester/DBS/06-transactions/#example_2","text":"","title":"Example"},{"location":"6-semester/DBS/06-transactions/#log-based-recovery","text":"Operations to recover from failures Redo : perform the changes to the database again Undo : restore database to state prior to execution","title":"Log-based Recovery"},{"location":"6-semester/DBS/06-transactions/#recovery-algorithm","text":"To recover from a failure Reproduce ( redo ) results for committed transactions Undo changes of transactions that did not commit Remarks In a multitasking system, more than one transaction may need to be undone. If a system crashes during the recovery stage, the new recovery must still give correct results ( idempotence ).","title":"Recovery Algorithm"},{"location":"6-semester/DBS/06-transactions/#phases-of-recovery","text":"Redo (repeat history) Forward scan through the log Repeat all updates in the same order as in the log file Determine \"undo\" transactions [ T_i T_i start] add T_i T_i to the \"undo list\" [ T_i T_i abort] or [ T_i T_i commit] remove T_i T_i from the \"undo list\" Undo (rollback) all transactions in the \"undo list\" Backwards scan through the log Undo all updates of transactions in the \"undo list\" create a compensating log record For a [ T_i T_i start] record of a transaction T_i T_i in the \"undo list\", add a [ T_i T_i abort] record to the log file, remove T_i T_i from the \"undo list\" Stop when \"undo list\" is empty","title":"Phases of Recovery"},{"location":"6-semester/DBS/06-transactions/#compensating-log-records","text":"[TID, DID, value] Created to undo (compensate) the changes of [TID, DID, value, newValue] Redo-only log record Can also be used to rollback a transaction during normal o","title":"Compensating Log Records"},{"location":"6-semester/DBS/06-transactions/#example_3","text":"Example can be seen in slides: DBS6 - Transactions - Slide 85 - p. 202","title":"Example"},{"location":"6-semester/DBS/07-physical-dbs/","text":"Physical Design \u00b6 Learning Goals Understand how tables are stored in files Understand basic indexing techniques Use knowledge of file/index to tune basic SQL queries File Organization \u00b6 Storage Hierarchy \u00b6 Volatile storage (cache, main memory): loses content when power is switched of Non-volatile storage : content persists even when power is switched of The higher the level, the faster the access Magnetic Hard Disk \u00b6 For each disk access Each track is divided into sectors (smallest unit of data that can be read/written) A block (page) is a contiguous sequence of sectors from a single track Basic unit of data transfer between disk and memory Optimization of disk block access Organize blocks in the way data is accessed Store related information close to each other Conceptually, relational data is stored as sequences of bits on disk. Functional requirements Processing records sequentially Efficient key-value search Insertion/deletion of records Performance objectives Little wasted space Fast response time High number of transactions Files \u00b6 Storing databases on disks The database is stored as a collection of files Each file corresponds to a set of records A record contains a number of fields Multiple records are \u201cgrouped\u201d into blocks (pages) Record size Fixed Variable Files reside on mass storage (usually a disk) Fast random access Non-volatile storage Example \u00b6 Blocks in a file system are not necessarily contiguous Speed Speed of reading a block (an I/O): \\sim 10 \\sim 10 msec to a non-contiguous block read \\sim 1 \\sim 1 msec for a contiguous block read DBMS/OS can reorganize blocks to make them contiguous Fixed Size Records \u00b6 All records use the same length if they occupy the whole space or not Read record i i Get record at location: \\mathrm{recordSize \\times i} \\mathrm{recordSize \\times i} Delete record i i Move records i+1,\\dots,n i+1,\\dots,n to location i,\\dots,n-1 i,\\dots,n-1 to close the gap or Move record n n to i i or Mark the gaps and fill them with future insertions Mark first gap in the file header Use the gaps to point to other gaps (free-list) Variable Size Records \u00b6 Records have different lengths and occupy different amounts of disk space Use case: variable-lengths attributes (e.g., varchar) Implementation alternatives If max. size is known: Map variable size records to fixed size records Slotted-page-structure Records stored contiguously Block header contains pointers to records Organizing Records in a File \u00b6 Determine the order of records within a file Heap file organization A record can be placed anywhere in the file Sequential file organization Records stored in sequential order by the value of the search key Hash file organization Hash the record to a block based on a hash function and a hash key Heap \u00b6 No apparent ordering on any of the columns in the table Search: linear scan always works Insert: find a free slot Sequential \u00b6 Example table is stored in order of the ID column The ordering column does not need to be the primary key Search: binary search when search on the ID column Insert: reorganization of the file Hashing \u00b6 Hash function used in this example is: mod(ID, 3) mod(ID, 3) Search: use the hash function on the search key value (ID) to find the right block Insert: use the hash function on the search key value (ID) to find the right block and append Indexes \u00b6 Assumptions Many rows are stored in the database Many queries only access a small fraction of the rows Must be able to modify the rows, i.e., insert, update, and delete Cannot take the database offline to reorganize the files used in the database Goal: access as little data as possible Overview \u00b6 Classification Variations Single-level index Multi-level index It is possible to have multiple indexes for the same relation. Primary Sparse Index \u00b6 Defined on a file ordered on the search key One index entry for each block in the file Secondary Dense Index \u00b6 Defined on a file not ordered on the search key One index entry for each record Overview \u00b6 Primary vs. secondary = clustering vs. non-clustering Is the file ordered on the search key? Dense vs. sparse A separate index entry for each record (unique search key value)? Yes \u2192 dense No \u2192 sparse Tradeoff Dense indexes: faster location of records Sparse indexes: smaller indexes Multi-level Index \u00b6 Goal: the outer index (sparse) fits into main memory The number of levels can be greater than two Limitations of index-sequential file organization Insertions and deletions Expensive reorganization of multiple levels of ordered files B^+^-trees Balanced search trees Number of lookups/levels is the same for all entries Leaving some space in each disk block Index Concepts Combined \u00b6 A secondary sparse index does not make sense! Since records are not sorted on that key, we cannot predict the location of a record from the location of any other record Thus, secondary indexes are always dense. Clustering Dense Index entry has a pointer to the first record Non-clustering Dense Index entry has pointers to all the records B+-Tree \u00b6 n = the fanout = the number of pointers = 3 in the example above \\dashv \\dashv = unused pointer nu = Unused search key value Leaf-level pointers: point to the data file Search: Both key lookups and range queries are well supported Node Structure \u00b6 Node structure used for root , internal , and leaf nodes In this example, the fanout n is 10 In each node: 10 (n) pointers (p_i) (p_i) In each node: 9 (n-1) search key values ( k_j k_j ) The last pointer in leaf nodes is used to chain together the leaf nodes in search key order Properties \u00b6 Balanced All paths from the root node to leaf nodes have the same length Bushy Each node has between \\lceil {n \\over 2} \\rceil \\lceil {n \\over 2} \\rceil and n children Exception: the root node has between 2 and n children Leaf nodes have between $\\lceil {n-1\\over2} \\rceil $ and n - 1 n - 1 pointers to file records and 1 pointer to the next leaf node Ordered The key values are sorted in each node, i.e, k_i < k_j k_i < k_j , if i < j i < j The subtrees are ordered Minimal B+-Tree Example \u00b6 A minimally filled B+-tree for n=3 B+-Tree in Practice \u00b6 Each B+-tree node has the size of one I/O block of data. A B+-tree node is at least 50% filled (by design) The B+-tree contains a rather small number of levels, usually logarithmic in the size of the main file The first one or two levels of the tree are stored in main memory \u201cLogically\u201d close does not imply \u201cphysically close\u201d Typically reading a node in a B+-tree requires one random I/O The non-leaf nodes are a hierarchy of sparse indexes When a primary key constraint for a table is created in a DBMS, then the uniqueness is checked by using a B+-tree. B+-Tree Updates \u00b6 Insertions Overflow, split The tree height may be increased by 1 Deletions Underflow, borrow, coalesce The tree height may be decreased by 1 Multiple examples in DBS7 Slide 30- p. 54- B+-Trees Online \u00b6 Interactive apps available on the Web https://goneill.co.nz/btree-demo.php https://www.cs.usfca.edu/~galles/visualization/BPlusTree.html third party material, not fully tested, might have a slightly different implementation (and errors) Unordered Indexes (Hashing) \u00b6 Static Hash Index \u00b6 Build an index based on a hash function (instead of based on a search key order) Choose an appropriate hash function h h Apply hash function h h to search key value k k to compute h(k) h(k) Bucket (disk block) for each value of h(k) h(k) Lookup One access to the bucket directory One access to the data file Good performance depends on a good hash function Bucket overflow Too many different key values hash to the same bucket Solution: overflow buckets and overflow chains Open vs Closed Hashing \u00b6 Open Hashing Overflow chains Closed Hashing Fixed number of buckets Overflow: use one of the other buckets (linear probing, further hash functions, etc.) Static Hashing \u00b6 Problem for static hashing: Hash function and number of buckets determined during initialization Databases grow or shrink over time! Initial number of buckets too small \\to \\to many overflows, performance degrades Initial number of buckets too big \\to \\to underflow, space is wasted Solutions Periodic re-organization Dynamic hashing: allows modifications dynamically Physical Design Tuning \u00b6 Design decisions Ordered/clustering indexes vs. hashing Sparse vs. dense index Clustering vs. non-clustering indexes Joins and indexes Tuning questions Are the costs of periodic reorganization acceptable? How frequent are insertions and deletions? Optimization goal: average access time or worst-case access time? What types of queries are expected? Use of Indexes \u00b6 Sometimes available indexes are not used, why? Catalog out of date optimizer may think table is small \u201cGood\u201d and \u201cbad\u201d query examples SELECT * FROM EMP WHERE salary/12 > 4000 Operations on conditions may prevent the optimizer from realizing the index could be useful SELECT * FROM EMP WHERE salary > 48000 Good SELECT * FROM EMP WHERE SUBSTR(name, 1, 1) = \u2019G\u2019 Functions on conditions require function-based indexes SELECT * FROM EMP WHERE name LIKE \u2019G%\u2019 Good SELECT * FROM EMP WHERE name = \u2019Smith\u2019 Good SELECT * FROM EMP WHERE salary IS NULL Requires an index on nullable values Nested sub-query Selection by negation Queries with OR","title":"Physical Design"},{"location":"6-semester/DBS/07-physical-dbs/#physical-design","text":"Learning Goals Understand how tables are stored in files Understand basic indexing techniques Use knowledge of file/index to tune basic SQL queries","title":"Physical Design"},{"location":"6-semester/DBS/07-physical-dbs/#file-organization","text":"","title":"File Organization"},{"location":"6-semester/DBS/07-physical-dbs/#storage-hierarchy","text":"Volatile storage (cache, main memory): loses content when power is switched of Non-volatile storage : content persists even when power is switched of The higher the level, the faster the access","title":"Storage Hierarchy"},{"location":"6-semester/DBS/07-physical-dbs/#magnetic-hard-disk","text":"For each disk access Each track is divided into sectors (smallest unit of data that can be read/written) A block (page) is a contiguous sequence of sectors from a single track Basic unit of data transfer between disk and memory Optimization of disk block access Organize blocks in the way data is accessed Store related information close to each other Conceptually, relational data is stored as sequences of bits on disk. Functional requirements Processing records sequentially Efficient key-value search Insertion/deletion of records Performance objectives Little wasted space Fast response time High number of transactions","title":"Magnetic Hard Disk"},{"location":"6-semester/DBS/07-physical-dbs/#files","text":"Storing databases on disks The database is stored as a collection of files Each file corresponds to a set of records A record contains a number of fields Multiple records are \u201cgrouped\u201d into blocks (pages) Record size Fixed Variable Files reside on mass storage (usually a disk) Fast random access Non-volatile storage","title":"Files"},{"location":"6-semester/DBS/07-physical-dbs/#example","text":"Blocks in a file system are not necessarily contiguous Speed Speed of reading a block (an I/O): \\sim 10 \\sim 10 msec to a non-contiguous block read \\sim 1 \\sim 1 msec for a contiguous block read DBMS/OS can reorganize blocks to make them contiguous","title":"Example"},{"location":"6-semester/DBS/07-physical-dbs/#fixed-size-records","text":"All records use the same length if they occupy the whole space or not Read record i i Get record at location: \\mathrm{recordSize \\times i} \\mathrm{recordSize \\times i} Delete record i i Move records i+1,\\dots,n i+1,\\dots,n to location i,\\dots,n-1 i,\\dots,n-1 to close the gap or Move record n n to i i or Mark the gaps and fill them with future insertions Mark first gap in the file header Use the gaps to point to other gaps (free-list)","title":"Fixed Size Records"},{"location":"6-semester/DBS/07-physical-dbs/#variable-size-records","text":"Records have different lengths and occupy different amounts of disk space Use case: variable-lengths attributes (e.g., varchar) Implementation alternatives If max. size is known: Map variable size records to fixed size records Slotted-page-structure Records stored contiguously Block header contains pointers to records","title":"Variable Size Records"},{"location":"6-semester/DBS/07-physical-dbs/#organizing-records-in-a-file","text":"Determine the order of records within a file Heap file organization A record can be placed anywhere in the file Sequential file organization Records stored in sequential order by the value of the search key Hash file organization Hash the record to a block based on a hash function and a hash key","title":"Organizing Records in a File"},{"location":"6-semester/DBS/07-physical-dbs/#heap","text":"No apparent ordering on any of the columns in the table Search: linear scan always works Insert: find a free slot","title":"Heap"},{"location":"6-semester/DBS/07-physical-dbs/#sequential","text":"Example table is stored in order of the ID column The ordering column does not need to be the primary key Search: binary search when search on the ID column Insert: reorganization of the file","title":"Sequential"},{"location":"6-semester/DBS/07-physical-dbs/#hashing","text":"Hash function used in this example is: mod(ID, 3) mod(ID, 3) Search: use the hash function on the search key value (ID) to find the right block Insert: use the hash function on the search key value (ID) to find the right block and append","title":"Hashing"},{"location":"6-semester/DBS/07-physical-dbs/#indexes","text":"Assumptions Many rows are stored in the database Many queries only access a small fraction of the rows Must be able to modify the rows, i.e., insert, update, and delete Cannot take the database offline to reorganize the files used in the database Goal: access as little data as possible","title":"Indexes"},{"location":"6-semester/DBS/07-physical-dbs/#overview","text":"Classification Variations Single-level index Multi-level index It is possible to have multiple indexes for the same relation.","title":"Overview"},{"location":"6-semester/DBS/07-physical-dbs/#primary-sparse-index","text":"Defined on a file ordered on the search key One index entry for each block in the file","title":"Primary Sparse Index"},{"location":"6-semester/DBS/07-physical-dbs/#secondary-dense-index","text":"Defined on a file not ordered on the search key One index entry for each record","title":"Secondary Dense Index"},{"location":"6-semester/DBS/07-physical-dbs/#overview_1","text":"Primary vs. secondary = clustering vs. non-clustering Is the file ordered on the search key? Dense vs. sparse A separate index entry for each record (unique search key value)? Yes \u2192 dense No \u2192 sparse Tradeoff Dense indexes: faster location of records Sparse indexes: smaller indexes","title":"Overview"},{"location":"6-semester/DBS/07-physical-dbs/#multi-level-index","text":"Goal: the outer index (sparse) fits into main memory The number of levels can be greater than two Limitations of index-sequential file organization Insertions and deletions Expensive reorganization of multiple levels of ordered files B^+^-trees Balanced search trees Number of lookups/levels is the same for all entries Leaving some space in each disk block","title":"Multi-level Index"},{"location":"6-semester/DBS/07-physical-dbs/#index-concepts-combined","text":"A secondary sparse index does not make sense! Since records are not sorted on that key, we cannot predict the location of a record from the location of any other record Thus, secondary indexes are always dense. Clustering Dense Index entry has a pointer to the first record Non-clustering Dense Index entry has pointers to all the records","title":"Index Concepts Combined"},{"location":"6-semester/DBS/07-physical-dbs/#b-tree","text":"n = the fanout = the number of pointers = 3 in the example above \\dashv \\dashv = unused pointer nu = Unused search key value Leaf-level pointers: point to the data file Search: Both key lookups and range queries are well supported","title":"B+-Tree"},{"location":"6-semester/DBS/07-physical-dbs/#node-structure","text":"Node structure used for root , internal , and leaf nodes In this example, the fanout n is 10 In each node: 10 (n) pointers (p_i) (p_i) In each node: 9 (n-1) search key values ( k_j k_j ) The last pointer in leaf nodes is used to chain together the leaf nodes in search key order","title":"Node Structure"},{"location":"6-semester/DBS/07-physical-dbs/#properties","text":"Balanced All paths from the root node to leaf nodes have the same length Bushy Each node has between \\lceil {n \\over 2} \\rceil \\lceil {n \\over 2} \\rceil and n children Exception: the root node has between 2 and n children Leaf nodes have between $\\lceil {n-1\\over2} \\rceil $ and n - 1 n - 1 pointers to file records and 1 pointer to the next leaf node Ordered The key values are sorted in each node, i.e, k_i < k_j k_i < k_j , if i < j i < j The subtrees are ordered","title":"Properties"},{"location":"6-semester/DBS/07-physical-dbs/#minimal-b-tree-example","text":"A minimally filled B+-tree for n=3","title":"Minimal B+-Tree Example"},{"location":"6-semester/DBS/07-physical-dbs/#b-tree-in-practice","text":"Each B+-tree node has the size of one I/O block of data. A B+-tree node is at least 50% filled (by design) The B+-tree contains a rather small number of levels, usually logarithmic in the size of the main file The first one or two levels of the tree are stored in main memory \u201cLogically\u201d close does not imply \u201cphysically close\u201d Typically reading a node in a B+-tree requires one random I/O The non-leaf nodes are a hierarchy of sparse indexes When a primary key constraint for a table is created in a DBMS, then the uniqueness is checked by using a B+-tree.","title":"B+-Tree in Practice"},{"location":"6-semester/DBS/07-physical-dbs/#b-tree-updates","text":"Insertions Overflow, split The tree height may be increased by 1 Deletions Underflow, borrow, coalesce The tree height may be decreased by 1 Multiple examples in DBS7 Slide 30- p. 54-","title":"B+-Tree Updates"},{"location":"6-semester/DBS/07-physical-dbs/#b-trees-online","text":"Interactive apps available on the Web https://goneill.co.nz/btree-demo.php https://www.cs.usfca.edu/~galles/visualization/BPlusTree.html third party material, not fully tested, might have a slightly different implementation (and errors)","title":"B+-Trees Online"},{"location":"6-semester/DBS/07-physical-dbs/#unordered-indexes-hashing","text":"","title":"Unordered Indexes (Hashing)"},{"location":"6-semester/DBS/07-physical-dbs/#static-hash-index","text":"Build an index based on a hash function (instead of based on a search key order) Choose an appropriate hash function h h Apply hash function h h to search key value k k to compute h(k) h(k) Bucket (disk block) for each value of h(k) h(k) Lookup One access to the bucket directory One access to the data file Good performance depends on a good hash function Bucket overflow Too many different key values hash to the same bucket Solution: overflow buckets and overflow chains","title":"Static Hash Index"},{"location":"6-semester/DBS/07-physical-dbs/#open-vs-closed-hashing","text":"Open Hashing Overflow chains Closed Hashing Fixed number of buckets Overflow: use one of the other buckets (linear probing, further hash functions, etc.)","title":"Open vs Closed Hashing"},{"location":"6-semester/DBS/07-physical-dbs/#static-hashing","text":"Problem for static hashing: Hash function and number of buckets determined during initialization Databases grow or shrink over time! Initial number of buckets too small \\to \\to many overflows, performance degrades Initial number of buckets too big \\to \\to underflow, space is wasted Solutions Periodic re-organization Dynamic hashing: allows modifications dynamically","title":"Static Hashing"},{"location":"6-semester/DBS/07-physical-dbs/#physical-design-tuning","text":"Design decisions Ordered/clustering indexes vs. hashing Sparse vs. dense index Clustering vs. non-clustering indexes Joins and indexes Tuning questions Are the costs of periodic reorganization acceptable? How frequent are insertions and deletions? Optimization goal: average access time or worst-case access time? What types of queries are expected?","title":"Physical Design Tuning"},{"location":"6-semester/DBS/07-physical-dbs/#use-of-indexes","text":"Sometimes available indexes are not used, why? Catalog out of date optimizer may think table is small \u201cGood\u201d and \u201cbad\u201d query examples SELECT * FROM EMP WHERE salary/12 > 4000 Operations on conditions may prevent the optimizer from realizing the index could be useful SELECT * FROM EMP WHERE salary > 48000 Good SELECT * FROM EMP WHERE SUBSTR(name, 1, 1) = \u2019G\u2019 Functions on conditions require function-based indexes SELECT * FROM EMP WHERE name LIKE \u2019G%\u2019 Good SELECT * FROM EMP WHERE name = \u2019Smith\u2019 Good SELECT * FROM EMP WHERE salary IS NULL Requires an index on nullable values Nested sub-query Selection by negation Queries with OR","title":"Use of Indexes"},{"location":"6-semester/DBS/08-query-optimization/","text":"Query Processing and Optimization \u00b6 Learning Goals Understand how selection statements are executed Understand the basic join algorithms Understand the basics of heuristic (logical) query optimization Understand the basics of physical query optimization \\newcommand{\\relationRaw}[3]{ \\newcommand{\\pk}{\\underline} #3\\mathbf{#1}#3#3:\\{[\\mathrm{#2}]\\} } \\newcommand{\\relation}[2]{\\relationRaw{#1}{#2}{}} \\newcommand{\\relational}[2]{\\relationRaw{#1}{#2}{&}} \\nonumber \\newcommand{\\relationRaw}[3]{ \\newcommand{\\pk}{\\underline} #3\\mathbf{#1}#3#3:\\{[\\mathrm{#2}]\\} } \\newcommand{\\relation}[2]{\\relationRaw{#1}{#2}{}} \\newcommand{\\relational}[2]{\\relationRaw{#1}{#2}{&}} \\nonumber Query Processing \u00b6 Evaluation of an SQL Statement \u00b6 The clauses are specified in the following order SELECT column(s) FROM table list WHERE condition GROUP BY grouping column(s) HAVING group condition ORDER BY sort list But the query is evaluated in a different order Cartesian product of tables in the from clause Predicates in the where clause Grouped according to the group by clause Predicate in the having clause applied to (eliminate) groups Compute aggregation functions for each remaining group Projection on columns enumerated in the select clause SQL is declarative! Steps of Query Processing \u00b6 Parsing a Query \u00b6 Parsing a query into an initial query plan \\pi_{title}(\\sigma_{name='Socrates' \\and empID=taughtBy}(professor \\times course)) \\pi_{title}(\\sigma_{name='Socrates' \\and empID=taughtBy}(professor \\times course)) Alternative Query Plan \u00b6 Query Optimization \u00b6 Alternatives Equivalent query execution plans Algorithms to compute an algebra operation Methods to access relations (indexes) Although the result is equivalent, execution costs might be different. Theory meets reality It is not the task of the user to write queries \u201cefficiently\u201d, it is the task of the query optimizer to execute them efficiently! But in reality. . . optimizers are not perfect. Query Costs \u00b6 Measures Total elapsed time for answering a query (response time) Many factors contribute to response time Disk access CPU costs Network communication Query load Parallel processing Disk access most dominant Block access time: seek time, rotation time Transfer time Query Optimization \u00b6 Logical query optimization Relational algebra Equivalence transformation Heuristic optimization Physical query optimization Algorithms and implementations of operations Cost model Heuristic (Logical) Query Optimization \u00b6 Logical query optimization Foundation: algebraic equivalences Algebraic equivalences span the potential search space Given an initial algebraic expression: apply algebraic equivalences to derive new equivalent algebraic expressions What is a good plan? Difficult to compare plans without a cost function \\to \\to logical query optimization relies on heuristics Main goal of logical query optimization Reduce the size of intermediate results Equivalences \u00b6 Break up conjunctions in selection predicates \\sigma_{c_1 \\and c_2 \\and \\cdots \\and c_n}(R) \\equiv \\sigma_{c_1}(\\sigma_{c_2}(\\dots(\\sigma_{c_n}(R))\\dots)) \\sigma_{c_1 \\and c_2 \\and \\cdots \\and c_n}(R) \\equiv \\sigma_{c_1}(\\sigma_{c_2}(\\dots(\\sigma_{c_n}(R))\\dots)) \\sigma \\sigma is commutative \\sigma_{c_1}(\\sigma_{c_2}(R)) \\equiv \\sigma_{c_2} ( \\sigma_{c_1}(R)) \\sigma_{c_1}(\\sigma_{c_2}(R)) \\equiv \\sigma_{c_2} ( \\sigma_{c_1}(R)) \\pi \\pi cascades If L_1 \\subseteq L_2 \\subseteq \\cdots \\subseteq L_n L_1 \\subseteq L_2 \\subseteq \\cdots \\subseteq L_n then \\pi_{L_1}(\\pi_{L_2}(\\dots(\\pi_{L_n}(R))\\dots)) \\equiv \\pi_{L_1}(R) \\pi_{L_1}(\\pi_{L_2}(\\dots(\\pi_{L_n}(R))\\dots)) \\equiv \\pi_{L_1}(R) Change the order of \\sigma \\sigma and \\pi \\pi If the selection involves only attributes A_1, \\dots, A_n A_1, \\dots, A_n contained in the projection list, the order of \\sigma \\sigma and \\pi \\pi can be changed \\pi_{A_1, \\dots, A_n}(\\sigma_c(R)) \\equiv \\sigma_c(\\pi_{A_1, \\dots, A_n}(R)) \\pi_{A_1, \\dots, A_n}(\\sigma_c(R)) \\equiv \\sigma_c(\\pi_{A_1, \\dots, A_n}(R)) \\cup, \\cap \\cup, \\cap and \\Join \\Join are commutative R\\Join_c S \\equiv S\\Join_c R R\\Join_c S \\equiv S\\Join_c R Change the order of \\sigma \\sigma and \\Join \\Join If the selection predicate c c involves only attributes of relation R R , the order of \\sigma \\sigma and \\Join \\Join can be changed \\sigma_c(R\\Join_j S) \\equiv \\sigma_c(R) \\Join_j S \\sigma_c(R\\Join_j S) \\equiv \\sigma_c(R) \\Join_j S If the selection predicate c c is a conjunction of the form c_1 \\and c_2 c_1 \\and c_2 and c_1 c_1 involves only attributes in R R and c_2 c_2 involves only attributes in S, then we need to split c c \\sigma_c(R\\Join S) \\equiv \\sigma_{c_1}(R) \\Join_j \\sigma_{c_2}(S) \\sigma_c(R\\Join S) \\equiv \\sigma_{c_1}(R) \\Join_j \\sigma_{c_2}(S) Change the order of \\pi \\pi and \\Join \\Join Given the projection list L=\\{A_1,\\dots,A_n, B_1,\\dots,B_m\\} L=\\{A_1,\\dots,A_n, B_1,\\dots,B_m\\} where A_i A_i represents attributes in R R and B_i B_i represents attributes in S S If the join predicate c c only references attributes in L L the following reformulation holds \\pi_L (R \\Join_c S) \\equiv (\\pi_{A_1,\\dots,A_n}(R)) \\Join_c (\\pi_{B_1, \\dots , B_m}(S)) \\pi_L (R \\Join_c S) \\equiv (\\pi_{A_1,\\dots,A_n}(R)) \\Join_c (\\pi_{B_1, \\dots , B_m}(S)) \\Join, \\cap, \\cup \\Join, \\cap, \\cup (in separate) are all associative I.e., with \\Phi \\Phi representing either of these operations, the following holds (R\\ \\Phi\\ S)\\ \\Phi\\ T \\equiv R\\ \\Phi\\ (S\\ \\Phi\\ T) (R\\ \\Phi\\ S)\\ \\Phi\\ T \\equiv R\\ \\Phi\\ (S\\ \\Phi\\ T) \\sigma \\sigma is distributive with \\cap, \\cup, - \\cap, \\cup, - I.e., with \\Phi \\Phi representing either of these operations, the following holds \\sigma_c(R\\ \\Phi\\ S) \\equiv (\\sigma_c(R))\\ \\Phi\\ (\\sigma_c(S)) \\sigma_c(R\\ \\Phi\\ S) \\equiv (\\sigma_c(R))\\ \\Phi\\ (\\sigma_c(S)) \\pi \\pi is distributive with \\cup \\cup \\pi_c(R\\cup S) \\equiv (\\pi_c(R))\\cup (\\pi_c(S)) \\pi_c(R\\cup S) \\equiv (\\pi_c(R))\\cup (\\pi_c(S)) Join and/or selection predicates can be reformulated based on De Morgan\u2019s laws \\neg(c_1 \\and c_2) \\equiv (\\neg c_1)\\or(\\neg c_2)\\label{demorgan1} \\neg(c_1 \\and c_2) \\equiv (\\neg c_1)\\or(\\neg c_2)\\label{demorgan1} \\neg(c_1 \\or c_2) \\equiv (\\neg c_1)\\and(\\neg c_2)\\label{demorgan2} \\neg(c_1 \\or c_2) \\equiv (\\neg c_1)\\and(\\neg c_2)\\label{demorgan2} Combination of Cartesian product and selection A Cartesian product followed by a selection whose predicate involves predicates of both involved operands can be combined to a join \\sigma_\\theta(R \\times S) \\equiv R \\Join_\\theta S \\sigma_\\theta(R \\times S) \\equiv R \\Join_\\theta S Remember the equivalent expressions for operators in relational algebra! Phases of Logical Query Optimization \u00b6 Break up conjunctive selection predicates Push selections down Introduce joins by combining selections and cross products Determine join order Heuristic : execute joins with input from selections before executing other joins Introduce and push down projections Not always useful Example in DBS8 slide 18 p. 35 Be Careful! \u00b6 Find the titles of reserved films 1 2 3 SELECT DISTINCT title FROM film F , reserved R WHERE F . filmID = R . filmID Find the titles of expensive reserved films 1 2 3 SELECT DISTINCT title FROM film F , reserved R WHERE F . filmID = R . filmID AND F . rentalPrice > 4 Summary: Heuristic Query Optimization \u00b6 Rules of thumb Perform selections as early as possible Perform projections as early as possible The optimization process Generate initial query plan from SQL statement Transform query plan into more efficient query plan via a series of modifications, each of which hopefully reducing execution time Note A single query plan provides all the results Sometimes also called rule-based query optimization Operator Implementations \u00b6 Sample Database \\begin{align*} \\relational{customer}{customerID, name, street, city, state}\\\\ \\relational{reserved}{customerID, filmID, resDate}\\\\ \\relational{film}{filmID, title, kind, rentalPrice} \\end{align*} \\begin{align*} \\relational{customer}{customerID, name, street, city, state}\\\\ \\relational{reserved}{customerID, filmID, resDate}\\\\ \\relational{film}{filmID, title, kind, rentalPrice} \\end{align*} Selection \u00b6 Taxonomy \u00b6 Primary key, point \\sigma_{filmID=2}(film) \\sigma_{filmID=2}(film) Here we could use an index Point \\sigma_{title='Terminator'}(film) \\sigma_{title='Terminator'}(film) We may or may not have an index here Range \\sigma_{1<rentalPrice<4}(film) \\sigma_{1<rentalPrice<4}(film) Conjunction (logical and) \\sigma_{kind='F' \\and rentalPrice=4}(film) \\sigma_{kind='F' \\and rentalPrice=4}(film) Disjunction (logical or) \\sigma_{rentalPrice<4 \\or kind='D'}(film) \\sigma_{rentalPrice<4 \\or kind='D'}(film) Selection Strategies - Point/Range \u00b6 Linear search Expensive but always applicable Binary search Applicable only when the file is appropriately ordered Primary hash index/table search Single record retrieval; does not work for range queries Retrieval of multiple records Primary/clustering index search Multiple records for each index item Implemented with single pointer to block with first associated record Secondary index search Implemented with multiple pointers, each to a single record Might become expensive Strategies for Conjunctive Queries \u00b6 1 2 3 4 SELECT * FROM customer WHERE name = \u2019 Jensen \u2019 AND street = \u2019 Elm \u2019 AND state = \u2019 Arizona \u2019 Can indexes on (name) and (street) be used? Yes Can an index on (name, street, state) be used? Yes Can an index on (name, street) be used? Yes Can an index on (name, street, city) be used? Yes Can an index on (city, name, street) be used? No Optimization of conjunctive queries Indexing provides good opportunities for improving performance Use available indexes Ideal: composite index is applicable If multiple are available \\to \\to use the most selective index, then check remaining conditions Use intersection of record pointers (if multiple indexes applicable) Index lookups to fetch sets of record pointers Intersect record pointers to perform conjunction Retrieve (and check) the qualifying records Disjunctive queries provide little opportunity for improving performance. Database tuning and the creation of indexes is important! Join \u00b6 Join strategies Nested loop join Index-based join Sort-merge join Hash join Strategies work on a per block (not per record) basis Estimate I/Os (block retrievals) Use of main memory buffer Table sizes and join selectivities influence join costs Query selectivity: sel = \\mathrm{\\# tuples\\ in\\ result \\over \\# candidates} sel = \\mathrm{\\# tuples\\ in\\ result \\over \\# candidates} For join, #candidates is the size of the Cartesian product Nested Loop Join \u00b6 See example in DBS8 slide 36 p. 68 PDF on moodle Block Nested Loop Join \u00b6 Not all blocks fit into main memory Parameters b_{inner}, b_{outer}: b_{inner}, b_{outer}: number of blocks n_B: n_B: size of main memory buffer Cost estimation (block transfers): b_{outer}+(\\lceil b_{outer} / (n_B-2)\\rceil) \\cdot b_{inner} b_{outer}+(\\lceil b_{outer} / (n_B-2)\\rceil) \\cdot b_{inner} If we know more system parameters (block transfer, disk seeks, CPU speed,. . . ) and the size of input relations, we can estimate the time to compute the join. Example reserved \\Join customer reserved \\Join customer number of blocks b_{reserved}=2.000, b_{customer}=10 b_{reserved}=2.000, b_{customer}=10 Size of main memory buffer n_B=6 n_B=6 Cost: reserved reserved as outer 2.000+\\lceil (2.000/4)\\rceil \\cdot 10 = 7.000 2.000+\\lceil (2.000/4)\\rceil \\cdot 10 = 7.000 customer customer as outer 10+\\lceil (10/4)\\rceil \\cdot 2000 = 6.010 10+\\lceil (10/4)\\rceil \\cdot 2000 = 6.010 Index-based Block Nested Loop Join \u00b6 Same principle as standard nested loop join Outer relation Inner relation Index lookups can replace file scans on the inner relation Merge Join \u00b6 Exploit sorted relations Assumption: Both input relations are sorted Example in DBS8 slide 42 p. 126 PDF on Moodle Cost \u00b6 Parameters b_1, b_2: b_1, b_2: number of blocks Cost estimation (block transfer) b_1 + b_2 b_1 + b_2 Extensions Combination with sorting if input relations are not sorted Not enough main memory Hash Join \u00b6 Apply hash functions to the join attributes \\to \\to partition tuples into buckets Hash each relation on the join attributes Each bucket must be small enough to fit into memory Join corresponding buckets from each relation Example Algorithm \u00b6 Parameters b_1, b_2 b_1, b_2 : number of blocks for tables R_1 R_1 and R_2 R_2 Steps Partitioning table R_1 R_1 with h_1 h_1 into buckets r_{1_i} r_{1_i} (read all / write all) 2\\times b_1 2\\times b_1 Partitioning table R_2 R_2 with h_1 h_1 into buckets r_{2_i} r_{2_i} (read all / write all) 2\\times b_2 2\\times b_2 Build phase: use h_2 h_2 to create an in-memory hash index on bucket r_{1_i} r_{1_i} (read all) b_1 b_1 Probe phase for corresponding r_{2_i} r_{2_i} , use h_2 h_2 to test in-memory index for matches (read all) b2 b2 Cost estimation (block transfer) 3 \\times b_1 + 3 \\times b_2 + \\epsilon 3 \\times b_1 + 3 \\times b_2 + \\epsilon \\epsilon \\epsilon : partially filled blocks Cost and Applicability of Join Strategies \u00b6 Nested loop join Can be used for all join types Can be quite expensive Merge join Files need to be sorted on the join attributes Sorting can be done for the purpose of the join Can use indexes Hash join Good hash functions are essential Performance best if smallest relation fits into main memory Cost-based (Physical) Query Optimization \u00b6 Objective For a given query, find the most efficient query execution plan Physical query optimization \u00b6 Generate alternative query execution plans Choose algorithms and access paths Compute costs Choose cheapest query execution plan Prerequisite Cost model Statistics on the input to each operation Statistics on leaf relations: stored in system catalog Statistics on intermediate relations must be estimated (cardinalities) Selectivity and Cardinality \u00b6 Statistics per Relation For relation r r Number of tuples (records): n_r n_r Tuple size in relation r r : l_r l_r Load factor (fill factor), percentage of space used in each block Blocking factor (number of records per block) Relation size in blocks: b_r b_r Relation organization Heap, hash, indexes, clustered Number of overflow blocks Statistics per Attribute For attribute A A in relation r r Size and type Number of distinct values for attribute A: V(A,r) V(A,r) The same as the size of \\pi_A(r) \\pi_A(r) Selection cardinality S(A,r) S(A,r) The same as the size of \\sigma_{A=a}(r) \\sigma_{A=a}(r) for an arbitrary value a a Probability distribution over the values Alternative: assume uniform distribution Statistics need to be updated when the table is updated! Statistics per Index Base relation Indexed attribute(s) Organization, eg. B+-tree, hash Clustering index? On key attribute(s)? Sparse or dense? Number of levels (if appropriate) Number of leaf-level index blocks Cost Estimation Example \u00b6 See example in DBS8 slide 53 p. 160 PDF on Moodle Cost Model \u00b6 Cost models consider more aspects than only disk access CPU time Communication time Main memory usage ... For this purpose, we need to estimate input/output sizes of each operator Statistics on leaf relations: stored in system catalog Statistics on intermediate relations must be estimated (cardinalities) Additional aspects Spanning search space (dynamic programming, exhaustive search, ... ) Bushy vs. left-deep join trees (parallelism vs. pipelining) Multiquery optimization (shared scans, ... ) ... Heuristic vs Cost-Based Query Optimization \u00b6 PostgreSQL \u00b6 EXPLAIN Display the execution plan that the PostgreSQL planner generates for the supplied statement 1 2 3 4 5 6 7 EXPLAIN SELECT DISTINCT s . semester FROM student s , takes h , course v , professor p WHERE p . name = \u2018 Socrates \u2018 AND v . taughtBy = p . empID AND v . courseID = h . courseID AND h . studID = s . studID ; EXPLAIN ANALYZE The additional ANALYZE option causes the statement to be actually executed, not only planned ANALYZE ANALYZE collects statistics about the contents of tables in the database Sequential Scans vs Indexes \u00b6 If an index is \u201cuseful\u201d or not depends on How much data is relevant to the query Size of the relation Properties of the index (clustered, multiple columns, ... ) What algorithm needs the data as input ... Until query optimization is perfected, the main task of database administrators will remain query tuning (creating indexes, etc.).","title":"Query Processing and Optimization"},{"location":"6-semester/DBS/08-query-optimization/#query-processing-and-optimization","text":"Learning Goals Understand how selection statements are executed Understand the basic join algorithms Understand the basics of heuristic (logical) query optimization Understand the basics of physical query optimization \\newcommand{\\relationRaw}[3]{ \\newcommand{\\pk}{\\underline} #3\\mathbf{#1}#3#3:\\{[\\mathrm{#2}]\\} } \\newcommand{\\relation}[2]{\\relationRaw{#1}{#2}{}} \\newcommand{\\relational}[2]{\\relationRaw{#1}{#2}{&}} \\nonumber \\newcommand{\\relationRaw}[3]{ \\newcommand{\\pk}{\\underline} #3\\mathbf{#1}#3#3:\\{[\\mathrm{#2}]\\} } \\newcommand{\\relation}[2]{\\relationRaw{#1}{#2}{}} \\newcommand{\\relational}[2]{\\relationRaw{#1}{#2}{&}} \\nonumber","title":"Query Processing and Optimization"},{"location":"6-semester/DBS/08-query-optimization/#query-processing","text":"","title":"Query Processing"},{"location":"6-semester/DBS/08-query-optimization/#evaluation-of-an-sql-statement","text":"The clauses are specified in the following order SELECT column(s) FROM table list WHERE condition GROUP BY grouping column(s) HAVING group condition ORDER BY sort list But the query is evaluated in a different order Cartesian product of tables in the from clause Predicates in the where clause Grouped according to the group by clause Predicate in the having clause applied to (eliminate) groups Compute aggregation functions for each remaining group Projection on columns enumerated in the select clause SQL is declarative!","title":"Evaluation of an SQL Statement"},{"location":"6-semester/DBS/08-query-optimization/#steps-of-query-processing","text":"","title":"Steps of Query Processing"},{"location":"6-semester/DBS/08-query-optimization/#parsing-a-query","text":"Parsing a query into an initial query plan \\pi_{title}(\\sigma_{name='Socrates' \\and empID=taughtBy}(professor \\times course)) \\pi_{title}(\\sigma_{name='Socrates' \\and empID=taughtBy}(professor \\times course))","title":"Parsing a Query"},{"location":"6-semester/DBS/08-query-optimization/#alternative-query-plan","text":"","title":"Alternative Query Plan"},{"location":"6-semester/DBS/08-query-optimization/#query-optimization","text":"Alternatives Equivalent query execution plans Algorithms to compute an algebra operation Methods to access relations (indexes) Although the result is equivalent, execution costs might be different. Theory meets reality It is not the task of the user to write queries \u201cefficiently\u201d, it is the task of the query optimizer to execute them efficiently! But in reality. . . optimizers are not perfect.","title":"Query Optimization"},{"location":"6-semester/DBS/08-query-optimization/#query-costs","text":"Measures Total elapsed time for answering a query (response time) Many factors contribute to response time Disk access CPU costs Network communication Query load Parallel processing Disk access most dominant Block access time: seek time, rotation time Transfer time","title":"Query Costs"},{"location":"6-semester/DBS/08-query-optimization/#query-optimization_1","text":"Logical query optimization Relational algebra Equivalence transformation Heuristic optimization Physical query optimization Algorithms and implementations of operations Cost model","title":"Query Optimization"},{"location":"6-semester/DBS/08-query-optimization/#heuristic-logical-query-optimization","text":"Logical query optimization Foundation: algebraic equivalences Algebraic equivalences span the potential search space Given an initial algebraic expression: apply algebraic equivalences to derive new equivalent algebraic expressions What is a good plan? Difficult to compare plans without a cost function \\to \\to logical query optimization relies on heuristics Main goal of logical query optimization Reduce the size of intermediate results","title":"Heuristic (Logical) Query Optimization"},{"location":"6-semester/DBS/08-query-optimization/#equivalences","text":"Break up conjunctions in selection predicates \\sigma_{c_1 \\and c_2 \\and \\cdots \\and c_n}(R) \\equiv \\sigma_{c_1}(\\sigma_{c_2}(\\dots(\\sigma_{c_n}(R))\\dots)) \\sigma_{c_1 \\and c_2 \\and \\cdots \\and c_n}(R) \\equiv \\sigma_{c_1}(\\sigma_{c_2}(\\dots(\\sigma_{c_n}(R))\\dots)) \\sigma \\sigma is commutative \\sigma_{c_1}(\\sigma_{c_2}(R)) \\equiv \\sigma_{c_2} ( \\sigma_{c_1}(R)) \\sigma_{c_1}(\\sigma_{c_2}(R)) \\equiv \\sigma_{c_2} ( \\sigma_{c_1}(R)) \\pi \\pi cascades If L_1 \\subseteq L_2 \\subseteq \\cdots \\subseteq L_n L_1 \\subseteq L_2 \\subseteq \\cdots \\subseteq L_n then \\pi_{L_1}(\\pi_{L_2}(\\dots(\\pi_{L_n}(R))\\dots)) \\equiv \\pi_{L_1}(R) \\pi_{L_1}(\\pi_{L_2}(\\dots(\\pi_{L_n}(R))\\dots)) \\equiv \\pi_{L_1}(R) Change the order of \\sigma \\sigma and \\pi \\pi If the selection involves only attributes A_1, \\dots, A_n A_1, \\dots, A_n contained in the projection list, the order of \\sigma \\sigma and \\pi \\pi can be changed \\pi_{A_1, \\dots, A_n}(\\sigma_c(R)) \\equiv \\sigma_c(\\pi_{A_1, \\dots, A_n}(R)) \\pi_{A_1, \\dots, A_n}(\\sigma_c(R)) \\equiv \\sigma_c(\\pi_{A_1, \\dots, A_n}(R)) \\cup, \\cap \\cup, \\cap and \\Join \\Join are commutative R\\Join_c S \\equiv S\\Join_c R R\\Join_c S \\equiv S\\Join_c R Change the order of \\sigma \\sigma and \\Join \\Join If the selection predicate c c involves only attributes of relation R R , the order of \\sigma \\sigma and \\Join \\Join can be changed \\sigma_c(R\\Join_j S) \\equiv \\sigma_c(R) \\Join_j S \\sigma_c(R\\Join_j S) \\equiv \\sigma_c(R) \\Join_j S If the selection predicate c c is a conjunction of the form c_1 \\and c_2 c_1 \\and c_2 and c_1 c_1 involves only attributes in R R and c_2 c_2 involves only attributes in S, then we need to split c c \\sigma_c(R\\Join S) \\equiv \\sigma_{c_1}(R) \\Join_j \\sigma_{c_2}(S) \\sigma_c(R\\Join S) \\equiv \\sigma_{c_1}(R) \\Join_j \\sigma_{c_2}(S) Change the order of \\pi \\pi and \\Join \\Join Given the projection list L=\\{A_1,\\dots,A_n, B_1,\\dots,B_m\\} L=\\{A_1,\\dots,A_n, B_1,\\dots,B_m\\} where A_i A_i represents attributes in R R and B_i B_i represents attributes in S S If the join predicate c c only references attributes in L L the following reformulation holds \\pi_L (R \\Join_c S) \\equiv (\\pi_{A_1,\\dots,A_n}(R)) \\Join_c (\\pi_{B_1, \\dots , B_m}(S)) \\pi_L (R \\Join_c S) \\equiv (\\pi_{A_1,\\dots,A_n}(R)) \\Join_c (\\pi_{B_1, \\dots , B_m}(S)) \\Join, \\cap, \\cup \\Join, \\cap, \\cup (in separate) are all associative I.e., with \\Phi \\Phi representing either of these operations, the following holds (R\\ \\Phi\\ S)\\ \\Phi\\ T \\equiv R\\ \\Phi\\ (S\\ \\Phi\\ T) (R\\ \\Phi\\ S)\\ \\Phi\\ T \\equiv R\\ \\Phi\\ (S\\ \\Phi\\ T) \\sigma \\sigma is distributive with \\cap, \\cup, - \\cap, \\cup, - I.e., with \\Phi \\Phi representing either of these operations, the following holds \\sigma_c(R\\ \\Phi\\ S) \\equiv (\\sigma_c(R))\\ \\Phi\\ (\\sigma_c(S)) \\sigma_c(R\\ \\Phi\\ S) \\equiv (\\sigma_c(R))\\ \\Phi\\ (\\sigma_c(S)) \\pi \\pi is distributive with \\cup \\cup \\pi_c(R\\cup S) \\equiv (\\pi_c(R))\\cup (\\pi_c(S)) \\pi_c(R\\cup S) \\equiv (\\pi_c(R))\\cup (\\pi_c(S)) Join and/or selection predicates can be reformulated based on De Morgan\u2019s laws \\neg(c_1 \\and c_2) \\equiv (\\neg c_1)\\or(\\neg c_2)\\label{demorgan1} \\neg(c_1 \\and c_2) \\equiv (\\neg c_1)\\or(\\neg c_2)\\label{demorgan1} \\neg(c_1 \\or c_2) \\equiv (\\neg c_1)\\and(\\neg c_2)\\label{demorgan2} \\neg(c_1 \\or c_2) \\equiv (\\neg c_1)\\and(\\neg c_2)\\label{demorgan2} Combination of Cartesian product and selection A Cartesian product followed by a selection whose predicate involves predicates of both involved operands can be combined to a join \\sigma_\\theta(R \\times S) \\equiv R \\Join_\\theta S \\sigma_\\theta(R \\times S) \\equiv R \\Join_\\theta S Remember the equivalent expressions for operators in relational algebra!","title":"Equivalences"},{"location":"6-semester/DBS/08-query-optimization/#phases-of-logical-query-optimization","text":"Break up conjunctive selection predicates Push selections down Introduce joins by combining selections and cross products Determine join order Heuristic : execute joins with input from selections before executing other joins Introduce and push down projections Not always useful Example in DBS8 slide 18 p. 35","title":"Phases of Logical Query Optimization"},{"location":"6-semester/DBS/08-query-optimization/#be-careful","text":"Find the titles of reserved films 1 2 3 SELECT DISTINCT title FROM film F , reserved R WHERE F . filmID = R . filmID Find the titles of expensive reserved films 1 2 3 SELECT DISTINCT title FROM film F , reserved R WHERE F . filmID = R . filmID AND F . rentalPrice > 4","title":"Be Careful!"},{"location":"6-semester/DBS/08-query-optimization/#summary-heuristic-query-optimization","text":"Rules of thumb Perform selections as early as possible Perform projections as early as possible The optimization process Generate initial query plan from SQL statement Transform query plan into more efficient query plan via a series of modifications, each of which hopefully reducing execution time Note A single query plan provides all the results Sometimes also called rule-based query optimization","title":"Summary: Heuristic Query Optimization"},{"location":"6-semester/DBS/08-query-optimization/#operator-implementations","text":"Sample Database \\begin{align*} \\relational{customer}{customerID, name, street, city, state}\\\\ \\relational{reserved}{customerID, filmID, resDate}\\\\ \\relational{film}{filmID, title, kind, rentalPrice} \\end{align*} \\begin{align*} \\relational{customer}{customerID, name, street, city, state}\\\\ \\relational{reserved}{customerID, filmID, resDate}\\\\ \\relational{film}{filmID, title, kind, rentalPrice} \\end{align*}","title":"Operator Implementations"},{"location":"6-semester/DBS/08-query-optimization/#selection","text":"","title":"Selection"},{"location":"6-semester/DBS/08-query-optimization/#taxonomy","text":"Primary key, point \\sigma_{filmID=2}(film) \\sigma_{filmID=2}(film) Here we could use an index Point \\sigma_{title='Terminator'}(film) \\sigma_{title='Terminator'}(film) We may or may not have an index here Range \\sigma_{1<rentalPrice<4}(film) \\sigma_{1<rentalPrice<4}(film) Conjunction (logical and) \\sigma_{kind='F' \\and rentalPrice=4}(film) \\sigma_{kind='F' \\and rentalPrice=4}(film) Disjunction (logical or) \\sigma_{rentalPrice<4 \\or kind='D'}(film) \\sigma_{rentalPrice<4 \\or kind='D'}(film)","title":"Taxonomy"},{"location":"6-semester/DBS/08-query-optimization/#selection-strategies-pointrange","text":"Linear search Expensive but always applicable Binary search Applicable only when the file is appropriately ordered Primary hash index/table search Single record retrieval; does not work for range queries Retrieval of multiple records Primary/clustering index search Multiple records for each index item Implemented with single pointer to block with first associated record Secondary index search Implemented with multiple pointers, each to a single record Might become expensive","title":"Selection Strategies - Point/Range"},{"location":"6-semester/DBS/08-query-optimization/#strategies-for-conjunctive-queries","text":"1 2 3 4 SELECT * FROM customer WHERE name = \u2019 Jensen \u2019 AND street = \u2019 Elm \u2019 AND state = \u2019 Arizona \u2019 Can indexes on (name) and (street) be used? Yes Can an index on (name, street, state) be used? Yes Can an index on (name, street) be used? Yes Can an index on (name, street, city) be used? Yes Can an index on (city, name, street) be used? No Optimization of conjunctive queries Indexing provides good opportunities for improving performance Use available indexes Ideal: composite index is applicable If multiple are available \\to \\to use the most selective index, then check remaining conditions Use intersection of record pointers (if multiple indexes applicable) Index lookups to fetch sets of record pointers Intersect record pointers to perform conjunction Retrieve (and check) the qualifying records Disjunctive queries provide little opportunity for improving performance. Database tuning and the creation of indexes is important!","title":"Strategies for Conjunctive Queries"},{"location":"6-semester/DBS/08-query-optimization/#join","text":"Join strategies Nested loop join Index-based join Sort-merge join Hash join Strategies work on a per block (not per record) basis Estimate I/Os (block retrievals) Use of main memory buffer Table sizes and join selectivities influence join costs Query selectivity: sel = \\mathrm{\\# tuples\\ in\\ result \\over \\# candidates} sel = \\mathrm{\\# tuples\\ in\\ result \\over \\# candidates} For join, #candidates is the size of the Cartesian product","title":"Join"},{"location":"6-semester/DBS/08-query-optimization/#nested-loop-join","text":"See example in DBS8 slide 36 p. 68 PDF on moodle","title":"Nested Loop Join"},{"location":"6-semester/DBS/08-query-optimization/#block-nested-loop-join","text":"Not all blocks fit into main memory Parameters b_{inner}, b_{outer}: b_{inner}, b_{outer}: number of blocks n_B: n_B: size of main memory buffer Cost estimation (block transfers): b_{outer}+(\\lceil b_{outer} / (n_B-2)\\rceil) \\cdot b_{inner} b_{outer}+(\\lceil b_{outer} / (n_B-2)\\rceil) \\cdot b_{inner} If we know more system parameters (block transfer, disk seeks, CPU speed,. . . ) and the size of input relations, we can estimate the time to compute the join. Example reserved \\Join customer reserved \\Join customer number of blocks b_{reserved}=2.000, b_{customer}=10 b_{reserved}=2.000, b_{customer}=10 Size of main memory buffer n_B=6 n_B=6 Cost: reserved reserved as outer 2.000+\\lceil (2.000/4)\\rceil \\cdot 10 = 7.000 2.000+\\lceil (2.000/4)\\rceil \\cdot 10 = 7.000 customer customer as outer 10+\\lceil (10/4)\\rceil \\cdot 2000 = 6.010 10+\\lceil (10/4)\\rceil \\cdot 2000 = 6.010","title":"Block Nested Loop Join"},{"location":"6-semester/DBS/08-query-optimization/#index-based-block-nested-loop-join","text":"Same principle as standard nested loop join Outer relation Inner relation Index lookups can replace file scans on the inner relation","title":"Index-based Block Nested Loop Join"},{"location":"6-semester/DBS/08-query-optimization/#merge-join","text":"Exploit sorted relations Assumption: Both input relations are sorted Example in DBS8 slide 42 p. 126 PDF on Moodle","title":"Merge Join"},{"location":"6-semester/DBS/08-query-optimization/#cost","text":"Parameters b_1, b_2: b_1, b_2: number of blocks Cost estimation (block transfer) b_1 + b_2 b_1 + b_2 Extensions Combination with sorting if input relations are not sorted Not enough main memory","title":"Cost"},{"location":"6-semester/DBS/08-query-optimization/#hash-join","text":"Apply hash functions to the join attributes \\to \\to partition tuples into buckets Hash each relation on the join attributes Each bucket must be small enough to fit into memory Join corresponding buckets from each relation Example","title":"Hash Join"},{"location":"6-semester/DBS/08-query-optimization/#algorithm","text":"Parameters b_1, b_2 b_1, b_2 : number of blocks for tables R_1 R_1 and R_2 R_2 Steps Partitioning table R_1 R_1 with h_1 h_1 into buckets r_{1_i} r_{1_i} (read all / write all) 2\\times b_1 2\\times b_1 Partitioning table R_2 R_2 with h_1 h_1 into buckets r_{2_i} r_{2_i} (read all / write all) 2\\times b_2 2\\times b_2 Build phase: use h_2 h_2 to create an in-memory hash index on bucket r_{1_i} r_{1_i} (read all) b_1 b_1 Probe phase for corresponding r_{2_i} r_{2_i} , use h_2 h_2 to test in-memory index for matches (read all) b2 b2 Cost estimation (block transfer) 3 \\times b_1 + 3 \\times b_2 + \\epsilon 3 \\times b_1 + 3 \\times b_2 + \\epsilon \\epsilon \\epsilon : partially filled blocks","title":"Algorithm"},{"location":"6-semester/DBS/08-query-optimization/#cost-and-applicability-of-join-strategies","text":"Nested loop join Can be used for all join types Can be quite expensive Merge join Files need to be sorted on the join attributes Sorting can be done for the purpose of the join Can use indexes Hash join Good hash functions are essential Performance best if smallest relation fits into main memory","title":"Cost and Applicability of Join Strategies"},{"location":"6-semester/DBS/08-query-optimization/#cost-based-physical-query-optimization","text":"Objective For a given query, find the most efficient query execution plan","title":"Cost-based (Physical) Query Optimization"},{"location":"6-semester/DBS/08-query-optimization/#physical-query-optimization","text":"Generate alternative query execution plans Choose algorithms and access paths Compute costs Choose cheapest query execution plan Prerequisite Cost model Statistics on the input to each operation Statistics on leaf relations: stored in system catalog Statistics on intermediate relations must be estimated (cardinalities)","title":"Physical query optimization"},{"location":"6-semester/DBS/08-query-optimization/#selectivity-and-cardinality","text":"Statistics per Relation For relation r r Number of tuples (records): n_r n_r Tuple size in relation r r : l_r l_r Load factor (fill factor), percentage of space used in each block Blocking factor (number of records per block) Relation size in blocks: b_r b_r Relation organization Heap, hash, indexes, clustered Number of overflow blocks Statistics per Attribute For attribute A A in relation r r Size and type Number of distinct values for attribute A: V(A,r) V(A,r) The same as the size of \\pi_A(r) \\pi_A(r) Selection cardinality S(A,r) S(A,r) The same as the size of \\sigma_{A=a}(r) \\sigma_{A=a}(r) for an arbitrary value a a Probability distribution over the values Alternative: assume uniform distribution Statistics need to be updated when the table is updated! Statistics per Index Base relation Indexed attribute(s) Organization, eg. B+-tree, hash Clustering index? On key attribute(s)? Sparse or dense? Number of levels (if appropriate) Number of leaf-level index blocks","title":"Selectivity and Cardinality"},{"location":"6-semester/DBS/08-query-optimization/#cost-estimation-example","text":"See example in DBS8 slide 53 p. 160 PDF on Moodle","title":"Cost Estimation Example"},{"location":"6-semester/DBS/08-query-optimization/#cost-model","text":"Cost models consider more aspects than only disk access CPU time Communication time Main memory usage ... For this purpose, we need to estimate input/output sizes of each operator Statistics on leaf relations: stored in system catalog Statistics on intermediate relations must be estimated (cardinalities) Additional aspects Spanning search space (dynamic programming, exhaustive search, ... ) Bushy vs. left-deep join trees (parallelism vs. pipelining) Multiquery optimization (shared scans, ... ) ...","title":"Cost Model"},{"location":"6-semester/DBS/08-query-optimization/#heuristic-vs-cost-based-query-optimization","text":"","title":"Heuristic vs Cost-Based Query Optimization"},{"location":"6-semester/DBS/08-query-optimization/#postgresql","text":"EXPLAIN Display the execution plan that the PostgreSQL planner generates for the supplied statement 1 2 3 4 5 6 7 EXPLAIN SELECT DISTINCT s . semester FROM student s , takes h , course v , professor p WHERE p . name = \u2018 Socrates \u2018 AND v . taughtBy = p . empID AND v . courseID = h . courseID AND h . studID = s . studID ; EXPLAIN ANALYZE The additional ANALYZE option causes the statement to be actually executed, not only planned ANALYZE ANALYZE collects statistics about the contents of tables in the database","title":"PostgreSQL"},{"location":"6-semester/DBS/08-query-optimization/#sequential-scans-vs-indexes","text":"If an index is \u201cuseful\u201d or not depends on How much data is relevant to the query Size of the relation Properties of the index (clustered, multiple columns, ... ) What algorithm needs the data as input ... Until query optimization is perfected, the main task of database administrators will remain query tuning (creating indexes, etc.).","title":"Sequential Scans vs Indexes"},{"location":"6-semester/VIT/","text":"VIT - Theory of Science \u00b6 Moodle side: https://www.moodle.aau.dk/course/view.php?id=33047","title":"Course"},{"location":"6-semester/VIT/#vit-theory-of-science","text":"Moodle side: https://www.moodle.aau.dk/course/view.php?id=33047","title":"VIT - Theory of Science"},{"location":"6-semester/VIT/deliverables/","text":"Deliverables \u00b6 Deliverable 1 \u00b6 Write one paragraph summary of the area where you do your current project. Find two suitable workshops and two suitable conferences where your current project work could be possibly published. The submission deadline for the conferences/workshops should be in this or next year. For each workshop/conference provide its full name, acronym, an URL to the home-page of the event and the submission deadline.","title":"Deliverables"},{"location":"6-semester/VIT/deliverables/#deliverables","text":"","title":"Deliverables"},{"location":"6-semester/VIT/deliverables/#deliverable-1","text":"Write one paragraph summary of the area where you do your current project. Find two suitable workshops and two suitable conferences where your current project work could be possibly published. The submission deadline for the conferences/workshops should be in this or next year. For each workshop/conference provide its full name, acronym, an URL to the home-page of the event and the submission deadline.","title":"Deliverable 1"}]}